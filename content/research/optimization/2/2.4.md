---
layout: NotesPage
title: 2.4 <br /> Singular-Stuff(Values)
permalink: /work_files/research/conv_opt/2_4
prevLink: /work_files/research/conv_opt.html
---

<div markdown="1" class = "TOC">
# Table of Contents

  * [The Singular Value Decomposition](#content1)
  {: .TOC1}
  * [SECOND](#content2)
  {: .TOC2}
  * [THIRD](#content3)
  {: .TOC3}
</div>

***
***

## The Singular Value Decomposition
{: #content1}

1. **What?**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11}
    :   Recall from here that any matrix $$A \in \mathbf{R}^{m \times n}$$ with rank one can be written as   $$A = \sigma u v^T$$,  where $$u \in \mathbf{R}^m, v \in \mathbf{R}^n,$$ and $$\sigma >0.$$
    :   It turns out that a similar result holds for matrices of arbitrary rank $$r$$.  :   That is, we can express any matrix $$A \in \mathbf{R}^{m \times n}$$ as sum of rank-one matrices:
    :   $$
        A = \sum_{i=1}^r \sigma_i u_i v_i^T,  
        $$
    :   where $$u_1, \ldots, u_r$$ are mutually orthogonal, $$v_1, \ldots, v_r$$ are also mutually orthogonal, and the $$\sigma_i$$â€™s are positive numbers called the singular values of $$A$$.

2. **The SVD Theorem:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12}
    :   An arbitrary matrix $$A \in \mathbf{R}^{m \times n}$$ admits a decomposition of the form:
    :   $$
        A = \sum_{i=1}^r \sigma_i u_i v_i^T = U \tilde{ {S}} V^T, \;\; \tilde{ {S}} := \left( \begin{array}{cc}  {S} & 0 \\ 0 & 0 \end{array} \right) ,  
        $$
    :   where $$U \in \mathbf{R}^{m \times m}, V \in \mathbf{R}^{n \times n}$$ are both orthogonal matrices, and the matrix $$S$$ is diagonal: 
    :   $$S = \mathbf{diag}(\sigma_1 , \ldots, \sigma_r),   $$
    :   where,  
        * The positive numbers $$\sigma_1 \ge \ldots \ge \sigma_r > 0$$ are unique, and are called the **_singular values_** of A.  
        * The number $$r \le min(m,n)$$ is equal to the rank of $$A$$.  
        * The triplet $$(U, \tilde{ {S}}, V)$$ is called a **_singular value decomposition_** (SVD) of $$A$$.  
        * The first $$r$$ columns of $$U: u_i, i=1, \ldots, r$$ (resp. $$V: v_i,  i=1, \ldots, r)$$ are called left (resp. right) singular vectors of $$A$$, and satisfy:  
    :   $$
        Av_i = \sigma_i u_i, \;\;\;\; A^Tu_i = \sigma_i v_i, \;\;\;\; i=1,...,r.
        $$

    :   <button>Proof.</button>{: .showText value="show"
        onclick="showTextPopHide(event);"}
        ![img](/main_files/conv_opt/2/2.4/pf0.png){: hidden="" width="100%"}

3. **Computing the SVD:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13}
    :   To find the SVD of a matrix $$A$$, we solve the following equation:
    :   $$
        \begin{align}
        & (1)\  A^TA = V\Lambda^T\Lambda V^T \\
        & (2)\  AV\  = U \Lambda
        \end{align}
        $$

4. **Complexity of the SVD:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents14} \\
    1. **Normal Matrices:** the complexity grows as $$\mathcal{O}(nm min(n,m))$$. 
    2. **Sparse Matrices:** good approximations can be calculated very efficiently.

5. **Geometric Interptation:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents15} 
    :   The theorem allows to decompose the action of A on a given input vector as a sequence of **three** _elementary transformations_.
        1. First, we form $$\tilde{x} := V^Tx \in \mathbf{R}^n$$.
            > $$V$$ orthogonal $$\implies \tilde{x}$$ is a rotated version of $$x$$, which still lies in the input space.
        2. Then we act on the rotated vector $$\tilde{x}$$ by scaling its elements
            > The first $$k$$ elements of $$\tilde{x}$$ are scaled by the singular values $$\sigma_1, \ldots, \sigma_r$$; the remaining $$n-r$$ elements are set to zero.  
            > This step results in a new vector $$\tilde{y}$$ which now belongs to the output space $$\mathbf{R}^m$$.
        3. Finally, we rotate the vector $$\tilde{y}$$ by the orthogonal matrix $$U$$, which results in $$y = U\tilde{y} = Ax$$.
    :   **Summary:**  
        1. A rotation in the input space
        2. A scaling that goes from the input space to the output space
        3. A rotation in the output space. 
        > In contrast with symmetric matrices, input and output directions are different.
    :   <button>Example: A $$4 \times 4$$ Matrix</button>{: .showText value="show"
        onclick="showTextPopHide(event);"}
        ![img](/main_files/conv_opt/2/2.4/pf1.png){: hidden="" width="100%"}

6. **Link with the Spectral Theorem:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents16}
    :   If $$A$$ admits an SVD, then the matrices $$AA^T$$ and $$A^TA$$ has the following SEDs:
    :   $$
        AA^T = U \Lambda_m U^T, \;\; A^TA = V \Lambda_n V^T,  
        $$
    :   where $$\Lambda_m := \tilde{ {S}}\tilde{ {S}}^T = \mathbf{ diag}(\sigma_1^2, \ldots, \sigma_r^2, 0, \ldots, 0)$$ is $$m \times m$$ (so it has $$m-r$$ trailing zeros),   
        and $$\Lambda_n := \tilde{ {S}}^T\tilde{ {S}} = \mathbf{ diag}(\sigma_1^2, \ldots, \sigma_r^2, 0, \ldots, 0)$$ is $$n \times n$$ (so it has $$n-r$$ trailing zeros). 
    :   > The eigenvalues of $$AA^T$$ and $$A^TA$$ are the same, and equal to the squared singular values of $$A$$.  
        > The corresponding eigenvectors are the left and right singular vectors of $$A$$.



7. **Asynchronous:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents17} \\

8. **Asynchronous:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents18} \\

***

## SECOND
{: #content2}

1. **Asynchronous:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents21} \\

2. **Asynchronous:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents22} \\

3. **Asynchronous:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents23} \\

4. **Asynchronous:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents24} \\

5. **Asynchronous:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents25} \\

6. **Asynchronous:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents26} \\

7. **Asynchronous:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents27} \\

8. **Asynchronous:**{: style="color: SteelBlue  "}{: .bodyContents2 #bodyContents28} \\

***

## THIRD
{: #content3}

1. **Asynchronous:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents31} \\

2. **Asynchronous:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents32} \\

3. **Asynchronous:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents33} \\

4. **Asynchronous:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents34} \\

5. **Asynchronous:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents35} \\

6. **Asynchronous:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents36} \\

7. **Asynchronous:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents37} \\

8. **Asynchronous:**{: style="color: SteelBlue  "}{: .bodyContents3 #bodyContents38} \\

***