---
layout: NotesPage
title: Maximum Margin Classifiers
permalink: /work_files/research/ml/1_3
prevLink: /work_files/research/ml.html
---

<div markdown="1" class = "TOC">
# Table of Contents

  * [Introduction and Set up](#content1)
  {: .TOC1}
</div>

***
***

## Introduction and Set up
{: #content1}

1. **The Margin:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents11} 
    :   The **margin** of a linear classifier is the distance from the decision boundary to the nearest sample point.

2. **The current Problem:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents12} 
    :   All the classifiers discussed thus far (i.e. Centroid, Perceptron) will converge to a correct classifier on linearly seprable data; however, the classifier they converge to is **not** unique nor the best.
    :   > _But what does it mean to be the "__best__" classifier?_
    :   We assume that if we can maximize the distance between the data points to be classified and the hyperplane that classifies them, then we have reached a boundary that allows for the "best-fit", i.e. allows for the most room for error.

3. **The Solution:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents13} 
    :   We enforce a constraint that achieves a classifier that has a maximum-margin.

4. **The Signed Distance:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents14}
    :   The _signed distance_ is the minimum distance from a point to a hyperplane.
    :   We solve for the signed distance to achieve the following formula for it:
    :   $$d = \dfrac{\| w \cdot x_0 + b \|}{\|w\|},$$  
    :   where we have an n-dimensional hyperplane: $$w \cdot x + b = 0$$ and a point $$x_0$$.
    > Also known as, **The Signed Distance**.  
    * **Proof.**  
        * Suppose we have an affine hyperplane defined by $$w \cdot x + b$$ and a point $$x_0$$.
        * Suppose that $$\vec{v} \in \mathbf{R}^n$$ is a point satisfying $$w \cdot \vec{v} + b = 0$$, i.e. it is a point on the plane.
        * We construct the vector $$x_0âˆ’\vec{v}$$ which points from $$\vec{v}$$ to $$x_0$$, and then, project it onto the unique vector perpendicular to the plane, i.e. $$w$$,  

            $$d=\| \text{proj}_{w} (x_0-\vec{v})\| = \left\| \frac{(x_0-\vec{v})\cdot w}{w \cdot w} w \right\| = \|x_0 \cdot w - \vec{v} \cdot w\|\frac{\|w\|}{\|w\|^2} = \frac{\|x_0 \cdot w - \vec{v} \cdot w\|}{\|w\|}.$$

        * We chose $$\vec{v}$$ such that $$w\cdot \vec{v}=-b$$ so we get  

            $$d=\| \text{proj}_{w} (x_0-\vec{v})\| = \frac{\|x_0 \cdot w +b\|}{\|w\|}$$  

    Thus, we conclude that if $$\|w\| = 1$$ then the _signed distance_ from a datapoint $$X_i$$ to the hyperplane is $$\|wX_i + b\|$$.

6. **Geometric Analysis:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents16}
    :   First, we notice that for any given plane $$w^Tx = 0$$, the equations, $$\gamma * w^Tx = 0$$, where $$\gamma \in \mathbf{R}$$ is a scalar, basically characterize the same plane and not many planes.  
        This is because $$w^Tx = 0 \iff \gamma * w^Tx = \gamma * 0 \iff \gamma * w^Tx = 0$$.
    :   The above implies that any model that takes input $$w$$ and produces a margin, will have to be **_Scale Invariant_**.  
    :   To get around this and simplify the analysis, I am going to consider all the representations of the same plane, and I am going to pick one where we normalize (re-scale) the weight $$w$$ such that the signed distance (distance to the point closest to the margin) is equal to one:
    :   $$\|w^TX_n\| > 0 \rightarrow \|w^TX_n\| = 1$$
    :   , where $$X_n$$ is the point closest to the plane.
    :   We constraint the hyperplane by normalizing $$w$$ to this equation $$\|w^TX_i\| = 1$$ or with added bias, $$\|w^TX_i + b\| = 1$$.
    :   This implies that there exists a "slab" of width $$\dfrac{2}{\|w\|}$$.

5. **The Margin, mathematically:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents15} \\
    :   Now, we can mathematically characterize the margin.
    :   By substituting the constraints $$\: y_i(w^TX_i+ b) \geq 1, \forall i \in [1,n]$$ and the signed distance:
    :   $$\min_i \dfrac{1}{\|w\|} \|w^TX_i + b\| \geq \dfrac{1}{w}$$

9. **The distance of the point closest to the hyperplane:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents19}
    :   We find the distance of the point closest to the hyperplane.
    :   Let $$X_n$$ be the point that is closest to the plane, and let $$\hat{w} = \dfrac{w}{\|w\|}$$.  
        Take any point $$X$$ on the plane, and let $$\vec{v}$$ be the vector $$\vec{v} = X_n - X$$.  
    :   Now, the distance, d is equal to 
    :   $$\begin{align}
            d & \ = \|\hat{w}\vec{v}\| \\
            & \ = \|\hat{w}(X_n - X)\| \\
            & \ = \|\hat{w}X_n - \hat{w}X)\| \\
            & \ = \dfrac{1}{\|w\|}\|wX_n + b - wX) - b\|,  & \text{we add and subtract the bias } b\\
            & \ = \dfrac{1}{\|w\|}\|(wX_n + b) - (wX + b)\| \\
            & \ = \dfrac{1}{\|w\|}\|(wX_n + b) - (0)\|,  & \text{from the eq. of the plane on a point on the plane} \\
            & \ = \dfrac{1}{\|w\|}\|(1) - (0)\|,  & \text{from the constraint on the distance of the closest point} \\
            & \ = \dfrac{1}{\|w\|}
            \end{align}
        $$

7. **Slab Existance:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents17}
    :   The analysis done above allows us to conclusively prove that there exists a slab of width $$\dfrac{2}{\|w\|}$$ containing no sample points where the hyperplane runs through (bisects) its center.

8. **Maximizing the Margin:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents18}
    :   To maximize the margin we need to maximize the width of the slab, i.e. maximize $$\dfrac{2}{\|w\|}$$,   
    or equivalently, 
    :   $$\max_w \dfrac{2}{\|w\|} = \min_w \dfrac{\|w\|}{2} = \min_w \dfrac{1}{2}\|w\| \min_w \dfrac{1}{2}\|w\|^2$$
    :   subject to the constraint mentioned earlier $$\min_i \|wX + b\| = 1, \forall i \in [1,n]$$, or equivalently
    :   $$y_i(wX_i + b) \geq 1, \forall i \in [1,n]$$
    :   since the equation $$y_i(wX_i + b)$$ enforces the absolute value condition as was our analysis for regular linear classifiers.

11. **The Optimization Problem for Maximum Margin Classifiers:**{: style="color: SteelBlue  "}{: .bodyContents1 #bodyContents111}
    :   $$\min_w \dfrac{1}{2}w^Tw \:\:\: : \:\: y_i(wX_i + b) \geq 1, \forall i \in [1,n]$$
    :   > The above problem is a Quadratic Program, in $$d + 1$$-diminsions and $$n$$-constraints, in standard form.
    :   > Notice that we use the quadratic $$w^Tw$$ instead of the linear $$w$$ as the objective because the quadratic function is smooth at zero as opposed to the linear objective which hinders the optimization.
