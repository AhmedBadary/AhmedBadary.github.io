---
layout: NotesPage
title: Variational Auto-Encoders
permalink: /work_files/research/dl/vae
prevLink: /work_files/research/dl/cv.html
---

<div markdown="1" class = "TOC">
# Table of Contents
  * [Variational Auto-Encoders](#content4)
  {: .TOC4}
</div>

***
***

## Variational Auto-Encoders
{: #content4}

[__Auto-Encoders__ (_click to read more_)](http://ahmedbadary.ml/work_files/research/dl/aencdrs) are unsupervised learning methods that aim to learn a representation (encoding) for a set of data in a smaller dimension.  
Auto-Encoders generate __Features__ that capture _factors of variation_ in the training data.

0. **Auto-Regressive Models VS Variational Auto-Encoders:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents40}  
    :   __Auto-Regressive Models__ defined a *__tractable__* (discrete) density function and, then, optimized the likelihood of training data:   
    :   $$p_\theta(x) = p(x_0) \prod_1^n p(x_i | x_{i<})$$  
    :   On the other hand, __VAEs__ defines an *__intractable__* (continuous) density function with latent variable $$z$$:  
    :   $$p_\theta(x) = \int p_\theta(z) p_\theta(x|z) dz$$
    :   but cannot optimize directly; instead, derive and optimiz a lower bound on likelihood instead.  

1. **Variational Auto-Encoders (VAEs):**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents41}  
    :   __Variational Autoencoder__ models inherit the autoencoder architecture, but make strong assumptions concerning the distribution of latent variables.  
    :   They use variational approach for latent representation learning, which results in an additional loss component and specific training algorithm called Stochastic Gradient Variational Bayes (SGVB).  

2. **Assumptions:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents42}  
    :   VAEs assume that: 
        * The data is generated by a directed __graphical model__ $$p(x\vert z)$$ 
        * The encoder is learning an approximation $$q_\phi(z|x)$$ to the posterior distribution $$p_\theta(z|x)$$  
            where $${\displaystyle \mathbf {\phi } }$$ and $${\displaystyle \mathbf {\theta } }$$ denote the parameters of the encoder (recognition model) and decoder (generative model) respectively.  
        * The training data $$\left\{x^{(i)}\right\}_{i=1}^N$$ is generated from underlying unobserved (latent) representation $$\mathbf{z}$$

3. **The Objective Function:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents43}    
    :   $${\displaystyle {\mathcal {L}}(\mathbf {\phi } ,\mathbf {\theta } ,\mathbf {x} )=D_{KL}(q_{\phi }(\mathbf {z} |\mathbf {x} )||p_{\theta }(\mathbf {z} ))-\mathbb {E} _{q_{\phi }(\mathbf {z} |\mathbf {x} )}{\big (}\log p_{\theta }(\mathbf {x} |\mathbf {z} ){\big )}}$$
    :   where $${\displaystyle D_{KL}}$$ is the __Kullbackâ€“Leibler divergence__ (KL-Div).  

4. **The Generation Process:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents44}  
    :   
    :   ![img](/main_files/cs231n/13/5.png){: width="40%"} 

5. **The Goal:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents45}  
    :   The goal is to estimate the true parameters $$\theta^\ast$$ of this generative model.

6. **Representing the Model:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents46}  
    :   * To represent the prior $$p(z)$$, we choose it to be simple, usually __Gaussian__  
        * To represent the conditional (which is very complex), we use a neural-network  

7. **Intractability:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents47}  
    :   The __Data Likelihood__:  
    :   $$p_\theta(x) = \int p_\theta(z) p_\theta(x|z) dz$$
    :   is intractable to compute for every $$z$$.  
    :   Thus, the __Posterior Density__:  
    :   $$p_\theta(z|x) = \dfrac{p_\theta(x|z) p_\theta(z)}{p_\theta(x)} = \dfrac{p_\theta(x|z) p_\theta(z)}{\int p_\theta(z) p_\theta(x|z) dz}$$ 
    :   is, also, intractable

8. **Dealing with Intractability:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents48}  
    :   In addition to decoder network modeling $$p_\theta(x\vert z)$$, define additional encoder network $$q_\phi(z\vert x)$$ that approximates $$p_\theta(z\vert x)$$
    :   This allows us to derive a __lower bound__ on the data likelihood that is tractable, which we can optimize.  

9. **The Model:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents49}  
    :   * The __Encoder__ (recognition/inference) and __Decoder__ (generation) networks are probabilistic and output means and variances of each the conditionals respectively:  
            ![img](/main_files/cs231n/13/6.png){: width="70%"}   
        * The generation (forward-pass) is done via sampling as follows:  
            ![img](/main_files/cs231n/13/7.png){: width="72%"}   

10. **The Log-Likelihood of Data:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents410}  
    :   * Deriving the Log-Likelihood:  
    :   ![img](/main_files/cs231n/13/8.png){: width="100%"}   

11. **Training the Model:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents411}  
    :   

12. **Pros, Cons and Research:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents412}  
    :   * __Pros__: 
            * Principled approach to generative models
            * Allows inference of $$q(z\vert x)$$, can be useful feature representation for other tasks  
    :   * __Cons__: 
            * Maximizing the lower bound of likelihood is okay, but not as good for evaluation as Auto-regressive models
            * Samples blurrier and lower quality compared to state-of-the-art (GANs)
    :   * __Active areas of research__:   
            * More flexible approximations, e.g. richer approximate posterior instead of diagonal Gaussian
            * Incorporating structure in latent variables