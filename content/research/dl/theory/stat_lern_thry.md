---
layout: NotesPage
title: Statistical Learning Theory
permalink: /work_files/research/dl/theory/stat_lern_thry
prevLink: /work_files/research/dl/theory.html
---

<div markdown="1" class = "TOC">
# Table of Contents

  * [Statistical Learning Theory](#content1)
  {: .TOC1}
  * [The Vapnik-Chervonenkis (VC) Theory](#content2)
  {: .TOC2}
  * [The Bias-Variance Decomposition Theory](#content3)
  {: .TOC3}
<!--  * [FOURTH](#content4)
  {: .TOC4}
  * [FIFTH](#content5)
  {: .TOC5}
  * [SIXTH](#content6)
  {: .TOC6} -->
</div>

***
***

* [Principles of Risk Minimization for Learning Theory (original papers)](http://papers.nips.cc/paper/506-principles-of-risk-minimization-for-learning-theory.pdf)  
* [Statistical Learning Theory from scratch (paper)](http://www.tml.cs.uni-tuebingen.de/team/luxburg/publications/StatisticalLearningTheory.pdf)    
* [The learning dynamics behind generalization and overfitting in Deep Networks](https://www.youtube.com/watch?v=pFWiauHOFpY)  
* [Notes on SLT](http://maxim.ece.illinois.edu/teaching/SLT/SLT.pdf)  
* [Mathematics of Learning (w/ proofs & necessary+sufficient conditions for learning)](http://web.mit.edu/9.s915/www/classes/dealing_with_data.pdf)  
* [Generalization Bounds for Hypothesis Spaces](https://courses.cs.washington.edu/courses/cse522/11wi/scribes/lecture4.pdf)  
* [Generalization Bound Derivation](https://mostafa-samir.github.io/ml-theory-pt2/)  
* [Overfitting isnâ€™t simple: Overfitting Re-explained with Priors, Biases, and No Free Lunch](http://mlexplained.com/2018/04/24/overfitting-isnt-simple-overfitting-re-explained-with-priors-biases-and-no-free-lunch/)  



## Statistical Learning Theory
{: #content1}

1. **Statistical Learning Theory:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    __Statistical Learning Theory__ is a framework for machine learning drawing from the fields of statistics and functional analysis. Under certain assumptions, this framework allows us to study the question:  
    > __How can we affect performance on the test set when we can only observe the training set?__{: style="color: blue"}  

    It is a _statistical_ approach to __Computational Learning Theory__.  

2. **Formal Definition:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}  
    Let:  
    * $$X$$: $$\:$$ the vector space of all possible __inputs__  
    * $$Y$$: $$\:$$ the vector space of all possible __outputs__  
    * $$Z = X \times Y$$: $$\:$$ the __product space__ of (input,output) pairs  
    * $$n$$: $$\:$$ the number of __samples__ in the __training set__  
    * $$S=\left\{\left(\vec{x}_{1}, y_{1}\right), \ldots,\left(\vec{x}_{n}, y_{n}\right)\right\}=\left\{\vec{z}_{1}, \ldots, \vec{z}_{n}\right\}$$: $$\:$$ the __training set__  
    * $$\mathcal{H} = f : X \rightarrow Y$$: $$\:$$ the __hypothesis space__ of all functions  
    * $$V(f(\vec{x}), y)$$: $$\:$$ an __error/loss function__  

    __Assumptions:__  
    {: #lst-p}
    * The training and test data are generated by an *__unknown, joint__* __probability distribution over datasets__ (over the product space $$Z$$, denoted: $$p_{\text{data}}(z)=p(\vec{x}, y)$$) called the __data-generating process__.  
        * $$p_{\text{data}}$$ is a __joint distribution__ so that it allows us to model _uncertainty in predictions_ (e.g. from noise in data) because $$y$$ is not a deterministic function of $$\vec{x}$$, but rather a _random variable_ with __conditional distribution__ $$p(y \vert \vec{x})$$ for a fixed $$\vec{x}$$.  
    * The __i.i.d. assumptions:__  
        * The examples in each dataset are __independent__ from each other  
        * The _training set_ and _test set_ are __identically distributed__ (drawn from the same probability distribution as each other)  

        > A collection of random variables is __independent and identically distributed__ if each random variable has the same probability distribution as the others and all are mutually independent.  
        > _Informally,_ it says that all the variables provide the same kind of information independently of each other.  
        > * [Discussion on the importance if i.i.d assumptions](https://stats.stackexchange.com/questions/213464/on-the-importance-of-the-i-i-d-assumption-in-statistical-learning/214220)  


    __The Inference Problem__{: style="color: red"}    
    Finding a function $$f : X \rightarrow Y$$ such that $$f(\vec{x}) \sim y$$.  

    __The Expected Risk:__  
    <p>$$I[f]=\mathbf{E}[V(f(\vec{x}), y)]=\int_{X \times Y} V(f(\vec{x}), y) p(\vec{x}, y) d \vec{x} d y$$</p>  

    __The Target Function:__  
    is the best possible function $$f$$ that can be chosen, is given by:  
    <p>$$f=\inf_{h \in \mathcal{H}} I[h]$$</p>  

    __The Empirical Risk:__  
    Is a *__proxy measure__* for the __expected risk__, based on the training set.  
    It is necessary because the probability distribution $$p(\vec{x}, y)$$ is _unknown_.  
    <p>$$I_{S}[f]=\frac{1}{n} \sum_{i=1}^{n} V\left(f\left(\vec{x}_{i}\right), y_{i}\right)$$</p>  

3. **Empirical risk minimization:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}  
    __Empirical Risk Minimization (ERM)__ is a principle in _statistical learning theory_ that is based on approximating the __Generalization Error (True Risk)__ by measuring the __Training Error (Empirical Risk)__, i.e. the performance on training data.  

    A _learning algorithm_ that chooses the function $$f_{S}$$ that minimizes the _empirical risk_ is called __empirical risk minimization__:  
    <p>$$R_{\mathrm{emp}}(h) = I_{S}[f]=\frac{1}{n} \sum_{i=1}^{n} V\left(f\left(\vec{x}_{i}\right), y_{i}\right)$$</p>    
    <p>$$f_{S} = \hat{h} = \arg \min _{h \in \mathcal{H}} R_{\mathrm{emp}}(h)$$</p>  

    __Complexity:__{: style="color: red"}  
    <button>Show</button>{: .showText value="show"
     onclick="showText_withParent_PopHide(event);"}
    * Empirical risk minimization for a classification problem with a _0-1 loss function_ is known to be an __NP-hard__ problem even for such a relatively simple class of functions as linear classifiers.  
        * [Paper Proof](https://arxiv.org/abs/1012.0729)  
    * Though, it can be solved efficiently when the minimal empirical risk is zero, i.e. data is linearly separable.  
    * __Coping with Hardness:__  
        * Employing a __convex approximation__ to the 0-1 loss: _Hinge Loss_, _SVM_  
        * Imposing __Assumptions on the data-generating distribution__ and thus, stop being an __agnostic learning algorithm__.  
    {: hidden=""}  


4. **Definitions:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents14}  
    * __Generalization Error:__{: style="color: red"}  
        AKA: __Expected Risk/Error__, __Out-of-Sample Error__[^2], __$$E_{\text{out}}$$__   
        It is a measure of how accurately an algorithm is able to predict outcome values for previously unseen data.  
    * __Generalization Gap:__{: style="color: red"}  
        It is a measure of how accurately an algorithm is able to predict outcome values for previously unseen data.  

        __Formally:__  
        The generalization gap is the __difference between the expected and empirical error__:  
        <p>$$G =I\left[f_{n}\right]-I_{S}\left[f_{n}\right]$$</p>  

        An Algorithm is said to __Generalize__ (achieve __Generalization__) if:  
        <p>$$\lim _{n \rightarrow \infty} G_n = \lim _{n \rightarrow \infty} I\left[f_{n}\right]-I_{S}\left[f_{n}\right]=0$$</p>  
        Equivalently:  
        <p>$$E_{\text { out }}(g) \approx E_{\text { in }}(g)$$</p>  
        or 
        <p>$$I\left[f_{n}\right] \approx I_{S}\left[f_{n}\right]$$</p>  

        __Computing the Generalization Gap:__  
        Since $$I\left[f_{n}\right]$$ cannot be computed for an unknown distribution, the generalization gap __cannot be computed__ either.  
        Instead the goal of __statistical learning theory__ is to _bound_ or _characterize_ the generalization gap in probability:  
        <p>$$P_{G}=P\left(I\left[f_{n}\right]-I_{S}\left[f_{n}\right] \leq \epsilon\right) \geq 1-\delta_{n}$$</p>  
        That is, the goal is to characterize the probability $${\displaystyle 1-\delta _{n}}$$ that the generalization gap is less than some error bound $${\displaystyle \epsilon }$$ (known as the __learning rate__ and generally dependent on $${\displaystyle \delta }$$ and $${\displaystyle n}$$).  
    * __The Empirical Distribution:__{: style="color: red"}  
        _AKA **Data-Generating Distribution**_  
        is the __discrete__ uniform distribution over the _sample points_.   
    * __The Approximation-Generalization Tradeoff:__{: style="color: red"}  
        * __Goal__:  
            Small $$E_{\text{out}}$$: Good approximation of $$f$$ *__out of sample__* (not in-sample).  
        * The tradeoff is characterized by the __complexity__ of the __hypothesis space $$\mathcal{H}$$__:  
            * __More Complex $$\mathcal{H}$$__: Better chance of approximating $$f$$  
            * __Less Complex $$\mathcal{H}$$__: Better chance of generalizing out-of-sample  
        * [**Abu-Mostafa**](https://www.youtube.com/embed/zrEyxfl2-a8?start=358){: value="show" onclick="iframePopA(event)"}
        <a href="https://www.youtube.com/embed/zrEyxfl2-a8?start=358"></a>
            <div markdown="1"> </div>    
        * [Lecture-Slides on Approximation-Generalization](https://mdav.ece.gatech.edu/ece-6254-spring2017/notes/13-bias-variance-marked.pdf)  
    * __Excess Risk (Generalization-Gap) Decomposition \| Estimation-Approximation Tradeoff:__{: style="color: red"}  
        __Excess Risk__ is defined as the difference between the expected-risk/generalization-error of any function $$\hat{f} = g^{\mathcal{D}}$$ that we learn from the data (exactly just bias-variance), and the expected-risk of the __target function__ $$f$$ (known as the __bayes optimal predictor__)
        * [**Excess Risk Decomposition Video**](https://www.youtube.com/embed/YA_CE9jat4I){: value="show" onclick="iframePopA(event)"}
        <a href="https://www.youtube.com/embed/YA_CE9jat4I"></a>
            <div markdown="1"> </div>    
        * [Excess Risk & Bias-Variance Lecture Slides](https://www.ics.uci.edu/~smyth/courses/cs274/readings/xing_singh_CMU_bias_variance.pdf)  
    <br>

8. **Notes:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents18}  
    * __Choices of Loss Functions__:  
        * __Regression__:  
            * __MSE__: $$\: V(f(\vec{x}), y)=(y-f(\vec{x}))^{2}$$ 
            * __MAE__: $$\: V(f(\vec{x}), y)=\vert{y-f(\vec{x})}\vert$$  
        * __Classification__:  
            * __Binary__: $$\: V(f(\vec{x}), y)=\theta(-y f(\vec{x}))$$  
                where $$\theta$$ is the _Heaviside Step Function_.  
    * __Training Data, Errors, and Risk__:  
        * __Training-Error__ is the __Empirical Risk__  
            * It is a __proxy__ for the __Generalization Error/Expected Risk__  
            * This is what we minimize
        * __Test-Error__ is an *__approximation__* to the __Generalization Error/Expected Risk__ 
            * This is what we (can) compute to ensure that minimizing Training-Err/Empirical-Risk (ERM) also minimized the Generalization-Err/Expected-Risk (which we can't compute directly)  
    * __Why the goal is NOT to minimize $$E_{\text{in}}$$ completely (intuition)__:  
        Basically, if you have noise in the data; then fitting the (finite) training-data completely; i.e. minimizing the in-sample-err completely will underestimate the out-of-sample-err.  
        Since, if noise existed AND you fit training-data completely $$E_{\text{in}} = 0$$ THEN you inherently have fitted the noise AND your performance on out-sample will be lower.   


***

## The Vapnik-Chervonenkis (VC) Theory
{: #content2}
<!-- 
1. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21}

2. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents22}

3. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents23}

4. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents24}

5. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents25}

6. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents26}

7. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents27}

8. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents28} -->

***

## The Bias-Variance Decomposition Theory
{: #content3}

11. **The Bias-Variance Decomposition Theory:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents311}    
    __The Bias-Variance Decomposition Theory__ is a way to quantify the __Approximation-Generalization Tradeoff__.  

    __Assumptions:__  
    {: #lst-p}
    * The analysis is done over the __entire data-distribution__  
    * The target function $$f$$ is already __known__; and you're trying to answer the question:  
        "How can $$\mathcal{H}$$ approximate $$f$$ over all? not just on your sample."  
    * Applies to __real-valued targets__ (can be extended)  
    * Use __Square Error__ (can be extended)  
    <br>

1. **The Bias-Variance Decomposition:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents31}    
    The __Bias-Variance Decomposition__ is a way of analyzing a learning algorithm's *__expected out-of-sample error__*[^1] as a _sum of three terms:_  
    * __Bias:__ is an error from erroneous assumptions in the learning algorithm.  
    * __Variance:__ is an error from sensitivity to small fluctuations in the training set.  
    * __Irreducible Error__ (resulting from noise in the problem itself)  

    Equivalently, __Bias__ and __Variance__ measure _two different sources of errors in an estimator:_   
    * __Bias:__ measures the expected deviation from the true value of the function or parameter.  
        > AKA: __Approximation Error__[^3]  (statistics)  How well can $$\mathcal{H}$$ approximate the target function '$$f$$'  
    * __Variance:__ measures the deviation from the expected estimator value that any particular sampling of the data is likely to cause.  
        > AKA: __Estimation (Generalization) Error__ (statistics) How well we can zoom in on a good $$h \in \mathcal{H}$$  


    __Bias-Variance Decomposition Formula:__  
    For any function $$\hat{f} = g^{\mathcal{D}}$$ we select, we can decompose its *__expected (out-of-sample) error__* on an _unseen sample $$x$$_ as:  
    <p>$$\mathrm{E}\left[(y-\hat{f}(x))^{2}\right]=(\operatorname{Bias}[\hat{f}(x)])^{2}+\operatorname{Var}[\hat{f}(x)]+\sigma^{2}$$</p>  
    Where:  
    * __Bias__:  
        <p>$$\operatorname{Bias}[\hat{f}(x)]=\mathrm{E}[\hat{f}(x)]-f(x)$$</p>  
    * __Variance__:  
        <p>$$\operatorname{Var}[\hat{f}(x)]=\mathrm{E}\left[\hat{f}(x)^{2}\right]-\mathrm{E}[\hat{f}(x)]^{2}$$</p>  
    and the expectation ranges over different realizations of the training set $$\mathcal{D}$$.  

2. **The Bias-Variance Tradeoff:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents32}  
    is the property of a set of predictive models whereby, models with a _lower bias_ (in parameter estimation) have a _higher variance_ (of the parameter estimates across samples) and vice-versa.  

    __Effects of Bias:__{: style="color: black"}  
    {: #lst-p}
    * __High Bias__: simple models, lead to *__underfitting__*{: style="color: red"}.  
    * __Low Bias__: complex models, lead to *__overfitting__*{: style="color: red"}.  
    
    __Effects of Variance:__{: style="color: black"}  
    {: #lst-p}
    * __High Variance__: complex models, lead to *__overfitting__*{: style="color: red"}.  
    * __Low Variance__: simple models, lead to *__underfitting__*{: style="color: red"}.  
    <br>

3. **Derivation:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents33}  
    <p>$${\displaystyle {\begin{aligned}\mathbb{E}_{\mathcal{D}} {\big [}I[g^{(\mathcal{D})}]{\big ]}&=\mathbb{E}_{\mathcal{D}} {\big [}\mathbb{E}_{x}{\big [}(g^{(\mathcal{D})}-y)^{2}{\big ]}{\big ]}\\
    &=\mathbb{E}_{x} {\big [}\mathbb{E}_{\mathcal{D}}{\big [}(g^{(\mathcal{D})}-y)^{2}{\big ]}{\big ]}\\
    &=\mathbb{E}_{x}{\big [}\mathbb{E}_{\mathcal{D}}{\big [}(g^{(\mathcal{D})}- f -\varepsilon)^{2}{\big ]}{\big ]}
    \\&=\mathbb{E}_{x}{\big [}\mathbb{E}_{\mathcal{D}} {\big [}(f+\varepsilon -g^{(\mathcal{D})}+\bar{g}-\bar{g})^{2}{\big ]}{\big ]}\\
    &=\mathbb{E}_{x}{\big [}\mathbb{E}_{\mathcal{D}} {\big [}(\bar{g}-f)^{2}{\big ]}+\mathbb{E}_{\mathcal{D}} [\varepsilon ^{2}]+\mathbb{E}_{\mathcal{D}} {\big [}(g^{(\mathcal{D})} - \bar{g})^{2}{\big ]}+2\: \mathbb{E}_{\mathcal{D}} {\big [}(\bar{g}-f)\varepsilon {\big ]}+2\: \mathbb{E}_{\mathcal{D}} {\big [}\varepsilon (g^{(\mathcal{D})}-\bar{g}){\big ]}+2\: \mathbb{E}_{\mathcal{D}} {\big [}(g^{(\mathcal{D})} - \bar{g})(\bar{g}-f){\big ]}{\big ]}\\
    &=\mathbb{E}_{x}{\big [}(\bar{g}-f)^{2}+\mathbb{E}_{\mathcal{D}} [\varepsilon ^{2}]+\mathbb{E}_{\mathcal{D}} {\big [}(g^{(\mathcal{D})} - \bar{g})^{2}{\big ]}+2(\bar{g}-f)\mathbb{E}_{\mathcal{D}} [\varepsilon ]\: +2\: \mathbb{E}_{\mathcal{D}} [\varepsilon ]\: \mathbb{E}_{\mathcal{D}} {\big [}g^{(\mathcal{D})}-\bar{g}{\big ]}+2\: \mathbb{E}_{\mathcal{D}} {\big [}g^{(\mathcal{D})}-\bar{g}{\big ]}(\bar{g}-f){\big ]}\\
    &=\mathbb{E}_{x}{\big [}(\bar{g}-f)^{2}+\mathbb{E}_{\mathcal{D}} [\varepsilon ^{2}]+\mathbb{E}_{\mathcal{D}} {\big [}(g^{(\mathcal{D})} - \bar{g})^{2}{\big ]}{\big ]}\\
    &=\mathbb{E}_{x}{\big [}(\bar{g}-f)^{2}+\operatorname {Var} [y]+\operatorname {Var} {\big [}g^{(\mathcal{D})}{\big ]}{\big ]}\\
    &=\mathbb{E}_{x}{\big [}\operatorname {Bias} [g^{(\mathcal{D})}]^{2}+\operatorname {Var} [y]+\operatorname {Var} {\big [}g^{(\mathcal{D})}{\big ]}{\big ]}\\
    &=\mathbb{E}_{x}{\big [}\operatorname {Bias} [g^{(\mathcal{D})}]^{2}+\sigma ^{2}+\operatorname {Var} {\big [}g^{(\mathcal{D})}{\big ]}{\big ]}\\
\end{aligned}}}$$</p>  
    where:  
    $$\overline{g}(\mathbf{x})=\mathbb{E}_{\mathcal{D}}\left[g^{(\mathcal{D})}(\mathbf{x})\right]$$ is the __average hypothesis__ over all realization of $$N$$ data-points $$\mathcal{D}_ i$$.  

    <button>Derivation with Wikipedia Notation</button>{: .showText value="show"
     onclick="showText_withParent_PopHide(event);"}
    <p hidden="">$${\displaystyle {\begin{aligned}\operatorname {E}_ {\mathcal{D}} {\big [}(y-{\hat {f}})^{2}{\big ]}&=\operatorname {E} {\big [}(f+\varepsilon -{\hat {f}})^{2}{\big ]}\\&=\operatorname {E} {\big [}(f+\varepsilon -{\hat {f}}+\operatorname {E} [{\hat {f}}]-\operatorname {E} [{\hat {f}}])^{2}{\big ]}\\&=\operatorname {E} {\big [}(f-\operatorname {E} [{\hat {f}}])^{2}{\big ]}+\operatorname {E} [\varepsilon ^{2}]+\operatorname {E} {\big [}(\operatorname {E} [{\hat {f}}]-{\hat {f}})^{2}{\big ]}+2\operatorname {E} {\big [}(f-\operatorname {E} [{\hat {f}}])\varepsilon {\big ]}+2\operatorname {E} {\big [}\varepsilon (\operatorname {E} [{\hat {f}}]-{\hat {f}}){\big ]}+2\operatorname {E} {\big [}(\operatorname {E} [{\hat {f}}]-{\hat {f}})(f-\operatorname {E} [{\hat {f}}]){\big ]}\\&=(f-\operatorname {E} [{\hat {f}}])^{2}+\operatorname {E} [\varepsilon ^{2}]+\operatorname {E} {\big [}(\operatorname {E} [{\hat {f}}]-{\hat {f}})^{2}{\big ]}+2(f-\operatorname {E} [{\hat {f}}])\operatorname {E} [\varepsilon ]+2\operatorname {E} [\varepsilon ]\operatorname {E} {\big [}\operatorname {E} [{\hat {f}}]-{\hat {f}}{\big ]}+2\operatorname {E} {\big [}\operatorname {E} [{\hat {f}}]-{\hat {f}}{\big ]}(f-\operatorname {E} [{\hat {f}}])\\&=(f-\operatorname {E} [{\hat {f}}])^{2}+\operatorname {E} [\varepsilon ^{2}]+\operatorname {E} {\big [}(\operatorname {E} [{\hat {f}}]-{\hat {f}})^{2}{\big ]}\\&=(f-\operatorname {E} [{\hat {f}}])^{2}+\operatorname {Var} [y]+\operatorname {Var} {\big [}{\hat {f}}{\big ]}\\&=\operatorname {Bias} [{\hat {f}}]^{2}+\operatorname {Var} [y]+\operatorname {Var} {\big [}{\hat {f}}{\big ]}\\&=\operatorname {Bias} [{\hat {f}}]^{2}+\sigma ^{2}+\operatorname {Var} {\big [}{\hat {f}}{\big ]}\\\end{aligned}}}$$</p>


4. **Results and Takeaways of the Decomposition:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents34}  
    Match the __"Model Complexity"__ to the *__Data Resources__*, NOT to the _Target Complexity_.  
    ![img](/main_files/dl/theory/stat_lern_thry/1.png){: width="80%"}  

    <button>Analogy to the Approximation-Generalization Tradeoff</button>{: .showText value="show"
    onclick="showText_withParent_PopHide(event);"}
    <p hidden="">Pretty much like I'm sitting in my office, and I want a document of some kind, an old letter. Someone has asked me for a letter of recommendation, and I don't want to rewrite it from scratch. So I want to take the older letter, and just see what I wrote, and then add the update to that.  <br>
    Before everything was archived in the computers, it used to be a piece of paper. So I know the letter of recommendation is somewhere. Now I face the question, should I write the letter of recommendation from scratch? Or should I look for the letter of recommendation? The recommendation is there. It's much easier when I find it. However, finding it is a big deal. So the question is not that the target function is there. The question is, can I find it?<br>  
    (Therefore, when I give you 100 examples, you choose the hypothesis set to match the 100 examples. If the 100 examples are terribly noisy, that's even worse. Because their information to guide you is worse.)  <br>
    <strong style="color: red">The data resources you have is, "what do you have in order to navigate the hypothesis set?". Let's pick a hypothesis set that we can afford to navigate. That is the game in learning. Done with the bias and variance.</strong></p>


5. **Measuring the Bias and Variance:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents35}  
    * __Training Error__: reflects Bias, NOT variance
    * __Test Error__: reflects Both


6. **Reducing the Bias and Variance, and Irreducible Err:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents36}  
    * __Adding Good Feature__:  
        * Decrease Bias  
    * __Adding Bad Feature__:  
        * Doesn't affect (increase) Bias much  
    * __Adding ANY Feature__:  
        * Increases Variance  
    * __Adding more Data__:  
        * Decreases Variance
        * (May) Decreases Bias: if $$h$$ can fit $$f$$ exactly.  
    * __Noise in Test Set__:  
        * Affects ONLY Irreducible Err
    * __Noise in Training Set__:  
        * Affects BOTH and ONLY Bias and Variance  
    * __Dimensionality Reduction__:  
        * Decrease Variance (by simplifying models)  
    * __Feature Selection__:  
        * Decrease Variance (by simplifying models)  
    * __Regularization__:  
        * Increase Bias
        * Decrease Variance
    * __Increasing # of Hidden Units in ANNs__:  
        * Decrease Bias
        * Increase Variance  
    * __Increasing # of Hidden Layers in ANNs__:  
        * Decrease Bias  
        * Increase Variance  
    * __Increasing $$k$$ in K-NN__:  
        * Increase Bias
        * Decrease Variance  
    * __Increasing Depth in Decision-Trees__:  
        * Increase Variance  
    * __Boosting__:  
        * Decreases Bias  
    * __Bagging__:  
        * Reduces Variance              

    * We __Cannot Reduce__ the __Irreducible Err__  
            
            



7. **Application of the Decomposition to Classification:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents37}  
    A similar decomposition exists for:  
    * Classification w/ $$0-1$$ loss  
    * Probabilistic Classification w/ Squared Error  

8. **Bias-Variance Decomposition and Risk (Excess Risk Decomposition):**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents38}  
    The __Bias-Variance Decomposition__ analyzes the behavior of the *__Expected Risk/Generalization Error__* for any function $$\hat{f}$$:  
    <p>$$R(\hat{f}) = \mathrm{E}\left[(y-\hat{f}(x))^{2}\right]=(\operatorname{Bias}[\hat{f}(x)])^{2}+\operatorname{Var}[\hat{f}(x)]+\sigma^{2}$$</p>  
    Assuming that $$y = f(x) + \epsilon$$.  

    The __Bayes Optimal Predictor__ is $$f(x) = \mathrm{E}[Y\vert X=x]$$.  

    The __Excess Risk__ is:  
    <p>$$\text{ExcessRisk}(\hat{f}) = R(\hat{f}) - R(f)$$</p>  

    __Excess Risk Decomposition:__{: style="color: red"}  
    We add and subtract the __target function $$f_{\text{target}}=\inf_{h \in \mathcal{H}} I[h]$$__ that minimizes the __(true) expected risk__:  
    <p>$$\text{ExcessRisk}(\hat{f}) = \underbrace{\left(R(\hat{f}) - R(f_{\text{target}})\right)}_ {\text { estimation error }} + \underbrace{\left(R(f_{\text{target}}) - R(f)\right)}_ {\text { approximation error }}$$</p>  



    The __Bias-Variance Decomposition__ for *__Excess Risk:__*  
    * Re-Writing __Excess Risk__:  
        <p>$$\text{ExcessRisk}(\hat{f}) = R(\hat{f}) - R(f) = \mathrm{E}\left[(y-\hat{f}(x))^{2}\right] - \mathrm{E}\left[(y-f(x))^{2}\right]$$</p>  
        which is equal to:  
        <p>$$R(\hat{f}) - R(f) = \mathrm{E}\left[(f(x)-\hat{f}(x))^{2}\right]$$</p>  
    <p>$$\mathrm{E}\left[(f(x)-\hat{f}(x))^{2}\right] = (\operatorname{Bias}[\hat{f}(x)])^{2}+\operatorname{Var}[\hat{f}(x)]$$</p>  


[^1]: with respect to a particular problem.  
[^2]: Note that Abu-Mostafa defines _out-sample error $$E_{\text{out}}$$_ as the _expected error/risk $$I[f]$$_; thus making $$G = E_{\text{out}} - E_{\text{in}}$$.  
[^3]: can be viewed as a measure of the __average network approximation error__ _over all possible training data sets $$\mathcal{D}$$_   

[  
if you dont want to mess with stat-jargon; lemme rephrase:  
is the minimizer $${\displaystyle f=\inf_{h \in \mathcal{H}} I[h]}$$ where $$I[h]$$ is the expected-risk/generalization-error (assume MSE);  
is it $$\overline{f}(\mathbf{x})=\mathbb{E}_{\mathcal{D}}\left[f^{(\mathcal{D})}(\mathbf{x})\right]$$ the average hypothesis over all realizations of $$N$$ data-points $$\mathcal{D}_i$$??  
]