---
layout: NotesPage
title: Concepts
permalink: /concepts_
prevLink: /
---

<div markdown="1" class = "TOC">
# Table of Contents

  * [ConvNets](#content1)
  {: .TOC1}
  * [RNNs](#content2)
  {: .TOC2}
  * [Math](#content3)
  {: .TOC3}
  * [Statistics](#content4)
  {: .TOC4}
  * [Optimization](#content5)
  {: .TOC5}
  * [Machine Learning](#content6)
  {: .TOC6}
  * [Computer Vision](#content7)
  {: .TOC7}
  * [NLP](#content8)
  {: .TOC8}
  * [Physics](#content9)
  {: .TOC9}
  * [Game Theory](#content10)
  {: .TOC10}
  * [Misc.](#content11)
  {: .TOC11}
</div>

***
***

## ConvNets
{: #content1}

1. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents11}  
    :   

<!-- 2. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}  
    :   

3. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}  
    :   

4. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents14}  
    :   

5. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents15}  
    :   

6. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents16}  
    :   

7. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents17}  
    :   

8. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents18}  
    :   
 -->
***

## RNNs
{: #content2}

1. **Gradient Clipping Intuition:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21}  
    :   ![img](/main_files/main/concepts/1.png){: width="100%"}  
    :   * The image above is that of the __Error Surface__ of a _single hidden unit RNN_ 
        * The observation here is that there exists __High Curvature Walls__ 
            This Curvature Wall will move the gradient to a very different/far, probably less useful area. 
            Thus, if we clip the gradients we will avoid the walls and will remain in the more useful area that we were exploring already. 
    :   Draw a line between the original point on the Error graph and the End (optimized) point then evaluate the Error on points on that line and look at the changes -> this shows changes in the curvature.


2. **PeepHole Connection:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents22}  
    :   is an addition on the equations of the __LSTM__ as follows: 
    :   $$ \Gamma_o = \sigma(W_o[a^{(t-1)}, x^{(t)}] + b_o) \\
        \implies 
        \sigma(W_o[a^{(t-1)}, x^{(t)}, c^{(t-1)}] + b_o)$$
    :   Thus, we add the term $$c^{(t-1)}$$ to the output gate.

3. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents23}  
    :   

<!-- 4. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents24}  
    :   

5. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents25}  
    :   

6. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents26}  
    :   

7. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents27}  
    :   

8. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents28}  
    :    -->

***

## Maths
{: #content3}

1. **Metrics and Quasi-Metrics:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents31}  
    :   A __Metric (distance function)__ $$d$$  is a function that defines a distance between each pair of elements of a set $$X$$.  
        A Metric induces a _topology_ on a set; BUT, not all topologies can be generated by a metric.  
        Mathematically, it is a function:  
    :   $${\displaystyle d:X\times X\to [0,\infty )},$$  
    :   that must satisfy the following properties:  
        1.  $${\displaystyle d(x,y)\geq 0}$$ $$\:\:\:\:\:\:\:$$   non-negativity or separation axiom  
        2.  $${\displaystyle d(x,y)=0\Leftrightarrow x=y}$$ $$\:\:\:\:\:\:\:$$  identity of indiscernibles  
        3.  $${\displaystyle d(x,y)=d(y,x)}$$ $$\:\:\:\:\:\:\:$$  symmetry  
        4.  $${\displaystyle d(x,z)\leq d(x,y)+d(y,z)}$$ $$\:\:\:\:\:\:\:$$  subadditivity or triangle inequality  
        > The first condition is implied by the others.  
    :   <button>Examples</button>{: .showText value="show"
     onclick="showTextPopHide(event);"}
    ![img](/main_files/concepts/1.png){: width="100%" hidden=""}  
    :   A __Quasi-Metric__ is a metric that lacks the _symmetry_ property.  
    :   One can form a Metric function $$d'$$  from a Quasi-metric function $$d$$ by taking:  
        $$d'(x, y) = ​1⁄2(d(x, y) + d(y, x))$$  

2. **Binary Relations (abstract algebra):**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents32}  
    :   A __binary relation__ on a set $$A$$ is a set of ordered pairs of elements of $$A$$. In other words, it is a subset of the Cartesian product $$A^2 = A ×A$$.  
    :   The number of binary relations on a set of $$N$$ elements is $$= 2^{N^2}$$
    :   __Examples:__  
        * "is greater than"  
        * "is equal to"  
        * A function $$f(x)$$  
    :   __Properties:__  (for a relation $$R$$ and set $$X$$)  
        * _Reflexive:_ for all $$x$$ in $$X$$ it holds that $$xRx$$  
        * _Symmetric:_ for all $$x$$ and $$y$$ in $$X$$ it holds that if $$xRy$$ then $$yRx$$  
        * _Transitive:_ for all $$x$$, $$y$$ and $$z$$ in $$X$$ it holds that if $$xRy$$ and $$yRz$$ then $$xRz$$  
    :   An __Equivalence Relation__ has all of the above properties.  


3. **Set Theory:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents33}  
    :   * __Number of subsets of a set of $$N$$ elements__ $$= 2^N$$  
        * __Number of pairs (e.g. $$(a,b)$$) of a set of $$N$$ elements__ $$= N^2$$  
            > e.g. $$\mathbb{R} \times \mathbb{R} = \mathbb{R}^2$$  

<!-- 
4. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents34}  
    :   

5. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents35}  
    :   

6. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents36}  
    :   

7. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents37}  
    :   

8. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents38}  
    :   
 -->
8. **Formulas:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents38}  
    :   * __Binomial Theorem__:  
    :   $$(x+y)^{n}=\sum_{k=0}^{n}{n \choose k}x^{n-k}y^{k}=\sum_{k=0}^{n}{n \choose k}x^{k}y^{n-k} \\={n \choose 0}x^{n}y^{0}+{n \choose 1}x^{n-1}y^{1}+{n \choose 2}x^{n-2}y^{2}+\cdots +{n \choose n-1}x^{1}y^{n-1}+{n \choose n}x^{0}y^{n},$$  
    :   * __Binomial Coefficient__:  
    :   $${\binom {n}{k}}={\frac {n!}{k!(n-k)!}}$$ 
    :   * __Expansion $$x^n - y^n$$__:  
    :   $$x^n - y^n = (x-y)(x^{n-1} + x^{n-2} y + ... + x y^{n-2} + y^{n-1})$$  
    :   * __Number of subsets of a set of $$N$$ elements__ $$= 2^N$$  
        * __Number of pairs (e.g. $$(a,b)$$) of a set of $$N$$ elements__ $$= N^2$$  
            


***

## Statistics
{: #content4}

1. **ROC Curve:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents41}  
    :   * A way to quantify how good a **binary classifier** separates two classes
        * True-Positive-Rate / False-Positive-Rate
        * Good classifier has a ROC curve that is near the top-left diagonal (hugging it)
        * A Bad Classifier has a ROC curve that is close to the diagonal line
        * It allows you to set the **classification threshold**
            * You can minimize False-positive rate or maximize the True-Positive Rate

2. **AUC - AUROC:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents42}  
    :   * Range $$ = 0.5 - 1.0$$ from poor to perfect 


3. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents43}  
    :   

4. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents44}  
    :   

5. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents45}  
    :   

6. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents46}  
    :   

7. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents47}  
    :   

8. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents48}  
    :    -->

***

## Optimization
{: #content5}

1. **Sigmoid:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents51}  
    :   $$\sigma(-x) = 1 - \sigma(x)$$

2. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents52}  
    :   

<!-- 3. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents53}  
    :   

4. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents54}  
    :   

5. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents55}  
    :   

6. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents56}  
    :   

7. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents57}  
    :   

8. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents58}  
    :    -->

***

## Machine Learning
{: #content6}

1. **Why NNs are not enough:**{: style="color: SteelBlue"}{: .bodyContents6 #bodyContents61}  
    :   The gist of it is this: neural nets do *pattern recognition*, which achieves *local generalization* (which works great for supervised perception). But many simple problems require some (small) amount of abstract modeling, which modern neural nets can't learn

2. **The Big Formulations:**{: style="color: SteelBlue"}{: .bodyContents6 #bodyContents62}  
    :   * __Sequence Labeling__: 
            * *__Problems__*:  
                * Speech Recognition 
                * OCR
                * Semantic Segmentation
            * *__Approaches__*:  
                * CTC - Bi-directional LSTM
                * Listen Attend and Spell (LAS)
                * HMMs 
                * CRFs
                
            
3. **What is ML?:**{: style="color: SteelBlue"}{: .bodyContents6 #bodyContents63}  
    :   Improve on __TASK T__ with respect to __PERFORMANCE METRIC P__ based on __EXPERIENCE E__.  
    :   __T:__ Categorize email messages as spam or legitimate 
        __P:__ Percentage of email messages correctly classified 
        __E:__ Database of emails, some with 
        human-given labels 

        __T:__ Recognizing hand-written words 
        __P:__ Percentage of words correctly classified 
        __E:__ Database of human-labeled images of 
        handwritten words 

        __T:__ playing checkers 
        __P:__ percentage of games won against an arbitrary opponent 
        __E:__ Playing practice games against itself 

        __T:__ Driving on four-lane highways using vision sensors 
        __P:__ Average distance traveled before a human-judged error 
        __E:__ A seq of images and steering commands recorded while observing a human driver  

4. **Graphical Models:**{: style="color: SteelBlue"}{: .bodyContents6 #bodyContents64}  
    :   

<!-- 5. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents6 #bodyContents65}  
    :   

6. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents6 #bodyContents66}  
    :   

7. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents6 #bodyContents67}  
    :   

8. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents6 #bodyContents68  
   -->  :   

***

## Computer Vision
{: #content7}

1. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents7 #bodyContents71}  
    :   

<!-- 2. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents7 #bodyContents72}  
    :   

3. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents7 #bodyContents73}  
    :   

4. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents7 #bodyContents74}  
    :   

5. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents7 #bodyContents75}  
    :   

6. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents7 #bodyContents76}  
    :   

7. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents7 #bodyContents77}  
    :   

8. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents7 #bodyContents78}  
  -->   :   

***

## NLP
{: #content8}

1. **Towards Better Language Modeling (Lec.9 highlight, 38m):**{: style="color: SteelBlue"}{: .bodyContents8 #bodyContents81}  
    :   To improve a _Language Model_:  
        1. __Better Inputs__: 
            Word -> Subword -> Char  
            ![img](/main_files/main/concepts/2.png){: width="100%"}  
            _Subword Language Modeling , Mikolov et al. 2012_  
            _Character-Aware Neural Language Model , Kim et al. 2015_.  
        2. __Better Regularization/Preprocessing__:  
            Similar to computer vision, we can do both Regularization and Preprocessing on the data to increase its relevance to the true distribution.  
            Preprocessing acts as a *__data augmentation__* technique. This allows us to achieve a __Smoother__ distribution, since we are removing more common words and re-enforcing rarer words.  
            _Zoneout, Kruger et al. 2016_  
            _Data Noising as Smoothing, Xie et al. 2016_       
            * *__Regularization__*:  
                * Use Dropout (Zaremba, et al. 2014). 
                * Use Stochastic FeedForward depth (Huang et al. 2016)
                * Use Norm Stabilization (Memisevic 2015)
                ...  
            * *__Preprocessing__*:  
                 * Randomly replacing words in a sentence with other words  
                 * Use bigram statistics to generate _Kneser-Ney_ inspired replacement (Xie et al. 2016). 
                 * Replace a word with __fixed__ drop rate
                 * Replace a word with __adaptive__ drop rate, by how rare two words appear together (i.e. "Humpty Dumpty"), and replace by a unigram draw over vocab
                 * Replace a word with __adaptive__ drop rate, and draw word from a __proposal distribution__ (i.e. "New York") 
        3. __Better Model__ (+ all above)

2. **Language Models:**{: style="color: SteelBlue"}{: .bodyContents8 #bodyContents82}  
    :   * __The ML-Estimate of $$p(w_i \vert w_{i-1})$$__ $$ = \dfrac{c(w_{i-1}\: w_i)}{\sum_{w_i} c(w_{i-1}\: w_i)}$$
            

3. **Neural Text Generation:**{: style="color: SteelBlue"}{: .bodyContents8 #bodyContents83}
    * __Traditionally__:  
        * Often Auto-regressive language models (ie. seq2seq)
        * These models generate text by sampling words sequentially, with each word conditioned on the previous word
        * Benchmarked on validation perplexity even though this is not a direct measure of the quality of the generated text 
        * The models are typically trained via __maximum likelihood__ and __teacher forcing__  
            > These methods are well-suited to optimizing perplexity but can result in poor sample quality since generating text requires conditioning on sequences of words that may have never been observed at training time  

<!-- 4. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents8 #bodyContents84}  
    :   

5. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents8 #bodyContents85}  
    :   

6. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents8 #bodyContents86}  
    :   

7. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents8 #bodyContents87}  
    :   

8. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents8 #bodyContents88}  
    :   --> 

***

## Physics
{: #content9}

1. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents9 #bodyContents91}  
    :   

<!-- 2. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents9 #bodyContents92}  
    :   

3. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents9 #bodyContents93}  
    :   

4. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents9 #bodyContents94}  
    :   

5. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents9 #bodyContents95}  
    :   

6. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents9 #bodyContents96}  
    :   

7. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents9 #bodyContents97}  
    :   

8. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents9 #bodyContents98}  
    :    -->

***

## Game Theory
{: #content10}

1. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents10 #bodyContents101}  
    :   

<!-- 2. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents10 #bodyContents102}  
    :   

3. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents10 #bodyContents103}  
    :   

4. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents10 #bodyContents104}  
    :   

5. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents10 #bodyContents105}  
    :   

6. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents10 #bodyContents106}  
    :   

7. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents10 #bodyContents107}  
    :   

8. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents10 #bodyContents108}  
    :    -->

***

## Misc.
{: #content11}

1. **Philosophy:**{: style="color: SteelBlue"}{: .bodyContents11 #bodyContents111}  
    :   __Occam's Razor:__  Suppose there exist two explanations for an occurrence. In this case the one that requires the least speculation is usually better.  
        Another way of saying it is that the more assumptions you have to make, the more unlikely an explanation.  

<!-- 2. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents11 #bodyContents112}  
    :   

3. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents11 #bodyContents113}  
    :   

4. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents11 #bodyContents114}  
    :   

5. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents11 #bodyContents115}  
    :   

6. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents11 #bodyContents116}  
    :   

7. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents11 #bodyContents117}  
    :   

8. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents11 #bodyContents118}  
    :    -->