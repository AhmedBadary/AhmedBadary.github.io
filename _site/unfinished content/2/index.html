<!DOCTYPE html>
<html lang="en-us">
  <head>
  <meta charset="UTF-8">
  <title>Ahmad Badary</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <link rel="stylesheet" href="/css/normalize.css">
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="/css/cayman.css">
  <link rel="stylesheet" href="/css/style.css">
  <link rel="shortcut icon" href="/main_files/favicon.ico" />
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="/css/customStyle.css">
  <title> » Ahmad Badary</title>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
</head>

  <body>
    <nav class="main-nav">
    <a href="https://ahmedbadary.github.io/" class="main-nav-logo">
        <img src="/main_files/logo.png">
    </a>
    <ul id="menu-main" class="main-nav-items">
        <li id="menu-item-1859" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-home menu-item-1859">
            <a href="/">Home</a>
        </li>
        <li id="menu-item-2869" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-2869">
            <a href="/work">Work</a>
        </li>
        <li id="menu-item-1892" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1892">
            <a href="/projects">Projects</a>
        </li>
        <li id="menu-item-1858" class="menu-item menu-item-type-post_type menu-item-object-page current_page_parent menu-item-1858">
            <a href="/blog">Blog</a>
        </li>
        <li id="menu-item-1862" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1862">
            <a href="/about">About</a>
        </li>
    </ul>
</nav>


<section class="page-header">
  <h1 class="project-name"></h1>
  <h2 class="project-tagline"></h2>
  <a href="https://github.com/AhmedBadary/" class="btn">GitHub</a>
  <a href="https://www.linkedin.com/in/ahmad-badary-656098121/" class="btn">LinkedIn</a>
  <a href=https://www.facebook.com/ahmed.thabet.94 class="btn">Facebook</a>
</section>

<!-- <div>
  <ul class="posts">
    
      <li><span>02 Jan 2014</span> &raquo; <a href="/2014/01/02/introducing-Ahmad/">Introducing Ahmad</a></li>
    
  </ul>
</div> -->


    <section class="main-content">
      
      <!-- <div class="page">
  <h1 class="page-title"></h1>
  <p>$$(.|\n)*?$$
<strong style="color: red">Fundamental Theorem of Statistical Learning (binary classification):</strong><br />
Let \(\mathcal{H}\) be a hypothesis class of functions from a domain \(X\) to \(\{0,1\}\) and let the loss function be the \(0-1\) loss.<br />
The following are equivalent:<br />
\(\begin{array}{l}{\text { 1. } \mathcal{H} \text { has uniform convergence. }} \\ {\text { 2. The ERM is a PAC learning algorithm for } \mathcal{H} \text { . }} \\ {\text { 3. } \mathcal{H} \text { is } PAC \text { learnable. }} \\ {\text { 4. } \mathcal{H} \text { has finite } VC \text { dimension. }}\end{array}\)<br />
This can be extended to <strong>regression</strong> and <strong>multiclass classification</strong>.</p>

<p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Proof.</button></p>
<div hidden="">
  <ul>
    <li>1 \(\Rightarrow 2\) We have seen uniform convergence implies that \(\mathrm{ERM}\) is \(\mathrm{PAC}\) learnable</li>
    <li>2 \(\Rightarrow 3\) Obvious.</li>
    <li>3 \(\Rightarrow 4\) We just proved that PAC learnability implies finite \(\mathrm{VC}\) dimension.</li>
    <li>4 \(\Rightarrow 1\) We proved that finite \(\mathrm{VC}\) dimension implies uniform convergence.</li>
  </ul>
</div>

<p><strong style="color: red">Notes:</strong></p>
<ul>
  <li>VC dimension fully determines <span style="color: goldenrod">learnability</span> for binary classification.</li>
  <li>The VC dimension doesn’t just determine <strong>learnability</strong>, it also gives a <span style="color: goldenrod">bound on the sample complexity</span> (which can be shown to be <strong style="color: goldenrod">tight</strong>).</li>
  <li><a href="https://www.cs.toronto.edu/~jlucas/teaching/csc411/lectures/lec23_24_handout.pdf">Lecture Slides (ref)</a></li>
  <li><button class="showText" value="show" onclick="showTextPopHide(event);">Extra Notes (what you should know)</button>
  <img src="https://cdn.mathpix.com/snip/images/9bYxbit2n1mvyrttH3CzgI-2CgdrrDNVkejd1fP5-AU.original.fullsize.png" alt="img" width="100%" hidden="" /></li>
</ul>

<p><strong style="color: red">Theoretical Concepts:</strong></p>
<ul>
  <li><strong>Kolmogorov Complexity</strong>:<br />
  In <strong>Algorithmic Information Theory</strong>, the <strong>Kolmogorov Complexity</strong> of an object, such as a piece of text, is the <span style="color: purple"><strong>length</strong> of the <em><strong>shortest</strong></em> <strong>computer program</strong></span> (in a predetermined programming language) that <span style="color: purple"><em>produces</em> the object as <strong>output</strong></span>.<br />
  It is a measure of the <strong>computational resources</strong> needed to <em><strong>specify</strong></em> the object.<br />
  It is also known as <strong>algorithmic complexity</strong>, <strong>Solomonoff–Kolmogorov complexity</strong>, <strong>program-size complexity</strong>, <strong>descriptive complexity</strong>, or <strong>algorithmic entropy</strong>.</li>
  <li><strong>Rademacher Complexity</strong>:</li>
  <li><strong>Generalization Bounds</strong>:</li>
  <li><strong>Sample Complexity</strong>:</li>
  <li><strong>PAC-Bayes Bound</strong>:</li>
  <li><strong>Kolmogorov Randomness</strong>:</li>
  <li><strong>Minimum Description Length (MDL)</strong>: <br />
  The <a href="https://en.wikipedia.org/wiki/Minimum_description_length"><strong>minimum description length (MDL) principle</strong></a> is a formalization of <strong>Occam’s razor</strong> in which the best hypothesis (a model and its parameters) for a given set of data is the one that leads to the <span style="color: purple">best compression of the data</span>.</li>
  <li><strong>Minimum Message Length (MML)</strong>:<br />
  <a href="https://en.wikipedia.org/wiki/Minimum_message_length"><strong>MML</strong></a> is a formal Information Theory restatement of <strong>Occam’s Razor</strong>: even when models are equal in goodness of fit accuracy to the observed data, the one generating the <span style="color: purple">shortest overall message</span> is more likely to be correct (where the message consists of a statement of the model, followed by a statement of data encoded concisely using that model).</li>
  <li><a href="https://web.stanford.edu/~hastie/Papers/ESLII.pdf#page=37">Statistical Decision Theory (ESL!)</a></li>
  <li><strong>PAC Learnability</strong>:<br />
  A hypothesis class \(\mathcal{H}\) is PAC learnable, if there exists a learning algorithm A, satisfying that for any \(\epsilon&gt;0\) and \(\delta \in(0,1)\) there exist \(\mathfrak{M}(\epsilon, \delta)=\) poly \(\left(\frac{1}{\epsilon}, \frac{1}{\delta}\right)\) such that for i.i.d samples \(S^{m}=\left\{\left(x_{i}, y_{i}\right)\right\}_ {i=1}^{m}\) drawn from any distribution \(\mathcal{D}\) and \(m \geq \mathfrak{M}(\epsilon, \delta)\) the algorithm returns a hypothesis \(A\left(S^{m}\right) \in \mathcal{H}\) satisfying
    <p>$$P_{S^{m} \sim \mathcal{D}^{m}}\left(L_{\mathcal{D}}(A(S))&gt;\min _{h \in \mathcal{H}} L_{\mathcal{D}}(h)+\epsilon\right)&lt;\delta$$</p>
    <p>To show that empirical risk minimization (ERM) is a PAC learning algorithm, we need to show that \(L_{S}(h) \approx L_{\mathcal{D}}(h)\) for all \(h\).</p>
  </li>
  <li>
    <p><strong>Uniform Convergence</strong>:<br />
  A hypothesis class \(\mathcal{H}\) has the uniform convergence property, if for any \(\epsilon&gt;0\) and \(\delta \in(0,1)\) there exist \(\mathfrak{M}(\epsilon, \delta)=\) \(\text{poly}\left(\frac{1}{\epsilon}, \frac{1}{\delta}\right)\) such that for any distribution \(\mathcal{D}\) and \(m \geq \mathfrak{M}(\epsilon, \delta)\) i.i.d samples \(S^{m}=\left\{\left(x_{i}, y_{i}\right)\right\}_ {i=1}^{m} \sim \mathcal{D}^{m}\) with probability at least \(1-\delta\), \(\left\vert L_{S}^{m}(h)-L_{\mathcal{D}}(h)\right\vert &lt;\epsilon\) for all \(h \in \mathcal{H}\).</p>

    <ul>
      <li>For a single \(h,\) law of large numbers says \(L_{S}^{m}(h) \stackrel{m \rightarrow \infty}{\rightarrow} L_{\mathcal{D}}(h)\)</li>
      <li>For loss bounded by 1 the Hoeffding inequality states:
        <p>$$P\left(\left\vert L_{S}^{m}(h)-L_{\mathcal{D}}(h)\right\vert &gt;\epsilon\right) \leq 2 e^{-2 \epsilon^{2} m}$$</p>
      </li>
      <li>The difficulty is to bound all the \(h \in \mathcal{H}\) <span style="color: purple">uniformly</span>.</li>
    </ul>
  </li>
  <li><strong>Complexity in ML</strong>:
    <ul>
      <li><strong>Definitions of the complexity of an object (\(h\))</strong>:
        <ul>
          <li><strong>Minimum Description Length (MDL)</strong>: the number of bits for specifying an object.</li>
          <li><strong>Order of a Polynomial</strong></li>
        </ul>
      </li>
      <li><strong>Definitions of the complexity of a class of objects (\(\mathcal{H}\))</strong>:
        <ul>
          <li><strong>Entropy</strong></li>
          <li><strong>VC-dim</strong></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="statistical-learning-theory">Statistical Learning Theory</h2>

<ol>
  <li><strong style="color: SteelBlue">Statistical Learning Theory:</strong>
 <strong>Statistical Learning Theory</strong> is a framework for machine learning drawing from the fields of statistics and functional analysis. Under certain assumptions, this framework allows us to study the question:
    <blockquote>
      <p><strong style="color: blue">How can we affect performance on the test set when we can only observe the training set?</strong></p>
    </blockquote>

    <p>It is a <em>statistical</em> approach to <strong>Computational Learning Theory</strong>.</p>
  </li>
  <li><strong style="color: SteelBlue">Formal Definition:</strong>
 Let:
    <ul>
      <li>\(X\): the vector space of all possible <strong>inputs</strong></li>
      <li>\(Y\): the vector space of all possible <strong>outputs</strong></li>
      <li>\(Z = X \times Y\): the <strong>product space</strong> of (input,output) pairs</li>
      <li>\(n\): the number of <strong>samples</strong> in the <strong>training set</strong></li>
      <li>\(S=\left\{\left(\vec{x}_{1}, y_{1}\right), \ldots,\left(\vec{x}_{n}, y_{n}\right)\right\}=\left\{\vec{z}_{1}, \ldots, \vec{z}_{n}\right\}\): the <strong>training set</strong></li>
      <li>\(\mathcal{H} = f : X \rightarrow Y\): the <strong>hypothesis space</strong> of all functions</li>
      <li>\(V(f(\vec{x}), y)\): an <strong>error/loss function</strong></li>
    </ul>

    <p><strong>Assumptions:</strong></p>
    <ul>
      <li>The training and test data are generated by an <em><strong>unknown, joint</strong></em> <strong>probability distribution over datasets</strong> (over the product space \(Z\), denoted: \(p_{\text{data}}(z)=p(\vec{x}, y)\)) called the <strong>data-generating process</strong>.
        <ul>
          <li>\(p_{\text{data}}\) is a <strong>joint distribution</strong> so that it allows us to model <em>uncertainty in predictions</em> (e.g. from noise in data) because \(y\) is not a deterministic function of \(\vec{x}\), but rather a <em>random variable</em> with <strong>conditional distribution</strong> \(p(y \vert \vec{x})\) for a fixed \(\vec{x}\).</li>
        </ul>
      </li>
      <li>The <strong>i.i.d. assumptions:</strong>
        <ul>
          <li>The examples in each dataset are <strong>independent</strong> from each other</li>
          <li>The <em>training set</em> and <em>test set</em> are <strong>identically distributed</strong> (drawn from the same probability distribution as each other)</li>
        </ul>

        <blockquote>
          <p>A collection of random variables is <strong>independent and identically distributed</strong> if each random variable has the same probability distribution as the others and all are mutually independent.<br />
<em>Informally,</em> it says that all the variables provide the same kind of information independently of each other.</p>
          <ul>
            <li><a href="https://stats.stackexchange.com/questions/213464/on-the-importance-of-the-i-i-d-assumption-in-statistical-learning/214220">Discussion on the importance if i.i.d assumptions</a></li>
          </ul>
        </blockquote>
      </li>
    </ul>

    <p><strong style="color: red">The Inference Problem</strong>  <br />
 Finding a function \(f : X \rightarrow Y\) such that \(f(\vec{x}) \sim y\).</p>

    <p><strong>The Expected Risk:</strong></p>
    <p>$$I[f]=\mathbf{E}[V(f(\vec{x}), y)]=\int_{X \times Y} V(f(\vec{x}), y) p(\vec{x}, y) d \vec{x} d y$$</p>

    <p><strong>The Target Function:</strong><br />
 is the best possible function \(f\) that can be chosen, is given by:</p>
    <p>$$f=\inf_{h \in \mathcal{H}} I[h]$$</p>

    <p><strong>The Empirical Risk:</strong><br />
 Is a <em><strong>proxy measure</strong></em> for the <strong>expected risk</strong>, based on the training set.<br />
 It is necessary because the probability distribution \(p(\vec{x}, y)\) is <em>unknown</em>.</p>
    <p>$$I_{S}[f]=\frac{1}{n} \sum_{i=1}^{n} V\left(f\left(\vec{x}_{i}\right), y_{i}\right)$$</p>
  </li>
  <li>
    <p><strong style="color: SteelBlue">Empirical risk minimization:</strong>
 <strong>Empirical Risk Minimization (ERM)</strong> is a principle in <em>statistical learning theory</em> that is based on approximating the <strong>Generalization Error (True Risk)</strong> by measuring the <strong>Training Error (Empirical Risk)</strong>, i.e. the performance on training data.</p>

    <p>A <em>learning algorithm</em> that chooses the function \(f_{S}\) that minimizes the <em>empirical risk</em> is called <strong>empirical risk minimization</strong>:</p>
    <p>$$R_{\mathrm{emp}}(h) = I_{S}[f]=\frac{1}{n} \sum_{i=1}^{n} V\left(f\left(\vec{x}_{i}\right), y_{i}\right)$$</p>
    <p>$$f_{S} = \hat{h} = \arg \min _{h \in \mathcal{H}} R_{\mathrm{emp}}(h)$$</p>

    <p><strong style="color: red">Complexity:</strong><br />
 <button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Show</button></p>
    <ul hidden="">
      <li>Empirical risk minimization for a classification problem with a <em>0-1 loss function</em> is known to be an <strong>NP-hard</strong> problem even for such a relatively simple class of functions as linear classifiers.
        <ul>
          <li><a href="https://arxiv.org/abs/1012.0729">Paper Proof</a></li>
        </ul>
      </li>
      <li>Though, it can be solved efficiently when the minimal empirical risk is zero, i.e. data is linearly separable.</li>
      <li><strong>Coping with Hardness:</strong>
        <ul>
          <li>Employing a <strong>convex approximation</strong> to the 0-1 loss: <em>Hinge Loss</em>, <em>SVM</em></li>
          <li>Imposing <strong>Assumptions on the data-generating distribution</strong> and thus, stop being an <strong>agnostic learning algorithm</strong>.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue">Definitions:</strong>
    <ul>
      <li><strong style="color: red">Generalization Error:</strong><br />
  AKA: <strong>Expected Risk/Error</strong>, <strong>Out-of-Sample Error</strong>[^2], <strong>\(E_{\text{out}}\)</strong> <br />
  It is a measure of how accurately an algorithm is able to predict outcome values for previously unseen data.</li>
      <li>
        <p><strong style="color: red">Generalization Gap:</strong><br />
  It is a measure of how accurately an algorithm is able to predict outcome values for previously unseen data.</p>

        <p><strong>Formally:</strong><br />
  The generalization gap is the <strong>difference between the expected and empirical error</strong>:</p>
        <p>$$G =I\left[f_{n}\right]-I_{S}\left[f_{n}\right]$$</p>

        <p>An Algorithm is said to <strong>Generalize</strong> (achieve <strong>Generalization</strong>) if:</p>
        <p>$$\lim _{n \rightarrow \infty} G_n = \lim _{n \rightarrow \infty} I\left[f_{n}\right]-I_{S}\left[f_{n}\right]=0$$</p>
        <p>Equivalently:</p>
        <p>$$E_{\text { out }}(g) \approx E_{\text { in }}(g)$$</p>
        <p>or</p>
        <p>$$I\left[f_{n}\right] \approx I_{S}\left[f_{n}\right]$$</p>

        <p><strong>Computing the Generalization Gap:</strong><br />
  Since \(I\left[f_{n}\right]\) cannot be computed for an unknown distribution, the generalization gap <strong>cannot be computed</strong> either.<br />
  Instead the goal of <strong>statistical learning theory</strong> is to <em>bound</em> or <em>characterize</em> the generalization gap in probability:</p>
        <p>$$P_{G}=P\left(I\left[f_{n}\right]-I_{S}\left[f_{n}\right] \leq \epsilon\right) \geq 1-\delta_{n}$$</p>
        <p>That is, the goal is to characterize the probability \({\displaystyle 1-\delta _{n}}\) that the generalization gap is less than some error bound \({\displaystyle \epsilon }\) (known as the <strong>learning rate</strong> and generally dependent on \({\displaystyle \delta }\) and \({\displaystyle n}\)).</p>
      </li>
      <li><strong style="color: red">The Empirical Distribution:</strong><br />
  <em>AKA <strong>Data-Generating Distribution</strong></em><br />
  is the <strong>discrete</strong> uniform distribution over the <em>sample points</em>.</li>
      <li><strong style="color: red">The Approximation-Generalization Tradeoff:</strong>
        <ul>
          <li><strong>Goal</strong>:<br />
  Small \(E_{\text{out}}\): Good approximation of \(f\) <em><strong>out of sample</strong></em> (not in-sample).</li>
          <li>The tradeoff is characterized by the <strong>complexity</strong> of the <strong>hypothesis space \(\mathcal{H}\)</strong>:
            <ul>
              <li><strong>More Complex \(\mathcal{H}\)</strong>: Better chance of approximating \(f\)</li>
              <li><strong>Less Complex \(\mathcal{H}\)</strong>: Better chance of generalizing out-of-sample</li>
            </ul>
          </li>
          <li><a href="https://www.youtube.com/embed/zrEyxfl2-a8?start=358" value="show" onclick="iframePopA(event)"><strong>Abu-Mostafa</strong></a>
  <a href="https://www.youtube.com/embed/zrEyxfl2-a8?start=358"></a>
            <div></div>
          </li>
          <li><a href="https://mdav.ece.gatech.edu/ece-6254-spring2017/notes/13-bias-variance-marked.pdf">Lecture-Slides on Approximation-Generalization</a></li>
        </ul>
      </li>
      <li><strong style="color: red">Excess Risk (Generalization-Gap) Decomposition | Estimation-Approximation Tradeoff:</strong><br />
  <strong>Excess Risk</strong> is defined as the difference between the expected-risk/generalization-error of any function \(\hat{f} = g^{\mathcal{D}}\) that we learn from the data (exactly just bias-variance), and the expected-risk of the <strong>target function</strong> \(f\) (known as the <strong>bayes optimal predictor</strong>)
        <ul>
          <li><a href="https://www.youtube.com/embed/YA_CE9jat4I" value="show" onclick="iframePopA(event)"><strong>Excess Risk Decomposition Video</strong></a>
  <a href="https://www.youtube.com/embed/YA_CE9jat4I"></a>
            <div></div>
          </li>
          <li><a href="https://www.ics.uci.edu/~smyth/courses/cs274/readings/xing_singh_CMU_bias_variance.pdf">Excess Risk &amp; Bias-Variance Lecture Slides</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue">Notes:</strong>
    <ul>
      <li><strong>Choices of Loss Functions</strong>:
        <ul>
          <li><strong>Regression</strong>:
            <ul>
              <li><strong>MSE</strong>: \(\: V(f(\vec{x}), y)=(y-f(\vec{x}))^{2}\)</li>
              <li><strong>MAE</strong>: \(\: V(f(\vec{x}), y)=\vert{y-f(\vec{x})}\vert\)</li>
            </ul>
          </li>
          <li><strong>Classification</strong>:
            <ul>
              <li><strong>Binary</strong>: \(\: V(f(\vec{x}), y)=\theta(-y f(\vec{x}))\)<br />
  where \(\theta\) is the <em>Heaviside Step Function</em>.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li><strong>Training Data, Errors, and Risk</strong>:
        <ul>
          <li><strong>Training-Error</strong> is the <strong>Empirical Risk</strong>
            <ul>
              <li>It is a <strong>proxy</strong> for the <strong>Generalization Error/Expected Risk</strong></li>
              <li>This is what we minimize</li>
            </ul>
          </li>
          <li><strong>Test-Error</strong> is an <em><strong>approximation</strong></em> to the <strong>Generalization Error/Expected Risk</strong>
            <ul>
              <li>This is what we (can) compute to ensure that minimizing Training-Err/Empirical-Risk (ERM) also minimized the Generalization-Err/Expected-Risk (which we can’t compute directly)</li>
            </ul>
          </li>
        </ul>
      </li>
      <li><strong>Why the goal is NOT to minimize \(E_{\text{in}}\) completely (intuition)</strong>:<br />
  Basically, if you have noise in the data; then fitting the (finite) training-data completely; i.e. minimizing the in-sample-err completely will underestimate the out-of-sample-err.<br />
  Since, if noise existed AND you fit training-data completely \(E_{\text{in}} = 0\) THEN you inherently have fitted the noise AND your performance on out-sample will be lower.</li>
    </ul>
  </li>
</ol>

<hr />

<h2 id="the-vapnik-chervonenkis-vc-theory">The Vapnik-Chervonenkis (VC) Theory</h2>

<hr />

<h2 id="the-bias-variance-decomposition-theory">The Bias-Variance Decomposition Theory</h2>

<ol>
  <li>
    <p><strong style="color: SteelBlue">The Bias-Variance Decomposition Theory:</strong>
<strong>The Bias-Variance Decomposition Theory</strong> is a way to quantify the <strong>Approximation-Generalization Tradeoff</strong>.</p>

    <p><strong>Assumptions:</strong></p>
    <ul>
      <li>The analysis is done over the <strong>entire data-distribution</strong></li>
      <li>The target function \(f\) is already <strong>known</strong>; and you’re trying to answer the question:<br />
  “How can \(\mathcal{H}\) approximate \(f\) over all? not just on your sample.”</li>
      <li>Applies to <strong>real-valued targets</strong> (can be extended)</li>
      <li>Use <strong>Square Error</strong> (can be extended)</li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue">The Bias-Variance Decomposition:</strong>
 The <strong>Bias-Variance Decomposition</strong> is a way of analyzing a learning algorithm’s <em><strong>expected out-of-sample error</strong></em>[^1] as a <em>sum of three terms:</em>
    <ul>
      <li><strong>Bias:</strong> is an error from erroneous assumptions in the learning algorithm.</li>
      <li><strong>Variance:</strong> is an error from sensitivity to small fluctuations in the training set.</li>
      <li><strong>Irreducible Error</strong> (resulting from noise in the problem itself)</li>
    </ul>

    <p>Equivalently, <strong>Bias</strong> and <strong>Variance</strong> measure <em>two different sources of errors in an estimator:</em></p>
    <ul>
      <li><strong>Bias:</strong> measures the expected deviation from the true value of the function or parameter.
        <blockquote>
          <p>AKA: <strong>Approximation Error</strong>[^3]  (statistics)  How well can \(\mathcal{H}\) approximate the target function ‘\(f\)’</p>
        </blockquote>
      </li>
      <li><strong>Variance:</strong> measures the deviation from the expected estimator value that any particular sampling of the data is likely to cause.
        <blockquote>
          <p>AKA: <strong>Estimation (Generalization) Error</strong> (statistics) How well we can zoom in on a good \(h \in \mathcal{H}\)</p>
        </blockquote>
      </li>
    </ul>

    <p><strong>Bias-Variance Decomposition Formula:</strong><br />
 For any function \(\hat{f} = g^{\mathcal{D}}\) we select, we can decompose its <em><strong>expected (out-of-sample) error</strong></em> on an <em>unseen sample \(x\)</em> as:</p>
    <p>$$\mathrm{E}\left[(y-\hat{f}(x))^{2}\right]=(\operatorname{Bias}[\hat{f}(x)])^{2}+\operatorname{Var}[\hat{f}(x)]+\sigma^{2}$$</p>
    <p>Where:</p>
    <ul>
      <li><strong>Bias</strong>:
        <p>$$\operatorname{Bias}[\hat{f}(x)]=\mathrm{E}[\hat{f}(x)]-f(x)$$</p>
      </li>
      <li><strong>Variance</strong>:
        <p>$$\operatorname{Var}[\hat{f}(x)]=\mathrm{E}\left[\hat{f}(x)^{2}\right]-\mathrm{E}[\hat{f}(x)]^{2}$$</p>
        <p>and the expectation ranges over different realizations of the training set \(\mathcal{D}\).</p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong style="color: SteelBlue">The Bias-Variance Tradeoff:</strong>
 is the property of a set of predictive models whereby, models with a <em>lower bias</em> (in parameter estimation) have a <em>higher variance</em> (of the parameter estimates across samples) and vice-versa.</p>

    <p><strong style="color: black">Effects of Bias:</strong></p>
    <ul>
      <li><strong>High Bias</strong>: simple models, lead to <em style="color: red"><strong>underfitting</strong></em>.</li>
      <li><strong>Low Bias</strong>: complex models, lead to <em style="color: red"><strong>overfitting</strong></em>.</li>
    </ul>

    <p><strong style="color: black">Effects of Variance:</strong></p>
    <ul>
      <li><strong>High Variance</strong>: complex models, lead to <em style="color: red"><strong>overfitting</strong></em>.</li>
      <li><strong>Low Variance</strong>: simple models, lead to <em style="color: red"><strong>underfitting</strong></em>.</li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue">Derivation:</strong>
    <p>$${\displaystyle {\begin{aligned}\mathbb{E}_{\mathcal{D}} {\big [}I[g^{(\mathcal{D})}]{\big ]}&amp;=\mathbb{E}_{\mathcal{D}} {\big [}\mathbb{E}_{x}{\big [}(g^{(\mathcal{D})}-y)^{2}{\big ]}{\big ]}\\
 &amp;=\mathbb{E}_{x} {\big [}\mathbb{E}_{\mathcal{D}}{\big [}(g^{(\mathcal{D})}-y)^{2}{\big ]}{\big ]}\\
 &amp;=\mathbb{E}_{x}{\big [}\mathbb{E}_{\mathcal{D}}{\big [}(g^{(\mathcal{D})}- f -\varepsilon)^{2}{\big ]}{\big ]}
 \\&amp;=\mathbb{E}_{x}{\big [}\mathbb{E}_{\mathcal{D}} {\big [}(f+\varepsilon -g^{(\mathcal{D})}+\bar{g}-\bar{g})^{2}{\big ]}{\big ]}\\
 &amp;=\mathbb{E}_{x}{\big [}\mathbb{E}_{\mathcal{D}} {\big [}(\bar{g}-f)^{2}{\big ]}+\mathbb{E}_{\mathcal{D}} [\varepsilon ^{2}]+\mathbb{E}_{\mathcal{D}} {\big [}(g^{(\mathcal{D})} - \bar{g})^{2}{\big ]}+2\: \mathbb{E}_{\mathcal{D}} {\big [}(\bar{g}-f)\varepsilon {\big ]}+2\: \mathbb{E}_{\mathcal{D}} {\big [}\varepsilon (g^{(\mathcal{D})}-\bar{g}){\big ]}+2\: \mathbb{E}_{\mathcal{D}} {\big [}(g^{(\mathcal{D})} - \bar{g})(\bar{g}-f){\big ]}{\big ]}\\
 &amp;=\mathbb{E}_{x}{\big [}(\bar{g}-f)^{2}+\mathbb{E}_{\mathcal{D}} [\varepsilon ^{2}]+\mathbb{E}_{\mathcal{D}} {\big [}(g^{(\mathcal{D})} - \bar{g})^{2}{\big ]}+2(\bar{g}-f)\mathbb{E}_{\mathcal{D}} [\varepsilon ]\: +2\: \mathbb{E}_{\mathcal{D}} [\varepsilon ]\: \mathbb{E}_{\mathcal{D}} {\big [}g^{(\mathcal{D})}-\bar{g}{\big ]}+2\: \mathbb{E}_{\mathcal{D}} {\big [}g^{(\mathcal{D})}-\bar{g}{\big ]}(\bar{g}-f){\big ]}\\
 &amp;=\mathbb{E}_{x}{\big [}(\bar{g}-f)^{2}+\mathbb{E}_{\mathcal{D}} [\varepsilon ^{2}]+\mathbb{E}_{\mathcal{D}} {\big [}(g^{(\mathcal{D})} - \bar{g})^{2}{\big ]}{\big ]}\\
 &amp;=\mathbb{E}_{x}{\big [}(\bar{g}-f)^{2}+\operatorname {Var} [y]+\operatorname {Var} {\big [}g^{(\mathcal{D})}{\big ]}{\big ]}\\
 &amp;=\mathbb{E}_{x}{\big [}\operatorname {Bias} [g^{(\mathcal{D})}]^{2}+\operatorname {Var} [y]+\operatorname {Var} {\big [}g^{(\mathcal{D})}{\big ]}{\big ]}\\
 &amp;=\mathbb{E}_{x}{\big [}\operatorname {Bias} [g^{(\mathcal{D})}]^{2}+\sigma ^{2}+\operatorname {Var} {\big [}g^{(\mathcal{D})}{\big ]}{\big ]}\\
\end{aligned}}}$$</p>
    <p>where:<br />
 \(\overline{g}(\mathbf{x})=\mathbb{E}_{\mathcal{D}}\left[g^{(\mathcal{D})}(\mathbf{x})\right]\) is the <strong>average hypothesis</strong> over all realization of \(N\) data-points \(\mathcal{D}_ i\), and \({\displaystyle \varepsilon }\) and \({\displaystyle {\hat {f}}} = g^{(\mathcal{D})}\) are <strong>independent</strong>.</p>

    <p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Derivation with Wikipedia Notation</button></p>
    <p hidden="">$${\displaystyle {\begin{aligned}\operatorname {E}_ {\mathcal{D}} {\big [}(y-{\hat {f}})^{2}{\big ]}&amp;=\operatorname {E} {\big [}(f+\varepsilon -{\hat {f}})^{2}{\big ]}\\&amp;=\operatorname {E} {\big [}(f+\varepsilon -{\hat {f}}+\operatorname {E} [{\hat {f}}]-\operatorname {E} [{\hat {f}}])^{2}{\big ]}\\&amp;=\operatorname {E} {\big [}(f-\operatorname {E} [{\hat {f}}])^{2}{\big ]}+\operatorname {E} [\varepsilon ^{2}]+\operatorname {E} {\big [}(\operatorname {E} [{\hat {f}}]-{\hat {f}})^{2}{\big ]}+2\operatorname {E} {\big [}(f-\operatorname {E} [{\hat {f}}])\varepsilon {\big ]}+2\operatorname {E} {\big [}\varepsilon (\operatorname {E} [{\hat {f}}]-{\hat {f}}){\big ]}+2\operatorname {E} {\big [}(\operatorname {E} [{\hat {f}}]-{\hat {f}})(f-\operatorname {E} [{\hat {f}}]){\big ]}\\&amp;=(f-\operatorname {E} [{\hat {f}}])^{2}+\operatorname {E} [\varepsilon ^{2}]+\operatorname {E} {\big [}(\operatorname {E} [{\hat {f}}]-{\hat {f}})^{2}{\big ]}+2(f-\operatorname {E} [{\hat {f}}])\operatorname {E} [\varepsilon ]+2\operatorname {E} [\varepsilon ]\operatorname {E} {\big [}\operatorname {E} [{\hat {f}}]-{\hat {f}}{\big ]}+2\operatorname {E} {\big [}\operatorname {E} [{\hat {f}}]-{\hat {f}}{\big ]}(f-\operatorname {E} [{\hat {f}}])\\&amp;=(f-\operatorname {E} [{\hat {f}}])^{2}+\operatorname {E} [\varepsilon ^{2}]+\operatorname {E} {\big [}(\operatorname {E} [{\hat {f}}]-{\hat {f}})^{2}{\big ]}\\&amp;=(f-\operatorname {E} [{\hat {f}}])^{2}+\operatorname {Var} [y]+\operatorname {Var} {\big [}{\hat {f}}{\big ]}\\&amp;=\operatorname {Bias} [{\hat {f}}]^{2}+\operatorname {Var} [y]+\operatorname {Var} {\big [}{\hat {f}}{\big ]}\\&amp;=\operatorname {Bias} [{\hat {f}}]^{2}+\sigma ^{2}+\operatorname {Var} {\big [}{\hat {f}}{\big ]}\\\end{aligned}}}$$</p>
  </li>
  <li>
    <p><strong style="color: SteelBlue">Results and Takeaways of the Decomposition:</strong>
 Match the <strong>“Model Complexity”</strong> to the <em><strong>Data Resources</strong></em>, NOT to the <em>Target Complexity</em>.<br />
 <img src="/main_files/dl/theory/stat_lern_thry/1.png" alt="img" width="80%" /></p>

    <p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Analogy to the Approximation-Generalization Tradeoff</button>
 <strong style="color: red">The data resources you have is, “what do you have in order to navigate the hypothesis set?”. Let’s pick a hypothesis set that we can afford to navigate. That is the game in learning. Done with the bias and variance.</strong>&lt;/p&gt;</p>
  </li>
  <li><strong style="color: SteelBlue">Measuring the Bias and Variance:</strong>
    <ul>
      <li><strong>Training Error</strong>: reflects Bias, NOT variance</li>
      <li><strong>Test Error</strong>: reflects Both</li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue">Reducing the Bias and Variance, and Irreducible Err:</strong>
    <ul>
      <li><strong>Adding Good Feature</strong>:
        <ul>
          <li>Decrease Bias</li>
        </ul>
      </li>
      <li><strong>Adding Bad Feature</strong>:
        <ul>
          <li>Doesn’t affect (increase) Bias much</li>
        </ul>
      </li>
      <li><strong>Adding ANY Feature</strong>:
        <ul>
          <li>Increases Variance</li>
        </ul>
      </li>
      <li><strong>Adding more Data</strong>:
        <ul>
          <li>Decreases Variance</li>
          <li>(May) Decreases Bias: if \(h\) can fit \(f\) exactly.</li>
        </ul>
      </li>
      <li><strong>Noise in Test Set</strong>:
        <ul>
          <li>Affects ONLY Irreducible Err</li>
        </ul>
      </li>
      <li><strong>Noise in Training Set</strong>:
        <ul>
          <li>Affects BOTH and ONLY Bias and Variance</li>
        </ul>
      </li>
      <li><strong>Dimensionality Reduction</strong>:
        <ul>
          <li>Decrease Variance (by simplifying models)</li>
        </ul>
      </li>
      <li><strong>Feature Selection</strong>:
        <ul>
          <li>Decrease Variance (by simplifying models)</li>
        </ul>
      </li>
      <li><strong>Regularization</strong>:
        <ul>
          <li>Increase Bias</li>
          <li>Decrease Variance</li>
        </ul>
      </li>
      <li><strong>Increasing # of Hidden Units in ANNs</strong>:
        <ul>
          <li>Decrease Bias</li>
          <li>Increase Variance</li>
        </ul>
      </li>
      <li><strong>Increasing # of Hidden Layers in ANNs</strong>:
        <ul>
          <li>Decrease Bias</li>
          <li>Increase Variance</li>
        </ul>
      </li>
      <li><strong>Increasing \(k\) in K-NN</strong>:
        <ul>
          <li>Increase Bias</li>
          <li>Decrease Variance</li>
        </ul>
      </li>
      <li><strong>Increasing Depth in Decision-Trees</strong>:
        <ul>
          <li>Increase Variance</li>
        </ul>
      </li>
      <li><strong>Boosting</strong>:
        <ul>
          <li>Decreases Bias</li>
        </ul>
      </li>
      <li><strong>Bagging</strong>:
        <ul>
          <li>Reduces Variance</li>
        </ul>
      </li>
      <li>We <strong>Cannot Reduce</strong> the <strong>Irreducible Err</strong></li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue">Application of the Decomposition to Classification:</strong>
 A similar decomposition exists for:
    <ul>
      <li>Classification w/ \(0-1\) loss</li>
      <li>Probabilistic Classification w/ Squared Error</li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue">Bias-Variance Decomposition and Risk (Excess Risk Decomposition):</strong>
 The <strong>Bias-Variance Decomposition</strong> analyzes the behavior of the <em><strong>Expected Risk/Generalization Error</strong></em> for any function \(\hat{f}\):
    <p>$$R(\hat{f}) = \mathrm{E}\left[(y-\hat{f}(x))^{2}\right]=(\operatorname{Bias}[\hat{f}(x)])^{2}+\operatorname{Var}[\hat{f}(x)]+\sigma^{2}$$</p>
    <p>Assuming that \(y = f(x) + \epsilon\).</p>

    <p>The <strong>Bayes Optimal Predictor</strong> is \(f(x) = \mathrm{E}[Y\vert X=x]\).</p>

    <p>The <strong>Excess Risk</strong> is:</p>
    <p>$$\text{ExcessRisk}(\hat{f}) = R(\hat{f}) - R(f)$$</p>

    <p><strong style="color: red">Excess Risk Decomposition:</strong><br />
 We add and subtract the <strong>target function \(f_{\text{target}}=\inf_{h \in \mathcal{H}} I[h]\)</strong> that minimizes the <strong>(true) expected risk</strong>:</p>
    <p>$$\text{ExcessRisk}(\hat{f}) = \underbrace{\left(R(\hat{f}) - R(f_{\text{target}})\right)}_ {\text { estimation error }} + \underbrace{\left(R(f_{\text{target}}) - R(f)\right)}_ {\text { approximation error }}$$</p>

    <p>The <strong>Bias-Variance Decomposition</strong> for <em><strong>Excess Risk:</strong></em></p>
    <ul>
      <li>Re-Writing <strong>Excess Risk</strong>:
        <p>$$\text{ExcessRisk}(\hat{f}) = R(\hat{f}) - R(f) = \mathrm{E}\left[(y-\hat{f}(x))^{2}\right] - \mathrm{E}\left[(y-f(x))^{2}\right]$$</p>
        <p>which is equal to:</p>
        <p>$$R(\hat{f}) - R(f) = \mathrm{E}\left[(f(x)-\hat{f}(x))^{2}\right]$$</p>
      </li>
    </ul>
    <p>$$\mathrm{E}\left[(f(x)-\hat{f}(x))^{2}\right] = (\operatorname{Bias}[\hat{f}(x)])^{2}+\operatorname{Var}[\hat{f}(x)]$$</p>

    <ul>
      <li>if you dont want to mess with stat-jargon; lemme rephrase:<br />
  is the minimizer \({\displaystyle f=\inf_{h \in \mathcal{H}} I[h]}\) where \(I[h]\) is the expected-risk/generalization-error (assume MSE);<br />
  is it \(\overline{f}(\mathbf{x})=\mathbb{E}_ {\mathcal{D}}\left[f^{(\mathcal{D})}(\mathbf{x})\right]\) the average hypothesis over all realizations of \(N\) data-points \(\mathcal{D}_ i\)??</li>
    </ul>
  </li>
</ol>

<hr />

<h2 id="generalization-theory">Generalization Theory</h2>

<ul>
  <li><a href="https://www.cs.princeton.edu/courses/archive/fall17/cos597A/lecnotes/generalize.pdf">Generalization Bounds: PAC-Bayes, Rademacher, ERM, etc. (Notes!)</a></li>
  <li><a href="http://www.offconvex.org/2017/12/08/generalization1/">Deep Learning Generalization (blog!)</a></li>
</ul>

<ol>
  <li>
    <p><strong style="color: SteelBlue">Generalization Theory:</strong></p>

    <p><strong style="color: red">Approaches to (Notions of) Quantitative Description of Generalization Theory:</strong></p>
    <ul>
      <li>VC Dimension</li>
      <li>Rademacher Complexity</li>
      <li>PAC-Bayes Bound</li>
    </ul>

    <p><strong style="color: red">Prescriptive vs Descriptive Theory:</strong></p>
    <ul>
      <li><strong>Prescriptive</strong>: only attaches a label to the problem, without giving any insight into how to solve the problem.</li>
      <li><strong>Descriptive</strong>: describes the problem in detail (e.g. by providing cause) and allows you to solve the problem.</li>
    </ul>

    <p>Generalization Theory Notions consist of attaching a descriptive label to the basic phenomenon of lack of generalization. They are hard to compute for today’s complicated ML models, let alone to use as a guide in designing learning systems.<br />
 <strong style="color: red">Generalization Bounds as Descriptive Labels:</strong></p>
    <ul>
      <li><strong>Rademacher Complexity</strong>:<br />
  <button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Assumptions</button>
        <ul hidden="">
          <li>labels and loss are 0,1,</li>
          <li>the badly generalizing \(h\) predicts perfectly on the training sample \(S\) and<br />
  is completely wrong on the heldout set \(S_2\), meaning:
            <p>$$\Delta_{S}(h)-\Delta_{S_{2}}(h) \approx-1$$</p>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p><strong style="color: SteelBlue">Overfitting:</strong>
 One way to summarize <strong>Overfitting</strong> is:<br />
 <span style="color: purple">discovering patterns in data that do not exist in the intended application</span>.<br />
 The typical case of overfitting “<span style="color: purple">decreasing loss only on the training set and increasing the loss on the validation set</span>” is only one example of this.</p>

    <p>The question then is how we prevent overfitting from occurring.<br />
 The problem is that <span style="color: purple">we cannot know apriori what patterns will generalize from the dataset</span>.</p>

    <p><strong style="color: red">No-Free-Lunch (NFL) Theorem:</strong></p>
    <ul>
      <li><strong>NFL Theorems for Supervised Machine Learning:</strong>
        <ul>
          <li>In his 1996 paper The Lack of A Priori Distinctions Between Learning Algorithms, David Wolpert examines if it is possible to get useful theoretical results with a training data set and a learning algorithm without making any assumptions about the target variable.</li>
          <li><em>Wolpert</em> proves that given a noise-free data set (i.e. no random variation, only trend) and a machine learning algorithm where the cost function is the error rate, all machine learning algorithms are equivalent when assessed with a generalization error rate (the model’s error rate on a validation data set).</li>
          <li>Extending this logic he demonstrates that for any two algorithms, A and B, there are as many scenarios where A will perform worse than B as there are where A will outperform B. This even holds true when one of the given algorithms is random guessing.</li>
          <li>This is because nearly all (non-rote) machine learning algorithms make some assumptions (known as inductive or learning bias) about the relationships between the predictor and target variables, introducing bias into the model.<br />
  The assumptions made by machine learning algorithms mean that some algorithms will fit certain data sets better than others. It also (by definition) means that there will be as many data sets that a given algorithm will not be able to model effectively.<br />
  How effective a model will be is directly dependent on how well the assumptions made by the model fit the true nature of the data.</li>
          <li>Implication: you can’t get good machine learning “for free”.<br />
  You must use knowledge about your data and the context of the world we live in (or the world your data lives in) to select an appropriate machine learning model.<br />
  There is no such thing as a single, universally-best machine learning algorithm, and there are no context or usage-independent (a priori) reasons to favor one algorithm over all others.</li>
        </ul>
      </li>
      <li><strong>NFL Theorem for Search/Optimization:</strong>
        <ul>
          <li>All algorithms that search for an extremum of a cost function perform exactly the same when averaged over all possible cost functions. So, for any search/optimization algorithm, any elevated performance over one class of problems is exactly paid for in performance over another class.</li>
          <li>It state that any two optimization algorithms are equivalent when their performance is averaged across all possible problems.</li>
          <li>Wolpert and McReady essentially say that <span style="color: purple">you need some prior knowledge that is encoded in your algorithm in order to search well</span>.<br />
  They created the theorem to give an easy counter example to researchers that would create a (tweak on an) algorithm and claimed that it would work better on all possible problems. It can’t. There’s a proof. If you have a good algorithm, it must in some way fit on your problem space, and you will have to investigate how.</li>
        </ul>
      </li>
      <li><button class="showText" value="show" onclick="showText_withParent_PopHide(event);"><strong>Explanation through a thought experiment:</strong></button>
        <ul hidden="">
          <li>Suppose you see someone toss a coin and get heads. What is the probability distribution over the next result of the coin toss?<br />
  Did you think heads 50% and tails 50%?<br />
  If so, you’re wrong: the answer is that we don’t know.<br />
  For all we know, the coin could have heads on both sides. Or, it might even obey some strange laws of physics and come out as tails every second toss.<br />
  The point is, <span style="color: purple">there is no way we can extract only patterns that will <strong>generalize</strong></span>: there will always be some scenarios where those patterns will not exist.<br />
  This is the essence of what is called the <strong>No Free Lunch Theorem</strong>: <span style="color: purple">any model that you create will always “overfit” and be completely wrong in some scenarios</span>.</li>
        </ul>
      </li>
      <li><strong>Main Concept:</strong> Generalizing is extrapolating to new data. To do that you need to make assumptions and for problems where the assumptions don’t hold you will be suboptimal.</li>
      <li><strong>Practical Implications</strong>:
        <ul>
          <li>Bias-free learning is futile because a learner that makes no a priori assumptions will have no rational basis for creating estimates when provided new, unseen input data.<br />
  Models are simplifications of a specific component of reality (observed with data). To simplify reality, a machine learning algorithm or statistical model needs to make assumptions and introduce bias (known specifically as inductive or learning bias).<br />
  The assumptions of an algorithm will work for some data sets but fail for others.</li>
          <li>This shows that we need some restriction on \(\mathcal{H}\) even for the realizable PAC setting. We cannot learn arbitrary set of hypothesis, there is no free lunch.</li>
          <li>There is no universal (one that works for all \(\mathcal{H}\)) learning algorithm.<br />
  I.e., if the algorithm \(A\) has no idea about \(\mathcal{H},\) even the singleton hypothesis class \(\mathcal{H}=\{h\}\) (as in the statement of the theorem) is not PAC learnable.</li>
          <li>Why do we have to fix a hypothesis class when coming up with a learning algorithm? Can we <em>“just learn”</em>?<br />
  The NFL theorem formally shows that the answer is <strong>NO</strong>.</li>
          <li>Counter the following claim: “My machine learning algorithm/optimization strategy is the best, always and forever, for all the scenarios”.</li>
          <li>Always check your assumptions before relying on a model or search algorithm.</li>
          <li>There is no “super algorithm” that will work perfectly for all datasets.</li>
          <li>Variable length and redundant encodings are not covered by the NFL theorems. Does Genetic Programming get a pass?</li>
          <li>NFL theorems are like a statistician sitting with his head in the fridge, and his feet in the oven. On average, his temperature is okay! NFL theorems prove that the arithmetic mean of performance is constant over all problems, it doesn’t prove that for other statistics this is the case. There has been an interesting ‘counter’ proof, where a researcher proved that for a particular problem space, a hill-climber would outperform a hill-descender on 90% of the problem instances, and did that by virtue of being exponentially worse on the remaining 10% of the problems. Its average performance abided by the NFL theorem and the two algorithms were equal when looking at mean performance, yet the hill-climber was better in 90% of the problems, i.e., it had a much better median performance. So is there maybe a free appetizer?</li>
          <li>While no one model/algorithm works best for every problem, it may be that one model/algorithm works best for all real-world problems, or all problems that we care about, practically speaking.</li>
        </ul>
      </li>
      <li><a href="http://www.no-free-lunch.org/">No Free Lunch Theorems Main Site</a></li>
      <li><a href="http://www.cs.utah.edu/~bhaskara/courses/theoryml/scribes/lecture3.pdf">No Free Lunch Theorem (Lec Notes)</a></li>
      <li><a href="http://www.cs.cornell.edu/courses/cs6783/2015fa/lec3.pdf">Machine Learning Theory - NFL Theorem (Lec Notes)</a></li>
      <li><a href="http://mlexplained.com/2018/04/24/overfitting-isnt-simple-overfitting-re-explained-with-priors-biases-and-no-free-lunch/">Overfitting isn’t simple: Overfitting Re-explained with Priors, Biases, and No Free Lunch (Blog)</a></li>
      <li><a href="https://artint.info/html/ArtInt_191.html">Learning as Refining the Hypothesis Space (Blog-Paper)</a></li>
      <li><a href="https://community.alteryx.com/t5/Data-Science-Blog/All-Models-Are-Wrong/ba-p/348080">All Models Are Wrong (Blog)</a></li>
      <li><a href="https://www.quora.com/Does-the-no-free-lunch-theorem-also-apply-to-deep-learning">Does the ‘no free lunch’ theorem also apply to deep learning? (Quora)</a></li>
    </ul>
  </li>
</ol>

</div> -->

<p>$$(.|\n)*?$$
<strong style="color: red">Fundamental Theorem of Statistical Learning (binary classification):</strong><br />
Let \(\mathcal{H}\) be a hypothesis class of functions from a domain \(X\) to \(\{0,1\}\) and let the loss function be the \(0-1\) loss.<br />
The following are equivalent:<br />
\(\begin{array}{l}{\text { 1. } \mathcal{H} \text { has uniform convergence. }} \\ {\text { 2. The ERM is a PAC learning algorithm for } \mathcal{H} \text { . }} \\ {\text { 3. } \mathcal{H} \text { is } PAC \text { learnable. }} \\ {\text { 4. } \mathcal{H} \text { has finite } VC \text { dimension. }}\end{array}\)<br />
This can be extended to <strong>regression</strong> and <strong>multiclass classification</strong>.</p>

<p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Proof.</button></p>
<div hidden="">
  <ul>
    <li>1 \(\Rightarrow 2\) We have seen uniform convergence implies that \(\mathrm{ERM}\) is \(\mathrm{PAC}\) learnable</li>
    <li>2 \(\Rightarrow 3\) Obvious.</li>
    <li>3 \(\Rightarrow 4\) We just proved that PAC learnability implies finite \(\mathrm{VC}\) dimension.</li>
    <li>4 \(\Rightarrow 1\) We proved that finite \(\mathrm{VC}\) dimension implies uniform convergence.</li>
  </ul>
</div>

<p><strong style="color: red">Notes:</strong></p>
<ul>
  <li>VC dimension fully determines <span style="color: goldenrod">learnability</span> for binary classification.</li>
  <li>The VC dimension doesn’t just determine <strong>learnability</strong>, it also gives a <span style="color: goldenrod">bound on the sample complexity</span> (which can be shown to be <strong style="color: goldenrod">tight</strong>).</li>
  <li><a href="https://www.cs.toronto.edu/~jlucas/teaching/csc411/lectures/lec23_24_handout.pdf">Lecture Slides (ref)</a></li>
  <li><button class="showText" value="show" onclick="showTextPopHide(event);">Extra Notes (what you should know)</button>
  <img src="https://cdn.mathpix.com/snip/images/9bYxbit2n1mvyrttH3CzgI-2CgdrrDNVkejd1fP5-AU.original.fullsize.png" alt="img" width="100%" hidden="" /></li>
</ul>

<p><strong style="color: red">Theoretical Concepts:</strong></p>
<ul>
  <li><strong>Kolmogorov Complexity</strong>:<br />
  In <strong>Algorithmic Information Theory</strong>, the <strong>Kolmogorov Complexity</strong> of an object, such as a piece of text, is the <span style="color: purple"><strong>length</strong> of the <em><strong>shortest</strong></em> <strong>computer program</strong></span> (in a predetermined programming language) that <span style="color: purple"><em>produces</em> the object as <strong>output</strong></span>.<br />
  It is a measure of the <strong>computational resources</strong> needed to <em><strong>specify</strong></em> the object.<br />
  It is also known as <strong>algorithmic complexity</strong>, <strong>Solomonoff–Kolmogorov complexity</strong>, <strong>program-size complexity</strong>, <strong>descriptive complexity</strong>, or <strong>algorithmic entropy</strong>.</li>
  <li><strong>Rademacher Complexity</strong>:</li>
  <li><strong>Generalization Bounds</strong>:</li>
  <li><strong>Sample Complexity</strong>:</li>
  <li><strong>PAC-Bayes Bound</strong>:</li>
  <li><strong>Kolmogorov Randomness</strong>:</li>
  <li><strong>Minimum Description Length (MDL)</strong>: <br />
  The <a href="https://en.wikipedia.org/wiki/Minimum_description_length"><strong>minimum description length (MDL) principle</strong></a> is a formalization of <strong>Occam’s razor</strong> in which the best hypothesis (a model and its parameters) for a given set of data is the one that leads to the <span style="color: purple">best compression of the data</span>.</li>
  <li><strong>Minimum Message Length (MML)</strong>:<br />
  <a href="https://en.wikipedia.org/wiki/Minimum_message_length"><strong>MML</strong></a> is a formal Information Theory restatement of <strong>Occam’s Razor</strong>: even when models are equal in goodness of fit accuracy to the observed data, the one generating the <span style="color: purple">shortest overall message</span> is more likely to be correct (where the message consists of a statement of the model, followed by a statement of data encoded concisely using that model).</li>
  <li><a href="https://web.stanford.edu/~hastie/Papers/ESLII.pdf#page=37">Statistical Decision Theory (ESL!)</a></li>
  <li><strong>PAC Learnability</strong>:<br />
  A hypothesis class \(\mathcal{H}\) is PAC learnable, if there exists a learning algorithm A, satisfying that for any \(\epsilon&gt;0\) and \(\delta \in(0,1)\) there exist \(\mathfrak{M}(\epsilon, \delta)=\) poly \(\left(\frac{1}{\epsilon}, \frac{1}{\delta}\right)\) such that for i.i.d samples \(S^{m}=\left\{\left(x_{i}, y_{i}\right)\right\}_ {i=1}^{m}\) drawn from any distribution \(\mathcal{D}\) and \(m \geq \mathfrak{M}(\epsilon, \delta)\) the algorithm returns a hypothesis \(A\left(S^{m}\right) \in \mathcal{H}\) satisfying
    <p>$$P_{S^{m} \sim \mathcal{D}^{m}}\left(L_{\mathcal{D}}(A(S))&gt;\min _{h \in \mathcal{H}} L_{\mathcal{D}}(h)+\epsilon\right)&lt;\delta$$</p>
    <p>To show that empirical risk minimization (ERM) is a PAC learning algorithm, we need to show that \(L_{S}(h) \approx L_{\mathcal{D}}(h)\) for all \(h\).</p>
  </li>
  <li>
    <p><strong>Uniform Convergence</strong>:<br />
  A hypothesis class \(\mathcal{H}\) has the uniform convergence property, if for any \(\epsilon&gt;0\) and \(\delta \in(0,1)\) there exist \(\mathfrak{M}(\epsilon, \delta)=\) \(\text{poly}\left(\frac{1}{\epsilon}, \frac{1}{\delta}\right)\) such that for any distribution \(\mathcal{D}\) and \(m \geq \mathfrak{M}(\epsilon, \delta)\) i.i.d samples \(S^{m}=\left\{\left(x_{i}, y_{i}\right)\right\}_ {i=1}^{m} \sim \mathcal{D}^{m}\) with probability at least \(1-\delta\), \(\left\vert L_{S}^{m}(h)-L_{\mathcal{D}}(h)\right\vert &lt;\epsilon\) for all \(h \in \mathcal{H}\).</p>

    <ul>
      <li>For a single \(h,\) law of large numbers says \(L_{S}^{m}(h) \stackrel{m \rightarrow \infty}{\rightarrow} L_{\mathcal{D}}(h)\)</li>
      <li>For loss bounded by 1 the Hoeffding inequality states:
        <p>$$P\left(\left\vert L_{S}^{m}(h)-L_{\mathcal{D}}(h)\right\vert &gt;\epsilon\right) \leq 2 e^{-2 \epsilon^{2} m}$$</p>
      </li>
      <li>The difficulty is to bound all the \(h \in \mathcal{H}\) <span style="color: purple">uniformly</span>.</li>
    </ul>
  </li>
  <li><strong>Complexity in ML</strong>:
    <ul>
      <li><strong>Definitions of the complexity of an object (\(h\))</strong>:
        <ul>
          <li><strong>Minimum Description Length (MDL)</strong>: the number of bits for specifying an object.</li>
          <li><strong>Order of a Polynomial</strong></li>
        </ul>
      </li>
      <li><strong>Definitions of the complexity of a class of objects (\(\mathcal{H}\))</strong>:
        <ul>
          <li><strong>Entropy</strong></li>
          <li><strong>VC-dim</strong></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="statistical-learning-theory">Statistical Learning Theory</h2>

<ol>
  <li><strong style="color: SteelBlue">Statistical Learning Theory:</strong>
 <strong>Statistical Learning Theory</strong> is a framework for machine learning drawing from the fields of statistics and functional analysis. Under certain assumptions, this framework allows us to study the question:
    <blockquote>
      <p><strong style="color: blue">How can we affect performance on the test set when we can only observe the training set?</strong></p>
    </blockquote>

    <p>It is a <em>statistical</em> approach to <strong>Computational Learning Theory</strong>.</p>
  </li>
  <li><strong style="color: SteelBlue">Formal Definition:</strong>
 Let:
    <ul>
      <li>\(X\): the vector space of all possible <strong>inputs</strong></li>
      <li>\(Y\): the vector space of all possible <strong>outputs</strong></li>
      <li>\(Z = X \times Y\): the <strong>product space</strong> of (input,output) pairs</li>
      <li>\(n\): the number of <strong>samples</strong> in the <strong>training set</strong></li>
      <li>\(S=\left\{\left(\vec{x}_{1}, y_{1}\right), \ldots,\left(\vec{x}_{n}, y_{n}\right)\right\}=\left\{\vec{z}_{1}, \ldots, \vec{z}_{n}\right\}\): the <strong>training set</strong></li>
      <li>\(\mathcal{H} = f : X \rightarrow Y\): the <strong>hypothesis space</strong> of all functions</li>
      <li>\(V(f(\vec{x}), y)\): an <strong>error/loss function</strong></li>
    </ul>

    <p><strong>Assumptions:</strong></p>
    <ul>
      <li>The training and test data are generated by an <em><strong>unknown, joint</strong></em> <strong>probability distribution over datasets</strong> (over the product space \(Z\), denoted: \(p_{\text{data}}(z)=p(\vec{x}, y)\)) called the <strong>data-generating process</strong>.
        <ul>
          <li>\(p_{\text{data}}\) is a <strong>joint distribution</strong> so that it allows us to model <em>uncertainty in predictions</em> (e.g. from noise in data) because \(y\) is not a deterministic function of \(\vec{x}\), but rather a <em>random variable</em> with <strong>conditional distribution</strong> \(p(y \vert \vec{x})\) for a fixed \(\vec{x}\).</li>
        </ul>
      </li>
      <li>The <strong>i.i.d. assumptions:</strong>
        <ul>
          <li>The examples in each dataset are <strong>independent</strong> from each other</li>
          <li>The <em>training set</em> and <em>test set</em> are <strong>identically distributed</strong> (drawn from the same probability distribution as each other)</li>
        </ul>

        <blockquote>
          <p>A collection of random variables is <strong>independent and identically distributed</strong> if each random variable has the same probability distribution as the others and all are mutually independent.<br />
<em>Informally,</em> it says that all the variables provide the same kind of information independently of each other.</p>
          <ul>
            <li><a href="https://stats.stackexchange.com/questions/213464/on-the-importance-of-the-i-i-d-assumption-in-statistical-learning/214220">Discussion on the importance if i.i.d assumptions</a></li>
          </ul>
        </blockquote>
      </li>
    </ul>

    <p><strong style="color: red">The Inference Problem</strong>  <br />
 Finding a function \(f : X \rightarrow Y\) such that \(f(\vec{x}) \sim y\).</p>

    <p><strong>The Expected Risk:</strong></p>
    <p>$$I[f]=\mathbf{E}[V(f(\vec{x}), y)]=\int_{X \times Y} V(f(\vec{x}), y) p(\vec{x}, y) d \vec{x} d y$$</p>

    <p><strong>The Target Function:</strong><br />
 is the best possible function \(f\) that can be chosen, is given by:</p>
    <p>$$f=\inf_{h \in \mathcal{H}} I[h]$$</p>

    <p><strong>The Empirical Risk:</strong><br />
 Is a <em><strong>proxy measure</strong></em> for the <strong>expected risk</strong>, based on the training set.<br />
 It is necessary because the probability distribution \(p(\vec{x}, y)\) is <em>unknown</em>.</p>
    <p>$$I_{S}[f]=\frac{1}{n} \sum_{i=1}^{n} V\left(f\left(\vec{x}_{i}\right), y_{i}\right)$$</p>
  </li>
  <li>
    <p><strong style="color: SteelBlue">Empirical risk minimization:</strong>
 <strong>Empirical Risk Minimization (ERM)</strong> is a principle in <em>statistical learning theory</em> that is based on approximating the <strong>Generalization Error (True Risk)</strong> by measuring the <strong>Training Error (Empirical Risk)</strong>, i.e. the performance on training data.</p>

    <p>A <em>learning algorithm</em> that chooses the function \(f_{S}\) that minimizes the <em>empirical risk</em> is called <strong>empirical risk minimization</strong>:</p>
    <p>$$R_{\mathrm{emp}}(h) = I_{S}[f]=\frac{1}{n} \sum_{i=1}^{n} V\left(f\left(\vec{x}_{i}\right), y_{i}\right)$$</p>
    <p>$$f_{S} = \hat{h} = \arg \min _{h \in \mathcal{H}} R_{\mathrm{emp}}(h)$$</p>

    <p><strong style="color: red">Complexity:</strong><br />
 <button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Show</button></p>
    <ul hidden="">
      <li>Empirical risk minimization for a classification problem with a <em>0-1 loss function</em> is known to be an <strong>NP-hard</strong> problem even for such a relatively simple class of functions as linear classifiers.
        <ul>
          <li><a href="https://arxiv.org/abs/1012.0729">Paper Proof</a></li>
        </ul>
      </li>
      <li>Though, it can be solved efficiently when the minimal empirical risk is zero, i.e. data is linearly separable.</li>
      <li><strong>Coping with Hardness:</strong>
        <ul>
          <li>Employing a <strong>convex approximation</strong> to the 0-1 loss: <em>Hinge Loss</em>, <em>SVM</em></li>
          <li>Imposing <strong>Assumptions on the data-generating distribution</strong> and thus, stop being an <strong>agnostic learning algorithm</strong>.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue">Definitions:</strong>
    <ul>
      <li><strong style="color: red">Generalization Error:</strong><br />
  AKA: <strong>Expected Risk/Error</strong>, <strong>Out-of-Sample Error</strong>[^2], <strong>\(E_{\text{out}}\)</strong> <br />
  It is a measure of how accurately an algorithm is able to predict outcome values for previously unseen data.</li>
      <li>
        <p><strong style="color: red">Generalization Gap:</strong><br />
  It is a measure of how accurately an algorithm is able to predict outcome values for previously unseen data.</p>

        <p><strong>Formally:</strong><br />
  The generalization gap is the <strong>difference between the expected and empirical error</strong>:</p>
        <p>$$G =I\left[f_{n}\right]-I_{S}\left[f_{n}\right]$$</p>

        <p>An Algorithm is said to <strong>Generalize</strong> (achieve <strong>Generalization</strong>) if:</p>
        <p>$$\lim _{n \rightarrow \infty} G_n = \lim _{n \rightarrow \infty} I\left[f_{n}\right]-I_{S}\left[f_{n}\right]=0$$</p>
        <p>Equivalently:</p>
        <p>$$E_{\text { out }}(g) \approx E_{\text { in }}(g)$$</p>
        <p>or</p>
        <p>$$I\left[f_{n}\right] \approx I_{S}\left[f_{n}\right]$$</p>

        <p><strong>Computing the Generalization Gap:</strong><br />
  Since \(I\left[f_{n}\right]\) cannot be computed for an unknown distribution, the generalization gap <strong>cannot be computed</strong> either.<br />
  Instead the goal of <strong>statistical learning theory</strong> is to <em>bound</em> or <em>characterize</em> the generalization gap in probability:</p>
        <p>$$P_{G}=P\left(I\left[f_{n}\right]-I_{S}\left[f_{n}\right] \leq \epsilon\right) \geq 1-\delta_{n}$$</p>
        <p>That is, the goal is to characterize the probability \({\displaystyle 1-\delta _{n}}\) that the generalization gap is less than some error bound \({\displaystyle \epsilon }\) (known as the <strong>learning rate</strong> and generally dependent on \({\displaystyle \delta }\) and \({\displaystyle n}\)).</p>
      </li>
      <li><strong style="color: red">The Empirical Distribution:</strong><br />
  <em>AKA <strong>Data-Generating Distribution</strong></em><br />
  is the <strong>discrete</strong> uniform distribution over the <em>sample points</em>.</li>
      <li><strong style="color: red">The Approximation-Generalization Tradeoff:</strong>
        <ul>
          <li><strong>Goal</strong>:<br />
  Small \(E_{\text{out}}\): Good approximation of \(f\) <em><strong>out of sample</strong></em> (not in-sample).</li>
          <li>The tradeoff is characterized by the <strong>complexity</strong> of the <strong>hypothesis space \(\mathcal{H}\)</strong>:
            <ul>
              <li><strong>More Complex \(\mathcal{H}\)</strong>: Better chance of approximating \(f\)</li>
              <li><strong>Less Complex \(\mathcal{H}\)</strong>: Better chance of generalizing out-of-sample</li>
            </ul>
          </li>
          <li><a href="https://www.youtube.com/embed/zrEyxfl2-a8?start=358" value="show" onclick="iframePopA(event)"><strong>Abu-Mostafa</strong></a>
  <a href="https://www.youtube.com/embed/zrEyxfl2-a8?start=358"></a>
            <div></div>
          </li>
          <li><a href="https://mdav.ece.gatech.edu/ece-6254-spring2017/notes/13-bias-variance-marked.pdf">Lecture-Slides on Approximation-Generalization</a></li>
        </ul>
      </li>
      <li><strong style="color: red">Excess Risk (Generalization-Gap) Decomposition | Estimation-Approximation Tradeoff:</strong><br />
  <strong>Excess Risk</strong> is defined as the difference between the expected-risk/generalization-error of any function \(\hat{f} = g^{\mathcal{D}}\) that we learn from the data (exactly just bias-variance), and the expected-risk of the <strong>target function</strong> \(f\) (known as the <strong>bayes optimal predictor</strong>)
        <ul>
          <li><a href="https://www.youtube.com/embed/YA_CE9jat4I" value="show" onclick="iframePopA(event)"><strong>Excess Risk Decomposition Video</strong></a>
  <a href="https://www.youtube.com/embed/YA_CE9jat4I"></a>
            <div></div>
          </li>
          <li><a href="https://www.ics.uci.edu/~smyth/courses/cs274/readings/xing_singh_CMU_bias_variance.pdf">Excess Risk &amp; Bias-Variance Lecture Slides</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue">Notes:</strong>
    <ul>
      <li><strong>Choices of Loss Functions</strong>:
        <ul>
          <li><strong>Regression</strong>:
            <ul>
              <li><strong>MSE</strong>: \(\: V(f(\vec{x}), y)=(y-f(\vec{x}))^{2}\)</li>
              <li><strong>MAE</strong>: \(\: V(f(\vec{x}), y)=\vert{y-f(\vec{x})}\vert\)</li>
            </ul>
          </li>
          <li><strong>Classification</strong>:
            <ul>
              <li><strong>Binary</strong>: \(\: V(f(\vec{x}), y)=\theta(-y f(\vec{x}))\)<br />
  where \(\theta\) is the <em>Heaviside Step Function</em>.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li><strong>Training Data, Errors, and Risk</strong>:
        <ul>
          <li><strong>Training-Error</strong> is the <strong>Empirical Risk</strong>
            <ul>
              <li>It is a <strong>proxy</strong> for the <strong>Generalization Error/Expected Risk</strong></li>
              <li>This is what we minimize</li>
            </ul>
          </li>
          <li><strong>Test-Error</strong> is an <em><strong>approximation</strong></em> to the <strong>Generalization Error/Expected Risk</strong>
            <ul>
              <li>This is what we (can) compute to ensure that minimizing Training-Err/Empirical-Risk (ERM) also minimized the Generalization-Err/Expected-Risk (which we can’t compute directly)</li>
            </ul>
          </li>
        </ul>
      </li>
      <li><strong>Why the goal is NOT to minimize \(E_{\text{in}}\) completely (intuition)</strong>:<br />
  Basically, if you have noise in the data; then fitting the (finite) training-data completely; i.e. minimizing the in-sample-err completely will underestimate the out-of-sample-err.<br />
  Since, if noise existed AND you fit training-data completely \(E_{\text{in}} = 0\) THEN you inherently have fitted the noise AND your performance on out-sample will be lower.</li>
    </ul>
  </li>
</ol>

<hr />

<h2 id="the-vapnik-chervonenkis-vc-theory">The Vapnik-Chervonenkis (VC) Theory</h2>

<hr />

<h2 id="the-bias-variance-decomposition-theory">The Bias-Variance Decomposition Theory</h2>

<ol>
  <li>
    <p><strong style="color: SteelBlue">The Bias-Variance Decomposition Theory:</strong>
<strong>The Bias-Variance Decomposition Theory</strong> is a way to quantify the <strong>Approximation-Generalization Tradeoff</strong>.</p>

    <p><strong>Assumptions:</strong></p>
    <ul>
      <li>The analysis is done over the <strong>entire data-distribution</strong></li>
      <li>The target function \(f\) is already <strong>known</strong>; and you’re trying to answer the question:<br />
  “How can \(\mathcal{H}\) approximate \(f\) over all? not just on your sample.”</li>
      <li>Applies to <strong>real-valued targets</strong> (can be extended)</li>
      <li>Use <strong>Square Error</strong> (can be extended)</li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue">The Bias-Variance Decomposition:</strong>
 The <strong>Bias-Variance Decomposition</strong> is a way of analyzing a learning algorithm’s <em><strong>expected out-of-sample error</strong></em>[^1] as a <em>sum of three terms:</em>
    <ul>
      <li><strong>Bias:</strong> is an error from erroneous assumptions in the learning algorithm.</li>
      <li><strong>Variance:</strong> is an error from sensitivity to small fluctuations in the training set.</li>
      <li><strong>Irreducible Error</strong> (resulting from noise in the problem itself)</li>
    </ul>

    <p>Equivalently, <strong>Bias</strong> and <strong>Variance</strong> measure <em>two different sources of errors in an estimator:</em></p>
    <ul>
      <li><strong>Bias:</strong> measures the expected deviation from the true value of the function or parameter.
        <blockquote>
          <p>AKA: <strong>Approximation Error</strong>[^3]  (statistics)  How well can \(\mathcal{H}\) approximate the target function ‘\(f\)’</p>
        </blockquote>
      </li>
      <li><strong>Variance:</strong> measures the deviation from the expected estimator value that any particular sampling of the data is likely to cause.
        <blockquote>
          <p>AKA: <strong>Estimation (Generalization) Error</strong> (statistics) How well we can zoom in on a good \(h \in \mathcal{H}\)</p>
        </blockquote>
      </li>
    </ul>

    <p><strong>Bias-Variance Decomposition Formula:</strong><br />
 For any function \(\hat{f} = g^{\mathcal{D}}\) we select, we can decompose its <em><strong>expected (out-of-sample) error</strong></em> on an <em>unseen sample \(x\)</em> as:</p>
    <p>$$\mathrm{E}\left[(y-\hat{f}(x))^{2}\right]=(\operatorname{Bias}[\hat{f}(x)])^{2}+\operatorname{Var}[\hat{f}(x)]+\sigma^{2}$$</p>
    <p>Where:</p>
    <ul>
      <li><strong>Bias</strong>:
        <p>$$\operatorname{Bias}[\hat{f}(x)]=\mathrm{E}[\hat{f}(x)]-f(x)$$</p>
      </li>
      <li><strong>Variance</strong>:
        <p>$$\operatorname{Var}[\hat{f}(x)]=\mathrm{E}\left[\hat{f}(x)^{2}\right]-\mathrm{E}[\hat{f}(x)]^{2}$$</p>
        <p>and the expectation ranges over different realizations of the training set \(\mathcal{D}\).</p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong style="color: SteelBlue">The Bias-Variance Tradeoff:</strong>
 is the property of a set of predictive models whereby, models with a <em>lower bias</em> (in parameter estimation) have a <em>higher variance</em> (of the parameter estimates across samples) and vice-versa.</p>

    <p><strong style="color: black">Effects of Bias:</strong></p>
    <ul>
      <li><strong>High Bias</strong>: simple models, lead to <em style="color: red"><strong>underfitting</strong></em>.</li>
      <li><strong>Low Bias</strong>: complex models, lead to <em style="color: red"><strong>overfitting</strong></em>.</li>
    </ul>

    <p><strong style="color: black">Effects of Variance:</strong></p>
    <ul>
      <li><strong>High Variance</strong>: complex models, lead to <em style="color: red"><strong>overfitting</strong></em>.</li>
      <li><strong>Low Variance</strong>: simple models, lead to <em style="color: red"><strong>underfitting</strong></em>.</li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue">Derivation:</strong>
    <p>$${\displaystyle {\begin{aligned}\mathbb{E}_{\mathcal{D}} {\big [}I[g^{(\mathcal{D})}]{\big ]}&amp;=\mathbb{E}_{\mathcal{D}} {\big [}\mathbb{E}_{x}{\big [}(g^{(\mathcal{D})}-y)^{2}{\big ]}{\big ]}\\
 &amp;=\mathbb{E}_{x} {\big [}\mathbb{E}_{\mathcal{D}}{\big [}(g^{(\mathcal{D})}-y)^{2}{\big ]}{\big ]}\\
 &amp;=\mathbb{E}_{x}{\big [}\mathbb{E}_{\mathcal{D}}{\big [}(g^{(\mathcal{D})}- f -\varepsilon)^{2}{\big ]}{\big ]}
 \\&amp;=\mathbb{E}_{x}{\big [}\mathbb{E}_{\mathcal{D}} {\big [}(f+\varepsilon -g^{(\mathcal{D})}+\bar{g}-\bar{g})^{2}{\big ]}{\big ]}\\
 &amp;=\mathbb{E}_{x}{\big [}\mathbb{E}_{\mathcal{D}} {\big [}(\bar{g}-f)^{2}{\big ]}+\mathbb{E}_{\mathcal{D}} [\varepsilon ^{2}]+\mathbb{E}_{\mathcal{D}} {\big [}(g^{(\mathcal{D})} - \bar{g})^{2}{\big ]}+2\: \mathbb{E}_{\mathcal{D}} {\big [}(\bar{g}-f)\varepsilon {\big ]}+2\: \mathbb{E}_{\mathcal{D}} {\big [}\varepsilon (g^{(\mathcal{D})}-\bar{g}){\big ]}+2\: \mathbb{E}_{\mathcal{D}} {\big [}(g^{(\mathcal{D})} - \bar{g})(\bar{g}-f){\big ]}{\big ]}\\
 &amp;=\mathbb{E}_{x}{\big [}(\bar{g}-f)^{2}+\mathbb{E}_{\mathcal{D}} [\varepsilon ^{2}]+\mathbb{E}_{\mathcal{D}} {\big [}(g^{(\mathcal{D})} - \bar{g})^{2}{\big ]}+2(\bar{g}-f)\mathbb{E}_{\mathcal{D}} [\varepsilon ]\: +2\: \mathbb{E}_{\mathcal{D}} [\varepsilon ]\: \mathbb{E}_{\mathcal{D}} {\big [}g^{(\mathcal{D})}-\bar{g}{\big ]}+2\: \mathbb{E}_{\mathcal{D}} {\big [}g^{(\mathcal{D})}-\bar{g}{\big ]}(\bar{g}-f){\big ]}\\
 &amp;=\mathbb{E}_{x}{\big [}(\bar{g}-f)^{2}+\mathbb{E}_{\mathcal{D}} [\varepsilon ^{2}]+\mathbb{E}_{\mathcal{D}} {\big [}(g^{(\mathcal{D})} - \bar{g})^{2}{\big ]}{\big ]}\\
 &amp;=\mathbb{E}_{x}{\big [}(\bar{g}-f)^{2}+\operatorname {Var} [y]+\operatorname {Var} {\big [}g^{(\mathcal{D})}{\big ]}{\big ]}\\
 &amp;=\mathbb{E}_{x}{\big [}\operatorname {Bias} [g^{(\mathcal{D})}]^{2}+\operatorname {Var} [y]+\operatorname {Var} {\big [}g^{(\mathcal{D})}{\big ]}{\big ]}\\
 &amp;=\mathbb{E}_{x}{\big [}\operatorname {Bias} [g^{(\mathcal{D})}]^{2}+\sigma ^{2}+\operatorname {Var} {\big [}g^{(\mathcal{D})}{\big ]}{\big ]}\\
\end{aligned}}}$$</p>
    <p>where:<br />
 \(\overline{g}(\mathbf{x})=\mathbb{E}_{\mathcal{D}}\left[g^{(\mathcal{D})}(\mathbf{x})\right]\) is the <strong>average hypothesis</strong> over all realization of \(N\) data-points \(\mathcal{D}_ i\), and \({\displaystyle \varepsilon }\) and \({\displaystyle {\hat {f}}} = g^{(\mathcal{D})}\) are <strong>independent</strong>.</p>

    <p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Derivation with Wikipedia Notation</button></p>
    <p hidden="">$${\displaystyle {\begin{aligned}\operatorname {E}_ {\mathcal{D}} {\big [}(y-{\hat {f}})^{2}{\big ]}&amp;=\operatorname {E} {\big [}(f+\varepsilon -{\hat {f}})^{2}{\big ]}\\&amp;=\operatorname {E} {\big [}(f+\varepsilon -{\hat {f}}+\operatorname {E} [{\hat {f}}]-\operatorname {E} [{\hat {f}}])^{2}{\big ]}\\&amp;=\operatorname {E} {\big [}(f-\operatorname {E} [{\hat {f}}])^{2}{\big ]}+\operatorname {E} [\varepsilon ^{2}]+\operatorname {E} {\big [}(\operatorname {E} [{\hat {f}}]-{\hat {f}})^{2}{\big ]}+2\operatorname {E} {\big [}(f-\operatorname {E} [{\hat {f}}])\varepsilon {\big ]}+2\operatorname {E} {\big [}\varepsilon (\operatorname {E} [{\hat {f}}]-{\hat {f}}){\big ]}+2\operatorname {E} {\big [}(\operatorname {E} [{\hat {f}}]-{\hat {f}})(f-\operatorname {E} [{\hat {f}}]){\big ]}\\&amp;=(f-\operatorname {E} [{\hat {f}}])^{2}+\operatorname {E} [\varepsilon ^{2}]+\operatorname {E} {\big [}(\operatorname {E} [{\hat {f}}]-{\hat {f}})^{2}{\big ]}+2(f-\operatorname {E} [{\hat {f}}])\operatorname {E} [\varepsilon ]+2\operatorname {E} [\varepsilon ]\operatorname {E} {\big [}\operatorname {E} [{\hat {f}}]-{\hat {f}}{\big ]}+2\operatorname {E} {\big [}\operatorname {E} [{\hat {f}}]-{\hat {f}}{\big ]}(f-\operatorname {E} [{\hat {f}}])\\&amp;=(f-\operatorname {E} [{\hat {f}}])^{2}+\operatorname {E} [\varepsilon ^{2}]+\operatorname {E} {\big [}(\operatorname {E} [{\hat {f}}]-{\hat {f}})^{2}{\big ]}\\&amp;=(f-\operatorname {E} [{\hat {f}}])^{2}+\operatorname {Var} [y]+\operatorname {Var} {\big [}{\hat {f}}{\big ]}\\&amp;=\operatorname {Bias} [{\hat {f}}]^{2}+\operatorname {Var} [y]+\operatorname {Var} {\big [}{\hat {f}}{\big ]}\\&amp;=\operatorname {Bias} [{\hat {f}}]^{2}+\sigma ^{2}+\operatorname {Var} {\big [}{\hat {f}}{\big ]}\\\end{aligned}}}$$</p>
  </li>
  <li>
    <p><strong style="color: SteelBlue">Results and Takeaways of the Decomposition:</strong>
 Match the <strong>“Model Complexity”</strong> to the <em><strong>Data Resources</strong></em>, NOT to the <em>Target Complexity</em>.<br />
 <img src="/main_files/dl/theory/stat_lern_thry/1.png" alt="img" width="80%" /></p>

    <p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Analogy to the Approximation-Generalization Tradeoff</button>
 <strong style="color: red">The data resources you have is, “what do you have in order to navigate the hypothesis set?”. Let’s pick a hypothesis set that we can afford to navigate. That is the game in learning. Done with the bias and variance.</strong>&lt;/p&gt;</p>
  </li>
  <li><strong style="color: SteelBlue">Measuring the Bias and Variance:</strong>
    <ul>
      <li><strong>Training Error</strong>: reflects Bias, NOT variance</li>
      <li><strong>Test Error</strong>: reflects Both</li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue">Reducing the Bias and Variance, and Irreducible Err:</strong>
    <ul>
      <li><strong>Adding Good Feature</strong>:
        <ul>
          <li>Decrease Bias</li>
        </ul>
      </li>
      <li><strong>Adding Bad Feature</strong>:
        <ul>
          <li>Doesn’t affect (increase) Bias much</li>
        </ul>
      </li>
      <li><strong>Adding ANY Feature</strong>:
        <ul>
          <li>Increases Variance</li>
        </ul>
      </li>
      <li><strong>Adding more Data</strong>:
        <ul>
          <li>Decreases Variance</li>
          <li>(May) Decreases Bias: if \(h\) can fit \(f\) exactly.</li>
        </ul>
      </li>
      <li><strong>Noise in Test Set</strong>:
        <ul>
          <li>Affects ONLY Irreducible Err</li>
        </ul>
      </li>
      <li><strong>Noise in Training Set</strong>:
        <ul>
          <li>Affects BOTH and ONLY Bias and Variance</li>
        </ul>
      </li>
      <li><strong>Dimensionality Reduction</strong>:
        <ul>
          <li>Decrease Variance (by simplifying models)</li>
        </ul>
      </li>
      <li><strong>Feature Selection</strong>:
        <ul>
          <li>Decrease Variance (by simplifying models)</li>
        </ul>
      </li>
      <li><strong>Regularization</strong>:
        <ul>
          <li>Increase Bias</li>
          <li>Decrease Variance</li>
        </ul>
      </li>
      <li><strong>Increasing # of Hidden Units in ANNs</strong>:
        <ul>
          <li>Decrease Bias</li>
          <li>Increase Variance</li>
        </ul>
      </li>
      <li><strong>Increasing # of Hidden Layers in ANNs</strong>:
        <ul>
          <li>Decrease Bias</li>
          <li>Increase Variance</li>
        </ul>
      </li>
      <li><strong>Increasing \(k\) in K-NN</strong>:
        <ul>
          <li>Increase Bias</li>
          <li>Decrease Variance</li>
        </ul>
      </li>
      <li><strong>Increasing Depth in Decision-Trees</strong>:
        <ul>
          <li>Increase Variance</li>
        </ul>
      </li>
      <li><strong>Boosting</strong>:
        <ul>
          <li>Decreases Bias</li>
        </ul>
      </li>
      <li><strong>Bagging</strong>:
        <ul>
          <li>Reduces Variance</li>
        </ul>
      </li>
      <li>We <strong>Cannot Reduce</strong> the <strong>Irreducible Err</strong></li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue">Application of the Decomposition to Classification:</strong>
 A similar decomposition exists for:
    <ul>
      <li>Classification w/ \(0-1\) loss</li>
      <li>Probabilistic Classification w/ Squared Error</li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue">Bias-Variance Decomposition and Risk (Excess Risk Decomposition):</strong>
 The <strong>Bias-Variance Decomposition</strong> analyzes the behavior of the <em><strong>Expected Risk/Generalization Error</strong></em> for any function \(\hat{f}\):
    <p>$$R(\hat{f}) = \mathrm{E}\left[(y-\hat{f}(x))^{2}\right]=(\operatorname{Bias}[\hat{f}(x)])^{2}+\operatorname{Var}[\hat{f}(x)]+\sigma^{2}$$</p>
    <p>Assuming that \(y = f(x) + \epsilon\).</p>

    <p>The <strong>Bayes Optimal Predictor</strong> is \(f(x) = \mathrm{E}[Y\vert X=x]\).</p>

    <p>The <strong>Excess Risk</strong> is:</p>
    <p>$$\text{ExcessRisk}(\hat{f}) = R(\hat{f}) - R(f)$$</p>

    <p><strong style="color: red">Excess Risk Decomposition:</strong><br />
 We add and subtract the <strong>target function \(f_{\text{target}}=\inf_{h \in \mathcal{H}} I[h]\)</strong> that minimizes the <strong>(true) expected risk</strong>:</p>
    <p>$$\text{ExcessRisk}(\hat{f}) = \underbrace{\left(R(\hat{f}) - R(f_{\text{target}})\right)}_ {\text { estimation error }} + \underbrace{\left(R(f_{\text{target}}) - R(f)\right)}_ {\text { approximation error }}$$</p>

    <p>The <strong>Bias-Variance Decomposition</strong> for <em><strong>Excess Risk:</strong></em></p>
    <ul>
      <li>Re-Writing <strong>Excess Risk</strong>:
        <p>$$\text{ExcessRisk}(\hat{f}) = R(\hat{f}) - R(f) = \mathrm{E}\left[(y-\hat{f}(x))^{2}\right] - \mathrm{E}\left[(y-f(x))^{2}\right]$$</p>
        <p>which is equal to:</p>
        <p>$$R(\hat{f}) - R(f) = \mathrm{E}\left[(f(x)-\hat{f}(x))^{2}\right]$$</p>
      </li>
    </ul>
    <p>$$\mathrm{E}\left[(f(x)-\hat{f}(x))^{2}\right] = (\operatorname{Bias}[\hat{f}(x)])^{2}+\operatorname{Var}[\hat{f}(x)]$$</p>

    <ul>
      <li>if you dont want to mess with stat-jargon; lemme rephrase:<br />
  is the minimizer \({\displaystyle f=\inf_{h \in \mathcal{H}} I[h]}\) where \(I[h]\) is the expected-risk/generalization-error (assume MSE);<br />
  is it \(\overline{f}(\mathbf{x})=\mathbb{E}_ {\mathcal{D}}\left[f^{(\mathcal{D})}(\mathbf{x})\right]\) the average hypothesis over all realizations of \(N\) data-points \(\mathcal{D}_ i\)??</li>
    </ul>
  </li>
</ol>

<hr />

<h2 id="generalization-theory">Generalization Theory</h2>

<ul>
  <li><a href="https://www.cs.princeton.edu/courses/archive/fall17/cos597A/lecnotes/generalize.pdf">Generalization Bounds: PAC-Bayes, Rademacher, ERM, etc. (Notes!)</a></li>
  <li><a href="http://www.offconvex.org/2017/12/08/generalization1/">Deep Learning Generalization (blog!)</a></li>
</ul>

<ol>
  <li>
    <p><strong style="color: SteelBlue">Generalization Theory:</strong></p>

    <p><strong style="color: red">Approaches to (Notions of) Quantitative Description of Generalization Theory:</strong></p>
    <ul>
      <li>VC Dimension</li>
      <li>Rademacher Complexity</li>
      <li>PAC-Bayes Bound</li>
    </ul>

    <p><strong style="color: red">Prescriptive vs Descriptive Theory:</strong></p>
    <ul>
      <li><strong>Prescriptive</strong>: only attaches a label to the problem, without giving any insight into how to solve the problem.</li>
      <li><strong>Descriptive</strong>: describes the problem in detail (e.g. by providing cause) and allows you to solve the problem.</li>
    </ul>

    <p>Generalization Theory Notions consist of attaching a descriptive label to the basic phenomenon of lack of generalization. They are hard to compute for today’s complicated ML models, let alone to use as a guide in designing learning systems.<br />
 <strong style="color: red">Generalization Bounds as Descriptive Labels:</strong></p>
    <ul>
      <li><strong>Rademacher Complexity</strong>:<br />
  <button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Assumptions</button>
        <ul hidden="">
          <li>labels and loss are 0,1,</li>
          <li>the badly generalizing \(h\) predicts perfectly on the training sample \(S\) and<br />
  is completely wrong on the heldout set \(S_2\), meaning:
            <p>$$\Delta_{S}(h)-\Delta_{S_{2}}(h) \approx-1$$</p>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p><strong style="color: SteelBlue">Overfitting:</strong>
 One way to summarize <strong>Overfitting</strong> is:<br />
 <span style="color: purple">discovering patterns in data that do not exist in the intended application</span>.<br />
 The typical case of overfitting “<span style="color: purple">decreasing loss only on the training set and increasing the loss on the validation set</span>” is only one example of this.</p>

    <p>The question then is how we prevent overfitting from occurring.<br />
 The problem is that <span style="color: purple">we cannot know apriori what patterns will generalize from the dataset</span>.</p>

    <p><strong style="color: red">No-Free-Lunch (NFL) Theorem:</strong></p>
    <ul>
      <li><strong>NFL Theorems for Supervised Machine Learning:</strong>
        <ul>
          <li>In his 1996 paper The Lack of A Priori Distinctions Between Learning Algorithms, David Wolpert examines if it is possible to get useful theoretical results with a training data set and a learning algorithm without making any assumptions about the target variable.</li>
          <li><em>Wolpert</em> proves that given a noise-free data set (i.e. no random variation, only trend) and a machine learning algorithm where the cost function is the error rate, all machine learning algorithms are equivalent when assessed with a generalization error rate (the model’s error rate on a validation data set).</li>
          <li>Extending this logic he demonstrates that for any two algorithms, A and B, there are as many scenarios where A will perform worse than B as there are where A will outperform B. This even holds true when one of the given algorithms is random guessing.</li>
          <li>This is because nearly all (non-rote) machine learning algorithms make some assumptions (known as inductive or learning bias) about the relationships between the predictor and target variables, introducing bias into the model.<br />
  The assumptions made by machine learning algorithms mean that some algorithms will fit certain data sets better than others. It also (by definition) means that there will be as many data sets that a given algorithm will not be able to model effectively.<br />
  How effective a model will be is directly dependent on how well the assumptions made by the model fit the true nature of the data.</li>
          <li>Implication: you can’t get good machine learning “for free”.<br />
  You must use knowledge about your data and the context of the world we live in (or the world your data lives in) to select an appropriate machine learning model.<br />
  There is no such thing as a single, universally-best machine learning algorithm, and there are no context or usage-independent (a priori) reasons to favor one algorithm over all others.</li>
        </ul>
      </li>
      <li><strong>NFL Theorem for Search/Optimization:</strong>
        <ul>
          <li>All algorithms that search for an extremum of a cost function perform exactly the same when averaged over all possible cost functions. So, for any search/optimization algorithm, any elevated performance over one class of problems is exactly paid for in performance over another class.</li>
          <li>It state that any two optimization algorithms are equivalent when their performance is averaged across all possible problems.</li>
          <li>Wolpert and McReady essentially say that <span style="color: purple">you need some prior knowledge that is encoded in your algorithm in order to search well</span>.<br />
  They created the theorem to give an easy counter example to researchers that would create a (tweak on an) algorithm and claimed that it would work better on all possible problems. It can’t. There’s a proof. If you have a good algorithm, it must in some way fit on your problem space, and you will have to investigate how.</li>
        </ul>
      </li>
      <li><button class="showText" value="show" onclick="showText_withParent_PopHide(event);"><strong>Explanation through a thought experiment:</strong></button>
        <ul hidden="">
          <li>Suppose you see someone toss a coin and get heads. What is the probability distribution over the next result of the coin toss?<br />
  Did you think heads 50% and tails 50%?<br />
  If so, you’re wrong: the answer is that we don’t know.<br />
  For all we know, the coin could have heads on both sides. Or, it might even obey some strange laws of physics and come out as tails every second toss.<br />
  The point is, <span style="color: purple">there is no way we can extract only patterns that will <strong>generalize</strong></span>: there will always be some scenarios where those patterns will not exist.<br />
  This is the essence of what is called the <strong>No Free Lunch Theorem</strong>: <span style="color: purple">any model that you create will always “overfit” and be completely wrong in some scenarios</span>.</li>
        </ul>
      </li>
      <li><strong>Main Concept:</strong> Generalizing is extrapolating to new data. To do that you need to make assumptions and for problems where the assumptions don’t hold you will be suboptimal.</li>
      <li><strong>Practical Implications</strong>:
        <ul>
          <li>Bias-free learning is futile because a learner that makes no a priori assumptions will have no rational basis for creating estimates when provided new, unseen input data.<br />
  Models are simplifications of a specific component of reality (observed with data). To simplify reality, a machine learning algorithm or statistical model needs to make assumptions and introduce bias (known specifically as inductive or learning bias).<br />
  The assumptions of an algorithm will work for some data sets but fail for others.</li>
          <li>This shows that we need some restriction on \(\mathcal{H}\) even for the realizable PAC setting. We cannot learn arbitrary set of hypothesis, there is no free lunch.</li>
          <li>There is no universal (one that works for all \(\mathcal{H}\)) learning algorithm.<br />
  I.e., if the algorithm \(A\) has no idea about \(\mathcal{H},\) even the singleton hypothesis class \(\mathcal{H}=\{h\}\) (as in the statement of the theorem) is not PAC learnable.</li>
          <li>Why do we have to fix a hypothesis class when coming up with a learning algorithm? Can we <em>“just learn”</em>?<br />
  The NFL theorem formally shows that the answer is <strong>NO</strong>.</li>
          <li>Counter the following claim: “My machine learning algorithm/optimization strategy is the best, always and forever, for all the scenarios”.</li>
          <li>Always check your assumptions before relying on a model or search algorithm.</li>
          <li>There is no “super algorithm” that will work perfectly for all datasets.</li>
          <li>Variable length and redundant encodings are not covered by the NFL theorems. Does Genetic Programming get a pass?</li>
          <li>NFL theorems are like a statistician sitting with his head in the fridge, and his feet in the oven. On average, his temperature is okay! NFL theorems prove that the arithmetic mean of performance is constant over all problems, it doesn’t prove that for other statistics this is the case. There has been an interesting ‘counter’ proof, where a researcher proved that for a particular problem space, a hill-climber would outperform a hill-descender on 90% of the problem instances, and did that by virtue of being exponentially worse on the remaining 10% of the problems. Its average performance abided by the NFL theorem and the two algorithms were equal when looking at mean performance, yet the hill-climber was better in 90% of the problems, i.e., it had a much better median performance. So is there maybe a free appetizer?</li>
          <li>While no one model/algorithm works best for every problem, it may be that one model/algorithm works best for all real-world problems, or all problems that we care about, practically speaking.</li>
        </ul>
      </li>
      <li><a href="http://www.no-free-lunch.org/">No Free Lunch Theorems Main Site</a></li>
      <li><a href="http://www.cs.utah.edu/~bhaskara/courses/theoryml/scribes/lecture3.pdf">No Free Lunch Theorem (Lec Notes)</a></li>
      <li><a href="http://www.cs.cornell.edu/courses/cs6783/2015fa/lec3.pdf">Machine Learning Theory - NFL Theorem (Lec Notes)</a></li>
      <li><a href="http://mlexplained.com/2018/04/24/overfitting-isnt-simple-overfitting-re-explained-with-priors-biases-and-no-free-lunch/">Overfitting isn’t simple: Overfitting Re-explained with Priors, Biases, and No Free Lunch (Blog)</a></li>
      <li><a href="https://artint.info/html/ArtInt_191.html">Learning as Refining the Hypothesis Space (Blog-Paper)</a></li>
      <li><a href="https://community.alteryx.com/t5/Data-Science-Blog/All-Models-Are-Wrong/ba-p/348080">All Models Are Wrong (Blog)</a></li>
      <li><a href="https://www.quora.com/Does-the-no-free-lunch-theorem-also-apply-to-deep-learning">Does the ‘no free lunch’ theorem also apply to deep learning? (Quora)</a></li>
    </ul>
  </li>
</ol>


      <footer class="site-footer">
    <!--   <span class="site-footer-owner"><a href="http://localhost:8889">Ahmad Badary</a> is maintained by <a href="http://ahmedbadary.ml">Ahmad Badary</a>.</span> -->
    
<!--  -->
    <div class="wrapper">
        <div class="footer-col-wrapper">
            <div class="footer-col footer-col-1">
            <span class="site-footer-owner"><a href="http://localhost:8889">Site</a> maintained by <a href="http://ahmedbadary.ml">Ahmad Badary</a>.</span>
    <span class="site-footer-credits">
        <p>
            &copy; 2017. All rights reserved.
        </p> 
    </span>
            </div>
            <div class="footer-col footer-col-2">
            <div><p>         </p></div>
            </div>
            <div class="footer-col footer-col-3">
                <ul class="social-media-list">
                    
                      <li>
                        <a href="https://github.com/AhmedBadary">
                          <i class="fa fa-github"></i> GitHub
                        </a>
                      </li>
                    
                    
                      <li>
                        <a href="https://linkedin.com/in/ahmad-badary-656098121/">
                          <i class="fa fa-linkedin"></i> LinkedIn
                        </a>
                      </li>
                    
                    
                      <li>
                        <a href="https://www.facebook.com/ahmed.thabet.94">
                          <i class="fa fa-facebook"></i> Facebook
                        </a>
                      </li>
                    
                </ul>
            </div>
        </div>
    </div>
<!--  -->
</footer>


    </section>

  </body>
</html>
