<!DOCTYPE html>
<html lang="en-us">
  <head>
  <meta charset="UTF-8">
  <title>Ahmad Badary</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <link rel="stylesheet" href="/css/normalize.css">
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="/css/cayman.css">
  <link rel="stylesheet" href="/css/style.css">
  <link rel="shortcut icon" href="/main_files/favicon.ico" />
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="/css/customStyle.css">
  <title> » Ahmad Badary</title>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
</head>

  <body>
    <nav class="main-nav">
    <a href="https://ahmedbadary.github.io/" class="main-nav-logo">
        <img src="/main_files/logo.png">
    </a>
    <ul id="menu-main" class="main-nav-items">
        <li id="menu-item-1859" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-home menu-item-1859">
            <a href="/">Home</a>
        </li>
        <li id="menu-item-2869" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-2869">
            <a href="/work">Work</a>
        </li>
        <li id="menu-item-1892" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1892">
            <a href="/projects">Projects</a>
        </li>
        <li id="menu-item-1858" class="menu-item menu-item-type-post_type menu-item-object-page current_page_parent menu-item-1858">
            <a href="/blog">Blog</a>
        </li>
        <li id="menu-item-1862" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1862">
            <a href="/about">About</a>
        </li>
    </ul>
</nav>


<section class="page-header">
  <h1 class="project-name">All</h1>
  <h2 class="project-tagline"></h2>
  <a href="/#" class="btn">Home</a>
  <a href="/work" class="btn">Work-Space</a>
  <a href= /work_files/research/dl/theory.html class="btn">Previous</a>
</section>

<!-- <div>
  <ul class="posts">
    
      <li><span>02 Jan 2014</span> &raquo; <a href="/2014/01/02/introducing-Ahmad/">Introducing Ahmad</a></li>
    
  </ul>
</div> -->


    <section class="main-content">
      
      <div class="TOC">
  <h1 id="table-of-contents">Table of Contents</h1>

</div>

<hr />
<hr />

<h2 id="content1">Data Drift/Shift</h2>

<ol>
  <li><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents11">Types of SHIFT:</strong>
    <ul>
      <li>Dataset shift happens when the i.i.d. assumptions are not valid for out problem space</li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents11">Types of SHIFT:</strong>
    <ul>
      <li>
        <p><strong>Covariate Shift:</strong></p>

        <p><img src="https://cdn.mathpix.com/snip/images/q4r9dKK9TcR2xk-GzjpGu9ufWzttm4idv18GCoz86eM.original.fullsize.png" alt="img" width="40%" /></p>
      </li>
      <li>
        <p><strong>Prior Probability Shift</strong>: \(P(x)\)<br />
  <img src="https://cdn.mathpix.com/snip/images/Qoxi81pa3lGfWuHOpMAKeth3IJZxA0UubqgBip2oWTU.original.fullsize.png" alt="img" width="40%" /></p>
      </li>
      <li>
        <p><strong>Covariate Shift</strong>:</p>

        <p><img src="https://cdn.mathpix.com/snip/images/SnCgPXP8t5kPsPh2XMo4elCO87crgEgXzFdmCV3P7vs.original.fullsize.png" alt="img" width="40%" /></p>
      </li>
      <li>
        <p><strong>Internal CS</strong>:<br />
  Researchers found that due to the variation in the distribution of activations from the output of a given hidden layer, which are used as the input to a subsequent layer, the network layers can suffer from covariate shift which can impede the training of deep neural networks.</p>

        <ul>
          <li>
            <p>Covariate shift is the change in the distribution of the covariates specifically, that is, the independent variables. This is normally due to changes in state of latent variables, which could be temporal (even changes to the stationarity of a temporal process), or spatial, or less obvious.</p>
          </li>
          <li>
            <p>IIt introduces BIAS to <em><strong>cross-validation</strong></em></p>
          </li>
        </ul>
      </li>
      <li>
        <p>The problem of dataset shift can stem from the way input features are utilized, the way training and test sets are selected, data sparsity, shifts in the data distribution due to non-stationary environments, and also from changes in the activation patterns within layers of deep neural networks.</p>
      </li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents12">General Data Distribution Shifts:</strong>
    <ul>
      <li><strong>Feature change</strong>, such as when new features are added, older features are removed, or the set of all possible values of a feature changes:
  months to years</li>
      <li>
        <table>
          <tbody>
            <tr>
              <td><strong>Label schema change</strong> is when the set of possible values for Y change. With label shift, P(Y) changes but P(X</td>
              <td>Y) remains the same. With label schema change, both P(Y) and P(X</td>
              <td>Y) change.</td>
            </tr>
          </tbody>
        </table>
        <ul>
          <li><em><strong>CREDIT:</strong></em> * With regression tasks, label schema change could happen because of changes in the possible range of label values. Imagine you’re building a model to predict someone’s credit score. Originally, you used a credit score system that ranged from 300 to 850, but you switched to a new system that ranges from 250 to 900.</li>
        </ul>
      </li>
      <li></li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents13">Causes of SHIFT:</strong><br />
 <img src="https://cdn.mathpix.com/snip/images/kOrj05WvKP5Za6xCtyQNlLv6kDvfErHCPWjVcCkWGt4.original.fullsize.png" alt="img" width="40%" />
    <ul>
      <li>Dataset shift resulting from sample selection bias is especially relevant when dealing with imbalanced classification, because, in highly imbalanced domains, the minority class is particularly sensitive to singular classification errors, due to the typically low number of samples it presents.</li>
    </ul>

    <p><img src="https://cdn.mathpix.com/snip/images/COPV9jUSmrT5WdsWaBhrUuzZKrdh2GmlL3uLwREiUqw.original.fullsize.png" alt="img" width="40%" /></p>

    <ul>
      <li><strong style="color: red">IN CREDIT</strong>
        <ul>
          <li>Non-stationary due to changing macro-economic state</li>
          <li>Adversarial Relationship / Fraud: people might try to game the system to get loans</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents14">Handling Data Distribution Shifts:</strong>
    <ol>
      <li>DETECTION</li>
      <li>HANDLING</li>
    </ol>

    <ul>
      <li><strong style="color: blue">DETECTION:</strong><br />
  <img src="https://cdn.mathpix.com/snip/images/Doczy4vS_FRRoH_miFbRcraLdzirPt8RHaWrNym5ReU.original.fullsize.png" alt="img" width="40%" /><br />
  <img src="https://cdn.mathpix.com/snip/images/38j2j-H44tajU8k5Khp1RVA7C42q1-Gb2Ga0LD2BDZ0.original.fullsize.png" alt="img" width="40%" />
        <ul>
          <li>monitor your model’s accuracy-related metrics30 in production to see whether they have changed.
            <ul>
              <li>
                <table>
                  <tbody>
                    <tr>
                      <td>When ground truth labels are unavailable or too delayed to be useful, we can monitor other distributions of interest instead. The distributions of interest are the input distribution P(X), the label distribution P(Y), and the conditional distributions P(X</td>
                      <td>Y) and P(Y</td>
                      <td>X).</td>
                    </tr>
                  </tbody>
                </table>
              </li>
              <li>In research, there have been efforts to understand and detect label shifts without labels from the target distribution. One such effort is Black Box Shift Estimation by Lipton et al., 2018.</li>
            </ul>
          </li>
          <li><strong style="color: blue">Statistical Methods:</strong>
            <ul>
              <li>a simple method many companies use to detect whether the two distributions are the same is to compare their statistics like mean, median, variance, quantiles, skewness, kurtosis, etc. (bad)
                <ul>
                  <li>If those metrics differ significantly, the inference distribution might have shifted from the training distribution. However, if those metrics are similar, there’s no guarantee that there’s no shift.</li>
                </ul>
              </li>
              <li></li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<h2 id="content2">Encoding</h2>

<ol>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents21">Linear Algebra:</strong></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents22">Linear Algebra:</strong></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents23">Linear Algebra:</strong></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents24">Linear Algebra:</strong></p>
  </li>
</ol>

<h2 id="content3">Feature Importance</h2>

<ol>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents31">Linear Algebra:</strong></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents32">Linear Algebra:</strong></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents33">Linear Algebra:</strong></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents34">Linear Algebra:</strong></p>
  </li>
</ol>

<h2 id="content4">Feature Selection</h2>

<ol>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents4" id="bodyContents41">Linear Algebra:</strong></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents4" id="bodyContents42">Linear Algebra:</strong></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents4" id="bodyContents43">Linear Algebra:</strong></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents4" id="bodyContents44">Linear Algebra:</strong></p>
  </li>
</ol>

<h2 id="content5">Data Preprocessing and Normalization</h2>

<ol>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents5" id="bodyContents51">Linear Algebra:</strong></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents5" id="bodyContents52">Linear Algebra:</strong></p>

    <p id="lst-p"><strong style="color: red">Data Regularization:</strong></p>
    <ul>
      <li>The <strong>Design Matrix</strong> contains sample points in each <em><strong>row</strong></em></li>
      <li><strong>Feature Scaling/Mean Normalization (of data)</strong>:
        <ul>
          <li>Define the mean \(\mu_j\) of each feature of the datapoints \(x^{(i)}\):</li>
        </ul>
        <p>$$\mu_{j}=\frac{1}{m} \sum_{i=1}^{m} x_{j}^{(i)}$$</p>
        <ul>
          <li>Replace each \(x_j^{(i)}\) with \(x_j - \mu_j\)</li>
        </ul>
      </li>
      <li><strong>Centering</strong>:  subtracting \(\mu\) from each row of \(X\)</li>
      <li><strong>Sphering</strong>:  applying the transform \(X' = X \Sigma^{-1/2}\)</li>
      <li><strong>Whitening</strong>:  Centering + Sphering (also known as <em><strong>Decorrelating feature space</strong></em>)</li>
    </ul>

    <p><strong>Why Normalize the Data/Signal?</strong><br />
 <img src="https://cdn.mathpix.com/snip/images/8aNuJetgTgCtv4pvqaI0dr96pDyUmfuX_d1aLK1lmaw.original.fullsize.png" alt="img" width="40%" /></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents5" id="bodyContents53">Linear Algebra:</strong></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents5" id="bodyContents54">CODE:</strong></p>
    <ul>
      <li><a href="https://hadrienj.github.io/posts/Preprocessing-for-deep-learning/">Preprocessing for deep learning: from covariance matrix to image whitening (Blog)</a>
        <ul>
          <li><a href="https://github.com/hadrienj/Preprocessing-for-deep-learning">NOTEBOOK</a></li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<h2 id="content6">Validation &amp; Evaluation - ROC, AUC, Reject Inference + Off-policy Evaluation</h2>

<ol>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents6" id="bodyContents61">Linear Algebra:</strong></p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents6" id="bodyContents62">ROC:</strong>
    <ul>
      <li>A way to quantify how good a <strong>binary classifier</strong> separates two classes</li>
      <li>True-Positive-Rate / False-Positive-Rate</li>
      <li>Good classifier has a ROC curve that is near the top-left diagonal (hugging it)</li>
      <li>A Bad Classifier has a ROC curve that is close to the diagonal line</li>
      <li>It allows you to set the <strong>classification threshold</strong>
        <ul>
          <li>You can minimize False-positive rate or maximize the True-Positive Rate</li>
        </ul>
      </li>
    </ul>

    <p id="lst-p"><strong style="color: red">Notes:</strong></p>
    <ul>
      <li>ROC curve is monotone increasing from 0 to 1 and is invariant to any monotone transformation of test results.</li>
      <li>ROC curves (&amp; AUC) are useful even if the <strong>predicted probabilities</strong> are not <em><strong>“properly calibrated”</strong></em></li>
      <li>ROC curves are not affected by monotonically increasing functions</li>
      <li><a href="https://builtin.com/data-science/roc-curves-auc">Scale and Threshold Invariance (Blog)</a></li>
      <li>Accuracy is neither a threshold-invariant metric nor a scale-invariant metric.</li>
      <li>
        <p>When to use <strong>PRECISION</strong>: when data is <em><strong>imbalanced</strong></em> E.G. when the number of <em><strong>negative</strong></em> examples is <strong>larger</strong> than <em><strong>positive</strong></em>.<br />
  Precision does not include <strong>TN (True Negatives)</strong> so NOT AFFECTED.<br />
  In PRACTICE, e.g. studying <em><strong>RARE Disease</strong></em>.<br />
 <br /></p>
      </li>
      <li>ROC Curve only cares about the <em><strong>ordering</strong></em> of the scores, not the values.</li>
      <li>
        <p><strong>Probability Calibration</strong> and ROC: The calibration doesn’t change the order of the scores, it just scales them to make a better match, and the ROC score only cares about the ordering of the scores.</p>
      </li>
      <li>
        <p><a href="https://kiwidamien.github.io/what-is-a-roc-curve-a-visualization-with-credit-scores.html">ROC and Credit Score Example (Blog)</a></p>
      </li>
      <li><strong>AUC</strong>: The AUC is also the probability that a randomly selected positive example has a higher score than a randomly selected negative example.</li>
    </ul>

    <p><button class="showText" value="show" onclick="showTextPopHide(event);">AUC Reliability (Equal AUC - different models)</button>
 <img src="https://cdn.mathpix.com/snip/images/GAyvZvN61xzDjklTeVepqrYYuWrXXfPnEHkNwM80p6k.original.fullsize.png" alt="img" width="100%" hidden="" /></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents6" id="bodyContents63">AUC:</strong><br />
 AUC provides an aggregate measure of performance across all possible classification thresholds. One way of interpreting AUC is as the probability that the model ranks a random positive example more highly than a random negative example.</p>

    <ul>
      <li>
        <p>Range \(= 0.5 - 1.0\), from poor to perfect</p>
      </li>
      <li><strong style="color: blue">Pros:</strong>
        <ul>
          <li>AUC is <em><strong>scale-invariant</strong></em>: It measures how well predictions are ranked, rather than their absolute values.</li>
          <li>AUC is <em><strong>classification-threshold-invariant</strong></em>: It measures the quality of the model’s predictions irrespective of what classification threshold is chosen.</li>
        </ul>

        <blockquote>
          <p>These properties make AUC pretty valuable for evaluating binary classifiers as it provides us with a way to compare them without caring about the classification threshold.</p>
        </blockquote>
      </li>
      <li>
        <p><strong style="color: red">Cons</strong> <br />
  However, both these reasons come with caveats, which may limit the usefulness of AUC in certain use cases:</p>

        <ul>
          <li>
            <p>Scale invariance is not always desirable. For example, sometimes we really do need well calibrated probability outputs, and AUC won’t tell us about that.</p>
          </li>
          <li>
            <p>Classification-threshold invariance is not always desirable. In cases where there are wide disparities in the cost of false negatives vs. false positives, it may be critical to minimize one type of classification error. For example, when doing email spam detection, you likely want to prioritize minimizing false positives (even if that results in a significant increase of false negatives). AUC isn’t a useful metric for this type of optimization.</p>
          </li>
        </ul>
      </li>
    </ul>

    <p id="lst-p"><strong style="color: red">Notes:</strong></p>
    <ul>
      <li><strong>Partial AUC</strong> can be used when only a portion of the entire ROC curve needs to be considered.<br />
 <br /></li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents6" id="bodyContents64">Reject Inference and Off-policy Evaluation:</strong>
    <ul>
      <li>
        <p><strong>Reject inference</strong> is a method for performing off-policy evaluation, which is a way to estimate the performance of a policy (a decision-making strategy) based on data generated by a different policy. In reject inference, the idea is to use importance sampling to weight the data in such a way that the samples generated by the behavior policy (the one that generated the data) are down-weighted, while the samples generated by the target policy (the one we want to evaluate) are up-weighted. This allows us to focus on the samples that are most relevant to the policy we are trying to evaluate, which can improve the accuracy of our estimates.</p>
      </li>
      <li>
        <p><strong>Off-policy evaluation</strong> is useful in situations where it is not possible or practical to directly evaluate the performance of a policy. For example, in a real-world setting, it may not be possible to directly evaluate a new policy because it could be risky or expensive to implement. In such cases, off-policy evaluation can help us estimate the performance of the policy using data generated by a different, perhaps safer or more easily implemented, policy.</p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents6" id="bodyContents65">Validation:</strong></p>

    <ol>
      <li>
        <p>Validate a model with given constraints (see above).</p>

        <p>Spoke to the recruiter who was super nice and transparent. Scheduled a technical screening afterwards. The question was to validate a model only knowing the true values and predicted values. The interviewer wanted to incorporate the business value of the model. I found this to be interesting and odd as how can the business value validate any model. As we walked through the problem, the interviewer did not care about traditional statistical error measures and techniques in model validation. The interviewer wanted to incorporate the business cases (i.e. <strong style="color: goldenrod">total loss in revenue and gains</strong>) to validate the model. To me, it felt more business intelligence rather than traditional statistics/machine learning model validation. I am uncertain if data scientists at Affirm are just BI with Python skills.</p>
      </li>
    </ol>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents6" id="bodyContents66">Evaluation:</strong>
    <ul>
      <li><strong style="color: blue">Precision vs Recall Tradeoff:</strong>
        <ul>
          <li>
            <p><strong>Recall</strong> is more important where Overlooked Cases (False Negatives) are more costly than False Alarms (False Positive). The focus in these problems is finding the positive cases.</p>
          </li>
          <li>
            <p><strong>Precision</strong> is more important where False Alarms (False Positives) are more costly than Overlooked Cases (False Negatives). The focus in these problems is in weeding out the negative cases.</p>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<h2 id="content7">Regularization</h2>

<ol>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents7" id="bodyContents71">Regularization:</strong></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents7" id="bodyContents72">Norms:</strong></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents7" id="bodyContents73">AUC:</strong></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents7" id="bodyContents74">Linear Algebra:</strong></p>
  </li>
</ol>

<h2 id="content8">Interpretability</h2>

<ol>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents8" id="bodyContents81">Regularization:</strong></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents8" id="bodyContents82">Norms:</strong></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents8" id="bodyContents83">AUC:</strong></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents8" id="bodyContents84">Linear Algebra:</strong></p>
  </li>
</ol>

<h2 id="content9">Decision Trees, Random Forests, XGB, and Gradient Boosting</h2>

<ol>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents9" id="bodyContents91">Regularization:</strong></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents9" id="bodyContents92">Norms:</strong></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents9" id="bodyContents93">AUC:</strong></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents9" id="bodyContents94">Boosting:</strong></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents9" id="bodyContents95">Boosting:</strong></p>
    <ul>
      <li><strong>Boosting</strong>: create different hypothesis \(h_i\)s sequentially + make each new hypothesis <strong>decorrelated</strong> with previous hypothesis.
        <ul>
          <li>Assumes that this will be combined/ensembled</li>
          <li>Ensures that each new model/hypothesis will give a different/independent output</li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<h2 id="content10"><a href="/work_files/calibration">Uncertainty and Probabilistic Calibration</a></h2>

<ul>
  <li><a href="/work_files/calibration">CALIBRATION (website)</a></li>
</ul>

<ol>
  <li><strong style="color: SteelBlue" class="bodyContents10" id="bodyContents101">Uncertainty:</strong>
    <ul>
      <li><strong style="color: red">Aleatoric vs Epistemic:</strong><br />
  Aleatoric uncertainty and epistemic uncertainty are two types of uncertainty that can arise in statistical modeling and machine learning. Aleatoric uncertainty is a type of uncertainty that arises from randomness or inherent noise in the data. It is inherent to the system being studied and cannot be reduced through additional data or better modeling. On the other hand, epistemic uncertainty is a type of uncertainty that arises from incomplete or imperfect knowledge about the system being studied. It can be reduced through additional data or better modeling.</li>
    </ul>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents10" id="bodyContents102">Model Uncertainty, Softmax, and Dropout:</strong> <br />
 <strong style="color: red">Interpreting Softmax Output Probabilities:</strong><br />
 Softmax outputs only measure <a href="https://en.wikipedia.org/wiki/Uncertainty_quantification#Aleatoric_and_epistemic_uncertainty"><strong>Aleatoric Uncertainty</strong></a>.<br />
 In the same way that in regression, a NN with two outputs, one representing mean and one variance, that parameterise a Gaussian, can capture aleatoric uncertainty, even though the model is deterministic.<br />
 Bayesian NNs (dropout included), aim to capture epistemic (aka model) uncertainty.</p>

    <p><strong style="color: red">Dropout for Measuring Model (epistemic) Uncertainty:</strong><br />
 Dropout can give us principled uncertainty estimates.<br />
 Principled in the sense that the uncertainty estimates basically approximate those of our <a href="/work_files/research/dl/archits/nns#bodyContents13">Gaussian process</a>.</p>

    <p id="lst-p"><strong>Theoretical Motivation:</strong> dropout neural networks are identical to <span style="color: purple">variational inference in Gaussian processes</span>.<br />
 <strong>Interpretations of Dropout:</strong></p>
    <ul>
      <li>Dropout is just a diagonal noise matrix with the diagonal elements set to either 0 or 1.</li>
      <li><a href="http://mlg.eng.cam.ac.uk/yarin/blog_3d801aa532c1ce.html">What My Deep Model Doesn’t Know (Blog! - Yarin Gal)</a><br />
 <br /></li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents10" id="bodyContents103">Calibration in Deep Networks:</strong>
    <ul>
      <li><a href="http://alondaks.com/2017/12/31/the-importance-of-calibrating-your-deep-model/">READ THIS (Blog!)</a></li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents10" id="bodyContents104">Linear Algebra:</strong></li>
</ol>

<h2 id="content11">Extra: Bandit, bootstrapping, and prediction interval estimation, Linear Models in Credit</h2>

<ol>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents11" id="bodyContents111">Bandit:</strong></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents11" id="bodyContents112">bootstrapping:</strong></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents11" id="bodyContents113">prediction interval estimation:</strong></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents11" id="bodyContents114">Linear Models in Credit Analysis:</strong><br />
 <img src="https://cdn.mathpix.com/snip/images/fa_yKNL9BXfeHhGkOvnXhNmgSYR8TF0B-hCfZn-0whc.original.fullsize.png" alt="img" width="40%" />  s</p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents11" id="bodyContents115">Errors vs Residuals:</strong><br />
 The <strong>Error</strong> of an observed value is the deviation of the observed value from the (unobservable) <strong><em>true</em></strong> value of a quantity of interest.</p>

    <p>The <strong>Residual</strong> of an observed value is the difference between the observed value and the <em><strong>estimated</strong></em> value of the quantity of interest.</p>
  </li>
</ol>

<h2 id="content12">Notes from Affirm Blog</h2>

<ol>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents12" id="bodyContents121">Bandit:</strong></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents12" id="bodyContents122">bootstrapping:</strong></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents12" id="bodyContents123">prediction interval estimation:</strong></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents12" id="bodyContents124">Linear Models in Credit Analysis:</strong><br />
 <img src="https://cdn.mathpix.com/snip/images/fa_yKNL9BXfeHhGkOvnXhNmgSYR8TF0B-hCfZn-0whc.original.fullsize.png" alt="img" width="40%" />  s</p>
  </li>
</ol>

<hr />
<hr />

<p><img src="https://cdn.mathpix.com/snip/images/TNzZfbJuHJsESt3Ds0LpsEBVdRsi56VBP8RK7r54Vc0.original.fullsize.png" alt="img" width="80%" /></p>

<h2 id="content1">Information Theory</h2>

<ol>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents11">Information Theory:</strong><br />
 <strong>Information theory</strong> is a branch of applied mathematics that revolves around quantifying how much information is present in a signal.<br />
 In the context of machine learning, we can also apply information theory to continuous variables where some of these message length interpretations do not apply, instead, we mostly use a few key ideas from information theory to characterize probability distributions or to quantify similarity between probability distributions.<br />
 <br /></p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents12">Motivation and Intuition:</strong><br />
 The basic intuition behind information theory is that learning that an unlikely event has occurred is more informative than learning that a likely event has occurred. A message saying “the sun rose this morning” is so uninformative as to be unnecessary to send, but a message saying “there was a solar eclipse this morning” is very informative.<br />
 Thus, information theory quantifies information in a way that formalizes this intuition:
    <ul>
      <li>Likely events should have low information content - in the extreme case, guaranteed events have no information at all</li>
      <li>Less likely events should have higher information content</li>
      <li>Independent events should have additive information. For example, finding out that a tossed coin has come up as heads twice should convey twice as much information as finding out that a tossed coin has come up as heads once.<br />
 <br /></li>
    </ul>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents133">Measuring Information:</strong><br />
In Shannons Theory, to <strong>transmit \(1\) bit of information</strong> means to <strong>divide the recipients <em>Uncertainty</em> by a factor of \(2\)</strong>.</p>

    <p>Thus, the <strong>amount of information</strong> transmitted is the <strong>logarithm</strong> (base \(2\)) of the <strong>uncertainty reduction factor</strong>.</p>

    <p>The <strong>uncertainty reduction factor</strong> is just the <strong>inverse of the probability</strong> of the event being communicated.</p>

    <p>Thus, the <strong>amount of information</strong> in an event \(\mathbf{x} = x\), called the <em><strong>Self-Information</strong></em>  is:</p>
    <p>$$I(x) = \log (1/p(x)) = -\log(p(x))$$</p>

    <p><strong>Shannons Entropy:</strong><br />
It is the <strong>expected amount of information</strong> of an uncertain/stochastic source. It acts as a measure of the amount of <em><strong>uncertainty</strong></em> of the events.<br />
Equivalently, the amount of information that you get from one sample drawn from a given probability distribution \(p\).<br />
<br /></p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents13">Self-Information:</strong><br />
 The <strong>Self-Information</strong> or <strong>surprisal</strong> is a synonym for the surprise when a random variable is sampled.<br />
 The <strong>Self-Information</strong> of an event \(\mathrm{x} = x\):
    <p>$$I(x) = - \log P(x)$$</p>
    <p>Self-information deals only with a single outcome.</p>

    <p><button class="showText" value="show" onclick="showTextPopHide(event);">Graph \(\log_2{1/x}\)</button>
 <img src="https://cdn.mathpix.com/snip/images/NQ7sPTwgM1cNWpDRoclXezOYWAE8lttajOy5ofO3UQ4.original.fullsize.png" alt="img" width="28%" hidden="" /><br />
 <br /></p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents14">Shannon Entropy:</strong><br />
 To quantify the amount of uncertainty in an entire probability distribution, we use <strong>Shannon Entropy</strong>.<br />
 <strong>Shannon Entropy</strong> is defined as the average amount of information produced by a stochastic source of data.
    <p>$$H(x) = {\displaystyle \operatorname {E}_{x \sim P} [I(x)]} = - {\displaystyle \operatorname {E}_{x \sim P} [\log P(X)] = -\sum_{i=1}^{n} p\left(x_{i}\right) \log p\left(x_{i}\right)}$$</p>
    <p><strong>Differential Entropy</strong> is Shannons entropy of a <strong>continuous</strong> random variable \(x\).<br />
 <img src="/main_files/math/prob/11.png" alt="img" width="60%" /><br />
 <br /></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents15">Distributions and Entropy:</strong><br />
 Distributions that are nearly deterministic (where the outcome is nearly certain) have low entropy; distributions that are closer to uniform have high entropy.
 <br /></p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents16">Relative Entropy | KL-Divergence:</strong><br />
 The <strong>Kullback–Leibler divergence</strong> (<strong>Relative Entropy</strong>) is a measure of how one probability distribution diverges from a second, expected probability distribution.<br />
 <strong>Mathematically:</strong>
    <p>$${\displaystyle D_{\text{KL}}(P\parallel Q)=\operatorname{E}_{x \sim P} \left[\log \dfrac{P(x)}{Q(x)}\right]=\operatorname{E}_{x \sim P} \left[\log P(x) - \log Q(x)\right]}$$</p>
    <ul>
      <li><strong>Discrete</strong>:</li>
    </ul>
    <p>$${\displaystyle D_{\text{KL}}(P\parallel Q)=\sum_{i}P(i)\log \left({\frac {P(i)}{Q(i)}}\right)}$$  </p>
    <ul>
      <li><strong>Continuous</strong>:</li>
    </ul>
    <p>$${\displaystyle D_{\text{KL}}(P\parallel Q)=\int_{-\infty }^{\infty }p(x)\log \left({\frac {p(x)}{q(x)}}\right)\,dx,}$$ </p>

    <p id="lst-p"><strong>Interpretation:</strong></p>
    <ul>
      <li><strong>Discrete variables</strong>:<br />
  it is the extra amount of information needed to send a message containing symbols drawn from probability distribution \(P\), when we use a code that was designed to minimize the length of messages drawn from probability distribution \(Q\).</li>
      <li><strong>Continuous variables</strong>:</li>
    </ul>

    <p id="lst-p"><strong>Properties:</strong></p>
    <ul>
      <li>Non-Negativity:<br />
      \({\displaystyle D_{\mathrm {KL} }(P\|Q) \geq 0}\)</li>
      <li>\({\displaystyle D_{\mathrm {KL} }(P\|Q) = 0 \iff}\) \(P\) and \(Q\) are:
        <ul>
          <li><em><strong>Discrete Variables</strong></em>:<br />
      the same distribution</li>
          <li><em><strong>Continuous Variables</strong></em>:<br />
      equal “almost everywhere”</li>
        </ul>
      </li>
      <li>Additivity of <em>Independent Distributions</em>:<br />
      \({\displaystyle D_{\text{KL}}(P\parallel Q)=D_{\text{KL}}(P_{1}\parallel Q_{1})+D_{\text{KL}}(P_{2}\parallel Q_{2}).}\)</li>
      <li>\({\displaystyle D_{\mathrm {KL} }(P\|Q) \neq D_{\mathrm {KL} }(Q\|P)}\)
        <blockquote>
          <p>This asymmetry means that there are important consequences to the choice of the ordering</p>
        </blockquote>
      </li>
      <li>Convexity in the pair of PMFs \((p, q)\) (i.e. \({\displaystyle (p_{1},q_{1})}\) and  \({\displaystyle (p_{2},q_{2})}\) are two pairs of PMFs):<br />
      \({\displaystyle D_{\text{KL}}(\lambda p_{1}+(1-\lambda )p_{2}\parallel \lambda q_{1}+(1-\lambda )q_{2})\leq \lambda D_{\text{KL}}(p_{1}\parallel q_{1})+(1-\lambda )D_{\text{KL}}(p_{2}\parallel q_{2}){\text{ for }}0\leq \lambda \leq 1.}\)</li>
    </ul>

    <p><strong>KL-Div as a Distance:</strong><br />
 Because the KL divergence is non-negative and measures the difference between two distributions, it is often conceptualized as measuring some sort of distance between these distributions.<br />
 However, it is <strong>not</strong> a true distance measure because it is <strong><em>not symmetric</em></strong>.</p>
    <blockquote>
      <p>KL-div is, however, a <em><strong>Quasi-Metric</strong></em>, since it satisfies all the properties of a distance-metric except symmetry</p>
    </blockquote>

    <p id="lst-p"><strong>Applications</strong><br />
 Characterizing:</p>
    <ul>
      <li>Relative (Shannon) entropy in information systems</li>
      <li>Randomness in continuous time-series</li>
      <li>It is a measure of <strong>Information Gain</strong>; used when comparing statistical models of inference</li>
    </ul>

    <p><strong>Example Application and Direction of Minimization</strong><br />
 Suppose we have a distribution \(p(x)\) and we wish to <em>approximate</em> it with another distribution \(q(x)\).<br />
 We have a choice of <em>minimizing</em> either:</p>
    <ol>
      <li>\({\displaystyle D_{\text{KL}}(p\|q)} \implies q^\ast = \operatorname {arg\,min}_q {\displaystyle D_{\text{KL}}(p\|q)}\)<br />
 Produces an approximation that usually places high probability anywhere that the true distribution places high probability.</li>
      <li>\({\displaystyle D_{\text{KL}}(q\|p)} \implies q^\ast \operatorname {arg\,min}_q {\displaystyle D_{\text{KL}}(q\|p)}\)<br />
 Produces an approximation that rarely places high probability anywhere that the true distribution places low probability.
        <blockquote>
          <p>which are different due to the <em>asymmetry</em> of the KL-divergence</p>
        </blockquote>
      </li>
    </ol>

    <p><button class="showText" value="show" onclick="showTextPopHide(event);">Choice of KL-div Direction</button>
 <img src="/main_files/math/infothry/1.png" alt="img" width="100%" hidden="" />
 <br /></p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents17">Cross Entropy:</strong><br />
 The <strong>Cross Entropy</strong> between two probability distributions \({\displaystyle p}\) and \({\displaystyle q}\) over the same underlying set of events measures the average number of bits needed to identify an event drawn from the set, if a coding scheme is used that is optimized for an “unnatural” probability distribution \({\displaystyle q}\), rather than the “true” distribution \({\displaystyle p}\).
    <p>$$H(p,q) = \operatorname{E}_{p}[-\log q]= H(p) + D_{\mathrm{KL}}(p\|q) =-\sum_{x }p(x)\,\log q(x)$$</p>

    <p>It is similar to <strong>KL-Div</strong> but with an additional quantity - the entropy of \(p\).</p>

    <p>Minimizing the cross-entropy with respect to \(Q\) is equivalent to minimizing the KL divergence, because \(Q\) does not participate in the omitted term.</p>

    <p>We treat \(0 \log (0)\) as \(\lim_{x \to 0} x \log (x) = 0\).<br />
 <br /></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents18">Mutual Information:</strong><br />
 The <strong>Mutual Information (MI)</strong> of two random variables is a measure of the mutual dependence between the two variables.<br />
 More specifically, it quantifies the “amount of information” (in bits) obtained about one random variable through observing the other random variable.</p>

    <p>It can be seen as a way of measuring the reduction in uncertainty (information content) of measuring a part of the system after observing the outcome of another parts of the system; given two R.Vs, knowing the value of one of the R.Vs in the system gives a corresponding reduction in (the uncertainty (information content) of) measuring the other one.</p>

    <p><strong>As KL-Divergence:</strong><br />
 Let \((X, Y)\) be a pair of random variables with values over the space \(\mathcal{X} \times \mathcal{Y}\) . If their joint distribution is \(P_{(X, Y)}\) and the marginal distributions are \(P_{X}\) and \(P_{Y},\) the mutual information is defined as:</p>
    <p>$$I(X ; Y)=D_{\mathrm{KL}}\left(P_{(X, Y)} \| P_{X} \otimes P_{Y}\right)$$</p>

    <p><strong>In terms of PMFs for discrete distributions:</strong><br />
 The mutual information of two jointly discrete random variables \(X\) and \(Y\) is calculated as a double sum:</p>
    <p>$$\mathrm{I}(X ; Y)=\sum_{y \in \mathcal{Y}} \sum_{x \in \mathcal{X}} p_{(X, Y)}(x, y) \log \left(\frac{p_{(X, Y)}(x, y)}{p_{X}(x) p_{Y}(y)}\right)$$</p>
    <p>where \({\displaystyle p_{(X,Y)}}\) is the joint probability mass function of \({\displaystyle X}\) X and \({\displaystyle Y}\), and \({\displaystyle p_{X}}\) and \({\displaystyle p_{Y}}\) are the marginal probability mass functions of \({\displaystyle X}\) and \({\displaystyle Y}\) respectively.</p>

    <p><strong>In terms of PDFs for continuous distributions:</strong><br />
 In the case of jointly continuous random variables, the double sum is replaced by a double integral:</p>
    <p>$$\mathrm{I}(X ; Y)=\int_{\mathcal{Y}} \int_{\mathcal{X}} p_{(X, Y)}(x, y) \log \left(\frac{p_{(X, Y)}(x, y)}{p_{X}(x) p_{Y}(y)}\right) d x d y$$</p>
    <p>where \(p_{(X, Y)}\) is now the joint probability density function of \(X\) and \(Y\) and \(p_{X}\) and \(p_{Y}\) are the marginal probability density functions of \(X\) and \(Y\) respectively.</p>

    <p id="lst-p"><strong style="color: red">Intuitive Definitions:</strong></p>
    <ul>
      <li>Measures the information that \(X\) and \(Y\) share:<br />
  It measures how much knowing one of these variables reduces uncertainty about the other.
        <ul>
          <li><strong>\(X, Y\) Independent</strong>  \(\implies I(X; Y) = 0\): their MI is zero</li>
          <li><strong>\(X\) deterministic function of \(Y\) and vice versa</strong> \(\implies I(X; Y) = H(X) = H(Y)\) their MI is equal to entropy of each variable</li>
        </ul>
      </li>
      <li>It’s a Measure of the inherent dependence expressed in the joint distribution of  \(X\) and  \(Y\) relative to the joint distribution of \(X\) and \(Y\) under the assumption of independence.<br />
  i.e. The price for encoding \({\displaystyle (X,Y)}\) as a pair of independent random variables, when in reality they are not.</li>
    </ul>

    <p id="lst-p"><strong style="color: red">Properties:</strong></p>
    <ul>
      <li>The KL-divergence shows that \(I(X; Y)\) is equal to zero precisely when <span style="color: goldenrod">the joint distribution conicides with the product of the marginals i.e. when </span> <strong style="color: goldenrod">\(X\) and \(Y\) are <em>independent</em></strong>.</li>
      <li>The MI is <strong>non-negative</strong>: \(I(X; Y) \geq 0\)
        <ul>
          <li>It is a measure of the price for encoding \({\displaystyle (X,Y)}\) as a pair of independent random variables, when in reality they are not.</li>
        </ul>
      </li>
      <li>It is <strong>symmetric</strong>: \(I(X; Y) = I(Y; X)\)</li>
      <li><strong>Related to conditional and joint entropies:</strong>
        <p>$${\displaystyle {\begin{aligned}\operatorname {I} (X;Y)&amp;{}\equiv \mathrm {H} (X)-\mathrm {H} (X|Y)\\&amp;{}\equiv \mathrm {H} (Y)-\mathrm {H} (Y|X)\\&amp;{}\equiv \mathrm {H} (X)+\mathrm {H} (Y)-\mathrm {H} (X,Y)\\&amp;{}\equiv \mathrm {H} (X,Y)-\mathrm {H} (X|Y)-\mathrm {H} (Y|X)\end{aligned}}}$$</p>
        <p>where \(\mathrm{H}(X)\) and \(\mathrm{H}(Y)\) are the marginal entropies, \(\mathrm{H}(X | Y)\) and \(\mathrm{H}(Y | X)\) are the conditional entopries, and \(\mathrm{H}(X, Y)\) is the joint entropy of \(X\) and \(Y\).</p>
        <ul>
          <li>Note the <em>analogy to the <strong>union, difference, and intersection of two sets</strong></em>:<br />
  <img src="https://cdn.mathpix.com/snip/images/aT2_JfK4TlRP9b5JawVqQigLD7dzxOrFjDIapoSF-F4.original.fullsize.png" alt="img" width="35%" class="center-image" /></li>
        </ul>
      </li>
      <li><strong>Related to KL-div of conditional distribution:</strong>
        <p>$$\mathrm{I}(X ; Y)=\mathbb{E}_{Y}\left[D_{\mathrm{KL}}\left(p_{X | Y} \| p_{X}\right)\right]$$</p>
      </li>
      <li><a href="https://www.youtube.com/watch?v=U9h1xkNELvY">MI (tutorial #1)</a></li>
      <li><a href="https://www.youtube.com/watch?v=d7AUaut6hso">MI (tutorial #2)</a></li>
    </ul>

    <p><strong style="color: red">Applications:</strong><br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">Show Lists</button></p>
    <ul hidden="">
      <li>In search engine technology, mutual information between phrases and contexts is used as a feature for k-means clustering to discover semantic clusters (concepts)</li>
      <li>Discriminative training procedures for hidden Markov models have been proposed based on the maximum mutual information (MMI) criterion.</li>
      <li>Mutual information has been used as a criterion for feature selection and feature transformations in machine learning. It can be used to characterize both the relevance and redundancy of variables, such as the minimum redundancy feature selection.</li>
      <li>Mutual information is used in determining the similarity of two different clusterings of a dataset. As such, it provides some advantages over the traditional Rand index.</li>
      <li>Mutual information of words is often used as a significance function for the computation of collocations in corpus linguistics.</li>
      <li>Detection of phase synchronization in time series analysis</li>
      <li>The mutual information is used to learn the structure of Bayesian networks/dynamic Bayesian networks, which is thought to explain the causal relationship between random variables</li>
      <li>Popular cost function in decision tree learning.</li>
      <li>In the infomax method for neural-net and other machine learning, including the infomax-based Independent component analysis algorithm</li>
    </ul>

    <p><strong style="color: red">Independence assumptions and low-rank matrix approximation (alternative definition):</strong><br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">show analysis</button>
  <img src="https://cdn.mathpix.com/snip/images/jzmGBSoIKS4x2IrykIaQR3P21Y2z8RS_VmZNKkOfjZQ.original.fullsize.png" alt="img" width="100%" hidden="" /></p>

    <p><strong style="color: red">As a Metric (relation to Jaccard distance):</strong><br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">show analysis</button>
 <img src="https://cdn.mathpix.com/snip/images/dOB7qr575sMswJ1MPEbHmFvlqvJp8ncf9ulYlR4aKDY.original.fullsize.png" alt="img" width="100%" hidden="" />
 <br /></p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents19">Pointwise Mutual Information (PMI):</strong><br />
 The PMI of a pair of outcomes \(x\) and \(y\) belonging to discrete random variables \(X\) and \(Y\) quantifies the discrepancy between the probability of their coincidence given their joint distribution and their individual distributions, assuming independence. Mathematically:
    <p>$$\operatorname{pmi}(x ; y) \equiv \log \frac{p(x, y)}{p(x) p(y)}=\log \frac{p(x | y)}{p(x)}=\log \frac{p(y | x)}{p(y)}$$</p>
    <p>In contrast to mutual information (MI) which builds upon PMI, it refers to single events, whereas MI refers to the average of all possible events.<br />
 The mutual information (MI) of the random variables \(X\) and \(Y\) is the expected value of the PMI (over all possible outcomes).</p>
  </li>
</ol>

<h3 id="more">More</h3>
<ul>
  <li><strong>Conditional Entropy</strong>: \(H(X \mid Y)=H(X)-I(X, Y)\)</li>
  <li><strong>Independence</strong>: \(I(X, Y)=0\)</li>
  <li><strong>Independence Relations</strong>:  \(H(X \mid Y)=H(X)\)</li>
</ul>

<hr />
<hr />

<h1 id="data-processing">DATA PROCESSING</h1>

<ul>
  <li>
    <p><a href="https://theprofessionalspoint.blogspot.com/2019/03/data-wrangling-techniques-steps.html">Data Wrangling Techniques (Blog!)</a></p>
  </li>
  <li><a href="http://mlexplained.com/2017/12/28/a-practical-introduction-to-nmf-nonnegative-matrix-factorization/">Non-Negative Matrix Factorization NMF Tutorial</a></li>
  <li>
    <p><a href="https://distill.pub/2016/misread-tsne/">How to Use t-SNE Effectively (distill blog!)</a></p>
  </li>
  <li><a href="https://umap-learn.readthedocs.io/en/latest/">UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction [better than t-sne?] (Library Code!)</a></li>
</ul>

<h2 style="font-size: 1.60em" id="content1">Dimensionality Reduction</h2>

<h3 id="dimensionality-reduction"><strong style="color: SteelBlue; font-size: 1.15em" class="bodyContents1" id="bodyContents11">Dimensionality Reduction</strong></h3>
<p><strong>Dimensionality Reduction</strong> is the process of reducing the number of random variables under consideration by obtaining a set of principal variables. It can be divided into <strong style="color: goldenrod">feature selection</strong> and <strong style="color: goldenrod">feature extraction</strong>.<br />
<br /></p>

<p id="lst-p"><strong>Dimensionality Reduction Methods:</strong></p>
<ul>
  <li>PCA</li>
  <li>Heatmaps</li>
  <li>t-SNE</li>
  <li>Multi-Dimensional Scaling (MDS)</li>
</ul>

<h3 id="t-sne--t-distributed-stochastic-neighbor-embeddings"><strong style="color: SteelBlue; font-size: 1.15em" class="bodyContents1" id="bodyContents12">t-SNE | T-distributed Stochastic Neighbor Embeddings</strong></h3>

<p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Paper</button></p>
<iframe hidden="" src="https://docs.google.com/viewer?url=http://jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf&amp;embedded=true" frameborder="0" height="840" width="646" title="Layer Normalization" scrolling="auto"></iframe>

<p><a href="https://medium.com/@layog/i-dont-understand-t-sne-part-1-50f507acd4f9">Understanding t-SNE Part 1: SNE algorithm and its drawbacks</a><br />
<a href="https://medium.com/@layog/i-do-not-understand-t-sne-part-2-b2f997d177e3">Understanding t-SNE Part 2: t-SNE improvements over SNE</a><br />
<a href="https://wiki.math.uwaterloo.ca/statwiki/index.php?title=visualizing_Data_using_t-SNE">t-SNE (statwiki)</a><br />
<a href="https://www.youtube.com/watch?v=W-9L6v_rFIE">t-SNE tutorial (video)</a><br />
<a href="https://www.youtube.com/watch?v=FQmCzpKWD48&amp;list=PLupD_xFct8mHqCkuaXmeXhe0ajNDu0mhZ">series (deleteme)</a></p>

<p><strong style="color: red">SNE - Stochastic Neighbor Embeddings:</strong><br />
<strong>SNE</strong> is a method that aims to <em>match</em> <strong>distributions of distances</strong> between points in high and low dimensional space via <strong>conditional probabilities</strong>.<br />
It Assumes distances in both high and low dimensional space are <strong>Gaussian-distributed</strong>.</p>
<ul>
  <li><a href="https://www.youtube.com/embed/ohQXphVSEQM?start=130" value="show" onclick="iframePopA(event)"><strong>Algorithm</strong></a>
<a href="https://www.youtube.com/embed/ohQXphVSEQM?start=130"></a>
    <div></div>
    <p><img src="/main_files/dl/concepts/data_proc/2.png" alt="img" width="65%" /><br />
<br /></p>
  </li>
</ul>

<p><strong style="color: red">t-SNE:</strong><br />
<strong>t-SNE</strong> is a machine learning algorithm for visualization developed by Laurens van der Maaten and Geoffrey Hinton.<br />
It is a <em><strong>nonlinear</strong></em> <em><strong>dimensionality reduction</strong></em> technique well-suited for <em>embedding high-dimensional data for visualization in a low-dimensional space</em> of <em>two or three dimensions</em>.<br />
Specifically, it models each high-dimensional object by a two- or three-dimensional point in such a way that <span style="color: goldenrod">similar objects are modeled by nearby points</span> and <span style="color: goldenrod">dissimilar objects are modeled by distant points</span>  <strong>with high probability</strong>.</p>
<blockquote>
  <p>It tends to <em>preserve <strong>local structure</strong></em>, while at the same time, <em>preserving the <strong>global structure</strong></em> as much as possible.</p>
</blockquote>

<p><br /></p>

<p id="lst-p"><strong style="color: red">Stages:</strong></p>
<ol>
  <li>It Constructs a probability distribution over pairs of high-dimensional objects in such a way that similar objects have a high probability of being picked while dissimilar points have an extremely small probability of being picked.</li>
  <li>It Defines a similar probability distribution over the points in the low-dimensional map, and it minimizes the <strong>Kullback–Leibler divergence</strong> between the two distributions with respect to the locations of the points in the map.<br />
<br /></li>
</ol>

<p id="lst-p"><strong style="color: red">Key Ideas:</strong><br />
It solves two big problems that <strong>SNE</strong> faces:</p>
<ol>
  <li><strong>The Crowding Problem:</strong><br />
 The “crowding problem” that are addressed in the paper is defined as: “the area of the two-dimensional map that is available to accommodate moderately distant datapoints will not be nearly large enough compared with the area available to accommodate nearby datepoints”. This happens when the datapoints are distributed in a region on a high-dimensional manifold around i, and we try to model the pairwise distances from i to the datapoints in a two-dimensional map. For example, it is possible to have 11 datapoints that are mutually equidistant in a ten-dimensional manifold but it is not possible to model this faithfully in a two-dimensional map. Therefore, if the small distances can be modeled accurately in a map, most of the moderately distant datapoints will be too far away in the two-dimensional map. In SNE, this will result in very small attractive force from datapoint i to these too-distant map points. The very large number of such forces collapses together the points in the center of the map and prevents gaps from forming between the natural clusters. This phenomena, crowding problem, is not specific to SNE and can be observed in other local techniques such as Sammon mapping as well.
    <ul>
      <li><strong>Solution - Student t-distribution for \(q\)</strong>:<br />
  Student t-distribution is used to compute the similarities between data points in the low dimensional space \(q\).</li>
    </ul>
  </li>
  <li><strong>Optimization Difficulty of KL-div:</strong><br />
 The KL Divergence is used over the conditional probability to calculate the error in the low-dimensional representation. So, the algorithm will be trying to minimize this loss and will calculate its gradient:
    <p>$$\frac{\delta C}{\delta y_{i}}=2 \sum_{j}\left(p_{j | i}-q_{j | i}+p_{i | j}-q_{i | j}\right)\left(y_{i}-y_{j}\right)$$</p>
    <p>This gradient involves all the probabilities for point \(i\) and \(j\). But, these probabilities were composed of the exponentials. The problem is that: We have all these exponentials in our gradient, which can explode (or display other unusual behavior) very quickly and hence the algorithm will take a long time to converge.</p>
    <ul>
      <li><strong>Solution - Symmetric SNE</strong>:<br />
  The Cost Function is a <strong>symmetrized</strong> version of that in SNE. i.e. \(p_{i\vert j} = p_{j\vert i}\) and \(q_{i\vert j} = q_{j\vert i}\).<br />
<br /></li>
    </ul>
  </li>
</ol>

<p><strong style="color: red">Application:</strong><br />
It is often used to visualize high-level representations learned by an <strong>artificial neural network</strong>.<br />
<br /></p>

<p><strong style="color: red">Motivation:</strong><br />
There are a lot of problems with traditional dimensionality reduction techniques that employ <em>feature projection</em>; e.g. <strong>PCA</strong>. These techniques attempt to <em><strong>preserve the global structure</strong></em>, and in that process they <em><strong>lose the local structure</strong></em>. Mainly, projecting the data on one axis or another, may (most likely) not preserve the <em>neighborhood structure</em> of the data; e.g. the clusters in the data:<br />
<img src="/main_files/dl/concepts/data_proc/1.png" alt="img" width="70%" /><br />
t-SNE finds a way to project data into a low dimensional space (1-d, in this case) such that the clustering (“local structure”) in the high dimensional space is preserved.<br />
<br /></p>

<p><strong style="color: red">t-SNE Clusters:</strong><br />
While t-SNE plots often seem to display clusters, the visual clusters can be influenced strongly by the chosen parameterization and therefore a good understanding of the parameters for t-SNE is necessary. Such “clusters” can be shown to even appear in non-clustered data, and thus may be false findings.<br />
It has been demonstrated that t-SNE is often able to <em>recover well-separated clusters</em>, and with special parameter choices, <a href="https://arxiv.org/abs/1706.02582">approximates a simple form of <strong>spectral clustering</strong></a>.<br />
<br /></p>

<p id="lst-p"><strong style="color: red">Properties:</strong></p>
<ul>
  <li>It preserves the <em>neighborhood structure</em> of the data</li>
  <li>Does NOT preserve <em>distances</em> nor <em>density</em></li>
  <li>Only to some extent preserves <em>nearest-neighbors</em>?<br />
  <a href="https://stats.stackexchange.com/questions/263539/clustering-on-the-output-of-t-sne/264647#264647">discussion</a></li>
  <li>It learns a <strong>non-parametric mapping</strong>, which means that it does NOT learn an <em>explicit function</em> that maps data from the input space to the map<br />
<br /></li>
</ul>

<p><strong style="color: red">Algorithm:</strong><br />
<button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Algorithm Details (wikipedia)</button></p>
<iframe hidden="" src="https://www.wikiwand.com/en/T-distributed_stochastic_neighbor_embedding#/Details" frameborder="0" height="840" width="646" title="Layer Normalization"></iframe>

<p><br /></p>

<p id="lst-p"><strong style="color: red">Issues/Weaknesses/Drawbacks:</strong></p>
<ol>
  <li>The paper only focuses on the date visualization using t-SNE, that is, embedding high-dimensional date into a two- or three-dimensional space. However, this behavior of t-SNE presented in the paper cannot readily be extrapolated to \(d&gt;3\) dimensions due to the heavy tails of the Student t-distribution.</li>
  <li>It might be less successful when applied to data sets with a high intrinsic dimensionality. This is a result of the <em><strong>local linearity assumption</strong> on the manifold</em> that t-SNE makes by employing Euclidean distance to present the similarity between the datapoints.</li>
  <li>The cost function is <strong>not convex</strong>. This leads to the problem that several optimization parameters (hyperparameters) need to be chosen (and tuned) and the constructed solutions depending on these parameters may be different each time t-SNE is run from an initial random configuration of the map points.</li>
  <li>It cannot work <strong>“online”</strong>. Since it learns a non-parametric mapping, which means that it does not learn an explicit function that maps data from the input space to the map. Therefore, it is not possible to embed test points in an existing map. You have to re-run t-SNE on the full dataset.<br />
 A potential approach to deal with this would be to train a multivariate regressor to predict the map location from the input data.<br />
 Alternatively, you could also <a href="https://lvdmaaten.github.io/publications/papers/AISTATS_2009.pdf">make such a regressor minimize the t-SNE loss directly (parametric t-SNE)</a>.</li>
</ol>

<p><br /></p>

<p id="lst-p"><strong style="color: red">t-SNE Optimization:</strong></p>
<ul>
  <li><a href="https://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf">Accelerating t-SNE using Tree-Based Algorithms</a></li>
  <li><a href="https://arxiv.org/pdf/1301.3342.pdf">Barnes-Hut-SNE Optimization</a></li>
</ul>

<p><br /></p>

<p id="lst-p"><strong style="color: red">Discussion and Information:</strong></p>
<ul>
  <li><strong>What is perplexity?</strong><br />
  Perplexity is a measure for information that is defined as 2 to the power of the Shannon entropy. The perplexity of a fair die with k sides is equal to k. In t-SNE, the perplexity may be viewed as a knob that sets the number of effective nearest neighbors. It is comparable with the number of nearest neighbors k that is employed in many manifold learners.</li>
  <li><strong>Choosing the perplexity hp:</strong> <br />
  The performance of t-SNE is fairly robust under different settings of the perplexity. The most appropriate value depends on the density of your data. Loosely speaking, one could say that a larger / denser dataset requires a larger perplexity. Typical values for the perplexity range between \(5\) and \(50\).</li>
  <li><strong>Every time I run t-SNE, I get a (slightly) different result?</strong><br />
  In contrast to, e.g., PCA, t-SNE has a non-convex objective function. The objective function is minimized using a gradient descent optimization that is initiated randomly. As a result, it is possible that different runs give you different solutions. Notice that it is perfectly fine to run t-SNE a number of times (with the same data and parameters), and to select the visualization with the lowest value of the objective function as your final visualization.</li>
  <li><strong>Assessing the “Quality of Embeddings/visualizations”:</strong><br />
  Preferably, just look at them! Notice that t-SNE does not retain distances but probabilities, so measuring some error between the Euclidean distances in high-D and low-D is useless. However, if you use the same data and perplexity, you can compare the Kullback-Leibler divergences that t-SNE reports. It is perfectly fine to run t-SNE ten times, and select the solution with the lowest KL divergence.</li>
</ul>

<!-- __Advantages:__{: style="color: red"}  
{: #lst-p}
1. Reduces time and 

### **Feature Selection**{: style="color: SteelBlue; font-size: 1.15em"}{: .bodyContents1 #bodyContents12}   -->
<p><br /></p>

<!--  
### **Asynchronous**{: style="color: SteelBlue; font-size: 1.15em"}{: .bodyContents1 #bodyContents13}  
<br>
### **Asynchronous**{: style="color: SteelBlue; font-size: 1.15em"}{: .bodyContents1 #bodyContents14}  
<br>
### **Asynchronous**{: style="color: SteelBlue; font-size: 1.15em"}{: .bodyContents1 #bodyContents15}  
<br>
### **Asynchronous**{: style="color: SteelBlue; font-size: 1.15em"}{: .bodyContents1 #bodyContents16}  
<br>
### **Asynchronous**{: style="color: SteelBlue; font-size: 1.15em"}{: .bodyContents1 #bodyContents17}  
<br>
### **Asynchronous**{: style="color: SteelBlue; font-size: 1.15em"}{: .bodyContents1 #bodyContents18}  
 -->

<hr />
<hr />

<h2 style="font-size: 1.60em" id="content2">Feature Selection</h2>

<h3 id="feature-selection"><strong style="color: SteelBlue; font-size: 1.15em" class="bodyContents2" id="bodyContents21">Feature Selection</strong></h3>
<p><strong>Feature Selection</strong> is the process of selecting a subset of relevant features (variables, predictors) for use in model construction.</p>

<p id="lst-p"><strong style="color: red">Applications:</strong></p>
<ul>
  <li>Simplification of models to make them easier to interpret by researchers/users</li>
  <li>Shorter training time</li>
  <li>A way to handle <em>curse of dimensionality</em></li>
  <li>Reduction of Variance \(\rightarrow\) Reduce Overfitting \(\rightarrow\) Enhanced Generalization</li>
</ul>

<p id="lst-p"><strong style="color: red">Strategies/Approaches:</strong></p>
<ul>
  <li><strong>Wrapper Strategy</strong>:<br />
  Wrapper methods use a predictive model to score feature subsets. Each new subset is used to train a model, which is tested on a hold-out set. Counting the number of mistakes made on that hold-out set (the error rate of the model) gives the score for that subset. As wrapper methods train a new model for each subset, they are very computationally intensive, but usually provide the best performing feature set for that particular type of model.<br />
  <strong>e.g.</strong> <strong style="color: goldenrod">Search Guided by Accuracy</strong>, <strong style="color: goldenrod">Stepwise Selection</strong></li>
  <li><strong>Filter Strategy</strong>:<br />
  Filter methods use a <em>proxy measure</em> instead of the error rate <em>to score a feature subset</em>. This measure is chosen to be fast to compute, while still capturing the usefulness of the feature set.<br />
  Filter methods produce a feature set which is <em>not tuned to a specific model</em>, usually giving lower prediction performance than a wrapper, but are more general and more useful for exposing the relationships between features.<br />
  <strong>e.g.</strong> <strong style="color: goldenrod">Information Gain</strong>, <strong style="color: goldenrod">pointwise-mutual/mutual information</strong>, <strong style="color: goldenrod">Pearson Correlation</strong></li>
  <li><strong>Embedded Strategy:</strong><br />
  Embedded methods are a catch-all group of techniques which perform feature selection as part of the model construction process.<br />
  <strong>e.g.</strong> <strong style="color: goldenrod">LASSO</strong></li>
</ul>

<p><br /></p>

<h3 id="correlation-feature-selection"><strong style="color: SteelBlue; font-size: 1.15em" class="bodyContents2" id="bodyContents22">Correlation Feature Selection</strong></h3>
<p>The <strong>Correlation Feature Selection (CFS)</strong> measure evaluates subsets of features on the basis of the following hypothesis:<br />
“<strong style="color: goldenrod">Good feature subsets contain features highly correlated with the classification, yet uncorrelated to each other</strong>”.</p>

<p>The following equation gives the <strong>merit of a feature subset</strong> \(S\) consisting of \(k\) features:</p>
<p>$${\displaystyle \mathrm {Merit} _{S_{k}}={\frac {k{\overline {r_{cf}}}}{\sqrt {k+k(k-1){\overline {r_{ff}}}}}}.}$$</p>
<p>where, \({\displaystyle {\overline {r_{cf}}}}\) is the average value of all feature-classification correlations, and \({\displaystyle {\overline {r_{ff}}}}\) is the average value of all feature-feature correlations.</p>

<p>The <strong>CFS criterion</strong> is defined as follows:</p>
<p>$$\mathrm {CFS} =\max _{S_{k}}\left[{\frac {r_{cf_{1}}+r_{cf_{2}}+\cdots +r_{cf_{k}}}{\sqrt {k+2(r_{f_{1}f_{2}}+\cdots +r_{f_{i}f_{j}}+\cdots +r_{f_{k}f_{1}})}}}\right]$$</p>

<p><br /></p>

<h3 id="feature-selection-embedded-in-learning-algorithms"><strong style="color: SteelBlue; font-size: 1.15em" class="bodyContents2" id="bodyContents23">Feature Selection Embedded in Learning Algorithms</strong></h3>
<ul>
  <li>\(l_{1}\)-regularization techniques, such as sparse regression, LASSO, and \({\displaystyle l_{1}}\)-SVM</li>
  <li>Regularized trees, e.g. regularized random forest implemented in the RRF package</li>
  <li>Decision tree</li>
  <li>Memetic algorithm</li>
  <li>Random multinomial logit (RMNL)</li>
  <li>Auto-encoding networks with a bottleneck-layer</li>
  <li>Submodular feature selection</li>
</ul>

<p><br /></p>

<h3 id="information-theory-based-feature-selection-mechanisms"><strong style="color: SteelBlue; font-size: 1.15em" class="bodyContents2" id="bodyContents24">Information Theory Based Feature Selection Mechanisms</strong></h3>
<p>There are different Feature Selection mechanisms around that <strong>utilize mutual information for scoring the different features</strong>.<br />
They all usually use the same algorithm:</p>
<ol>
  <li>Calculate the mutual information as score for between all features (\({\displaystyle f_{i}\in F}\)) and the target class (\(c\))</li>
  <li>Select the feature with the largest score (e.g. \({\displaystyle argmax_{f_{i}\in F}(I(f_{i},c))}\)) and add it to the set of selected features (\(S\))</li>
  <li>Calculate the score which might be derived form the mutual information</li>
  <li>Select the feature with the largest score and add it to the set of select features (e.g. \({\displaystyle {\arg \max }_{f_{i}\in F}(I_{derived}(f_{i},c))}\))</li>
  <li>Repeat 3. and 4. until a certain number of features is selected (e.g. \({\displaystyle \vert S\vert =l}\))</li>
</ol>

<!-- <br> ### **Asynchronous**{: style="color: SteelBlue; font-size: 1.15em"}{: .bodyContents2 #bodyContents25}  
### **Asynchronous**{: style="color: SteelBlue; font-size: 1.15em"}{: .bodyContents2 #bodyContents26}   -->

<hr />
<hr />

<h2 style="font-size: 1.60em" id="content3">Feature Extraction</h2>

<h3 id="feature-extraction"><strong style="color: SteelBlue; font-size: 1.15em" class="bodyContents3" id="bodyContents31">Feature Extraction</strong></h3>
<p><strong>Feature Extraction</strong> starts from an initial set of measured data and builds derived values (features) intended to be informative and non-redundant, facilitating the subsequent learning and generalization steps, and in some cases leading to better human interpretations.</p>

<p>In <strong>dimensionality reduction</strong>, feature extraction is also called <strong>Feature Projection</strong>, which is a method that transforms the data in the high-dimensional space to a space of fewer dimensions. The data transformation may be linear, as in principal component analysis (PCA), but many nonlinear dimensionality reduction techniques also exist.</p>

<p id="lst-p"><strong style="color: red">Methods/Algorithms:</strong></p>
<ul>
  <li>Independent component analysis</li>
  <li>Isomap</li>
  <li>Kernel PCA</li>
  <li>Latent semantic analysis</li>
  <li>Partial least squares</li>
  <li>Principal component analysis</li>
  <li>Autoencoder</li>
  <li>Linear Discriminant Analysis (LDA)</li>
  <li>Non-negative matrix factorization (NMF)</li>
</ul>

<p><br /></p>

<h3 id="data-imputation"><strong style="color: SteelBlue; font-size: 1.15em" class="bodyContents3" id="bodyContents32">Data Imputation</strong></h3>

<p id="lst-p"><strong>Resources:</strong></p>
<ul>
  <li><a href="https://www.interpretable.ai/products/optimpute/">Imputation Solutions (Product)</a></li>
  <li><a href="https://www.interpretable.ai/solutions/data-pipeline/">Robust Data Pipeline Design (Product/Case)</a></li>
</ul>

<!-- ### ****{: style="color: SteelBlue; font-size: 1.15em"}{: .bodyContents3 #bodyContents32}  
### ****{: style="color: SteelBlue; font-size: 1.15em"}{: .bodyContents3 #bodyContents33}   -->

<ul>
  <li><a href="https://heartbeat.fritz.ai/how-to-make-your-machine-learning-models-robust-to-outliers-44d404067d07">How to Make Your Machine Learning Models Robust to Outliers (Blog!)</a></li>
</ul>

<p><a href="https://en.wikipedia.org/wiki/Outlier#Working_with_outliers">Outliers</a><br />
<a href="https://en.wikipedia.org/wiki/Robust_statistics#Replacing_outliers_and_missing_values">Replacing Outliers</a><br />
<a href="https://en.wikipedia.org/wiki/Data_transformation_(statistics)">Data Transformation - Outliers - Standardization</a><br />
<a href="https://hadrienj.github.io/posts/Preprocessing-for-deep-learning/">PreProcessing in DL - Data Normalization</a><br />
<a href="https://towardsdatascience.com/the-complete-beginners-guide-to-data-cleaning-and-preprocessing-2070b7d4c6d">Imputation and Feature Scaling</a><br />
<a href="https://en.wikipedia.org/wiki/Missing_data#Techniques_of_dealing_with_missing_data">Missing Data - Imputation</a><br />
<a href="https://en.wikipedia.org/wiki/Random_projection">Dim-Red - Random Projections</a><br />
<a href="https://en.wikipedia.org/wiki/Relief_(feature_selection)">F-Selection - Relief</a><br />
<a href="https://www.statisticshowto.datasciencecentral.com/box-cox-transformation/">Box-Cox Transf - outliers</a><br />
<a href="https://en.wikipedia.org/wiki/Analysis_of_covariance">ANCOVA</a><br />
<a href="https://www.analyticsvidhya.com/blog/2016/12/introduction-to-feature-selection-methods-with-an-example-or-how-to-select-the-right-variables/">Feature Selection Methods</a></p>

<hr />
<hr />

<h1 id="practiical-concepts">PRactiical Concepts</h1>

<ul>
  <li><a href="https://www.inference.vc/design-patterns/">A Cookbook for Machine Learning: Vol 1 (Blog!)</a>
    <ul>
      <li><a href="https://www.reddit.com/r/MachineLearning/comments/7dd45h/d_a_cookbook_for_machine_learning_a_list_of_ml/">Reddit Blog</a></li>
    </ul>
  </li>
  <li><a href="http://noracook.io/Books/MachineLearning/deeplearningcookbook.pdf">Deep Learning Cookbook (book)</a></li>
</ul>

<!-- ## FIRST
{: #content1} -->

<ol>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents11">Data Snooping:</strong></p>

    <p><strong>The Principle:</strong><br />
 If a data set has <strong><em>affected</em></strong> any step in the <strong>learning process</strong>, its <strong>ability to <em>assess the outcome</em></strong> has been compromised.</p>

    <p id="lst-p"><strong>Analysis:</strong></p>
    <ul>
      <li>Making decision by <strong>examining the dataset</strong> makes <em><strong>you</strong></em> a part of the learning algorithm.<br />
  However, you didn’t consider your contribution to the learning algorithm when making e.g. VC-Analysis for Generalization.</li>
      <li>Thus, you are <strong>vulnerable</strong> to designing the model (or choices of learning) according to the <em><strong>idiosyncrasies</strong></em> of the <strong>dataset</strong>.</li>
      <li>The real problem is that you are not <em>“charging” for the decision you made by examining the dataset</em>.</li>
    </ul>

    <p id="lst-p"><strong>What’s allowed?</strong></p>
    <ul>
      <li>You are allowed (even encouraged) to look at all other information related to the <strong>target function</strong> and <strong>input space</strong>.<br />
  e.g. number/range/dimension/scale/etc. of the inputs, correlations, properties (monotonicity), etc.</li>
      <li>EXCEPT, for the <strong><em>specific</em> realization of the training dataset</strong>.</li>
    </ul>

    <p id="lst-p"><strong style="color: red">Manifestations of Data Snooping with Examples (one/manifestation):</strong></p>
    <ul>
      <li><strong>Changing the Parameters of the model (Tricky)</strong>:
        <ul>
          <li><strong>Complexity</strong>:<br />
  Decreasing the order of the fitting polynomial by observing geometric properties of the <strong>training set</strong>.</li>
        </ul>
      </li>
      <li><strong>Using statistics of the Entire Dataset (Tricky)</strong>:
        <ul>
          <li><strong>Normalization</strong>:<br />
  Normalizing the data with the mean and variance of the <strong>entire dataset (training+testing)</strong>.
            <ul>
              <li>E.g. In Financial Forecasting; the average affects the outcome by exposing the trend.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li><strong>Reuse of a Dataset</strong>:<br />
  If you keep Trying one model after the other <em>on the</em> <strong>same data set</strong>, you will eventually ‘succeed’.<br />
  <em>“If you torture the data long enough, it will confess”</em>.<br />
  This bad because the final model you selected, is the <strong><em>union</em> of all previous models</strong>: since some of those models were <em><strong>rejected</strong></em> by <strong>you</strong> (a <em><strong>learning algorithm</strong></em>).
        <ul>
          <li><strong>Fixed (deterministic) training set for Model Selection</strong>:<br />
  Selecting a model by trying many models on the <strong>same <em>fixed (deterministic)</em> Training dataset</strong>.</li>
        </ul>
      </li>
      <li><strong>Bias via Snooping</strong>:<br />
  By looking at the data in the future when you are not allowed to have the data (it wouldn’t have been possible); you are creating <strong>sampling bias</strong> caused by <em>“snooping”</em>.
        <ul>
          <li>E.g. Testing a <strong>Trading</strong> algorithm using the <em><strong>currently</strong></em> <strong>traded companies</strong> (in S&amp;P500).<br />
  You shouldn’t have been able to know which companies are being <em><strong>currently</strong></em> traded (future).</li>
        </ul>
      </li>
    </ul>

    <p id="lst-p"><strong style="color: red">Remedies/Solutions to Data Snooping:</strong></p>
    <ol>
      <li><strong>Avoid</strong> Data Snooping:<br />
 A strict discipline (very hard).</li>
      <li><strong>Account for</strong> Data Snooping:<br />
 By quantifying “How much <strong>data contamination</strong>”.</li>
    </ol>

    <p><br /></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents12">Mismatched Data:</strong></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents13">Mismatched Classes:</strong></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents14">Sampling Bias:</strong><br />
 <strong>Sampling Bias</strong> occurs when: \(\exists\) Region with zero-probability \(P=0\) in training, but with positive-probability \(P&gt;0\) in testing.</p>

    <p><strong>The Principle:</strong><br />
 If the data is sampled in a biased way, learning will produce a similarly biased outcome.</p>

    <p id="lst-p"><strong>Example: 1948 Presidential Elections</strong></p>
    <ul>
      <li>Newspaper conducted a <em><strong>Telephone</strong></em> poll between: <strong>Jackson</strong> and <strong>Truman</strong></li>
      <li><strong>Jackson</strong> won the poll <strong>decisively</strong>.</li>
      <li>The result was NOT <strong>unlucky</strong>:<br />
  No matter how many times the poll was re-conducted, and no matter how many times the sample sized is increased; the outcome will be fixed.</li>
      <li>The reason is the <em><strong>Telephone</strong></em>:<br />
  (1) Telephones were <strong>expensive</strong> and only <strong>rich people</strong> had Telephones.<br />
  (2) Rich people favored <strong>Jackson</strong>.<br />
  Thus, the result was <strong>well reflective</strong> of the (mini) population being sampled.</li>
    </ul>

    <p><strong style="color: red">How to sample:</strong><br />
 Sample in a way that <span style="color: purple">matches the <strong>distributions</strong> of <strong>train</strong> and <strong>test</strong></span> samples.</p>

    <p>The solution <strong>Fails</strong> (doesn’t work) if:<br />
 \(\exists\) Region with zero-probability \(P=0\) in training, but with positive-probability \(P&gt;0\) in testing.</p>
    <blockquote>
      <p>This is when sampling bias exists.</p>
    </blockquote>

    <p id="lst-p"><strong style="color: red">Notes:</strong></p>
    <ul>
      <li>Medical sources sometimes refer to sampling bias as <strong>ascertainment bias</strong>.</li>
      <li>Sampling bias could be viewed as a subtype of <strong>selection bias</strong>.<br />
 <br /></li>
    </ul>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents15">Model Uncertainty:</strong></p>

    <p><strong style="color: red">Interpreting Softmax Output Probabilities:</strong><br />
 Softmax outputs only measure <a href="https://en.wikipedia.org/wiki/Uncertainty_quantification#Aleatoric_and_epistemic_uncertainty"><strong>Aleatoric Uncertainty</strong></a>.<br />
 In the same way that in regression, a NN with two outputs, one representing mean and one variance, that parameterise a Gaussian, can capture aleatoric uncertainty, even though the model is deterministic.<br />
 Bayesian NNs (dropout included), aim to capture epistemic (aka model) uncertainty.</p>

    <p><strong style="color: red">Dropout for Measuring Model (epistemic) Uncertainty:</strong><br />
 Dropout can give us principled uncertainty estimates.<br />
 Principled in the sense that the uncertainty estimates basically approximate those of our <a href="/work_files/research/dl/archits/nns#bodyContents13">Gaussian process</a>.</p>

    <p id="lst-p"><strong>Theoretical Motivation:</strong> dropout neural networks are identical to <span style="color: purple">variational inference in Gaussian processes</span>.<br />
 <strong>Interpretations of Dropout:</strong></p>
    <ul>
      <li>Dropout is just a diagonal noise matrix with the diagonal elements set to either 0 or 1.</li>
      <li><a href="http://mlg.eng.cam.ac.uk/yarin/blog_3d801aa532c1ce.html">What My Deep Model Doesn’t Know (Blog! - Yarin Gal)</a><br />
 <br /></li>
    </ul>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents16">Probability Calibration:</strong><br />
 Modern NN are <strong>miscalibrated</strong>: not well-calibrated. They tend to be very confident. We cannot interpret the softmax probabilities as reflecting the true probability distribution or as a measure of confidence.</p>

    <p><strong>Miscalibration:</strong> is the discrepancy between model confidence and model accuracy.<br />
 You assume that if a model gives \(80\%\) confidence for 100 images, then \(80\) of them will be accurate and the other \(20\) will be inaccurate.<br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">Miscalibration in Modern Neural Networks</button>
 <img src="https://cdn.mathpix.com/snip/images/boMaW8Wx2tXfUYTJpd-rhcVGWnrtpC4_2AGbXPxtocc.original.fullsize.png" alt="img" width="100%" hidden="" /></p>

    <p><strong>Model Confidence:</strong> probability of correctness.<br />
 <strong>Calibrated Confidence (softmax scores) \(\hat{p}\):</strong> \(\hat{p}\) represents a true probability.</p>

    <p><button class="showText" value="show" onclick="showTextPopHide(event);">Bias of Different Classical ML Models</button>
 <img src="https://cdn.mathpix.com/snip/images/m91-I3AcQ52sbAjr2gzeBlv_SlmZSh5Hb_knOLkTOMk.original.fullsize.png" alt="img" width="100%" hidden="" /></p>

    <p><button class="showText" value="show" onclick="showTextPopHide(event);">Summary On Practical Use of Model Scores (sklearn)</button>
 <img src="https://cdn.mathpix.com/snip/images/f7hsQi4QKi0wejzS4YNwKVf_AaYVjOjqZFdt5UcSvDc.original.fullsize.png" alt="img" width="100%" hidden="" /></p>

    <p><strong style="color: red">Probability Calibration:</strong><br />
 Predicted scores (model outputs) of many classifiers do not represent <em>“true” probabilities</em>.<br />
 They only respect the <em>mathematical definition</em> (conditions) of what a probability function is:</p>
    <ol>
      <li>Each “probability” is between 0 and 1</li>
      <li>When you sum the probabilities of an observation being in any particular class, they sum to 1.</li>
    </ol>

    <ul>
      <li>
        <p><strong>Calibration Curves</strong>: A calibration curve plots the predicted probabilities against the actual rate of occurance.<br />
  I.E. It plots the <em><strong>predicted</strong></em> probabilities against the <em><strong>actual</strong></em> probabilities.<br />
  <button class="showText" value="show" onclick="showTextPopHide(event);">Example: Rain Prediction with Naive Bayes Model</button>
  <img src="https://cdn.mathpix.com/snip/images/SgB0b51NGZ0_McTgF0d25jWXx43A-cA8MSSco6jvzZA.original.fullsize.png" alt="img" width="100%" hidden="" /></p>
      </li>
      <li>
        <p><strong>Approach</strong>:<br />
  Calibrating a classifier consists of fitting a regressor (called a calibrator) that maps the output of the classifier (as given by <code class="language-plaintext highlighter-rouge">decision_function</code> or <code class="language-plaintext highlighter-rouge">predict_proba</code> - sklearn) to a calibrated probability in \([0, 1]\).<br />
  Denoting the output of the classifier for a given sample by \(f_i\), the calibrator tries to predict \(p\left(y_i=1 \mid f_i\right)\).</p>
      </li>
      <li><a href="https://scikit-learn.org/stable/modules/calibration.html"><strong>Methods</strong></a>:
        <ul>
          <li>
            <p><strong>Platt Scaling</strong>: Platt scaling basically fits a logistic regression on the original model’s.<br />
  The closer the calibration curve is to a sigmoid, the more effective the scaling will be in correcting the model.<br />
  <button class="showText" value="show" onclick="showTextPopHide(event);">Model Definition</button>
  <img src="https://cdn.mathpix.com/snip/images/F_DDq98LBIJPNFg6AjRQI4OpJJ_ozb4ZM1NdrOaxfOk.original.fullsize.png" alt="img" width="100%" hidden="" /></p>

            <ul>
              <li>
                <p><strong>Assumptions</strong>:<br />
  The sigmoid method assumes the calibration curve can be corrected by applying a sigmoid function to the raw predictions.<br />
  This assumption has been empirically justified in the case of <strong>Support Vector Machines</strong> with <strong>common kernel functions</strong> on various benchmark datasets but does not necessarily hold in general.</p>
              </li>
              <li>
                <p><strong>Limitations</strong>:</p>
                <ul>
                  <li>The logistic model works best if the <strong>calibration error</strong> is <em><strong>symmetrical</strong></em>, meaning the classifier output for each binary class is <em><strong>normally distributed</strong></em> with the <em><strong>same variance</strong></em>.<br />
  This can be a problem for highly imbalanced classification problems, where outputs do not have equal variance.</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>
            <p><strong>Isotonic Method</strong>:  The ‘isotonic’ method fits a non-parametric isotonic regressor, which outputs a step-wise non-decreasing function.<br />
  <button class="showText" value="show" onclick="showTextPopHide(event);">Objective/Loss</button>
  <img src="https://cdn.mathpix.com/snip/images/XEjg4c6wis3M51_xsrTRzg00BRdtFK8_4CNOd7IcZ-I.original.fullsize.png" alt="img" width="100%" hidden="" /></p>

            <p>This method is more general when compared to ‘sigmoid’ as the only restriction is that the mapping function is monotonically increasing. It is thus more powerful as it can correct any monotonic distortion of the un-calibrated model. However, it is more prone to overfitting, especially on small datasets.</p>
          </li>
          <li>
            <p><strong style="color: blue">Comparison:</strong></p>
            <ul>
              <li>Platt Scaling is most effective when the un-calibrated model is under-confident and has similar calibration errors for both high and low outputs.</li>
              <li>Isotonic Method is more powerful than Platt Scaling:  Overall, ‘isotonic’ will perform as well as or better than ‘sigmoid’ when there is enough data (greater than ~ 1000 samples) to avoid overfitting.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        <p><a href="https://kiwidamien.github.io/are-you-sure-thats-a-probability.html#Limitations-of-recalibration">Limitations of recalibration:</a><br />
  Different calibration methods have different weaknesses depending on the shape of the <em>calibration curve</em>.<br />
  E.g. <em>Platt Scaling</em> works better the more the <em>calibration curve</em> resembles a <em><strong>sigmoid</strong></em>.</p>

        <p><button class="showText" value="show" onclick="showTextPopHide(event);">Example of Platt Scaling Failure</button>
  <img src="https://cdn.mathpix.com/snip/images/I8sRhwL5JnmbjJ39hRcuIc5jomlUD4O2rrC1wMA6H6M.original.fullsize.png" alt="img" width="100%" hidden="" /></p>
      </li>
      <li><a href="https://scikit-learn.org/stable/modules/calibration.html#multiclass-support" style="color: blue"><strong>Multi-Class Support:</strong></a></li>
    </ul>

    <p><strong>Note:</strong> The samples that are used to fit the calibrator should not be the same samples used to fit the classifier, as this would introduce bias. This is because performance of the classifier on its training data would be better than for novel data. Using the classifier output of training data to fit the calibrator would thus result in a biased calibrator that maps to probabilities closer to 0 and 1 than it should.</p>

    <ul>
      <li><a href="https://arxiv.org/pdf/1706.04599.pdf">On Calibration of Modern Neural Networks</a>  <br />
  Paper that defines the problem and gives multiple effective solution for calibrating Neural Networks.</li>
      <li><a href="file:///Users/ahmadbadary/Downloads/Kängsepp_ComputerScience_2018.pdf">Calibration of Convolutional Neural Networks (Thesis!)</a></li>
      <li>For calibrating output probabilities in Deep Nets; Temperature scaling outperforms Platt scaling. <a href="https://arxiv.org/pdf/1706.04599.pdf">paper</a></li>
      <li><a href="https://scikit-learn.org/stable/modules/calibration.html">Plot and Explanation</a></li>
      <li><a href="http://alondaks.com/2017/12/31/the-importance-of-calibrating-your-deep-model/">Blog on How to do it</a></li>
      <li><a href="https://kiwidamien.github.io/are-you-sure-thats-a-probability.html">Interpreting outputs of a logistic classifier (Blog)</a>  <br />
 <br /></li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents17">Debugging Strategies for Deep ML Models:</strong><br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">Strategies</button>
    <ol hidden="">
      <li><strong>Visualize the model in action:</strong><br />
 Directly observing qualitative results of a model (e.g. located objects, generated speech) can help avoid <strong>evaluation bugs</strong> or <strong>mis-leading evaluation results</strong>. It can also help guide the expected quantitative performance of the model.</li>
      <li><strong>Visualize the worst mistakes:</strong><br />
 By viewing the training set examples that are the hardest to model correctly by using a confidence measure (e.g. softmax probabilities), one can often discover problems with the way the data have been <strong>preprocessed</strong> or <strong>labeled</strong>.</li>
      <li><strong>Reason about Software using Training and Test <em>Error</em>:</strong><br />
 It is hard to determine whether the underlying software is correctly implemented.<br />
 We can use the training/test errors to help guide us:
        <ul>
          <li>If training error is low but test error is high, then:
            <ul>
              <li>it is likely that that the training procedure works correctly,and the model is overfitting for fundamental algorithmic reasons.</li>
              <li>or that the test error is measured incorrectly because of a problem with saving the model after training then reloading it for test set evaluation, or because the test data was prepared differently from the training data.</li>
            </ul>
          </li>
          <li>If both training and test errors are high, then:<br />
  it is difficult to determine whether there is a software defect or whether the model is underfitting due to fundamental algorithmic reasons.<br />
  This scenario requires further tests, described next.</li>
        </ul>
      </li>
      <li><strong>Fit a <em>Tiny Dataset:</em></strong><br />
 If you have high error on the training set, determine whether it is due to genuine underfitting or due to a software defect.<br />
 Usually even small models can be guaranteed to be able fit a suﬃciently small dataset. For example, a classification dataset with only one example can be fit just by setting the biase sof the output layer correctly.<br />
 This test can be extended to a small dataset with few examples.</li>
      <li><strong>Monitor histograms of <em>Activations</em> and <em>Gradients:</em></strong><br />
 It is often useful to visualize statistics of neural network activations and gradients, collected over a large amount of training iterations (maybe one epoch).<br />
 The <strong>preactivation value</strong> of <strong>hidden units</strong> can tell us if the units <span style="color: purple"><strong>saturate</strong></span>, or how often they do.<br />
 For example, for rectifiers,how often are they off? Are there units that are always off?<br />
 For tanh units,the average of the absolute value of the preactivations tells us how saturated the unit is.<br />
 In a deep network where the propagated gradients quickly grow or quickly vanish, optimization may be hampered.<br />
 Finally, it is useful to compare the magnitude of parameter gradients to the magnitude of the parameters themselves. As suggested by Bottou (2015), we would like the magnitude of parameter updates over a minibatch to represent something like 1 percent of the magnitude of the parameter, not 50 percent or 0.001 percent (which would make the parametersmove too slowly). It may be that some groups of parameters are moving at a good pace while others are stalled. When the data is sparse (like in natural language) some parameters may be very rarely updated, and this should be kept in mind when monitoring their evolution.</li>
      <li>Finally, many deep learning algorithms provide some sort of guarantee about the results produced at each step.<br />
 For example, in part III, we will see some approximate inference algorithms that work by using algebraic solutions to optimization problems.<br />
 Typically these can be debugged by testing each of their guarantees.Some guarantees that some optimization algorithms offer include that the objective function will never increase after one step of the algorithm, that the gradient with respect to some subset of variables will be zero after each step of the algorithm,and that the gradient with respect to all variables will be zero at convergence.Usually due to rounding error, these conditions will not hold exactly in a digital computer, so the debugging test should include some tolerance parameter.</li>
    </ol>
    <p><br /></p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents18">The Machine Learning Algorithm Recipe:</strong><br />
 Nearly all deep learning algorithms can be described as particular instances of a fairly simple recipe in both Supervised and Unsupervised settings:
    <ul>
      <li>A combination of:
        <ul>
          <li>A specification of a dataset</li>
          <li>A cost function</li>
          <li>An optimization procedure</li>
          <li>A model</li>
        </ul>
      </li>
      <li><strong>Ex: Linear Regression</strong><br />
  <button class="showText" value="show" onclick="showTextPopHide(event);">Example</button>
        <ul hidden="">
          <li>A specification of a dataset:<br />
  The Dataset consists of \(X\) and \(y\).</li>
          <li>A cost function:<br />
  \(J(\boldsymbol{w}, b)=-\mathbb{E}_{\mathbf{x}, \mathbf{y} \sim \hat{p}_{\text {data }}} \log p_{\text {model }}(y | \boldsymbol{x})\)</li>
          <li>An optimization procedure:<br />
  in most cases, the optimization algorithm is defined by solving for where the gradient of the cost is zero using the normal equation.</li>
          <li>A model:<br />
  The Model Specification is:<br />
  \(p_{\text {model}}(y \vert \boldsymbol{x})=\mathcal{N}\left(y ; \boldsymbol{x}^{\top} \boldsymbol{w}+b, 1\right)\)</li>
        </ul>
      </li>
      <li><strong>Ex: PCA</strong><br />
  <button class="showText" value="show" onclick="showTextPopHide(event);">Example</button>
        <ul hidden="">
          <li>A specification of a dataset:<br />
  \(X\)</li>
          <li>A cost function:<br />
  \(J(\boldsymbol{w})=\mathbb{E}_{\mathbf{x} \sim \hat{p}_{\text {data }}}\|\boldsymbol{x}-r(\boldsymbol{x} ; \boldsymbol{w})\|_ {2}^{2}\)</li>
          <li>An optimization procedure:<br />
  Constrained Convex optimization or Gradient Descent.</li>
          <li>A model:<br />
  Defined to have \(\boldsymbol{w}\) with <strong>norm</strong> \(1\) and <strong>reconstruction function</strong> \(r(\boldsymbol{x})=\boldsymbol{w}^{\top} \boldsymbol{x} \boldsymbol{w}\).</li>
        </ul>
      </li>
      <li><strong>Specification of a Dataset</strong>:<br />
  Could be <strong>labeled (supervised)</strong> or <strong>unlabeled (unsupervised)</strong>.</li>
      <li><strong>Cost Function</strong>:<br />
  The cost function typically includes at least one term that causes the learning process to perform <strong>statistical estimation</strong>. The most common cost function is the negative log-likelihood, so that minimizing the cost function causes maximum likelihood estimation.</li>
      <li><strong>Optimization Procedure</strong>:<br />
  Could be <strong>closed-form</strong> or <strong>iterative</strong> or <strong>special-case</strong>.<br />
  If the cost function does not allow for <strong>closed-form</strong> solution (e.g. if the model is specified as <strong>non-linear</strong>), then we usually need <strong>iterative</strong> optimization algorithms e.g. <strong>gradient descent</strong>.<br />
  If the cost can’t be computed for <strong>computational problems</strong> then we can approximate it with an iterative numerical optimization as long as we have some way to <span style="color: purple">approximating its <strong>gradients</strong></span>.</li>
      <li><strong>Model</strong>:<br />
  Could be <strong>linear</strong> or <strong>non-linear</strong>.</li>
    </ul>

    <p>If a machine learning algorithm seems especially unique or hand designed, it can usually be understood as using a <strong>special-case optimizer</strong>.<br />
 Some models, such as <strong>decision trees</strong> and <strong>k-means</strong>, require <em><strong>special-case optimizers</strong></em> because their <strong>cost functions</strong> have <em><strong>flat regions</strong></em> that make them inappropriate for minimization by gradient-based optimizers.</p>

    <p>Recognizing that most machine learning algorithms can be described using this recipe helps to see the different algorithms as part of a taxonomy of methods for doing related tasks that work for similar reasons, rather than as a long list of algorithms that each have separate justifications.</p>

    <p><br /></p>
  </li>
</ol>

<p><strong>Recall</strong> is more important where Overlooked Cases (False Negatives) are more costly than False Alarms (False Positive). The focus in these problems is finding the positive cases.</p>

<p><strong>Precision</strong> is more important where False Alarms (False Positives) are more costly than Overlooked Cases (False Negatives). The focus in these problems is in weeding out the negative cases.</p>

<ul>
  <li><a href="https://kiwidamien.github.io/interview-practice-with-precision-and-recall.html">Interview practice with P and R (Blog)</a></li>
</ul>

<p><strong>ROC Curve and AUC:</strong></p>

<p>Note:</p>
<ul>
  <li>ROC Curve only cares about the <em><strong>ordering</strong></em> of the scores, not the values.
    <ul>
      <li><strong>Probability Calibration</strong> and ROC: The calibration doesn’t change the order of the scores, it just scales them to make a better match, and the ROC score only cares about the ordering of the scores.</li>
    </ul>
  </li>
  <li>
    <p><a href="https://kiwidamien.github.io/what-is-a-roc-curve-a-visualization-with-credit-scores.html">ROC and Credit Score Example (Blog)</a></p>
  </li>
  <li><strong>AUC</strong>: The AUC is also the probability that a randomly selected positive example has a higher score than a randomly selected negative example.</li>
</ul>

<p><button class="showText" value="show" onclick="showTextPopHide(event);">AUC Reliability (Equal AUC - different models)</button>
<img src="https://cdn.mathpix.com/snip/images/GAyvZvN61xzDjklTeVepqrYYuWrXXfPnEHkNwM80p6k.original.fullsize.png" alt="img" width="100%" hidden="" /></p>

<p><button class="showText" value="show" onclick="showTextPopHide(event);">ROC Diagonal</button>
<img src="https://cdn.mathpix.com/snip/images/GPRX_7Ca-eJdTm04-HQO1Mc8E0cLi1FbFqjnp1z04Yk.original.fullsize.png" alt="img" width="100%" hidden="" /></p>

<p><button class="showText" value="show" onclick="showTextPopHide(event);">Comparing Thresholds on an ROC</button>
<img src="https://cdn.mathpix.com/snip/images/QPD_iV87ZGNW8Mpk-apO69uGKfiy400nrHgO8Cuc-cc.original.fullsize.png" alt="img" width="100%" hidden="" /></p>

<p><button class="showText" value="show" onclick="showTextPopHide(event);">AUC to Compare two Classifiers</button>
<img src="https://cdn.mathpix.com/snip/images/L3X5BVmBkVbzzZmHlQRM4r69hOzfo3EFLWEvm6VeZVM.original.fullsize.png" alt="img" width="100%" hidden="" /></p>

<p><button class="showText" value="show" onclick="showTextPopHide(event);">PR Curve</button>
<img src="https://cdn.mathpix.com/snip/images/fMMLgJHugy3Ebj7OG7Lq4X3HX5ZTGL6gzFi4IaP8hT4.original.fullsize.png" alt="img" width="100%" hidden="" /></p>

<p><button class="showText" value="show" onclick="showTextPopHide(event);">When to use Precision instead of FPR</button>
<img src="https://cdn.mathpix.com/snip/images/lZkRZCj8kSX4xK6JcEcnm8wVVEe5856uFO5mNs0VRY8.original.fullsize.png" alt="img" width="100%" hidden="" /></p>

<p><button class="showText" value="show" onclick="showTextPopHide(event);">Why use Precision</button>
<img src="https://cdn.mathpix.com/snip/images/IQ7O2yvLTJqG70NIzPxMdZySzEXQI39YgIcz5D4YXrQ.original.fullsize.png" alt="img" width="100%" hidden="" /></p>

<p><button class="showText" value="show" onclick="showTextPopHide(event);">Example when Precision is favorable to FPR</button>
<img src="https://cdn.mathpix.com/snip/images/8ZcbFolG4VmYWakNzewSVx4oThTkxP1jS4pGZJ3oUbc.original.fullsize.png" alt="img" width="100%" hidden="" /></p>

<ul>
  <li><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2698108/">ROC in Radiology (Paper)</a><br />
  Includes discussion for <em><strong>Partial AUC</strong></em> when only a portion of the entire ROC curve needs to be considered.</li>
</ul>

<hr />

<h2 id="content2">SECOND</h2>

<!-- 
1. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21}
2. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents22}
3. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents23}
4. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents24}
 -->

<hr />
<hr />

<h1 id="calibration">Calibration</h1>

<p>Modern NN are <strong>miscalibrated</strong>: not well-calibrated. They tend to be very confident. We cannot interpret the softmax probabilities as reflecting the true probability distribution or as a measure of confidence.</p>

<p><strong>Miscalibration:</strong> is the discrepancy between model confidence and model accuracy.<br />
You assume that if a model gives \(80\%\) confidence for 100 images, then \(80\) of them will be accurate and the other \(20\) will be inaccurate.<br />
<button class="showText" value="show" onclick="showTextPopHide(event);">Miscalibration in Modern Neural Networks</button>
<img src="https://cdn.mathpix.com/snip/images/boMaW8Wx2tXfUYTJpd-rhcVGWnrtpC4_2AGbXPxtocc.original.fullsize.png" alt="img" width="40%" /></p>

<p><strong>Model Confidence:</strong> probability of correctness.<br />
<strong>Calibrated Confidence (softmax scores) \(\hat{p}\):</strong> \(\hat{p}\) represents a true probability.</p>

<p><button class="showText" value="show" onclick="showTextPopHide(event);">Bias of Different Classical ML Models</button>
<img src="https://cdn.mathpix.com/snip/images/m91-I3AcQ52sbAjr2gzeBlv_SlmZSh5Hb_knOLkTOMk.original.fullsize.png" alt="img" width="40%" /></p>

<p><button class="showText" value="show" onclick="showTextPopHide(event);">Summary On Practical Use of Model Scores (sklearn)</button>
<img src="https://cdn.mathpix.com/snip/images/f7hsQi4QKi0wejzS4YNwKVf_AaYVjOjqZFdt5UcSvDc.original.fullsize.png" alt="img" width="40%" /></p>

<p><strong style="color: red">Probability Calibration:</strong><br />
Predicted scores (model outputs) of many classifiers do not represent <em>“true” probabilities</em>.<br />
They only respect the <em>mathematical definition</em> (conditions) of what a probability function is:</p>
<ol>
  <li>Each “probability” is between 0 and 1</li>
  <li>When you sum the probabilities of an observation being in any particular class, they sum to 1.</li>
</ol>

<ul>
  <li>
    <p><strong>Calibration Curves</strong>: A calibration curve plots the predicted probabilities against the actual rate of occurance.<br />
  I.E. It plots the <em><strong>predicted</strong></em> probabilities against the <em><strong>actual</strong></em> probabilities.<br />
  <button class="showText" value="show" onclick="showTextPopHide(event);">Example: Rain Prediction with Naive Bayes Model</button>
  <img src="https://cdn.mathpix.com/snip/images/SgB0b51NGZ0_McTgF0d25jWXx43A-cA8MSSco6jvzZA.original.fullsize.png" alt="img" width="40%" /></p>
  </li>
  <li>
    <p><strong>Approach</strong>:<br />
  Calibrating a classifier consists of fitting a regressor (called a calibrator) that maps the output of the classifier (as given by <code class="language-plaintext highlighter-rouge">decision_function</code> or <code class="language-plaintext highlighter-rouge">predict_proba</code> - sklearn) to a calibrated probability in \([0, 1]\).<br />
  Denoting the output of the classifier for a given sample by \(f_i\), the calibrator tries to predict \(p\left(y_i=1 \mid f_i\right)\).</p>
  </li>
  <li><a href="https://scikit-learn.org/stable/modules/calibration.html"><strong>Methods</strong></a>:
    <ul>
      <li>
        <p><strong>Platt Scaling</strong>: Platt scaling basically fits a logistic regression on the original model’s.<br />
  The closer the calibration curve is to a sigmoid, the more effective the scaling will be in correcting the model.<br />
  <button class="showText" value="show" onclick="showTextPopHide(event);">Model Definition</button>
  <img src="https://cdn.mathpix.com/snip/images/F_DDq98LBIJPNFg6AjRQI4OpJJ_ozb4ZM1NdrOaxfOk.original.fullsize.png" alt="img" width="40%" /></p>

        <ul>
          <li>
            <p><strong>Assumptions</strong>:<br />
  The sigmoid method assumes the calibration curve can be corrected by applying a sigmoid function to the raw predictions.<br />
  This assumption has been empirically justified in the case of <strong>Support Vector Machines</strong> with <strong>common kernel functions</strong> on various benchmark datasets but does not necessarily hold in general.</p>
          </li>
          <li>
            <p><strong>Limitations</strong>:</p>
            <ul>
              <li>The logistic model works best if the <strong>calibration error</strong> is <em><strong>symmetrical</strong></em>, meaning the classifier output for each binary class is <em><strong>normally distributed</strong></em> with the <em><strong>same variance</strong></em>.<br />
  This can be a problem for highly imbalanced classification problems, where outputs do not have equal variance.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        <p><strong>Isotonic Method</strong>:  The ‘isotonic’ method fits a non-parametric isotonic regressor, which outputs a step-wise non-decreasing function.<br />
  <button class="showText" value="show" onclick="showTextPopHide(event);">Objective/Loss</button>
  <img src="https://cdn.mathpix.com/snip/images/XEjg4c6wis3M51_xsrTRzg00BRdtFK8_4CNOd7IcZ-I.original.fullsize.png" alt="img" width="40%" /></p>

        <p>This method is more general when compared to ‘sigmoid’ as the only restriction is that the mapping function is monotonically increasing. It is thus more powerful as it can correct any monotonic distortion of the un-calibrated model. However, it is more prone to overfitting, especially on small datasets.</p>
      </li>
      <li>
        <p><strong style="color: blue">Comparison:</strong></p>
        <ul>
          <li>Platt Scaling is most effective when the un-calibrated model is under-confident and has similar calibration errors for both high and low outputs.</li>
          <li>Isotonic Method is more powerful than Platt Scaling:  Overall, ‘isotonic’ will perform as well as or better than ‘sigmoid’ when there is enough data (greater than ~ 1000 samples) to avoid overfitting.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p><a href="https://kiwidamien.github.io/are-you-sure-thats-a-probability.html#Limitations-of-recalibration">Limitations of recalibration:</a><br />
  Different calibration methods have different weaknesses depending on the shape of the <em>calibration curve</em>.<br />
  E.g. <em>Platt Scaling</em> works better the more the <em>calibration curve</em> resembles a <em><strong>sigmoid</strong></em>.</p>

    <p><button class="showText" value="show" onclick="showTextPopHide(event);">Example of Platt Scaling Failure</button>
  <img src="https://cdn.mathpix.com/snip/images/I8sRhwL5JnmbjJ39hRcuIc5jomlUD4O2rrC1wMA6H6M.original.fullsize.png" alt="img" width="40%" /></p>
  </li>
  <li><a href="https://scikit-learn.org/stable/modules/calibration.html#multiclass-support" style="color: blue"><strong>Multi-Class Support:</strong></a></li>
</ul>

<p><strong>Note:</strong> The samples that are used to fit the calibrator should not be the same samples used to fit the classifier, as this would introduce bias. This is because performance of the classifier on its training data would be better than for novel data. Using the classifier output of training data to fit the calibrator would thus result in a biased calibrator that maps to probabilities closer to 0 and 1 than it should.</p>

<ul>
  <li><a href="https://arxiv.org/pdf/1706.04599.pdf">On Calibration of Modern Neural Networks</a>  <br />
  Paper that defines the problem and gives multiple effective solution for calibrating Neural Networks.</li>
  <li><a href="file:///Users/ahmadbadary/Downloads/Kängsepp_ComputerScience_2018.pdf">Calibration of Convolutional Neural Networks (Thesis!)</a></li>
  <li>For calibrating output probabilities in Deep Nets; Temperature scaling outperforms Platt scaling. <a href="https://arxiv.org/pdf/1706.04599.pdf">paper</a></li>
  <li><a href="https://scikit-learn.org/stable/modules/calibration.html">Plot and Explanation</a></li>
  <li><a href="http://alondaks.com/2017/12/31/the-importance-of-calibrating-your-deep-model/">Blog on How to do it</a></li>
  <li><a href="https://kiwidamien.github.io/are-you-sure-thats-a-probability.html">Interpreting outputs of a logistic classifier (Blog)</a>  <br />
<br /></li>
</ul>

<hr />
<hr />

<h1 id="answers-hidden">Answers Hidden</h1>

<h1 id="data-processing-and-analysis">Data Processing and Analysis</h1>
<p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Data Processing and Analysis</button></p>
<ol hidden="">
  <li><strong style="color: red">What are 3 data preprocessing techniques to handle outliers?</strong>
    <ol>
      <li>Winsorizing/Winsorization (cap at threshold).</li>
      <li>Transform to reduce skew (using Box-Cox or similar).</li>
      <li>Remove outliers if you’re certain they are anomalies or measurement errors.</li>
    </ol>
  </li>
  <li><strong style="color: red">Describe the strategies to dimensionality reduction?</strong>
    <ol>
      <li>Feature Selection</li>
      <li>Feature Projection/Extraction</li>
    </ol>
  </li>
  <li><strong style="color: red">What are 3 ways of reducing dimensionality?</strong>
    <ol>
      <li>Removing Collinear Features</li>
      <li>Performing PCA, ICA, etc.</li>
      <li>Feature Engineering</li>
      <li>AutoEncoder</li>
      <li>Non-negative matrix factorization (NMF)</li>
      <li>LDA</li>
      <li>MSD</li>
    </ol>
  </li>
  <li><strong style="color: red">List methods for Feature Selection</strong>
    <ol>
      <li>Variance Threshold: normalize first (variance depends on scale)</li>
      <li>Correlation Threshold: remove the one with larger mean absolute correlation with other features.</li>
      <li>Genetic Algorithms</li>
      <li>Stepwise Search: bad performance, regularization much better, it’s a greedy algorithm (can’t account for future effects of each change)</li>
      <li>LASSO, Elastic-Net</li>
    </ol>
  </li>
  <li><strong style="color: red">List methods for Feature Extraction</strong>
    <ol>
      <li>PCA, ICA, CCA</li>
      <li>AutoEncoders</li>
      <li>LDA: LDA is a supervised linear transformation technique since the dependent variable (or the class label) is considered in the model. It Extracts the k new independent variables that <strong>maximize the separation between the classes of the dependent variable</strong>.
        <ol>
          <li>Linear discriminant analysis is used to find a linear combination of features that characterizes or separates two or more classes (or levels) of a categorical variable.</li>
          <li>Unlike PCA, LDA extracts the k new independent variables that <strong>maximize the separation between the classes of the dependent variable</strong>. LDA is a supervised linear transformation technique since the dependent variable (or the class label) is considered in the model.</li>
        </ol>
      </li>
      <li>Latent Semantic Analysis</li>
      <li>Isomap</li>
    </ol>
  </li>
  <li><strong style="color: red">How to detect correlation of “categorical variables”?</strong>
    <ol>
      <li>Chi-Squared test: it is a statistical test applied to the groups of categorical features to evaluate the likelihood of correlation or association between them using their frequency distribution.</li>
    </ol>
  </li>
  <li><strong style="color: red">Feature Importance</strong>
    <ol>
      <li>Use linear regression and select variables based on \(p\) values</li>
      <li>Use Random Forest, Xgboost and plot variable importance chart</li>
      <li>Lasso</li>
      <li>Measure information gain for the available set of features and select top \(n\) features accordingly.</li>
      <li>Use Forward Selection, Backward Selection, Stepwise Selection</li>
      <li>Remove the correlated variables prior to selecting important variables</li>
      <li>In linear models, feature importance can be calculated by the scale of the coefficients</li>
      <li>In tree-based methods (such as random forest), important features are likely to appear closer to the root of the tree. We can get a feature’s importance for random forest by computing the averaging depth at which it appears across all trees in the forest</li>
    </ol>
  </li>
  <li><strong style="color: red">Capturing the correlation between continuous and categorical variable? If yes, how?</strong><br />
 Yes, we can use ANCOVA (analysis of covariance) technique to capture association between continuous and categorical variables.<br />
 <a href="https://www.youtube.com/watch?v=a61mkzQRf6c&amp;t=2s">ANCOVA Explained</a></li>
  <li><strong style="color: red">What cross validation technique would you use on time series data set?</strong><br />
 <a href="https://en.wikipedia.org/wiki/Forward_chaining">Forward chaining strategy</a> with k folds.</li>
  <li><strong style="color: red">How to deal with missing features? (Imputation?)</strong>
    <ol>
      <li>Assign a unique category to missing values, who knows the missing values might decipher some trend.</li>
      <li>Remove them blatantly</li>
      <li>we can sensibly check their distribution with the target variable, and if found any pattern we’ll keep those missing values and assign them a new category while removing others.</li>
    </ol>
  </li>
  <li><strong style="color: red">Do you suggest that treating a categorical variable as continuous variable would result in a better predictive model?</strong><br />
 For better predictions, categorical variable can be considered as a continuous variable only when the variable is ordinal in nature.</li>
  <li><strong style="color: red">What are collinearity and multicollinearity?</strong>
    <ol>
      <li><strong>Collinearity</strong> occurs when two predictor variables (e.g., \(x_1\) and \(x_2\)) in a multiple regression have some correlation.</li>
      <li><strong>Multicollinearity</strong> occurs when more than two predictor variables (e.g., \(x_1, x_2, \text{ and } x_3\)) are inter-correlated.</li>
    </ol>
  </li>
  <li><strong style="color: red">What is data normalization and why do we need it?</strong><br />
 <img src="https://cdn.mathpix.com/snip/images/8aNuJetgTgCtv4pvqaI0dr96pDyUmfuX_d1aLK1lmaw.original.fullsize.png" alt="img" width="80%" /></li>
</ol>

<hr />

<h1 id="mlstatistical-models">ML/Statistical Models</h1>
<p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">ML/Statistical Models</button></p>
<ol hidden="">
  <li><strong style="color: red">What are parametric models?</strong><br />
 Parametric models are those with a finite number of parameters. To predict new data, you only need to know the parameters of the model. Examples include linear regression, logistic regression, and linear SVMs.</li>
  <li><strong style="color: red">What is a classifier?</strong><br />
 A function that maps…</li>
</ol>

<hr />

<hr />
<hr />

<h1 id="effect-size">Effect Size</h1>

<p>list two methods for calculating the effect size in applied machine learning and when they might be useful</p>

<p>There are several different methods for calculating the effect size in applied machine learning, and which one is most appropriate to use depends on the specific context and type of data being analyzed. Some common methods for calculating effect size include:</p>

<p>Cohen’s d: This method is commonly used when comparing the means of two groups, such as in a controlled experiment where one group receives a treatment and the other serves as a control. It is calculated by taking the difference between the two group means and dividing by the pooled standard deviation.</p>

<p>Pearson’s r: This method is commonly used when analyzing the relationship between two continuous variables, such as the relationship between a person’s height and weight. It is calculated by taking the covariance of the two variables and dividing by the product of their standard deviations.</p>

<p>These methods might be useful in applied machine learning when you are trying to evaluate the performance of a model or the impact of a specific feature on the outcome of a model. For example, if you are comparing the performance of two different machine learning algorithms on a classification task, you might use Cohen’s d to compare the mean accuracy of the two algorithms and determine which one is more effective. Alternatively, if you are trying to understand the relationship between a particular feature and the outcome of a regression model, you might use Pearson’s r to determine the strength of that relationship.</p>

<hr />
<hr />

<h1 id="affirm-interview-qa">Affirm Interview QA</h1>

<p>how would you implement a machine learning data processing pipeline?</p>

<p>To implement a machine learning data processing pipeline, you can follow these steps:</p>

<p>Collect the data that you want to process. This data should be relevant to the problem that you want to solve with machine learning.</p>

<p>Clean and preprocess the data to prepare it for machine learning. This may include tasks such as removing missing or invalid data points, transforming the data into a suitable format, and scaling the data so that all features have the same range.</p>

<p>Split the preprocessed data into training, validation, and test sets. The training set is used to train the machine learning model, the validation set is used to evaluate the model during training, and the test set is used to evaluate the final model.</p>

<p>Choose a machine learning model that is suitable for the problem you want to solve. You may need to experiment with different models and hyperparameters to find the best one.</p>

<p>Train the machine learning model on the training set. This involves using the training data to adjust the model’s parameters so that it can make accurate predictions on new data.</p>

<p>Evaluate the trained model on the validation set. This will give you an idea of how well the model is performing and whether it is overfitting or underfitting the data.</p>

<p>Fine-tune the model by adjusting its hyperparameters, training it on different subsets of the training data, or using regularization techniques to prevent overfitting.</p>

<p>Once you are satisfied with the model’s performance on the validation set, evaluate it on the test set to get an estimate of its performance on unseen data.</p>

<p>Use the trained and fine-tuned model to make predictions on new data. This may involve using the model to make predictions on the entire dataset, or using it to make predictions on individual data points as they are received.</p>

<p>Overall, implementing a machine learning data processing pipeline requires a combination of data preparation, model selection and training, and model evaluation and fine-tuning. By following these steps, you can build a pipeline that can process data efficiently and accurately for a variety of machine learning tasks.</p>

<p>AJ
how would you process data to be fed to gradient boosting models?</p>

<p>To process data for use with gradient boosting models, you can follow these steps:</p>

<p>Collect the data that you want to process. This data should be relevant to the problem that you want to solve with gradient boosting.</p>

<p>Clean and preprocess the data to prepare it for use with gradient boosting. This may include tasks such as removing missing or invalid data points, transforming the data into a suitable format, and scaling the data so that all features have the same range.</p>

<p>Split the preprocessed data into training, validation, and test sets. The training set is used to train the gradient boosting model, the validation set is used to evaluate the model during training, and the test set is used to evaluate the final model.</p>

<p>Choose a gradient boosting model that is suitable for the problem you want to solve. You may need to experiment with different models and hyperparameters to find the best one.</p>

<p>Train the gradient boosting model on the training set. This involves using the training data to adjust the model’s parameters so that it can make accurate predictions on new data.</p>

<p>Evaluate the trained model on the validation set. This will give you an idea of how well the model is performing and whether it is overfitting or underfitting the data.</p>

<p>Fine-tune the model by adjusting its hyperparameters, training it on different subsets of the training data, or using regularization techniques to prevent overfitting.</p>

<p>Once you are satisfied with the model’s performance on the validation set, evaluate it on the test set to get an estimate of its performance on unseen data.</p>

<p>Use the trained and fine-tuned model to make predictions on new data. This may involve using the model to make predictions on the entire dataset, or using it to make predictions on individual data points as they are received.</p>

<p>Overall, processing data for use with gradient boosting models involves many of the same steps as processing data for other machine learning models. The key difference is that you need to carefully choose a gradient boosting model that is suitable for the problem you want to solve, and you may need to experiment with different hyperparameters to achieve the best performance.</p>

<p>AJ
How should data be formatted to be fed to xgboost?</p>

<p>To be fed to XGBoost, data should be formatted as a matrix with rows representing the samples and columns representing the features. Each sample should be labeled with the correct class, if the problem is a supervised learning task. The matrix should be stored in a NumPy array or a Pandas DataFrame, which are the two most common formats for storing data in Python.</p>

<p>Additionally, XGBoost requires that the data be preprocessed in a specific way. For example, all categorical variables should be encoded as numeric values, and all continuous variables should be scaled so that they have the same range. You can use the DMatrix class in XGBoost to convert the data into the appropriate format for training and testing.</p>

<p>Overall, the key to formatting data for use with XGBoost is to ensure that it is organized as a matrix with rows representing samples and columns representing features, and that it is preprocessed in a way that is suitable for gradient boosting. By following these guidelines, you can prepare your data for use with XGBoost and other gradient boosting models.</p>

<p>AJ
How we can measure the importance of features in regression? and then some special cases.</p>

<p>2 / 2</p>

<p>There are several ways to measure the importance of features in regression. Some common methods include:</p>

<p>Coefficient of determination (R²): This is a measure of how well the model fits the data. It ranges from 0 to 1, with a higher value indicating a better fit.</p>

<p>Coefficient of variation (CV): This is a measure of how much the model’s predictions vary from the true values. It is calculated as the standard deviation of the residuals (the difference between the predicted values and the true values) divided by the mean of the true values.</p>

<p>F-test: This is a statistical test that measures how well the model fits the data compared to a simpler model. The F-statistic is calculated as the ratio of the variance of the residuals in the full model to the variance of the residuals in the simpler model.</p>

<p>Feature importance: Some machine learning algorithms, such as random forests and gradient boosting, can calculate the importance of each feature in the model. This is typically measured as the decrease in model performance when the feature is removed or perturbed.</p>

<p>In some special cases, such as when the data has multicollinearity (meaning that the features are highly correlated with each other), it may be difficult to accurately measure the importance of individual features. In these cases, it may be necessary to use regularization techniques or other methods to improve the model’s ability to accurately measure feature importance.</p>

<hr />
<hr />

<h1 id="regularization">Regularization</h1>

<h2 id="content2">Parameter Norm Penalties</h2>

<ol>
  <li><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents21">Parameter Norms:</strong><br />
 Many regularization approaches are based on limiting the capacity of models by adding a parameter norm penalty \(\Omega(\boldsymbol{\theta})\) to the objective function \(J\). We denote the regularized objective function by \(\tilde{J}\):
    <p>$$\tilde{J}(\boldsymbol{\theta} ; \boldsymbol{X}, \boldsymbol{y})=J(\boldsymbol{\theta} ; \boldsymbol{X}, \boldsymbol{y})+\alpha \Omega(\boldsymbol{\theta}) \tag{7.1}$$</p>
    <p>where \(\alpha \in[0, \infty)\) is a HP that weights the relative contribution of the norm penalty term, \(\Omega\), relative to the standard objective function \(J\).</p>
    <ul>
      <li><strong>Effects of \(\alpha\)</strong>:
        <ul>
          <li>\(\alpha = 0\) results in NO regularization</li>
          <li>Larger values of \(\alpha\) correspond to MORE regularization</li>
        </ul>
      </li>
    </ul>

    <p>The <strong>effect of minimizing the regularized objective function</strong> is that it will <em><strong>decrease</strong></em>, both, <em>the original objective \(J\)</em> on the training data and some <em>measure of the size of the parameters \(\boldsymbol{\theta}\)</em>.</p>

    <p>Different choices for the parameter norm \(\Omega\) can result in different solutions being preferred.<br />
 <br /></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents22">Parameter Penalties and the Bias parameter:</strong><br />
 In NN, we usually penalize <strong>only the weights</strong> of the affine transformation at each layer and we leave the <strong>biases unregularized</strong>.<br />
 Biases typically require less data than the weights to fit accurately. The reason is that <em>each weight specifies how TWO variables interact</em> so fitting the weights well, requires observing both variables in a variety of conditions. However, <em>each bias controls only a single variable</em>, thus, we don’t induce too much <em>variance</em> by leaving the biases unregularized. If anything, regularizing the bias can introduce a significant amount of <em>underfitting</em>.<br />
 <br /></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents23">Note on the \(\alpha\) parameter for different hidden layers:</strong><br />
 In the context of neural networks, it is sometimes desirable to use a separate penalty with a different \(\alpha\) coefficient for each layer of the network. Because it can be expensive to search for the correct value of multiple hyperparameters, it is still reasonable to use the same weight decay at all layers just to reduce the size of search space.<br />
 <br /></p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents24">\(L^2\) Parameter Regularization (Weight Decay):</strong><br />
 It is a regularization strategy that <em>drives the weights closer to the origin</em><sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">1</a></sup> by adding a regularization term:
    <p>$$\Omega(\mathbf{\theta}) = \frac{1}{2}\|\boldsymbol{w}\|_ {2}^{2}$$</p>
    <p>to the objective function.</p>

    <p>In statistics, \(L^2\) regularization is also known as <strong>Ridge Regression</strong> or <strong>Tikhonov Regularization</strong>.</p>

    <p><strong style="color: red">Analyzing Weight Decay:</strong><br />
 <button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Show Analysis</button></p>
    <ul hidden="">
      <li><strong>What happens in a Single Step</strong>:<br />
  We can gain some insight into the behavior of weight decay regularization by studying the gradient of the regularized objective function.<br />
  Take the models objective function:
        <p>$$\tilde{J}(\boldsymbol{w} ; \boldsymbol{X}, \boldsymbol{y})=\frac{\alpha}{2} \boldsymbol{w}^{\top} \boldsymbol{w}+J(\boldsymbol{w} ; \boldsymbol{X}, \boldsymbol{y}) \tag{7.2}$$</p>
        <p>with the corresponding <em>parameter gradient</em>:</p>
        <p>$$\nabla_{\boldsymbol{w}} \tilde{J}(\boldsymbol{w} ; \boldsymbol{X}, \boldsymbol{y})=\alpha \boldsymbol{w}+\nabla_{\boldsymbol{w}} J(\boldsymbol{w} ; \boldsymbol{X}, \boldsymbol{y}) \tag{7.3}$$</p>
        <p>The gradient descent update:</p>
        <p>$$\boldsymbol{w} \leftarrow \boldsymbol{w}-\epsilon\left(\alpha \boldsymbol{w}+\nabla_{\boldsymbol{w}} J(\boldsymbol{w} ; \boldsymbol{X}, \boldsymbol{y})\right) \tag{7.4}$$</p>
        <p>Equivalently:</p>
        <p>$$\boldsymbol{w} \leftarrow(1-\epsilon \alpha) \boldsymbol{w}-\epsilon \nabla_{\boldsymbol{w}} J(\boldsymbol{w} ; \boldsymbol{X}, \boldsymbol{y}) \tag{7.5}$$</p>

        <p>Observe that the addition of the weight decay term has modified the learning rule to <strong>multiplicatively shrink the weight vector by  a constant factor on each step</strong>, just before performing the usual gradient update.</p>
      </li>
      <li><strong>What happens over the Entire course of training</strong>:<br />
  We simplify the analysis by making a quadratic (2nd-order Taylor) approximation to the objective function in the neighborhood of the optimal wight-parameter of the unregularized objective \(\mathbf{w}^{\ast} = \arg \min_{\boldsymbol{w}} J(\boldsymbol{w})\).<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">2</a></sup><br />
  The approximation \(\hat{J}\):
        <p>$$\hat{J}(\boldsymbol{\theta})=J\left(\boldsymbol{w}^{\ast}\right)+\frac{1}{2}\left(\boldsymbol{w}-\boldsymbol{w}^{\ast}\right)^{\top} \boldsymbol{H}(J(\boldsymbol{w}^{\ast}))\left(\boldsymbol{w}-\boldsymbol{w}^{\ast}\right)  \tag{7.6}$$</p>
        <p>where \(\boldsymbol{H}\) is the Hessian matrix of \(J\) with respect to \(\mathbf{w}\) evaluated at \(\mathbf{w}^{\ast}\).</p>

        <p><strong>Notice:</strong></p>
        <ul>
          <li>There is no first-order term in this quadratic approximation, because \(\boldsymbol{w}^{\ast}\)  is defined to be a minimum, where the gradient vanishes.</li>
          <li>Because \(\boldsymbol{w}^{\ast}\) is the location of a minimum of \(J\), we can conclude that \(\boldsymbol{H}\) is <strong>positive semidefinite</strong>.</li>
        </ul>

        <p>The <strong>gradient</strong> of \(\hat{J} + \Omega(\mathbf{\theta})\):</p>
        <p>$$\nabla_{\boldsymbol{w}} \hat{J}(\boldsymbol{w})=\boldsymbol{H}(J(\boldsymbol{w}^{\ast}))\left(\tilde{\boldsymbol{w}}-\boldsymbol{w}^{\ast}\right) + \alpha \tilde{\boldsymbol{w}} \tag{7.7}$$</p>
        <p>And the <strong>minimum</strong> is achieved at \(\nabla_{\boldsymbol{w}} \hat{J}(\boldsymbol{w}) = 0\):</p>
        <p>$$\tilde{\boldsymbol{w}}=(\boldsymbol{H}+\alpha \boldsymbol{I})^{-1} \boldsymbol{H} \boldsymbol{w}^{\ast} \tag{7.10}$$</p>

        <p><strong>Effects:</strong></p>
        <ul>
          <li>As \(\alpha\) approaches \(0\): the regularized solution \(\tilde{\boldsymbol{w}}\) approaches \(\boldsymbol{w}^{\ast}\).</li>
          <li>As \(\alpha\) grows: we apply <strong>spectral decomposition</strong> to the <strong>real and symmetric</strong> \(\boldsymbol{H} = \boldsymbol{Q} \boldsymbol{\Lambda} \boldsymbol{Q}^{\top}\):
            <p>$$\begin{aligned} \tilde{\boldsymbol{w}} &amp;=\left(\boldsymbol{Q} \mathbf{\Lambda} \boldsymbol{Q}^{\top}+\alpha \boldsymbol{I}\right)^{-1} \boldsymbol{Q} \boldsymbol{\Lambda} \boldsymbol{Q}^{\top} \boldsymbol{w}^{\ast} \\ &amp;=\left[\boldsymbol{Q}(\boldsymbol{\Lambda}+\alpha \boldsymbol{I}) \boldsymbol{Q}^{\top}\right]^{-1} \boldsymbol{Q} \boldsymbol{\Lambda} \boldsymbol{Q}^{\top} \boldsymbol{w}^{\ast} \\ &amp;=\boldsymbol{Q}(\boldsymbol{\Lambda}+\alpha \boldsymbol{I})^{-1} \boldsymbol{\Lambda} \boldsymbol{Q}^{\top} \boldsymbol{w}^{\ast} \end{aligned} \tag{7.13}$$</p>
          </li>
        </ul>

        <p>Thus, we see that the effect of weight decay is to rescale \(\boldsymbol{w}^{\ast}\) along the axes defined by the eigenvector of \(\boldsymbol{H}\). Specifically, the component of \(\boldsymbol{w}^{\ast}\) that is aligned with the \(i\)-th eigenvector of \(\boldsymbol{H}\)  is rescaled by a factor of \(\frac{\lambda_{i}}{\lambda_{i}+\alpha}\).</p>

        <p><img src="/main_files/dl_book/regularization/1.png" alt="img" width="100%" /></p>

        <p><strong>Summary:</strong></p>

        <table>
          <tbody>
            <tr>
              <td><strong>Condition</strong></td>
              <td><strong>Effect of Regularization</strong></td>
            </tr>
            <tr>
              <td>\(\lambda_{i}&gt;&gt;\alpha\)</td>
              <td>Not much</td>
            </tr>
            <tr>
              <td>\(\lambda_{i}&lt;&lt;\alpha\)</td>
              <td>The weight value almost shrunk to \(0\)</td>
            </tr>
          </tbody>
        </table>
      </li>
      <li><strong>Applying \(L^2\) regularization to <em>Linear Regression</em> :</strong>
        <ul>
          <li><button class="showText" value="show" onclick="showTextPopHide(event);">Application to Linear Regression</button>
  <img src="/main_files/dl_book/regularization/2.png" alt="img" width="100%" hidden="" /></li>
        </ul>
      </li>
      <li><a href="https://himarora.github.io/regularization/understanding-the-mathematics-of-weight-decay/">Weight Decay Analysis (blog)</a></li>
    </ul>
    <p><br /></p>

    <p><strong style="color: red">\(L^2\) Regularization Derivation:</strong><br />
 \(L^2\) regularization is equivalent to <strong>MAP Bayesian inference with a Gaussian prior on the weights</strong>.</p>

    <p><strong>The MAP Estimate:</strong><br />
 <button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Show MAP Estimate Derivation</button></p>
    <p hidden="">$$\begin{aligned} \hat{\theta}_ {\mathrm{MAP}} &amp;=\arg \max_{\theta} P(\theta \vert y) \\ &amp;=\arg \max_{\theta} \frac{P(y \vert \theta) P(\theta)}{P(y)} \\ &amp;=\arg \max_{\theta} P(y \vert \theta) P(\theta) \\ &amp;=\arg \max_{\theta} \log (P(y \vert \theta) P(\theta)) \\ &amp;=\arg \max_{\theta} \log P(y \vert \theta)+\log P(\theta) \end{aligned}$$</p>

    <p>We place a <strong>Gaussian Prior</strong> on the weights, with <strong>zero mean</strong> and <strong>equal variance \(\tau^2\)</strong>:</p>
    <p>$$\begin{aligned} \hat{\theta}_ {\mathrm{MAP}} &amp;=\arg \max_{\theta} \log P(y \vert \theta)+\log P(\theta) \\ &amp;=\arg \max _{\boldsymbol{w}}\left[\log \prod_{i=1}^{n} \dfrac{1}{\sigma \sqrt{2 \pi}} e^{-\dfrac{\left(y_{i}-\boldsymbol{w}^{\top}\boldsymbol{x}_i\right)^{2}}{2 \sigma^{2}}}+\log \prod_{j=0}^{p} \dfrac{1}{\tau \sqrt{2 \pi}} e^{-\dfrac{w_{j}^{2}}{2 \tau^{2}}} \right] \\ &amp;=\arg \max _{\boldsymbol{w}} \left[-\sum_{i=1}^{n} \dfrac{\left(y_{i}-\boldsymbol{w}^{\top}\boldsymbol{x}_i\right)^{2}}{2 \sigma^{2}}-\sum_{j=0}^{p} \dfrac{w_{j}^{2}}{2 \tau^{2}}\right] \\ &amp;=\arg \min_{\boldsymbol{w}} \dfrac{1}{2 \sigma^{2}}\left[\sum_{i=1}^{n}\left(y_{i}-\boldsymbol{w}^{\top}\boldsymbol{x}_i\right)^{2}+\dfrac{\sigma^{2}}{\tau^{2}} \sum_{j=0}^{p} w_{j}^{2}\right] \\ &amp;=\arg \min_{\boldsymbol{w}} \left[\sum_{i=1}^{n}\left(y_{i}-\boldsymbol{w}^{\top}\boldsymbol{x}_i\right)^{2}+\lambda \sum_{j=0}^{p} w_{j}^{2}\right] \\ &amp;= \arg \min_{\boldsymbol{w}} \left[ \|XW - \boldsymbol{y}\|^2 + \lambda {\|\boldsymbol{w}\|_ 2}^2\right]\end{aligned}$$</p>
    <p><button class="showText" value="show" onclick="showTextPopHide(event);">Different Notation</button>
 <img src="/main_files/dl_book/regularization/4.png" alt="img" width="100%" hidden="" /> <br />
 <br /></p>

    <p id="lst-p"><strong style="color: red">Properties:</strong></p>
    <ul>
      <li>Notice that L2-regularization has a rotational invariance. This actually makes it more sensitive to irrelevant features.  <a href="https://www.cs.ubc.ca/~schmidtm/Courses/540-W18/L6.pdf">[Ref]</a>
        <blockquote>
          <p><a href="https://icml.cc/Conferences/2004/proceedings/papers/354.pdf">Paper</a></p>
        </blockquote>
      </li>
      <li>Adding L2-regularization to a convex function gives a strongly-convex function. So L2-regularization can make gradient descent converge much faster.  (^ same ref)</li>
    </ul>

    <p id="lst-p"><strong style="color: red">Notes:</strong></p>
    <ul>
      <li><a href="https://thomas-tanay.github.io/post--L2-regularization/">L2-reg and Adversarial Examples (New Angle)</a></li>
      <li><a href="https://himarora.github.io/regularization/understanding-the-mathematics-of-weight-decay/">Weight Decay Analysis (blog)</a></li>
      <li><strong>Interpreting the Penalty term \(\lambda\):</strong><br />
  In the Bayesian Interpretation of Weight Decay; by assuming a <strong>Gaussian Distribution</strong> on the errors:<br />
  the Penalty term \(\lambda\) can be interpreted as the (squared) <span style="color: purple">ratio of two <strong>variances</strong></span> of the <strong>data</strong> and the <strong>weights</strong> respectively: \(\lambda = \frac{\sigma_{D}^{2}}{\sigma_{W}^{2}}\).
        <ul>
          <li><a href="https://www.youtube.com/watch?v=_WZAD2uhvUM&amp;list=PLiPvV5TNogxKKwvKb1RKwkq2hm7ZvpHz0&amp;index=60">Learning the <strong>VARIANCES</strong> of the distributions/Priors (Hinton!)</a></li>
        </ul>
      </li>
      <li><a href="https://explained.ai/regularization/index.html">A visual explanation for regularization of linear models (Blog)</a><br />
 <br /></li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents25">\(L^1\) Regularization:</strong><br />
 \(L^1\) Regularization is another way to regulate the model by <em>penalizing the size of its parameters</em>; the technique adds a regularization term:
    <p>$$\Omega(\boldsymbol{\theta})=\|\boldsymbol{w}\|_{1}=\sum_{i}\left|w_{i}\right| \tag{7.18}$$</p>
    <p>which is a sum of absolute values of the individual parameters.</p>

    <p>The regularized objective function is given by:</p>
    <p>$$\tilde{J}(\boldsymbol{w} ; \boldsymbol{X}, \boldsymbol{y})=\alpha\|\boldsymbol{w}\|_ {1}+J(\boldsymbol{w} ; \boldsymbol{X}, \boldsymbol{y}) \tag{7.19}$$</p>
    <p>with the corresponding (sub) gradient:</p>
    <p>$$\nabla_{\boldsymbol{w}} \tilde{J}(\boldsymbol{w} ; \boldsymbol{X}, \boldsymbol{y})=\alpha \operatorname{sign}(\boldsymbol{w})+\nabla_{\boldsymbol{w}} J(\boldsymbol{X}, \boldsymbol{y} ; \boldsymbol{w}) \tag{7.20}$$</p>

    <p>Notice that the regularization contribution to the gradient, <strong>no longer scales linearly with each \(w_i\)</strong>; instead it is a <strong>constant factor with a sign = \(\text{sign}(w_i)\)</strong>.</p>

    <p>[Analysis]</p>

    <p><strong>Sparsity of the \(L^1\) regularization:</strong><br />
 In comparison to \(L^2\), \(L^1\) regularization results in a solution that is more <strong>sparse</strong>.<br />
 The <em>sparsity property</em> has been used extensively as a <strong>feature selection</strong> mechanism.</p>
    <ul>
      <li><strong>LASSO</strong>: The Least Absolute Shrinkage and Selection Operator integrates an \(L^1\) penalty with a <em>linear model</em> and a <em>least-squares cost function</em>.<br />
  The \(L^1\) penalty causes a subset of the weights to become <strong>zero</strong>, suggesting that the corresponding features may safely be discarded.</li>
    </ul>

    <p><strong style="color: red">\(L^1\) Regularization Derivation:</strong><br />
 \(L^1\) regularization is equivalent to (the log-prior term in) <strong>MAP Bayesian inference with an isotropic Laplace distribution prior on the weights</strong>:</p>
    <p>$$\log p(\boldsymbol{w})=\sum_{i} \log \operatorname{Laplace}\left(w_{i} ; 0, \frac{1}{\alpha}\right)=-\alpha\|\boldsymbol{w}\|_ {1}+n \log \alpha-n \log 2 \tag{7.24}$$</p>
    <p>note that we can ignore the terms \(\log \alpha-\log 2\) because they do not depend on \(\boldsymbol{w}\).    <br />
 <button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Derivation</button></p>
    <p hidden="">$$\begin{aligned} \hat{\theta}_ {\mathrm{MAP}} &amp;=\arg \max_{\theta} \log P(y \vert \theta)+\log P(\theta) \\  &amp;=\arg \max _{\boldsymbol{w}}\left[\log \prod_{i=1}^{n} \dfrac{1}{\sigma \sqrt{2 \pi}} e^{-\dfrac{\left(y_{i}-\boldsymbol{w}^{\top}\boldsymbol{x}_i\right)^{2}}{2 \sigma^{2}}}+\log \prod_{j=0}^{p} \dfrac{1}{2 b} e^{-\dfrac{\left|\theta_{j}\right|}{2 b}} \right] \\    &amp;=\arg \max _{\boldsymbol{w}} \left[-\sum_{i=1}^{n} \dfrac{\left(y_{i}-\boldsymbol{w}^{\top}\boldsymbol{x}_i\right)^{2}}{2 \sigma^{2}}-\sum_{j=0}^{p} \dfrac{\left|w_{j}\right|}{2 b}\right] \\    &amp;=\arg \min_{\boldsymbol{w}} \dfrac{1}{2 \sigma^{2}}\left[\sum_{i=1}^{n}\left(y_{i}-\boldsymbol{w}^{\top}\boldsymbol{x}_i\right)^{2}+\dfrac{\sigma^{2}}{b} \sum_{j=0}^{p}\left|w_{j}\right|\right] \\    &amp;=\arg \min_{\boldsymbol{w}} \left[\sum_{i=1}^{n}\left(y_{i}-\boldsymbol{w}^{\top}\boldsymbol{x}_i\right)^{2}+\lambda \sum_{j=0}^{p}\left|w_{j}\right|\right] \\    &amp;= \arg \min_{\boldsymbol{w}} \left[ \|XW - \boldsymbol{y}\|^2 + \lambda \|\boldsymbol{w}\|_ 1\right]\end{aligned}$$</p>

    <p id="lst-p"><strong style="color: red">Properties:</strong></p>
    <ul>
      <li>\(L^1\) regularization can occasionally produce non-unique solutions. A simple example is provided in the figure when the space of possible solutions lies on a 45 degree line. 
 <br /></li>
    </ul>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents26">\(L^1\) VS \(L^2\) Regularization:</strong></p>

    <ul>
      <li><strong>Feature Correlation and Sparsity</strong>:
        <ul>
          <li><strong>Identical features</strong>:
            <ul>
              <li>\(L^1\) regularization spreads weight arbitrarily (all weights same sign)</li>
              <li>\(L^2\) regularization spreads weight evenly</li>
            </ul>
          </li>
          <li><strong>Linearly related features</strong>:
            <ul>
              <li>\(L^1\) regularization chooses variable with larger scale, \(0\) weight to others</li>
              <li>\(L^2\) prefers variables with larger scale — spreads weight proportional to scale
                <blockquote>
                  <p><a href="https://www.youtube.com/watch?v=KIoz_aa1ed4&amp;list=PLnZuxOufsXnvftwTB1HL6mel1V32w0ThI&amp;index=7">Reference</a></p>
                </blockquote>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>

    <p><strong style="color: red">Interpreting Sparsity with an Example:</strong><br />
 Let’s imagine we are estimating two coefficients in a regression. In \(L^2\) regularization, the solution \(\boldsymbol{w} =(0,1)\) has the same weight as \(\boldsymbol{w}=(\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}})\)  so they are both treated equally. In \(L^1\) regularization, the same two solutions favor the sparse one:</p>
    <p>$$\|(1,0)\|_{1}=1&lt;\left\|\left(\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}}\right)\right\|_{1}=\sqrt{2}$$</p>
    <p>So \(L^2\) regularization doesn’t have any specific built in mechanisms to favor zeroed out coefficients, while \(L^1\) regularization actually favors these sparser solutions.</p>
    <blockquote>
      <p><a href="https://www.quora.com/What-is-the-difference-between-L1-and-L2-regularization-How-does-it-solve-the-problem-of-overfitting-Which-regularizer-to-use-and-when">Extensive Discussions on Sparsity (Quora)</a></p>
    </blockquote>

    <p><br /></p>
  </li>
</ol>

<p><strong style="color: red">Notes:</strong></p>
<ul>
  <li><strong>Elastic Net Regularization:</strong>
    <p>$$\Omega = \lambda\left(\alpha\|w\|_{1}+(1-\alpha)\|w\|_{2}^{2}\right), \alpha \in[0,1]$$</p>
    <ul>
      <li>Combines both \(L^1\) and \(L^2\)</li>
      <li>Used to <strong>produce sparse solutions</strong>, but to avoid the problem of \(L^1\) solutions being sometimes <strong>Non-Unique</strong>
        <ul>
          <li>The problem mainly arises with <strong>correlated features</strong></li>
        </ul>
      </li>
      <li>Elastic net regularization tends to have a grouping effect, where correlated input features are assigned equal weights.</li>
    </ul>
  </li>
</ul>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:2" role="doc-endnote">
      <p>More generally, we could regularize the parameters to be near any specific point in space and, surprisingly, still get a regularization effect, but better results will be obtained for a value closer to the true one, with zero being a default value that makes sense when we do not know if the correct value should be positive or negative. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>The approximation is perfect if the objective function is truly quadratic, as in the case of <strong>linear regression w/ MSE</strong>. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>


      <footer class="site-footer">
    <!--   <span class="site-footer-owner"><a href="http://localhost:8889">Ahmad Badary</a> is maintained by <a href="https://ahmedbadary.github.io/">Ahmad Badary</a>.</span> -->
    
<!--  -->
    <div class="wrapper">
        <div class="footer-col-wrapper">
            <div class="footer-col footer-col-1">
            <span class="site-footer-owner"><a href="http://localhost:8889">Site</a> maintained by <a href="https://ahmedbadary.github.io/">Ahmad Badary</a>.</span>
    <span class="site-footer-credits">
        <p>
            &copy; 2017. All rights reserved.
        </p> 
    </span>
            </div>
            <div class="footer-col footer-col-2">
            <div><p>         </p></div>
            </div>
            <div class="footer-col footer-col-3">
                <ul class="social-media-list">
                    
                      <li>
                        <a href="https://github.com/AhmedBadary">
                          <i class="fa fa-github"></i> GitHub
                        </a>
                      </li>
                    
                    
                      <li>
                        <a href="https://linkedin.com/in/ahmad-badary-656098121/">
                          <i class="fa fa-linkedin"></i> LinkedIn
                        </a>
                      </li>
                    
                    
                      <li>
                        <a href="https://www.facebook.com/ahmed.thabet.94">
                          <i class="fa fa-facebook"></i> Facebook
                        </a>
                      </li>
                    
                </ul>
            </div>
        </div>
    </div>
<!--  -->
</footer>


    </section>

  </body>

<!-- Table of Content Script -->
<script type="text/javascript">
var bodyContents = $(".bodyContents1");
$("<ol>").addClass("TOC1ul").appendTo(".TOC1");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC1ul");
     });
// 
var bodyContents = $(".bodyContents2");
$("<ol>").addClass("TOC2ul").appendTo(".TOC2");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC2ul");
     });
// 
var bodyContents = $(".bodyContents3");
$("<ol>").addClass("TOC3ul").appendTo(".TOC3");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC3ul");
     });
//
var bodyContents = $(".bodyContents4");
$("<ol>").addClass("TOC4ul").appendTo(".TOC4");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC4ul");
     });
//
var bodyContents = $(".bodyContents5");
$("<ol>").addClass("TOC5ul").appendTo(".TOC5");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC5ul");
     });
//
var bodyContents = $(".bodyContents6");
$("<ol>").addClass("TOC6ul").appendTo(".TOC6");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC6ul");
     });
//
var bodyContents = $(".bodyContents7");
$("<ol>").addClass("TOC7ul").appendTo(".TOC7");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC7ul");
     });
//
var bodyContents = $(".bodyContents8");
$("<ol>").addClass("TOC8ul").appendTo(".TOC8");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC8ul");
     });
//
var bodyContents = $(".bodyContents9");
$("<ol>").addClass("TOC9ul").appendTo(".TOC9");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC9ul");
     });

</script>

<!-- VIDEO BUTTONS SCRIPT -->
<script type="text/javascript">
  function iframePopInject(event) {
    var $button = $(event.target);
    // console.log($button.parent().next());
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $figure = $("<div>").addClass("video_container");
        $iframe = $("<iframe>").appendTo($figure);
        $iframe.attr("src", $button.attr("src"));
        // $iframe.attr("frameborder", "0");
        $iframe.attr("allowfullscreen", "true");
        $iframe.css("padding", "4px 6px");
        $button.next().css("display", "block");
        $figure.appendTo($button.next());
        $button.text("Hide Video")
    } else {
        $button.attr('value', 'show');
        $button.next().html("");
        $button.text("Show Video")
    }
}
</script>

<!-- BUTTON TRY -->
<script type="text/javascript">
  function iframePopA(event) {
    event.preventDefault();
    var $a = $(event.target).parent();
    console.log($a);
    if ($a.attr('value') == 'show') {
        $a.attr('value', 'hide');
        $figure = $("<div>");
        $iframe = $("<iframe>").addClass("popup_website_container").appendTo($figure);
        $iframe.attr("src", $a.attr("href"));
        $iframe.attr("frameborder", "1");
        $iframe.attr("allowfullscreen", "true");
        $iframe.css("padding", "4px 6px");
        $a.next().css("display", "block");
        $figure.appendTo($a.next().next());
        // $a.text("Hide Content")
        $('html, body').animate({
            scrollTop: $a.offset().top
        }, 1000);
    } else {
        $a.attr('value', 'show');
        $a.next().next().html("");
        // $a.text("Show Content")
    }

    $a.next().css("display", "inline");
}
</script>


<!-- TEXT BUTTON SCRIPT - INJECT -->
<script type="text/javascript">
  function showTextPopInject(event) {
    var $button = $(event.target);
    var txt = $button.attr("input");
    console.log(txt);
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $p = $("<p>");
        $p.html(txt);
        $button.next().css("display", "block");
        $p.appendTo($button.next());
        $button.text("Hide Content")
    } else {
        $button.attr('value', 'show');
        $button.next().html("");
        $button.text("Show Content")
    }

}
</script>

<!-- TEXT BUTTON SCRIPT - HIDDEN / HIDE / SHOW / HIDE/SHOW -->
<script type="text/javascript">
  function showTextPopHide(event) {
    var $button = $(event.target);
    // var txt = $button.attr("input");
    var txt = $button.text();
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $button.next().removeAttr("hidden");
        $button.text(txt + " - Hide Content");
    } else {
        $button.attr('value', 'show');
        $button.next().attr("hidden", "");
        $button.text(txt.replace(" - Hide Content",""));
    }
}
</script>

<!-- TEXT BUTTON SCRIPT - HIDDEN / HIDE / SHOW / HIDE/SHOW -->
<script type="text/javascript">
  function showText_withParent_PopHide(event) {
    var $button = $(event.target);
    var $parent = $button.parent();
    var txt = $button.text();
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $parent.next().removeAttr("hidden");
        $button.text(txt + " - Hide Content");
    } else {
        $button.attr('value', 'show');
        $parent.next().attr("hidden", "");
        $button.text(txt.replace(" - Hide Content",""));
    }
}
</script>

<!-- Print / Printing / printme -->
<!-- <script type="text/javascript">
i = 0

for (var i = 1; i < 6; i++) {
    var bodyContents = $(".bodyContents" + i);
    $("<p>").addClass("TOC1ul")  .appendTo(".TOC1");
    bodyContents.each(function(index, element) {
        var paragraph = $(element);
        $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC1ul");
         });
} 
</script>
 -->
 
</html>

