<!DOCTYPE html>
<html lang="en-us">
  <head>
  <meta charset="UTF-8">
  <title>Ahmad Badary</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <link rel="stylesheet" href="/css/normalize.css">
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="/css/cayman.css">
  <link rel="stylesheet" href="/css/style.css">
  <link rel="shortcut icon" href="/main_files/favicon.ico" />
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="/css/customStyle.css">
  <title> » Ahmad Badary</title>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
</head>

  <body>
    <nav class="main-nav">
    <a href="https://ahmedbadary.github.io/" class="main-nav-logo">
        <img src="/main_files/logo.png">
    </a>
    <ul id="menu-main" class="main-nav-items">
        <li id="menu-item-1859" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-home menu-item-1859">
            <a href="/">Home</a>
        </li>
        <li id="menu-item-2869" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-2869">
            <a href="/work">Work</a>
        </li>
        <li id="menu-item-1892" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1892">
            <a href="/projects">Projects</a>
        </li>
        <li id="menu-item-1858" class="menu-item menu-item-type-post_type menu-item-object-page current_page_parent menu-item-1858">
            <a href="/blog">Blog</a>
        </li>
        <li id="menu-item-1862" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1862">
            <a href="/about">About</a>
        </li>
    </ul>
</nav>


<section class="page-header">
  <h1 class="project-name">Text Classification</h1>
  <h2 class="project-tagline"></h2>
  <a href="/#" class="btn">Home</a>
  <a href="/work" class="btn">Work-Space</a>
  <a href= /work_files/research/nlp.html class="btn">Previous</a>
</section>

<!-- <div>
  <ul class="posts">
    
      <li><span>02 Jan 2014</span> &raquo; <a href="/2014/01/02/introducing-Ahmad/">Introducing Ahmad</a></li>
    
  </ul>
</div> -->


    <section class="main-content">
      
      <div class="TOC">
  <h1 id="table-of-contents">Table of Contents</h1>

  <ul class="TOC1">
    <li><a href="#content1">Introduction and Definitions</a></li>
  </ul>
  <ul class="TOC2">
    <li><a href="#content2">The Naive Bayes Classifier</a></li>
  </ul>
  <ul class="TOC3">
    <li><a href="#content3">Evalutaion of Text Classification</a></li>
  </ul>
  <ul class="TOC4">
    <li><a href="#content4">General Discussion of Issues in Text Classification</a></li>
  </ul>
</div>

<hr />
<hr />

<h2 id="content1">Introduction and Definitions</h2>

<ol>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents11">Text Classification:</strong></dt>
      <dd>The task of assigning a piece of text to one or more classes or categories.</dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents12">Applications:</strong></dt>
      <dd>
        <ul>
          <li><strong>Spam Filtering</strong>: discerning spam emails form legitimate emails.</li>
          <li><strong>Email Routing</strong>: sending an email sento to a genral address to a specfic affress based on the topic.</li>
          <li><strong>Language Identification</strong>: automatiacally determining the genre of a piece of text.</li>
          <li>Readibility Assessment__: determining the degree of readability of a piece of text.</li>
          <li><strong>Sentiment Analysis</strong>: determining the general emotion/feeling/attitude of the author of a piece of text.</li>
          <li><strong>Authorship Attribution</strong>: determining which author wrote which piece of text.</li>
          <li><strong>Age/Gender Identification</strong>: determining the age and/or gender of the author of a piece of text.</li>
        </ul>
      </dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents13">Classification Methods:</strong></dt>
      <dd>
        <ul>
          <li><strong>(Hand-Coded)Rules-Based Algorithms</strong>: use rules based on combinations of words or other features.
            <ul>
              <li>Can have high accuracy if the rules are carefully refined and maintained by experts.</li>
              <li>However, building and maintaining these rules is very hard.</li>
            </ul>
          </li>
          <li><strong>Supervised Machine Learning</strong>: using an ML algorithm that trains on a training set of (document, class) elements to train a classifier.
            <ul>
              <li><em>Types of Classifiers</em>:
                <ul>
                  <li>Naive Bayes</li>
                  <li>Logistic Regression</li>
                  <li>SVMs</li>
                  <li>K-NNs</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </dd>
    </dl>
  </li>
</ol>

<hr />

<h2 id="content2">The Naive Bayes Classifier</h2>

<ol>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents21">Naive Bayes Classifiers:</strong></dt>
      <dd>are a family of simple probabilitic classifiers based on applying <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem"><em>Bayes’ Theorem</em></a> with strong (naive) independence assumptions between the features.</dd>
      <dd><strong>The Probabilistic Model</strong>:<br />
Abstractly, naive Bayes is a conditional probability model: given a problem instance to be classified, represented by a vector <script type="math/tex">{\displaystyle \mathbf {x} =(x_{1},\dots ,x_{n})}}=(x_{1},\dots ,x_{n})</script> representing some n features (independent variables), it assigns to this instance probabilities</dd>
      <dd>
        <script type="math/tex; mode=display">{\displaystyle p(C_{k}\mid x_{1},\dots ,x_{n})\,}</script>
      </dd>
      <dd>for each of the <script type="math/tex">k</script> possible outcome or classes <script type="math/tex">C_k</script>.</dd>
      <dd>Now, using <em>Bayes’ Theorem</em> we decompose the conditional probability as:</dd>
      <dd>
        <script type="math/tex; mode=display">{\displaystyle p(C_{k}\mid \mathbf {x} )={\frac {p(C_{k})\ p(\mathbf {x} \mid C_{k})}{p(\mathbf {x} )}}\,}</script>
      </dd>
      <dd>Or, equivalenty, and more intuitively:</dd>
      <dd>
        <script type="math/tex; mode=display">{\displaystyle {\mbox{posterior}}={\dfrac{\text{prior} \times \text{likelihood}}{\text{evidence}}}\,}</script>
      </dd>
      <dd>We can disregard the <em>Denomenator</em> since it does <strong>not</strong> depend on the classes <script type="math/tex">C</script>, making it a constant.</dd>
      <dd>Now, using the <em>Chain-Rule</em> for repeated application of the conditional probability,   the joint probability model can be rewritten as:</dd>
      <dd>
        <script type="math/tex; mode=display">p(C_{k},x_{1},\dots ,x_{n})\, = p(x_{1}\mid x_{2},\dots ,x_{n},C_{k})p(x_{2}\mid x_{3},\dots ,x_{n},C_{k})\dots p(x_{n-1}\mid x_{n},C_{k})p(x_{n}\mid C_{k})p(C_{k})</script>
      </dd>
      <dd>Applying the naive conditional independence assumptions,
        <blockquote>
          <p>i.e. assume that each feature <script type="math/tex">{\displaystyle x_{i}}</script> is conditionally independent of every other feature <script type="math/tex">{\displaystyle x_{j}}</script> for <script type="math/tex">{\displaystyle j\neq i}</script>, given the category <script type="math/tex">{\displaystyle C}</script></p>
        </blockquote>
      </dd>
      <dd>
        <script type="math/tex; mode=display">\implies \\ 
 {\displaystyle p(x_{i}\mid x_{i+1},\dots ,x_{n},C_{k})=p(x_{i}\mid C_{k})\,}.</script>
      </dd>
      <dd>Thus, we can write the join probability model as:</dd>
      <dd>
        <script type="math/tex; mode=display">{\displaystyle p(C_{k}\mid x_{1},\dots ,x_{n})={\frac {1}{Z}}p(C_{k})\prod _{i=1}^{n}p(x_{i}\mid C_{k})}</script>
      </dd>
      <dd>Where, <script type="math/tex">{\displaystyle Z=p(\mathbf {x} )=\sum _{k}p(C_{k})\ p(\mathbf {x} \mid C_{k})}</script> is a <strong>constant</strong> scaling factor, a function of the, <em>known</em>, feature variables.</dd>
      <dd>
        <p><strong>The Decision Rule</strong>: we commonly use the <a href="https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation"><em>Maximum A Posteriori (MAP)</em></a> hypothesis, as the decision rule.</p>
      </dd>
      <dd>Thus, <strong>the classifier</strong> becomes:</dd>
      <dd>
        <script type="math/tex; mode=display">{\displaystyle {\hat {y}}={\underset {k\in \{1,\dots ,K\}}{\operatorname {argmax} }}\ p(C_{k})\displaystyle \prod _{i=1}^{n}p(x_{i}\mid C_{k}).}</script>
      </dd>
      <dd>A function that assigns a class label <script type="math/tex">{\displaystyle {\hat {y}}=C_{k}}</script> for some <script type="math/tex">k</script>.</dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents22">Multinomial Naive Bayes:</strong></dt>
      <dd>With a multinomial event model, samples (feature vectors) represent the frequencies with which certain events have been generated by a multinomial <script type="math/tex">{\displaystyle (p_{1},\dots ,p_{n})}</script> where <script type="math/tex">{\displaystyle p_{i}}</script> is the probability that event <script type="math/tex">i</script> occurs.</dd>
      <dd>The likelihood of observing a feature vector (histogram) <script type="math/tex">\mathbf{x}</script> is given by:</dd>
      <dd>
        <script type="math/tex; mode=display">{\displaystyle p(\mathbf {x} \mid C_{k})={\frac {(\sum _{i}x_{i})!}{\prod _{i}x_{i}!}}\prod _{i}{p_{ki}}^{x_{i}}}</script>
      </dd>
      <dd>The multinomial naive Bayes classifier becomes a linear classifier when expressed in log-space:</dd>
      <dd>
        <script type="math/tex; mode=display">% <![CDATA[
{\displaystyle {\begin{aligned}\log p(C_{k}\mid \mathbf {x} )&\varpropto \log \left(p(C_{k})\prod _{i=1}^{n}{p_{ki}}^{x_{i}}\right)\\&=\log p(C_{k})+\sum _{i=1}^{n}x_{i}\cdot \log p_{ki}\\&=b+\mathbf {w} _{k}^{\top }\mathbf {x} \end{aligned}}} %]]></script>
      </dd>
      <dd>where <script type="math/tex">{\displaystyle b=\log p(C_{k})}</script> and <script type="math/tex">{\displaystyle w_{ki}=\log p_{ki}}</script>.</dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents23">Bag-of-Words:</strong></dt>
      <dd>The <strong>bag-of-words model</strong> (or <strong>vector-space-model</strong>) is a simplifying representation of text/documents.</dd>
      <dd>A text is represented as the bag (Multi-Set) of its words with multiplicity, disregarding any grammatrical rules and word-orderings.</dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents24">The Simplifying Assumptions Used:</strong></dt>
      <dd>
        <ul>
          <li><strong>Bag-of-Words</strong>: we assume that the position of the words does <em>not</em> matter.</li>
          <li><strong>Naive Independence</strong>: the feature probabilities are indpendenet given a class <script type="math/tex">c</script>.</li>
        </ul>
      </dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents25">Learning the Multi-Nomial Naive Bayes Model:</strong></dt>
      <dd>
        <ul>
          <li><strong>The Maximum Likelihood Estimate</strong>: we simply use the frequencies in the date.
            <ul>
              <li><script type="math/tex">\hat{P}(c_j) = \dfrac{\text{doc-count}(C=c_j)}{N_\text{doc}}</script>
                <blockquote>
                  <p>The <em>Prior Probability</em> of a document being in class <script type="math/tex">c_j</script>, is the fraction of the documents in the training data that are in class <script type="math/tex">c_j</script>.</p>
                </blockquote>
              </li>
              <li><script type="math/tex">\hat{P}(w_i | c_i) = \dfrac{\text{count}(w_i,c_j)}{\sum_{w \in V} \text{count}(w, c_j)}</script>
                <blockquote>
                  <p>The <em>likelihood</em> of the word <script type="math/tex">w_i</script> given a class <script type="math/tex">c_j</script>, is the fraction of the occurunces of the word <script type="math/tex">w_i</script> in class <script type="math/tex">c_j</script> over all words in the class.</p>
                </blockquote>
              </li>
            </ul>
          </li>
        </ul>
      </dd>
      <dd>
        <ul>
          <li><strong>The Problem with Maximum Likelihood</strong>:<br />
  If a certain word occurs in the test-set but <strong>not</strong> in the training set, the likelihood of that word given the equation above will be set to <script type="math/tex">0</script>.<br />
  Now, since we are multiplying all the likelihood terms together, the MAP estimate will be set to <script type="math/tex">0</script> as well, regardless of the other values.</li>
        </ul>
      </dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents26">Solutions to the MLE Problem:</strong></dt>
      <dd>Usually the problem of reducing the estimate to zero is solved by adding a regularization technique known as <em>smoothing</em>.</dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents27">Lidstone Smoothing (additive smoothing):</strong></dt>
      <dd>is a technique used to smooth categorical data as the following:</dd>
      <dd>Given an observation vector <script type="math/tex">x = (x_1, \ldots, x_d)</script> from a multinomial distribution with <script type="math/tex">N</script> trials, a <em>smoothed</em> version of the data produces the estimators:</dd>
      <dd>
        <script type="math/tex; mode=display">{\hat {\theta }}_{i}={\frac {x_{i}+\alpha }{N+\alpha d}}\qquad (i=1,\ldots ,d),</script>
      </dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents28">Laplace Smoothing:</strong></dt>
      <dd>is a special case of additive smoothing (Lidstone Smoothing) with <script type="math/tex">\alpha = 1</script>:</dd>
      <dd>
        <script type="math/tex; mode=display">{\hat {\theta }}_{i}={\frac {x_{i}+1 }{N+ d}}\qquad (i=1,\ldots ,d),</script>
      </dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents29">The Algorithm:</strong></dt>
      <dd>
        <ul>
          <li>Extract the <em>Vocabulary</em> from the trianing data</li>
          <li>Calculate <script type="math/tex">P(c_j)</script> terms
            <ul>
              <li>For each <script type="math/tex">c_j \in C</script> do
                <ul>
                  <li><script type="math/tex">\text{docs}_j \leftarrow</script> all docs with class <script type="math/tex">=c_j</script></li>
                  <li>
                    <script type="math/tex; mode=display">P(c_j) \leftarrow \dfrac{\|\text{docs}_j\|}{\|\text{total # docs}\|}</script>
                  </li>
                </ul>
              </li>
            </ul>
          </li>
          <li>Calculate <script type="math/tex">P(w_k \| c_j)</script> terms
            <ul>
              <li><script type="math/tex">\text{Text}_j \leftarrow</script> single doc containing all <script type="math/tex">\text{docs}_j</script></li>
              <li>For each word <script type="math/tex">w_k \in</script> Vocab.
                <ul>
                  <li><script type="math/tex">n_k \leftarrow</script> # of occurunces of <script type="math/tex">w_k \in \text{Text}_j</script></li>
                  <li>
                    <script type="math/tex; mode=display">P(w_k \| c_j) \leftarrow \dfrac{n_k + \alpha}{n + \alpha \|Vocab.\|}</script>
                  </li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents210">Summary:</strong></dt>
      <dd>
        <ul>
          <li>Very fast</li>
          <li>Low storage requirements</li>
          <li>Robust to Irrelevant Features
            <ul>
              <li>Irrelevant features cancel each other out.</li>
            </ul>
          </li>
          <li>Works well in domains with many equally important features
            <ul>
              <li>Decision Trees_ suffer from fragmentation in such cases - especially if there is little data.</li>
            </ul>
          </li>
          <li>It is <em>Optimal</em> if the independence conditions hold.</li>
        </ul>
      </dd>
    </dl>
  </li>
</ol>

<hr />

<h2 id="content3">Evaluation of Text Classification</h2>

<ol>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents31">The <script type="math/tex">2x2</script> Contingency Table:</strong></dt>
      <dd>
        <table>
          <tbody>
            <tr>
              <td> </td>
              <td><strong>correct</strong> (Spam)</td>
              <td><strong>not correct</strong> (not Spam)</td>
            </tr>
            <tr>
              <td><strong>selected</strong> (Spam)</td>
              <td>tp</td>
              <td>fp</td>
            </tr>
            <tr>
              <td><strong>not selected</strong> (not Spam)</td>
              <td>fn</td>
              <td>tn</td>
            </tr>
          </tbody>
        </table>
      </dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents32">Accuracy:</strong></dt>
      <dd>
        <script type="math/tex; mode=display">\text{Acc} = \dfrac{\text{tp} + \text{tn}}{\text{tp} + \text{fp} + \text{fn} + \text{tn}}</script>
      </dd>
      <dd><strong>The Problem</strong>:<br />
Accuracy can be easily fooled (i.e. produce a very high number) in a scenario where the number of occurrences of a class we desire is much less than the data we are searching.</dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents33">Precision (positive predictive value (PPV)):</strong></dt>
      <dd>is the fraction of relevant instances among the retrieved instances.</dd>
      <dd><strong>Equivalently</strong>,<br />
the % of selected items that are correct.</dd>
      <dd>
        <script type="math/tex; mode=display">{\displaystyle {\text{Precision}}={\frac {tp}{tp+fp}}\,}</script>
      </dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents34">Recall (True Positive Rate), (Sensitivity):</strong></dt>
      <dd>Also referred to as the <strong>true positive rate</strong> or <strong>sensitivity</strong>.</dd>
      <dd>is the fraction of relevant instances that have been retrieved over the total amount of relevant instances.</dd>
      <dd><strong>Equivalently</strong>,<br />
the % of correct items that are selected.</dd>
      <dd>
        <script type="math/tex; mode=display">{\displaystyle {\text{Recall}}={\frac {tp}{tp+fn}}\,}</script>
      </dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents35">The Trade-Off:</strong></dt>
      <dd>Usually, the two measures discusses above have an inverse relation between them due to the quantities they measure.</dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents36">The F-measure:</strong></dt>
      <dd>is a measure that combines <em>precision</em> and <em>recall</em>.</dd>
      <dd>It is the <em>harmonic mean</em> of precision and recall:</dd>
      <dd>
        <script type="math/tex; mode=display">{\displaystyle F=2\cdot {\frac {\mathrm {precision} \cdot \mathrm {recall} }{\mathrm {precision} +\mathrm {recall} }}}</script>
      </dd>
    </dl>
  </li>
</ol>

<hr />

<h2 id="content4">General Discussion of Issues in Text Classification</h2>

<ol>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents4" id="bodyContents41">Very Little Data:</strong></dt>
      <dd>
        <ul>
          <li>Use Naive Bayes
            <ul>
              <li>Naive Bayes is a “high-bias” algorithm; it tends to <strong>not</strong> overfit the data.</li>
            </ul>
          </li>
          <li>Use Semi-Supervised Learning
            <ul>
              <li>Try Bootstrapping or EM over unlabeled documents</li>
            </ul>
          </li>
        </ul>
      </dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents4" id="bodyContents42">Reasonable Amount of Data:</strong></dt>
      <dd>
        <ul>
          <li>Use:
            <ul>
              <li>SVM</li>
              <li>Regularized Logistic Regression</li>
              <li>(try) Decision Trees</li>
            </ul>
          </li>
        </ul>
      </dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents4" id="bodyContents43">Huge Amount of Data:</strong></dt>
      <dd>Be careful of the run-time:
        <ul>
          <li>SVM: slow train time</li>
          <li>KNN: slow test time</li>
          <li>Reg. Log. Regr.: somewhat faster</li>
          <li>Naive-Bayes: might be good to be used.</li>
        </ul>
      </dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents4" id="bodyContents44">Underflow Prevention:</strong></dt>
      <dd><strong>Problem:</strong> Due to the “<em>multiplicative</em>” nature of the algorithms we are using, we might run into a floating-point underflow problem.</dd>
      <dd><strong>Solution</strong>: transfer the calculations to the <em>log-space</em> where all the multiplications are transformed into additions.</dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents4" id="bodyContents45">Tweaking the Performance of the Algorithms:</strong></dt>
      <dd>
        <ul>
          <li>Utilize <strong><em>Domain-Specific</em> features</strong> and weights</li>
          <li><strong>Upweighting</strong>: counting a word as if it occurred multiple times.</li>
        </ul>
      </dd>
    </dl>
  </li>
</ol>



      <footer class="site-footer">
    <!--   <span class="site-footer-owner"><a href="http://localhost:8889">Ahmad Badary</a> is maintained by <a href="http://ahmedbadary.ml">Ahmad Badary</a>.</span> -->
    
<!--  -->
    <div class="wrapper">
        <div class="footer-col-wrapper">
            <div class="footer-col footer-col-1">
            <span class="site-footer-owner"><a href="http://localhost:8889">Site</a> maintained by <a href="http://ahmedbadary.ml">Ahmad Badary</a>.</span>
    <span class="site-footer-credits">
        <p>
            &copy; 2017. All rights reserved.
        </p> 
    </span>
            </div>
            <div class="footer-col footer-col-2">
            <div><p>         </p></div>
            </div>
            <div class="footer-col footer-col-3">
                <ul class="social-media-list">
                    
                      <li>
                        <a href="https://github.com/AhmedBadary">
                          <i class="fa fa-github"></i> GitHub
                        </a>
                      </li>
                    
                    
                      <li>
                        <a href="https://linkedin.com/in/ahmad-badary-656098121/">
                          <i class="fa fa-linkedin"></i> LinkedIn
                        </a>
                      </li>
                    
                    
                      <li>
                        <a href="https://www.facebook.com/ahmed.thabet.94">
                          <i class="fa fa-facebook"></i> Facebook
                        </a>
                      </li>
                    
                </ul>
            </div>
        </div>
    </div>
<!--  -->
</footer>


    </section>

  </body>

<!-- Table of Content Script -->
<script type="text/javascript">
var bodyContents = $(".bodyContents1");
$("<ol>").addClass("TOC1ul").appendTo(".TOC1");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC1ul");
     });
// 
var bodyContents = $(".bodyContents2");
$("<ol>").addClass("TOC2ul").appendTo(".TOC2");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC2ul");
     });
// 
var bodyContents = $(".bodyContents3");
$("<ol>").addClass("TOC3ul").appendTo(".TOC3");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC3ul");
     });
//
var bodyContents = $(".bodyContents4");
$("<ol>").addClass("TOC4ul").appendTo(".TOC4");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC4ul");
     });
//
var bodyContents = $(".bodyContents5");
$("<ol>").addClass("TOC5ul").appendTo(".TOC5");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC5ul");
     });
//
var bodyContents = $(".bodyContents6");
$("<ol>").addClass("TOC6ul").appendTo(".TOC6");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC6ul");
     });
//
var bodyContents = $(".bodyContents7");
$("<ol>").addClass("TOC7ul").appendTo(".TOC7");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC7ul");
     });
//
var bodyContents = $(".bodyContents8");
$("<ol>").addClass("TOC8ul").appendTo(".TOC8");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC8ul");
     });
//
var bodyContents = $(".bodyContents9");
$("<ol>").addClass("TOC9ul").appendTo(".TOC9");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC9ul");
     });

</script>

<!-- VIDEO BUTTONS SCRIPT -->
<script type="text/javascript">
  function iframePopInject(event) {
    var $button = $(event.target);
    // console.log($button.parent().next());
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $figure = $("<div>").addClass("video_container");
        $iframe = $("<iframe>").appendTo($figure);
        $iframe.attr("src", $button.attr("src"));
        // $iframe.attr("frameborder", "0");
        $iframe.attr("allowfullscreen", "true");
        $iframe.css("padding", "4px 6px");
        $button.next().css("display", "block");
        $figure.appendTo($button.next());
        $button.text("Hide Video")
    } else {
        $button.attr('value', 'show');
        $button.next().html("");
        $button.text("Show Video")
    }
}
</script>

<!-- BUTTON TRY -->
<script type="text/javascript">
  function iframePopA(event) {
    event.preventDefault();
    var $a = $(event.target).parent();
    console.log($a);
    if ($a.attr('value') == 'show') {
        $a.attr('value', 'hide');
        $figure = $("<div>");
        $iframe = $("<iframe>").addClass("popup_website_container").appendTo($figure);
        $iframe.attr("src", $a.attr("href"));
        $iframe.attr("frameborder", "1");
        $iframe.attr("allowfullscreen", "true");
        $iframe.css("padding", "4px 6px");
        $a.next().css("display", "block");
        $figure.appendTo($a.next().next());
        // $a.text("Hide Content")
        $('html, body').animate({
            scrollTop: $a.offset().top
        }, 1000);
    } else {
        $a.attr('value', 'show');
        $a.next().next().html("");
        // $a.text("Show Content")
    }

    $a.next().css("display", "inline");
}
</script>


<!-- TEXT BUTTON SCRIPT - INJECT -->
<script type="text/javascript">
  function showTextPopInject(event) {
    var $button = $(event.target);
    var txt = $button.attr("input");
    console.log(txt);
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $p = $("<p>");
        $p.html(txt);
        $button.next().css("display", "block");
        $p.appendTo($button.next());
        $button.text("Hide Content")
    } else {
        $button.attr('value', 'show');
        $button.next().html("");
        $button.text("Show Content")
    }

}
</script>

<!-- TEXT BUTTON SCRIPT - HIDDEN / HIDE / SHOW / HIDE/SHOW -->
<script type="text/javascript">
  function showTextPopHide(event) {
    var $button = $(event.target);
    var txt = $button.attr("input");
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $button.next().removeAttr("hidden");
        $button.text("Hide Content");
    } else {
        $button.attr('value', 'show');
        $button.next().attr("hidden", "");
        $button.text("Show Content");
    }
}
</script>

<!-- TEXT BUTTON SCRIPT - HIDDEN / HIDE / SHOW / HIDE/SHOW -->
<script type="text/javascript">
  function showText_withParent_PopHide(event) {
    var $button = $(event.target);
    var $parent = $button.parent();
    var txt = $button.attr("input");
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $parent.next().removeAttr("hidden");
        $button.text("Hide Content");
    } else {
        $button.attr('value', 'show');
        $parent.next().attr("hidden", "");
        $button.text("Show Content");
    }
}
</script>

<!-- Print / Printing / printme -->
<!-- <script type="text/javascript">
i = 0

for (var i = 1; i < 6; i++) {
    var bodyContents = $(".bodyContents" + i);
    $("<p>").addClass("TOC1ul")  .appendTo(".TOC1");
    bodyContents.each(function(index, element) {
        var paragraph = $(element);
        $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC1ul");
         });
} 
</script>
 -->
 
</html>

