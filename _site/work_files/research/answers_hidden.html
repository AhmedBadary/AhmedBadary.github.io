<!DOCTYPE html>
<html lang="en-us">
  <head>
  <meta charset="UTF-8">
  <title>Ahmad Badary</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <link rel="stylesheet" href="/css/normalize.css">
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="/css/cayman.css">
  <link rel="stylesheet" href="/css/style.css">
  <link rel="shortcut icon" href="/main_files/favicon.ico" />
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="/css/customStyle.css">
  <title> » Ahmad Badary</title>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
</head>

  <body>
    <nav class="main-nav">
    <a href="https://ahmedbadary.github.io/" class="main-nav-logo">
        <img src="/main_files/logo.png">
    </a>
    <ul id="menu-main" class="main-nav-items">
        <li id="menu-item-1859" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-home menu-item-1859">
            <a href="/">Home</a>
        </li>
        <li id="menu-item-2869" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-2869">
            <a href="/work">Work</a>
        </li>
        <li id="menu-item-1892" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1892">
            <a href="/projects">Projects</a>
        </li>
        <li id="menu-item-1858" class="menu-item menu-item-type-post_type menu-item-object-page current_page_parent menu-item-1858">
            <a href="/blog">Blog</a>
        </li>
        <li id="menu-item-1862" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1862">
            <a href="/about">About</a>
        </li>
    </ul>
</nav>


<section class="page-header">
  <h1 class="project-name">Answers to Prep Questions (Learning)</h1>
  <h2 class="project-tagline"></h2>
  <a href="/#" class="btn">Home</a>
  <a href="/work" class="btn">Work-Space</a>
  <a href= /work_files/research.html class="btn">Previous</a>
</section>

<!-- <div>
  <ul class="posts">
    
      <li><span>02 Jan 2014</span> &raquo; <a href="/2014/01/02/introducing-Ahmad/">Introducing Ahmad</a></li>
    
  </ul>
</div> -->


    <section class="main-content">
      
      <h1 id="gradient-based-optimization">Gradient-Based Optimization</h1>
<p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Gradient-Based Optimization</button></p>
<ol hidden="">
  <li><strong style="color: red">Define Gradient Methods:</strong><br />
 Gradient Methods are algorithms for solving (optimization) minimization problems of the form:
    <p>$$\min_{x \in \mathbb{R}^{n}} f(x)$$</p>
    <p>by doing <span style="color: goldenrod">local search</span>  (~ hill-climbing) with the search directions defined by the <strong>gradient</strong> of the function at the current point.</p>
  </li>
  <li><strong style="color: red">Give examples of Gradient-Based Algorithms:</strong>
    <ol>
      <li>Gradient Descent: minimizes arbitrary differentiable functions</li>
      <li>Conjugate Gradient: minimizes sparse linear systems w/ symmetric &amp; PD matrices</li>
      <li>Coordinate Descent: minimizes functions of two variables</li>
    </ol>
  </li>
  <li><strong style="color: red">What is Gradient Descent:</strong><br />
 GD is a <em>first order</em>, <em>iterative</em> algorithm to minimize an objective function \(J(\theta)\) parametrized by a models parameters \(\theta \in \mathbb{R}^d\) by updating the parameters in the opposite direction of the gradient of the objective function \(\nabla_{\theta} J(\theta)\)  w.r.t. to the parameters.</li>
  <li>
    <p><strong style="color: red">Explain it intuitively:</strong><br />
 1) <strong style="color: DarkMagenta">Local Search from a starting location on a hill</strong><br />
 2) <strong style="color: DarkGray">Feel around how a small movement/step around your location would change the height of the surrounding hill (is the ground higher or lower)</strong><br />
 3) <strong style="color: Olive">Make the movement/step consistent as a small fixed step along some direction</strong><br />
 4) <strong style="color: MediumBlue">Measure the steepness of the hill at the new location in the chosen direction</strong><br />
 5) <strong style="color: Crimson">Do so by Approximating the steepness with some local information</strong><br />
 6) <strong style="color: Purple">Measure the change in steepness from your current point to the new point</strong><br />
 6) <strong style="color: SpringGreen">Find the direction that decreases the steepness the most</strong>  <br />
 \(\iff\)</p>

    <p>1) <strong style="color: DarkMagenta">Local Search from an initial point \(x_0\) on a function</strong><br />
 2) <strong style="color: DarkGray">Explore the value of the function at different small nudges around \(x_0\)</strong><br />
 3) <strong style="color: Olive">Make the nudges consistent as a small fixed step \(\delta\) along a normalized direction \(\hat{\boldsymbol{u}}\)</strong><br />
 4) <strong style="color: MediumBlue">Evaluate the function at the new location \(x_0 + \delta \hat{\boldsymbol{u}}\)</strong><br />
 5) <strong style="color: Crimson">Do so by Approximating the function w/ first-order information (Taylor expansion)</strong><br />
 6) <strong style="color: Purple">Measure the change in value of the objective \(\Delta f\), from current point to the new point</strong><br />
 7) <strong style="color: SpringGreen">Find the direction \(\hat{\boldsymbol{u}}\) that minimizes the function the most</strong></p>
  </li>
  <li><strong style="color: red">Give its derivation:</strong><br />
 1) <strong style="color: DarkMagenta">Start at \(\boldsymbol{x} = x_0\)</strong><br />
 2) <strong style="color: DarkGray">We would like to know how would a small change in \(\boldsymbol{x}\), namely \(\Delta \boldsymbol{x}\) would affect the value of the function \(f(x)\). This will allow us to evaluate the function:</strong>
    <p>$$f(\mathbf{x}+\Delta \mathbf{x})$$</p>
    <p><strong style="color: DarkGray">to find the direction that makes \(f\) decrease the fastest</strong><br />
 3) <strong style="color: Olive">Let’s set up \(\Delta \boldsymbol{x}\), the change in \(\boldsymbol{x}\), as a fixed step \(\delta\) along some normalized direction \(\hat{\boldsymbol{u}}\):</strong></p>
    <p>$$\Delta \boldsymbol{x} = \delta \hat{\boldsymbol{u}}$$</p>
    <p>4) <strong style="color: MediumBlue">Evaluate the function at the new location:</strong></p>
    <p>$$f(\mathbf{x}+\Delta \mathbf{x}) = f(\mathbf{x}+\delta \hat{\boldsymbol{u}})$$</p>
    <p>5) <strong style="color: Crimson">Using the first-order approximation:</strong></p>
    <p>$$\begin{aligned} f(\mathbf{x}+\delta \hat{\boldsymbol{u}}) &amp;\approx f({\boldsymbol{x}}_0) + \left(({\boldsymbol{x}}_0 - \delta \hat{\boldsymbol{u}}) - {\boldsymbol{x}}_0 \right)^T \nabla_{\boldsymbol{x}} f({\boldsymbol{x}}_0) \\
     &amp;\approx f(x_0) + \delta \hat{\boldsymbol{u}}^T \nabla_x f(x_0) \end{aligned}$$</p>
    <p>6) <strong style="color: Purple">The change in \(f\) is:</strong></p>
    <p>$$\begin{aligned} \Delta f &amp;= f(\boldsymbol{x}_ 0 + \Delta \boldsymbol{x}) - f(\boldsymbol{x}_ 0) \\
 &amp;= f(\boldsymbol{x}_ 0 + \delta \hat{\boldsymbol{u}}) - f(\boldsymbol{x}_ 0)\\
 &amp;= \delta \nabla_x f(\boldsymbol{x}_ 0)^T\hat{\boldsymbol{u}} + \mathcal{O}(\delta^2) \\
 &amp;= \delta \nabla_x f(\boldsymbol{x}_ 0)^T\hat{\boldsymbol{u}} \\
 &amp;\geq -\delta\|\nabla f(\boldsymbol{x}_ 0)\|_ 2
 \end{aligned}$$</p>
    <p><strong style="color: Purple">where</strong></p>
    <p>$$\nabla_x f(\boldsymbol{x}_ 0)^T\hat{\boldsymbol{u}} \in \left[-\|\nabla f(\boldsymbol{x}_ 0)\|_ 2, \|\nabla f(\boldsymbol{x}_ 0)\|_ 2\right]$$</p>
    <p><strong style="color: Purple">since \(\hat{\boldsymbol{u}}\) is a unit vector; either aligned with \(\nabla_x f(\boldsymbol{x}_ 0)\) or in the opposite direction; it contributes nothing to the magnitude of the dot product.</strong>  <br />
 7) <strong style="color: SpringGreen">So, the \(\hat{\boldsymbol{u}}\) that <span style="color: goldenrod">changes the above inequality to equality, achieves the largest negative value</span> (moves the most downhill). That vector \(\hat{\boldsymbol{u}}\) is, then, the one in the negative direction of \(\nabla_x f(\boldsymbol{x}_ 0)\); the opposite direction of the gradient.</strong>  <br />
 \(\implies\)</p>
    <p>$$\hat{\boldsymbol{u}} = - \dfrac{\nabla_x f(x)}{\|\nabla_x f(x)\|_ 2}$$</p>

    <p>Finally, <strong>The method of steepest/gradient descent</strong> proposes a new point to decrease the value of \(f\):</p>
    <p>$$\boldsymbol{x}^{\prime}=\boldsymbol{x}-\epsilon \nabla_{\boldsymbol{x}} f(\boldsymbol{x})$$</p>
    <p>where \(\epsilon\) is the <strong>learning rate</strong>, defined as:</p>
    <p>$$\epsilon = \dfrac{\delta}{\left\|\nabla_{x} f(x)\right\|_ {2}}$$</p>
  </li>
  <li><strong style="color: red">What is the learning rate?</strong><br />
 Learning rate is a hyper-parameter that controls how much we are adjusting the weights of our network with respect the loss gradient; defined as:
    <p>$$\epsilon = \dfrac{\delta}{\left\|\nabla_{x} f(x)\right\|_ {2}}$$</p>
    <ol>
      <li><strong style="color: blue">Where does it come from?</strong><br />
 The learning rate comes from a modification of the step-size in the GD derivation.<br />
 We get the learning rate by employing a simple idea:<br />
 We have a <strong>fixed step-size</strong> \(\delta\) that dictated how much we should be moving in the direction of steepest descent. However, we would like to keep the step-size from being too small or overshooting. The idea is to <em><strong>make the step-size proportional to the magnitude of the gradient</strong></em> (i.e. some constant multiplied by the magnitude of the gradient):
        <p>$$\delta = \epsilon \left\|\nabla_{x} f(x)\right\|_ {2}$$</p>
        <p>If we do so, we get a nice cancellation as follows:</p>
        <p>$$\begin{aligned}\Delta \boldsymbol{x} &amp;= \delta \hat{\boldsymbol{u}}  \\
     &amp;= -\delta \dfrac{\nabla_x f(x)}{\|\nabla_x f(x)\|_ 2} \\
     &amp;= - \epsilon \left\|\nabla_{x} f(x)\right\|_ {2} \dfrac{\nabla_x f(x)}{\|\nabla_x f(x)\|_ 2} \\
     &amp;= - \dfrac{\epsilon \left\|\nabla_{x} f(x)\right\|_ {2}}{\|\nabla_x f(x)\|_ 2} \nabla_x f(x) \\
     &amp;= - \epsilon \nabla_x f(x)
 \end{aligned}$$</p>
        <p>where now we have a <em><strong>fixed learning rate</strong></em> instead of a <em>fixed step-size</em>.</p>
      </li>
    </ol>

    <p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Show lr questions</button></p>
    <ol hidden="">
      <li><strong style="color: blue">How does it relate to the step-size?</strong><br />
 [Answer above]</li>
      <li><strong style="color: blue">We go from having a fixed step-size to [blank]:</strong><br />
 a fixed learning rate.</li>
    </ol>

    <ol>
      <li><strong style="color: blue">What is its range?</strong><br />
 \([0.0001, 0.4]\)</li>
      <li><strong style="color: blue">How do we choose the learning rate?</strong>
        <ol>
          <li>Choose a starting lr from the range \([0.0001, 0.4]\) and adjust using cross-validation (lr is a HP).</li>
          <li>Do
            <ol>
              <li><strong>Set it to a small constant</strong></li>
              <li><strong>Smooth Functions (non-NN)</strong>:
                <ol>
                  <li><strong>Line Search</strong>: evaluate \(f\left(\boldsymbol{x}-\epsilon \nabla_{\boldsymbol{x}} f(\boldsymbol{x})\right)\) for several values of \(\epsilon\) and choose the one that results in the smallest objective value.<br />
 E.g. <strong>Secant Method</strong>, <strong>Newton-Raphson Method</strong> (may need Hessian, hard for large dims)</li>
                  <li><strong>Trust Region Method</strong></li>
                </ol>
              </li>
              <li><strong>Non-Smooth Functions (non-NN)</strong>:
                <ol>
                  <li><strong>Direct Line Search</strong><br />
 E.g. <strong>golden section search</strong></li>
                </ol>
              </li>
              <li><strong>Grid Search</strong>: is simply an exhaustive searching through a manually specified subset of the hyperparameter space of a learning algorithm. It is guided by some performance metric, typically measured by cross-validation on the training set or evaluation on a held-out validation set.</li>
              <li><strong>Population-Based Training (PBT)</strong>: is an elegant implementation of using a genetic algorithm for hyper-parameter choice.</li>
              <li><strong>Bayesian Optimization</strong>: is a global optimization method for noisy black-box functions.</li>
            </ol>
          </li>
          <li>Use larger lr and use a <strong>learning rate schedule</strong></li>
        </ol>
      </li>
    </ol>

    <p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Show lr questions #2</button></p>
    <ol hidden="">
      <li><strong style="color: blue">Compare Line Search vs Trust Region:</strong><br />
 Trust-region methods are in some sense dual to line-search methods: trust-region methods first choose a step size (the size of the trust region) and then a step direction, while line-search methods first choose a step direction and then a step size.</li>
      <li><strong style="color: blue">Learning Rate Schedule:</strong>
        <ol>
          <li><strong style="color: blue">Define:</strong><br />
 A learning rate schedule changes the learning rate during learning and is most often changed between epochs/iterations. This is mainly done with two parameters: <strong>decay</strong> and <strong>momentum</strong>.</li>
          <li><strong style="color: blue">List Types:</strong>
            <ol>
              <li><strong>Time-based</strong> learning schedules alter the learning rate depending on the learning rate of the previous time iteration. Factoring in the decay the mathematical formula for the learning rate is:
                <p>$${\displaystyle \eta_{n+1}={\frac {\eta_{n}}{1+dn}}}$$</p>
                <p>where \(\eta\) is the learning rate, \(d\) is a decay parameter and \(n\) is the iteration step.</p>
              </li>
              <li><strong>Step-based</strong> learning schedules changes the learning rate according to some pre defined steps:
                <p>$${\displaystyle \eta_{n}=\eta_{0}d^{floor({\frac {1+n}{r}})}}$$</p>
                <p>where \({\displaystyle \eta_{n}}\) is the learning rate at iteration \(n\), \(\eta_{0}\) is the initial learning rate, \(d\) is how much the learning rate should change at each drop (0.5 corresponds to a halving) and \(r\) corresponds to the droprate, or how often the rate should be dropped (\(10\) corresponds to a drop every \(10\) iterations). The floor function here drops the value of its input to \(0\) for all values smaller than \(1\).</p>
              </li>
              <li><strong>Exponential</strong> learning schedules are similar to step-based but instead of steps a decreasing exponential function is used. The mathematical formula for factoring in the decay is:
                <p>$$ {\displaystyle \eta_{n}=\eta_{0}e^{-dn}}$$</p>
                <p>where \(d\) is a decay parameter.</p>
              </li>
            </ol>
          </li>
        </ol>
      </li>
    </ol>
  </li>
  <li>
    <p><strong style="color: red">Describe the convergence of the algorithm:</strong><br />
 Gradient Descent converges when every element of the gradient is zero, or very close to zero within some threshold.</p>

    <p>With certain assumptions on \(f\) (convex, \(\nabla f\) lipschitz) and particular choices of \(\epsilon\) (chosen via line-search etc.), convergence to a local minimum can be guaranteed.<br />
 Moreover, if \(f\) is convex, all local minima are global minimia, so convergence is to the global minimum.</p>
  </li>
  <li><strong style="color: red">How does GD relate to Euler?</strong><br />
 Gradient descent can be viewed as <strong>applying Euler’s method for solving ordinary differential equations \({\displaystyle x'(t)=-\nabla f(x(t))}\) to a gradient flow</strong>.</li>
  <li><strong style="color: red">List the variants of GD:</strong>
    <ol>
      <li><strong>Batch Gradient Descent</strong></li>
      <li><strong>Stochastic GD</strong></li>
      <li><strong>Mini-Batch GD</strong></li>
      <li><strong style="color: blue">How do they differ? Why?:</strong><br />
 There are three variants of gradient descent, which differ in the amount of data used to compute the gradient. The amount of data imposes a trade-off between the accuracy of the parameter updates and the time it takes to perform the update.</li>
    </ol>

    <p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Define the Following w/ parameter updates and properties:</button></p>
    <ol hidden="">
      <li><strong style="color: blue">BGD:</strong><br />
 <strong>Batch Gradient Descent</strong> AKA <strong>Vanilla Gradient Descent</strong>, computes the gradient of the objective wrt. the parameters \(\theta\) for the entire dataset:
        <p>$$\theta=\theta-\epsilon \cdot \nabla_{\theta} J(\theta)$$</p>
        <ol>
          <li><strong>Properties</strong>:
            <ol>
              <li>Since we need to compute the gradient for the entire dataset for each update, this approach can be very slow and is intractable for datasets that can’t fit in memory.</li>
              <li>Moreover, batch-GD doesn’t allow for an <em>online</em> learning approach.</li>
            </ol>
          </li>
        </ol>
      </li>
      <li><strong style="color: blue">SGD:</strong><br />
 <strong>SGD</strong> performs a parameter update for each data-point:
        <p>$$\theta=\theta-\epsilon \cdot \nabla_{\theta} J\left(\theta ; x^{(i)} ; y^{(i)}\right)$$</p>
        <ol>
          <li><strong>Properties</strong>:
            <ol>
              <li>SGD exhibits a lot of fluctuation and has a lot of variance in the parameter updates. However, although, SGD can potentially move in the wrong direction due to limited information; in-practice, if we slowly decrease the learning-rate, it shows the same convergence behavior as batch gradient descent, almost certainly converging to a local or the global minimum for non-convex and convex optimization respectively.</li>
              <li>Moreover, the fluctuations it exhibits enables it to jump to new and potentially better local minima.</li>
            </ol>
          </li>
          <li><strong style="color: blue">How should we handle the lr in this case? Why?</strong><br />
 We should reduce the lr after every epoch:<br />
 This is due to the fact that the random sampling of batches acts as a source of noise which might make SGD keep oscillating around the minima without actually reaching it.</li>
          <li><strong style="color: blue">What conditions guarantee convergence of SGD?</strong> 
 <strong>The following conditions guarantee convergence under convexity conditions for SGD</strong>:
            <p>$$\begin{array}{l}{\sum_{k=1}^{\infty} \epsilon_{k}=\infty, \quad \text { and }} \\ {\sum_{k=1}^{\infty} \epsilon_{k}^{2}&lt;\infty}\end{array}$$</p>
          </li>
        </ol>
      </li>
      <li><strong style="color: blue">M-BGD:</strong><br />
 A hybrid approach that perform updates for a, pre-specified, mini-batch of \(n\) training examples:
        <p>$$\theta=\theta-\epsilon \cdot \nabla_{\theta} J\left(\theta ; x^{(i : i+n)} ; y^{(i : i+n)}\right)$$</p>
        <ol>
          <li><strong style="color: blue">What advantages does it have?</strong>
            <ol>
              <li>Reduce the variance of the parameter updates \(\rightarrow\) more stable convergence</li>
              <li>Makes use of matrix-vector highly optimized libraries</li>
              <li>Computationally efficient compared to stochastic gradient descent.</li>
              <li>Improve generalization by finding flat minima.</li>
              <li>Improving convergence, by using mini-batches we approximating the gradient of the entire training set, which might help to avoid local minima.</li>
            </ol>
          </li>
        </ol>
      </li>
      <li><strong style="color: blue">Explain the different kinds of gradient-descent optimization procedures:</strong>
        <ol>
          <li><strong>Batch Gradient Descent</strong> AKA <strong>Vanilla Gradient Descent</strong>, computes the gradient of the objective wrt. the parameters \(\theta\) for the entire dataset:
            <p>$$\theta=\theta-\epsilon \cdot \nabla_{\theta} J(\theta)$$</p>
          </li>
          <li><strong>SGD</strong> performs a parameter update for each data-point:
            <p>$$\theta=\theta-\epsilon \cdot \nabla_{\theta} J\left(\theta ; x^{(i)} ; y^{(i)}\right)$$</p>
          </li>
          <li><strong>Mini-batch Gradient Descent</strong> a hybrid approach that perform updates for a, pre-specified, mini-batch of \(n\) training examples:
            <p>$$\theta=\theta-\epsilon \cdot \nabla_{\theta} J\left(\theta ; x^{(i : i+n)} ; y^{(i : i+n)}\right)$$</p>
          </li>
        </ol>
      </li>
      <li><strong style="color: blue">State the difference between SGD and GD?</strong><br />
 <strong>Gradient Descent</strong>’s cost-function iterates over ALL training samples.<br />
 <strong>Stochastic Gradient Descent</strong>’s cost-function only accounts for ONE training sample, chosen at random.</li>
      <li><strong style="color: blue">When would you use GD over SDG, and vice-versa?</strong><br />
 GD theoretically minimizes the error function better than SGD. However, SGD converges much faster once the dataset becomes large.<br />
 That means GD is preferable for small datasets while SGD is preferable for larger ones.</li>
    </ol>
  </li>
  <li>
    <p><strong style="color: red">What is the problem of vanilla approaches to GD?</strong><br />
 All the variants described above, however, <strong>do not guarantee <em>“good” convergence</em></strong> due to some challenges.</p>

    <p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Show</button></p>
    <ol hidden="">
      <li><strong style="color: blue">List the challenges that account for the problem above:</strong>
        <ol>
          <li><strong>Choosing a proper learning rate is usually difficult:</strong><br />
 A learning rate that is too small leads to painfully slow convergence, while a learning rate that is too large can hinder convergence and cause the loss function to fluctuate around the minimum or even to diverge.</li>
          <li><strong>Inability to adapt to the <em>specific</em> characteristics of the dataset:</strong><br />
 Learning rate schedules try to adjust the learning rate during training by e.g. annealing, i.e. reducing the learning rate according to a pre-defined schedule or when the change in objective between epochs falls below a threshold. These schedules and thresholds, however, have to be defined in advance and are thus unable to adapt to a dataset’s characteristics.</li>
          <li><strong>The learning rate is <em>fixed</em> for all parameter updates:</strong><br />
 If our data is <em><strong>sparse</strong></em>  and our <em><strong>features have very different frequencies</strong></em>, we might not want to update all of them to the same extent, <span style="color: goldenrod">but perform a larger update for rarely occurring features</span>.</li>
          <li>Another key challenge of minimizing highly non-convex error functions common for neural networks is <strong>avoiding getting trapped in their numerous suboptimal local minima</strong>. Dauphin et al. argue that the difficulty arises in fact not from local minima but from saddle points, i.e. points where one dimension slopes up and another slopes down. These saddle points are usually surrounded by a plateau of the same error, which makes it notoriously hard for SGD to escape, as the gradient is close to zero in all dimensions.</li>
        </ol>
      </li>
    </ol>
  </li>
  <li><strong style="color: red">List the different strategies for optimizing GD:</strong>
    <ol>
      <li><strong>Adapt our updates to the slope of our error function</strong></li>
      <li><strong>Adapt our updates to the importance of each individual parameter</strong></li>
    </ol>
  </li>
  <li><strong style="color: red">List the different variants for optimizing GD:</strong>
    <ol>
      <li><strong>Adapt our updates to the slope of our error function:</strong>
        <ol>
          <li>Momentum</li>
          <li>Nesterov Accelerated Gradient</li>
        </ol>
      </li>
      <li><strong>Adapt our updates to the importance of each individual parameter:</strong>
        <ol>
          <li>Adagrad</li>
          <li>Adadelta</li>
          <li>RMSprop</li>
          <li>Adam</li>
          <li>AdaMax</li>
          <li>Nadam</li>
          <li>AMSGrad</li>
        </ol>
      </li>
    </ol>
  </li>
</ol>
<p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Show</button></p>
<ol hidden="">
  <li><strong style="color: red">Momentum:</strong>
    <ol>
      <li><strong style="color: blue">Motivation:</strong><br />
 SGD has trouble navigating <em>ravines</em> (i.e. areas where the surface curves much more steeply in one dimension than in another) which are common around local optima.<br />
 In these scenarios, SGD oscillates across the slopes of the ravine while only making hesitant progress along the bottom towards the local optimum.</li>
      <li><strong style="color: blue">Definitions/Algorithm:</strong><br />
 <strong>Momentum</strong> is a method that helps accelerate SGD in the relevant direction and dampens oscillations (image^). It does this by adding a fraction \(\gamma\) of the update vector of the past time step to the current update vector:
        <p>$$\begin{aligned} v_{t} &amp;=\gamma v_{t-1}+\eta \nabla_{\theta} J(\theta) \\ \theta &amp;=\theta-v_{t} \end{aligned}$$</p>
      </li>
      <li>
        <p><strong style="color: blue">Intuition:</strong><br />
 Essentially, when using momentum, we push a ball down a hill. The ball accumulates momentum as it rolls downhill, becoming faster and faster on the way (until it reaches its terminal velocity if there is air resistance, i.e.  \(\gamma &lt; 1\)). The same thing happens to our parameter updates: The momentum term increases for dimensions whose gradients point in the same directions and reduces updates for dimensions whose gradients change directions. As a result, we gain faster convergence and reduced oscillation.</p>

        <p>In this case we think of the equation as:</p>
        <p>$$v_{t} =\underbrace{\gamma}_{\text{friction }} \: \underbrace{v_{t-1}}_{\text{velocity}}+\eta \underbrace{\nabla_{\theta} J(\theta)}_ {\text{acceleration}}$$</p>
      </li>
      <li><strong style="color: blue">Parameter Settings:</strong><br />
 The momentum term \(\gamma\) is usually set to \(0.9\) or a similar value, and \(v_0 = 0\).</li>
    </ol>
  </li>
  <li><strong style="color: red">Nesterov Accelerated Gradient (Momentum):</strong>
    <ol>
      <li><strong style="color: blue">Motivation:</strong><br />
 Momentum is good, however, a ball that rolls down a hill, blindly following the slope, is highly unsatisfactory. We’d like to have a smarter ball, a ball that has a notion of where it is going so that it knows to slow down before the hill slopes up again.</li>
      <li><strong style="color: blue">Definitions/Algorithm:</strong><br />
 <strong>NAG</strong> is a way to five our momentum term this kind of prescience. Since we know that we will use the momentum term \(\gamma v_{t-1}\) to move the parameters \(\theta\), we can compute a rough approximation of the next position of the parameters with \(\theta - \gamma v_{t-1}\) (w/o the gradient). This allows us to, effectively, look ahead by calculating the gradient not wrt. our current parameters \(\theta\) but wrt. the approximate future position of our parameters:
        <p>$$\begin{aligned} v_{t} &amp;=\gamma v_{t-1}+\eta \nabla_{\theta} J\left(\theta-\gamma v_{t-1}\right) \\ \theta &amp;=\theta-v_{t} \end{aligned}$$</p>
      </li>
      <li><strong style="color: blue">Intuition:</strong><br />
 While Momentum first computes the current gradient (small blue vector) and then takes a big jump in the direction of the updated accumulated gradient (big blue vector), NAG first makes a big jump in the direction of the previous accumulated gradient (brown vector), measures the gradient and then makes a correction (red vector), which results in the complete NAG update (green vector). This anticipatory update prevents us from going too fast and results in increased responsiveness, which has significantly increased the performance of RNNs on a number of tasks.</li>
      <li><strong style="color: blue">Parameter Settings:</strong><br />
 \(\gamma = 0.9\),</li>
      <li><strong style="color: blue">Successful Applications:</strong><br />
 This really helps the optimization of <strong>recurrent neural networks</strong></li>
    </ol>
  </li>
  <li><strong style="color: red">Adagrad</strong>
    <ol>
      <li><strong style="color: blue">Motivation:</strong><br />
 Now that we are able to <strong style="color: goldenrod">adapt our updates to the slope of our error function</strong> and speed up SGD in turn, we would also like to <strong style="color: goldenrod">adapt our updates to each individual parameter</strong> to perform larger or smaller updates depending on their importance.</li>
      <li><strong style="color: blue">Definitions/Algorithm:</strong><br />
 <strong>Adagrad</strong> is an algorithm for gradient-based optimization that does just this: It adapts the learning rate to the parameters, performing smaller updates (i.e. low learning rates) for parameters associated with frequently occurring features, and larger updates (i.e. high learning rates) for parameters associated with infrequent features.</li>
      <li><strong style="color: blue">Intuition:</strong><br />
 Adagrad uses a different learning rate for every parameter \(\theta_i\) at every time step \(t\), so, we first show Adagrad’s per-parameter update.<br />
 <strong>Adagrad per-parameter update:</strong><br />
 The SGD update for every parameter \(\theta_i\) at each time step \(t\) is:
        <p>$$\theta_{t+1, i}=\theta_{t, i}-\eta \cdot g_{t, i}$$</p>
        <p>where \(g_{t, i}=\nabla_{\theta} J\left(\theta_{t, i}\right.\) is the partial derivative of the objective function w.r.t. to the parameter \(\theta_i\) at time step \(t\), and \(g_{t}\) is the gradient at time-step \(t\).</p>

        <p>In its update rule, Adagrad modifies the general learning rate \(\eta\) at each time step \(t\) for every parameter \(\theta_i\) based on the past gradients that have been computed for \(\theta_i\):</p>
        <p>$$\theta_{t+1, i}=\theta_{t, i}-\frac{\eta}{\sqrt{G_{t, i i}+\epsilon}} \cdot g_{t, i}$$</p>

        <p>\(G_t \in \mathbb{R}^{d \times d}\) here is a diagonal matrix where each diagonal element \(i, i\) is the sum of the squares of the gradients wrt \(\theta_i\) up to time step \(t\)[^12], while \(\epsilon\) is a smoothing term that avoids division by zero (\(\approx 1e - 8\)).</p>

        <p>As \(G_t\) contains the sum of the squares of the past gradients w.r.t. to all parameters \(\theta\) along its diagonal, we can now vectorize our implementation by performing a matrix-vector product \(\odot\) between \(G_t\) and  \(g_t\):</p>
        <p>$$\theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{G_{t}+\epsilon}} \odot g_{t}$$</p>
      </li>
      <li><strong style="color: blue">Parameter Settings:</strong><br />
 For the lr, Most implementations use a default value of \(0.01\) and leave it at that.</li>
      <li><strong style="color: blue">Successful Application:</strong><br />
 Well-suited for dealing with <strong>sparse data</strong> (because it adapts the lr of each parameter wrt <strong>feature frequency</strong>)</li>
      <li><strong style="color: blue">Properties:</strong>
        <ol>
          <li>Well-suited for dealing with <strong>sparse data</strong> (because it adapts the lr of each parameter wrt <strong>feature frequency</strong>)</li>
          <li><strong>Eliminates need for manual tuning of lr</strong>:<br />
 One of Adagrad’s main benefits is that it eliminates the need to manually tune the learning rate. Most implementations use a default value of \(0.01\) and leave it at that.</li>
          <li><strong>Weakness \(\rightarrow\) Accumulation of the squared gradients in the denominator</strong>:<br />
 Since every added term is positive, the accumulated sum keeps growing during training. This in turn causes the learning rate to shrink and eventually become infinitesimally small, at which point the algorithm is no longer able to acquire additional knowledge.</li>
          <li><strong>Without the sqrt, the algorithm performs <em>much worse</em></strong></li>
        </ol>
      </li>
    </ol>
  </li>
  <li><strong style="color: red">Adadelta</strong>
    <ol>
      <li><strong style="color: blue">Motivation:</strong><br />
 <strong>Adagrad</strong> has a weakness where it suffers from <strong style="color: goldenrod">aggressive, monotonically decreasing lr</strong> by <em>accumulation of the squared gradients in the denominator</em>. The following algorithm aims to resolve this flow.</li>
      <li><strong style="color: blue">Definitions/Algorithm:</strong></li>
      <li><strong style="color: blue">Intuition:</strong></li>
      <li><strong style="color: blue">Parameter Settings:</strong><br />
 We set \(\gamma\) to a similar value as the momentum term, around \(0.9\).</li>
      <li><strong style="color: blue">Properties:</strong>
        <ol>
          <li><strong>Eliminates need for lr completely</strong>:<br />
 With Adadelta, we do not even need to set a default learning rate, as it has been eliminated from the update rule.</li>
        </ol>
      </li>
    </ol>
  </li>
  <li><strong style="color: red">RMSprop</strong>
    <ol>
      <li><strong style="color: blue">Motivation:</strong><br />
 RMSprop and Adadelta have both been developed independently around the same time stemming from the need to <strong>resolve Adagrad’s radically diminishing learning rates</strong>.</li>
      <li><strong style="color: blue">Definitions/Algorithm:</strong><br />
 <strong>RMSprop</strong> is an unpublished, adaptive learning rate method proposed by Geoff Hinton in <a href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">Lecture 6e of his Coursera Class</a>.</li>
      <li><strong style="color: blue">Intuition:</strong><br />
 RMSprop in fact is identical to the first update vector of Adadelta that we derived above:
        <p>$$\begin{align}  \begin{split}  E[g^2]_t &amp;= 0.9 E[g^2]_{t-1} + 0.1 g^2_t \\  \theta_{t+1} &amp;= \theta_{t} - \dfrac{\eta}{\sqrt{E[g^2]_t + \epsilon}} g_{t}  \end{split}  \end{align}$$</p>

        <p>RMSprop as well <strong style="color: goldenrod">divides the learning rate by an exponentially decaying average of squared gradients</strong>.</p>
      </li>
      <li><strong style="color: blue">Parameter Settings:</strong><br />
 Hinton suggests \(\gamma\) to be set to \(0.9\), while a good default value for the learning rate \(\eta\) is \(0.001\).</li>
      <li><strong style="color: blue">Properties:</strong><br />
 Works well with RNNs.</li>
    </ol>
  </li>
  <li><strong style="color: red">Adam</strong>
    <ol>
      <li><strong style="color: blue">Motivation:</strong><br />
 Adding <strong>momentum</strong> to Adadelta/RMSprop.</li>
      <li><strong style="color: blue">Definitions/Algorithm:</strong><br />
 <strong>Adaptive Moment Estimation (Adam)</strong> is another method that computes adaptive learning rates for each parameter. In addition to storing an exponentially decaying average of past squared gradients \(v_t\) like Adadelta and RMSprop, Adam also keeps an exponentially decaying average of past gradients \(m_t\), similar to momentum.</li>
      <li><strong style="color: blue">Intuition:</strong><br />
 <strong>Adam VS Momentum:</strong><br />
 Whereas momentum can be seen as a ball running down a slope, Adam behaves like a heavy ball with friction, which thus prefers flat minima in the error surface.</li>
      <li><strong style="color: blue">Parameter Settings:</strong><br />
 The authors propose default values of \(0.9\) for \(\beta_1\), \(0.999\) for \(\beta_2\), and \(10^{ −8}\) for \(\epsilon\).</li>
      <li><strong style="color: blue">Properties:</strong>
        <ol>
          <li>They show empirically that Adam works well in practice and compares favorably to other adaptive learning-method algorithms.</li>
          <li>Kingma et al. show that its bias-correction helps Adam slightly outperform RMSprop towards the end of optimization as gradients become sparser. Insofar, Adam might be the best overall choice.</li>
        </ol>
      </li>
    </ol>
  </li>
  <li><strong style="color: red">Which methods have trouble with saddle points?</strong><br />
 SGD, Momentum, and NAG find it difficulty to break symmetry, although the two latter eventually manage to escape the saddle point, while Adagrad, RMSprop, and Adadelta quickly head down the negative slope.</li>
  <li><strong style="color: red">How should you choose your optimizer?</strong>
    <ol>
      <li>As we can see, the adaptive learning-rate methods, i.e. Adagrad, Adadelta, RMSprop, and Adam are most suitable and provide the best convergence for these scenarios.</li>
      <li><strong>Sparse Input Data</strong>:<br />
 If your input data is sparse, then you likely achieve the best results using one of the adaptive learning-rate methods. An additional benefit is that you won’t need to tune the learning rate but likely achieve the best results with the default value.</li>
    </ol>
  </li>
  <li><strong style="color: red">Summarize the different variants listed above. How do they compare to each other?</strong><br />
 In summary, RMSprop is an extension of Adagrad that deals with its radically diminishing learning rates. It is identical to Adadelta, except that Adadelta uses the RMS of parameter updates in the numerator update rule. Adam, finally, adds bias-correction and momentum to RMSprop. Insofar, RMSprop, Adadelta, and Adam are very similar algorithms that do well in similar circumstances.</li>
  <li><strong style="color: red">What’s a common choice in many research papers?</strong><br />
 Interestingly, many recent papers use vanilla SGD without momentum and a simple learning rate annealing schedule.</li>
  <li><strong style="color: red">List additional strategies for optimizing SGD:</strong>
    <ol>
      <li>Shuffling and Curriculum Learning</li>
      <li>Batch Normalization</li>
      <li>Early Stopping</li>
      <li>Gradient Noise</li>
    </ol>
  </li>
</ol>

<hr />

<h1 id="maximum-margin-classifiers">Maximum Margin Classifiers</h1>
<p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Maximum Margin Classifiers</button></p>
<ol hidden="">
  <li><strong style="color: red">Define Margin Classifiers:</strong><br />
 A <strong>margin classifier</strong> is a classifier which is able to give an associated distance from the decision boundary for each example.</li>
  <li><strong style="color: red">What is a Margin for a linear classifier?</strong><br />
 The <strong>margin</strong> of a linear classifier is the distance from the decision boundary to the nearest sample point.</li>
  <li>
    <p><strong style="color: red">Give the motivation for margin classifiers:</strong><br />
 Non-margin classifiers (e.g. Centroid, Perceptron, LR) will converge to a correct classifier on linearly separable data; however, the classifier they converge to is <strong>not</strong> unique nor the best.</p>
  </li>
  <li><strong style="color: red">Define the notion of the “best” possible classifier</strong><br />
 We assume that if we can maximize the distance between the data points to be classified and the hyperplane that classifies them, then we have reached a boundary that allows for the “best-fit”, i.e. allows for the most room for error.</li>
  <li><strong style="color: red">How can we achieve the “best” classifier?</strong><br />
 We enforce a constraint that achieves a classifier that has a maximum-margin.</li>
  <li><strong style="color: red">What unique vector is orthogonal to the hp? Prove it:</strong><br />
 The weight vector \(\mathbf{w}\) is orthogonal to the separating-plane/decision-boundary, defined by \(\mathbf{w}^T\mathbf{x} + b = 0\), in the \(\mathcal{X}\) space; Reason:<br />
 Since if you take any two points \(\mathbf{x}^\prime\) and \(\mathbf{x}^{\prime \prime}\) on the plane, and create the vector \(\left(\mathbf{x}^{\prime}-\mathbf{x}^{\prime \prime}\right)\)  parallel to the plane by subtracting the two points, then the following equations must hold:
    <p>$$\mathbf{w}^{\top} \mathbf{x}^{\prime}+b=0 \wedge \mathbf{w}^{\top} \mathbf{x}^{\prime \prime}+b=0 \implies \mathbf{w}^{\top}\left(\mathbf{x}^{\prime}-\mathbf{x}^{\prime \prime}\right)=0$$</p>
  </li>
  <li><strong style="color: red">What do we mean by “signed distance”? Derive its formula:</strong><br />
 The <em>signed distance</em> is the minimum distance from a point to a hyperplane.
 We solve for the signed distance to achieve the following formula for it:
 \(d = \dfrac{\| w \cdot x_0 + b \|}{\|w\|},\)<br />
 where we have an n-dimensional hyperplane: \(w \cdot x + b = 0\) and a point \(\mathbf{x}_ n\).<br />
 <strong>Derivation:</strong>
    <ol>
      <li>Suppose we have an affine hyperplane defined by \(w \cdot x + b\) and a point \(\mathbf{x}_ n\).</li>
      <li>Suppose that \(\mathbf{x} \in \mathbf{R}^n\) is a point satisfying \(w \cdot \mathbf{x} + b = 0\), i.e. it is a point on the plane.</li>
      <li>We construct the vector \(\mathbf{x}_ n−\mathbf{x}\) which points from \(\mathbf{x}\) to \(\mathbf{x}_ n\), and then, project (scalar projection==signed distance) it onto the unique vector perpendicular to the plane, i.e. \(w\),
        <p>$$d=| \text{comp}_{w} (\mathbf{x}_ n-\mathbf{x})| = \left| \frac{(\mathbf{x}_ n-\mathbf{x})\cdot w}{\|w\|} \right| = \frac{|\mathbf{x}_ n \cdot w - \mathbf{x} \cdot w|}{\|w\|}.$$</p>
      </li>
      <li>Since \(\mathbf{x}\)  is a vector on the plane, it must satisfy \(w\cdot \mathbf{x}=-b\) so we get
        <p>$$d=| \text{comp}_{w} (\mathbf{x}_ n-\mathbf{x})| = \frac{|\mathbf{x}_ n \cdot w +b|}{\|w\|}$$</p>
      </li>
    </ol>

    <p>Thus, we conclude that if \(\|w\| = 1\) then the <em>signed distance</em> from a datapoint \(X_i\) to the hyperplane is \(\|wX_i + b\|\).</p>
  </li>
  <li><strong style="color: red">Given the formula for signed distance, calculate the “distance of the point closest to the hyperplane”:</strong><br />
 Let \(X_n\) be the point that is closest to the plane, and let \(\hat{w} = \dfrac{w}{\|w\|}\).<br />
 Take any point \(X\) on the plane, and let \(\vec{v}\) be the vector \(\vec{v} = X_n - X\).<br />
 Now, the distance, \(d\) is equal to
    <p>$$\begin{align}
     d &amp; \ = \vert\hat{w}^{\top}\vec{v}\vert \\
     &amp; \ = \vert\hat{w}^{\top}(X_n - X)\vert \\
     &amp; \ = \vert\hat{w}^{\top}X_n - \hat{w}^{\top}X)\vert \\
     &amp; \ = \dfrac{1}{\|w\|}\vert w^{\top}X_n + b - w^{\top}X) - b\vert ,  &amp; \text{we add and subtract the bias } b\\
     &amp; \ = \dfrac{1}{\|w\|}\vert (w^{\top}X_n + b) - (w^{\top}X + b)\vert  \\
     &amp; \ = \dfrac{1}{\|w\|}\vert (w^{\top}X_n + b) - (0)\vert ,  &amp; \text{from the eq. of the plane on a point on the plane} \\
     &amp; \ = \dfrac{\vert (w^{\top}X_n + b)\vert}{\|w\|}
     \end{align}
 $$</p>
  </li>
  <li><strong style="color: red">Use geometric properties of the hp to Simplify the expression for the distance of the closest point to the hp, above</strong><br />
 First, we notice that for any given plane \(w^Tx = 0\), the equations, \(\gamma * w^Tx = 0\), where \(\gamma \in \mathbf{R}\) is a scalar, basically characterize the same plane and not many planes.<br />
 This is because \(w^Tx = 0 \iff \gamma * w^Tx = \gamma * 0 \iff \gamma * w^Tx = 0\).<br />
 The above implies that any model that takes input \(w\) and produces a margin, will have to be <strong><em>Scale Invariant</em></strong>.<br />
 To get around this and simplify the analysis, I am going to consider all the representations of the same plane, and I am going to pick one where we normalize (re-scale) the weight \(w\) such that the signed distance (distance to the point closest to the margin) is equal to one:
    <p>$$|w^Tx_n| &gt; 0 \rightarrow |w^Tx_n| = 1$$</p>
    <p>, where \(x_n\) is the point closest to the plane.<br />
 We constraint the hyperplane by normalizing \(w\) to this equation \(|w^Tx_i| = 1\) or with added bias, \(|w^Tx_i + b| = 1\).<br />
 \(\implies\)</p>
    <p>$$\begin{align}
     d &amp; \ = \dfrac{\vert (w^{\top}X_n + b)\vert}{\|w\|} \\
     &amp; \ = \dfrac{\vert (1)\vert}{\|w\|} ,  &amp; \text{from the constraint on the distance of the closest point} \\
     &amp; \ = \dfrac{1}{\|w\|}
     \end{align}
 $$</p>
  </li>
  <li><strong style="color: red">Characterize the margin, mathematically:</strong><br />
 we can characterize the margin, with its size, as the distance, \(\frac{1}{\|\mathbf{w}\|}\), between the hyperplane/boundary and the closest point to the plane \(\mathbf{x}_ n\), in both directions (multiply by 2) \(= \frac{2}{\|\mathbf{w}\|}\) ; given the condition we specified earlier \(\left|\mathbf{w}^{\top} \mathbf{x}_ {n} + b\right|=1\) for the closest point \(\mathbf{x}_ n\).</li>
  <li><strong style="color: red">Characterize the “Slab Existence”:</strong><br />
 The analysis done above allows us to conclusively prove that there exists a slab of width \(\dfrac{2}{\|w\|}\) containing no sample points where the hyperplane runs through (bisects) its center.</li>
  <li><strong style="color: red">Formulate the optimization problem of <em>maximizing the margin</em> wrt analysis above:</strong><br />
 We formulate the optimization problem of <em><strong>maximizing the margin</strong></em> by <em>maximizing the distance</em>, subject to the condition on how we derived the distance:
    <p>$$\max_{\mathbf{w}} \dfrac{2}{\|\mathbf{w}\|} \:\:\: : \:\: \min _{n=1,2, \ldots, N}\left|\mathbf{w}^{\top} \mathbf{x}_{n}+b\right|=1$$</p>
  </li>
  <li><strong style="color: red">Reformulate the optimization problem above to a more “friendly” version (wrt optimization -&gt; put in standard form):</strong><br />
 We can reformulate by (1) Flipping and <strong>Minimizing</strong>, (2) Taking a square since it’s monotonic and convex, and (3) noticing that \(\left|\mathbf{w}^{T} \mathbf{x}_ {n}+b\right|=y_{n}\left(\mathbf{w}^{T} \mathbf{x}_ {n}+b\right)\) (since the signal and label must agree, their product will always be positive) and the \(\min\) operator can be replaced by ensuring that for all the points the condition \(y_{n}\left(\mathbf{w}^{\top} \mathbf{x}_ {n}+b\right) \geq 1\) holds <a href="https://www.youtube.com/watch?v=eHsErlPJWUU&amp;t=1555">proof (by contradiction)</a> as:
    <p>$$\min_w \dfrac{1}{2} \mathbf{w}^T\mathbf{w} \:\:\: : \:\: y_{n}\left(\mathbf{w}^{\top} \mathbf{x}_ {n}+b\right) \geq 1 \:\: \forall i \in [1,N]$$</p>
    <p>Now when we solve the “friendly” equation above, we will get the <strong>separating plane</strong> with the <em><strong>best possible margin</strong></em> (best=biggest).</p>

    <ol>
      <li><strong style="color: blue">Give the final (standard) formulation of the “Optimization problem for maximum margin classifiers”:</strong>
        <p>$$\min_w \dfrac{1}{2} \mathbf{w}^T\mathbf{w} \:\:\: : \:\: y_{n}\left(\mathbf{w}^{\top} \mathbf{x}_ {n}+b\right) \geq 1 \:\: \forall i \in [1,N]$$</p>
      </li>
      <li><strong style="color: blue">What kind of formulation is it (wrt optimization)? What are the parameters?</strong><br />
 The above problem is a Quadratic Program, in \(d + 1\)-dimensions and \(n\)-constraints, in standard form.</li>
    </ol>
  </li>
</ol>

<hr />

<h1 id="hard-margin-svms">Hard-Margin SVMs</h1>
<p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Hard-Margin SVMs</button></p>
<ol hidden="">
  <li><strong style="color: red">Define:</strong>
    <ol>
      <li><strong style="color: blue">SVMs:</strong><br />
 1.*Support Vector Machines** (SVMs) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis.<br />
 The SVM is a <a href="/work_files/research/ml/1/3"><em>Maximum Margin Classifier</em></a> that aims to find the “maximum-margin hyperplane” that divides the group of points \({\displaystyle {\vec {x}}_{i}} {\vec {x}}_{i}\) for which \({\displaystyle y_{i}=1}\) from the group of points for which \({\displaystyle y_{i}=-1}\).</li>
      <li><strong style="color: blue">Support Vectors:</strong><br />
 1.*Support Vectors** are the data-points that lie exactly on the margin (i.e. on the boundary of the slab).<br />
 They satisfy \(\|w^TX' + b\| = 1, \forall\) support vectors \(X'\)</li>
      <li><strong style="color: blue">Hard-Margin SVM:</strong><br />
 The <em>Hard-Margin SVM</em> is just a maximum-margin classifier with features and kernels (discussed later).</li>
    </ol>
  </li>
  <li><strong style="color: red">Define the following wrt hard-margin SVM:</strong>
    <ol>
      <li><strong style="color: blue">Goal:</strong><br />
 Find weights ‘\(w\)’ and scalar ‘\(b\)’ that correctly classifies the data-points and, moreover, does so in the “<em>best</em>” possible way.</li>
      <li><strong style="color: blue">Procedure:</strong><br />
 (1) Use a linear classifier
 (2) But, Maximize the Margin
 (3) Do so by Minimizing \(\|w\|\)</li>
      <li><strong style="color: blue">Decision Function:</strong>
        <p>$${\displaystyle f(x)={\begin{cases}1&amp;{\text{if }}\ w\cdot X_i+\alpha&gt;0\\0&amp;{\text{otherwise}}\end{cases}}}$$</p>
      </li>
      <li><strong style="color: blue">Constraints:</strong>
        <p>$$y_i(wX_i + b) \geq 1, \forall i \in [1,n]$$</p>
      </li>
      <li><strong style="color: blue">The Optimization Problem:</strong><br />
 Find weights ‘\(w\)’ and scalar ‘\(b\)’ that minimize
        <p>$$ \dfrac{1}{2} w^Tw$$</p>
        <p>Subject to</p>
        <p>$$y_i(wX_i + b) \geq 1, \forall i \in [1,n]$$</p>
        <p>Formally,</p>
        <p>$$\min_w \dfrac{1}{2}w^Tw \:\:\: : \:\: y_i(wX_i + b) \geq 1, \forall i \in [1,n]$$</p>
      </li>
      <li><strong style="color: blue">The Optimization Method:</strong><br />
 The SVM optimization problem reduces to a <a href="work_files/research/conv_opt/3_3">Quadratic Program</a>.</li>
    </ol>
  </li>
  <li><strong style="color: red">Elaborate on the generalization analysis:</strong><br />
 We notice that, geometrically, the hyperplane (the maximum margin classifier) is completely characterized by the <em>support vectors</em> (the vectors that lie on the margin).<br />
 A very important conclusion arises.<br />
 The maximum margin classifier (SVM) depends <strong>only</strong> on the number of support vectors and   <strong><em>not</em></strong> on the dimension of the problem.<br />
 This implies that the computation doesn’t scale up with the dimension and, also implies, that the <em>kernel trick</em> works very well.</li>
  <li><strong style="color: red">List the properties:</strong>
    <ol>
      <li>In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.</li>
      <li>The hyperplane is determined solely by its support vectors.</li>
      <li>The SVM always converges on linearly separable data.</li>
      <li>The Hard-Margin SVM fails if the data is not linearly separable.</li>
      <li>The Hard-Margin SVM is quite sensitive to outliers</li>
    </ol>
  </li>
  <li><strong style="color: red">Give the solution to the optimization problem for H-M SVM:</strong><br />
 To solve the above problem, we need something that deals with <strong>inequality constraints</strong>; thus, we use the <strong>KKT method</strong> for solving a <em><strong>Lagrnagian under inequality constraints</strong></em>.<br />
 The <strong>Lagrange Formulation</strong>:
    <ol>
      <li>Formulate the Lagrangian:
        <ol>
          <li>Take each inequality constraint and put them in the <em>zero-form</em> (equality with Zero)</li>
          <li>Multiply each inequality by a Lagrange Multiplier \(\alpha_n\)</li>
          <li>Add them to the objective function \(\min_w \dfrac{1}{2} \mathbf{w}^T\mathbf{w}\)<br />
 The sign will be \(-\) (negative) simply because the inequality is \(\geq 0\)</li>
        </ol>
        <p>$$\min_{w, b} \max_{\alpha_n} \mathcal{L}(\mathbf{w}, b, \boldsymbol{\alpha}) = \dfrac{1}{2} \mathbf{w}^T\mathbf{w} -\sum_{n=1}^{N} \alpha_{n}\left(y_{n}\left(\mathbf{w}^{\top} \mathbf{x}_ {n}+b\right)-1\right) \:\:\: : \:\: \alpha_n \geq 0$$</p>
      </li>
      <li>Optimize the objective independently, for each of the unconstrained variables:
        <ol>
          <li>Gradient w.r.t. \(\mathbf{w}\):
            <p>$$\nabla_{\mathrm{w}} \mathcal{L}=\mathrm{w}-\sum_{n=1}^{N} \alpha_{n} y_{n} \mathrm{x}_ {n}=0 \\ \implies \\ \mathbf{w}=\sum_{n=1}^{N} \alpha_{n} y_{n} \mathbf{x}_ {n}$$</p>
          </li>
          <li>Derivative w.r.t. \(b\):
            <p>$$\frac{\partial \mathcal{L}}{\partial b}=-\sum_{n=1}^{N} \alpha_{n} y_{n}=0 \\ \implies \\ \sum_{n=1}^{N} \alpha_{n} y_{n}=0$$</p>
          </li>
        </ol>
      </li>
      <li>Get the <em><strong>Dual Formulation</strong></em> w.r.t. the (<em>tricky</em>) <strong>constrained</strong> variable \(\alpha_n\):
        <ol>
          <li>Substitute with the above conditions in the original lagrangian (such that the optimization w.r.t. \(\alpha_n\) will become free of \(\mathbf{w}\) and \(b\):
            <p>$$\mathcal{L}(\boldsymbol{\alpha})=\sum_{n=1}^{N} \alpha_{n}-\frac{1}{2} \sum_{n=1}^{N} \sum_{m=1}^{N} y_{n} y_{m} \alpha_{n} \alpha_{m} \mathbf{x}_{n}^{\mathrm{T}} \mathbf{x}_{m}$$</p>
          </li>
          <li>Notice that the first constraint \(\mathbf{w}=\sum_{n=1}^{N} \alpha_{n} y_{n} \mathbf{x}_ {n}\) has-no-effect/doesn’t-constraint \(\alpha_n\) so it’s a vacuous constraint. However, not the second constraint \(\sum_{n=1}^{N} \alpha_{n} y_{n}=0\).</li>
          <li>Set the optimization objective and the constraints, a <strong>quadratic function in \(\alpha_n\)</strong>:</li>
        </ol>
        <p>$$\max_{\alpha} \mathcal{L}(\boldsymbol{\alpha})=\sum_{n=1}^{N} \alpha_{n}-\frac{1}{2} \sum_{n=1}^{N} \sum_{m=1}^{N} y_{n} y_{m} \alpha_{n} \alpha_{m} \mathbf{x}_{n}^{\mathrm{T}} \mathbf{x}_{m} \\ \:\:\:\:\:\:\:\:\:\: : \:\: \alpha_n \geq 0 \:\: \forall \: n= 1, \ldots, N \:\: \wedge \:\: \sum_{n=1}^{N} \alpha_{n} y_{n}=0$$</p>
      </li>
      <li>Set the problem as a <strong>Quadratic Programming</strong> problem:
        <ol>
          <li>Change the <em>maximization</em> to <em>minimization</em> by flipping the signs:
            <p>$$\min _{\alpha} \frac{1}{2} \sum_{n=1}^{N} \sum_{m=1}^{N} y_{n} y_{m} \alpha_{n} \alpha_{m} \mathbf{x}_{0}^{\mathrm{T}} \mathbf{x}_{m}-\sum_{n=1}^{N} \alpha_{n}$$</p>
          </li>
          <li><strong>Isolate the Coefficients from the \(\alpha_n\)s</strong> and set in <em>matrix-form</em>:
            <p>$$\min _{\alpha} \frac{1}{2} \alpha^{\top} 
     \underbrace{\begin{bmatrix}
         y_{1} y_{1} \mathbf{x}_{1}^{\top} \mathbf{x}_{1} &amp; y_{1} y_{2} \mathbf{x}_{1}^{\top} \mathbf{x}_{2} &amp; \ldots &amp; y_{1} y_{N} \mathbf{x}_{1}^{\top} \mathbf{x}_{N}  \\
         y_{2} y_{1} \mathbf{x}_{2}^{\top} \mathbf{x}_{1} &amp; y_{2} y_{2} \mathbf{x}_{2}^{\top} \mathbf{x}_{2} &amp; \ldots &amp; y_{2} y_{N} \mathbf{x}_{2}^{\top} \mathbf{x}_{N} \\
         \ldots &amp; \ldots &amp; \ldots &amp; \ldots \\
         y_{N} y_{1} \mathbf{x}_{N}^{\top} \mathbf{x}_{1} &amp; y_{N} y_{2} \mathbf{x}_{N}^{\top} \mathbf{x}_{2} &amp; \ldots &amp; y_{N} y_{N} \mathbf{x}_{N}^{\top} \mathbf{x}_{N} 
     \end{bmatrix}}_{\text{quadratic coefficients}}
 \alpha+\underbrace{\left(-1^{\top}\right)}_ {\text {linear }} \alpha \\ 
 \:\:\:\:\:\:\:\:\:\: : \:\: \underbrace{\mathbf{y}^{\top} \boldsymbol{\alpha}=0}_{\text {linear constraint }} \:\: \wedge \:\: \underbrace{0}_{\text {lower bounds }} \leq \alpha \leq \underbrace{\infty}_{\text {upper bounds }}  $$</p>
            <blockquote>
              <p>The <em>Quadratic Programming Package</em> asks you for the <strong>Quadratic Term (Matrix)</strong> and the <strong>Linear Term</strong>, and for the <strong>Linear Constraint</strong> and the <strong>Range of \(\alpha_n\)s</strong>; and then, gives you back an \(\mathbf{\alpha}\).</p>
            </blockquote>
          </li>
        </ol>

        <p>Equivalently:</p>
        <p>$$\min _{\alpha} \frac{1}{2} \boldsymbol{\alpha}^{\mathrm{T}} \mathrm{Q} \boldsymbol{\alpha}-\mathbf{1}^{\mathrm{T}} \boldsymbol{\alpha} \quad \text {subject to } \quad \mathbf{y}^{\mathrm{T}} \boldsymbol{\alpha}=0 ; \quad \boldsymbol{\alpha} \geq \mathbf{0}$$</p>
      </li>
      <li><strong style="color: blue">What method does it require to be solved:</strong><br />
 To solve the above problem, we need something that deals with <strong>inequality constraints</strong>; thus, we use the <strong>KKT method</strong> for solving a <em><strong>Lagrnagian under inequality constraints</strong></em>.</li>
      <li><strong style="color: blue">Formulate the Lagrangian:</strong>
        <ol>
          <li>Take each inequality constraint and put them in the <em>zero-form</em> (equality with Zero)</li>
          <li>Multiply each inequality by a Lagrange Multiplier \(\alpha_n\)</li>
          <li>Add them to the objective function \(\min_w \dfrac{1}{2} \mathbf{w}^T\mathbf{w}\)<br />
 The sign will be \(-\) (negative) simply because the inequality is \(\geq 0\)</li>
        </ol>
        <p>$$\min_{w, b} \max_{\alpha_n} \mathcal{L}(\mathbf{w}, b, \boldsymbol{\alpha}) = \dfrac{1}{2} \mathbf{w}^T\mathbf{w} -\sum_{n=1}^{N} \alpha_{n}\left(y_{n}\left(\mathbf{w}^{\top} \mathbf{x}_ {n}+b\right)-1\right) \:\:\: : \:\: \alpha_n \geq 0$$</p>
      </li>
      <li><strong style="color: blue">Optimize the objective for each variable:</strong>
        <ol>
          <li>Gradient w.r.t. \(\mathbf{w}\):
            <p>$$\nabla_{\mathrm{w}} \mathcal{L}=\mathrm{w}-\sum_{n=1}^{N} \alpha_{n} y_{n} \mathrm{x}_ {n}=0 \\ \implies \\ \mathbf{w}=\sum_{n=1}^{N} \alpha_{n} y_{n} \mathbf{x}_ {n}$$</p>
          </li>
          <li>Derivative w.r.t. \(b\):
            <p>$$\frac{\partial \mathcal{L}}{\partial b}=-\sum_{n=1}^{N} \alpha_{n} y_{n}=0 \\ \implies \\ \sum_{n=1}^{N} \alpha_{n} y_{n}=0$$</p>
          </li>
        </ol>
      </li>
      <li><strong style="color: blue">Get the <em>Dual Formulation</em> w.r.t. the (<em>tricky</em>) constrained variable \(\alpha_n\):</strong>
        <ol>
          <li>Substitute with the above conditions in the original lagrangian (such that the optimization w.r.t. \(\alpha_n\) will become free of \(\mathbf{w}\) and \(b\):
            <p>$$\mathcal{L}(\boldsymbol{\alpha})=\sum_{n=1}^{N} \alpha_{n}-\frac{1}{2} \sum_{n=1}^{N} \sum_{m=1}^{N} y_{n} y_{m} \alpha_{n} \alpha_{m} \mathbf{x}_{n}^{\mathrm{T}} \mathbf{x}_{m}$$</p>
          </li>
          <li>Notice that the first constraint \(\mathbf{w}=\sum_{n=1}^{N} \alpha_{n} y_{n} \mathbf{x}_ {n}\) has-no-effect/doesn’t-constraint \(\alpha_n\) so it’s a vacuous constraint. However, not the second constraint \(\sum_{n=1}^{N} \alpha_{n} y_{n}=0\).</li>
          <li>Set the optimization objective and the constraints, a <strong>quadratic function in \(\alpha_n\)</strong>:</li>
        </ol>
        <p>$$\max_{\alpha} \mathcal{L}(\boldsymbol{\alpha})=\sum_{n=1}^{N} \alpha_{n}-\frac{1}{2} \sum_{n=1}^{N} \sum_{m=1}^{N} y_{n} y_{m} \alpha_{n} \alpha_{m} \mathbf{x}_{n}^{\mathrm{T}} \mathbf{x}_{m} \\ \:\:\:\:\:\:\:\:\:\: : \:\: \alpha_n \geq 0 \:\: \forall \: n= 1, \ldots, N \:\: \wedge \:\: \sum_{n=1}^{N} \alpha_{n} y_{n}=0$$</p>
      </li>
      <li><strong style="color: blue">Set the problem as a <em>Quadratic Programming</em> problem:</strong>
        <ol>
          <li>Change the <em>maximization</em> to <em>minimization</em> by flipping the signs:
            <p>$$\min _{\alpha} \frac{1}{2} \sum_{n=1}^{N} \sum_{m=1}^{N} y_{n} y_{m} \alpha_{n} \alpha_{m} \mathbf{x}_{0}^{\mathrm{T}} \mathbf{x}_{m}-\sum_{n=1}^{N} \alpha_{n}$$</p>
          </li>
          <li><strong>Isolate the Coefficients from the \(\alpha_n\)s</strong> and set in <em>matrix-form</em>:
            <p>$$\min _{\alpha} \frac{1}{2} \alpha^{\top} 
     \underbrace{\begin{bmatrix}
         y_{1} y_{1} \mathbf{x}_{1}^{\top} \mathbf{x}_{1} &amp; y_{1} y_{2} \mathbf{x}_{1}^{\top} \mathbf{x}_{2} &amp; \ldots &amp; y_{1} y_{N} \mathbf{x}_{1}^{\top} \mathbf{x}_{N}  \\
         y_{2} y_{1} \mathbf{x}_{2}^{\top} \mathbf{x}_{1} &amp; y_{2} y_{2} \mathbf{x}_{2}^{\top} \mathbf{x}_{2} &amp; \ldots &amp; y_{2} y_{N} \mathbf{x}_{2}^{\top} \mathbf{x}_{N} \\
         \ldots &amp; \ldots &amp; \ldots &amp; \ldots \\
         y_{N} y_{1} \mathbf{x}_{N}^{\top} \mathbf{x}_{1} &amp; y_{N} y_{2} \mathbf{x}_{N}^{\top} \mathbf{x}_{2} &amp; \ldots &amp; y_{N} y_{N} \mathbf{x}_{N}^{\top} \mathbf{x}_{N} 
     \end{bmatrix}}_{\text{quadratic coefficients}}
 \alpha+\underbrace{\left(-1^{\top}\right)}_ {\text {linear }} \alpha \\ 
 \:\:\:\:\:\:\:\:\:\: : \:\: \underbrace{\mathbf{y}^{\top} \boldsymbol{\alpha}=0}_{\text {linear constraint }} \:\: \wedge \:\: \underbrace{0}_{\text {lower bounds }} \leq \alpha \leq \underbrace{\infty}_ {\text {upper bounds }}  $$</p>
          </li>
        </ol>
      </li>
      <li><strong style="color: blue">What are the inputs and outputs to the Quadratic Program Package?</strong><br />
 The <em>Quadratic Programming Package</em> asks you for the <strong>Quadratic Term (Matrix)</strong> and the <strong>Linear Term</strong>, and for the <strong>Linear Constraint</strong> and the <strong>Range of \(\alpha_n\)s</strong>; and then, gives you back an \(\mathbf{\alpha}\).</li>
      <li><strong style="color: blue">Give the final form of the optimization problem in standard form:</strong>
        <p>$$\min_{\alpha} \frac{1}{2} \boldsymbol{\alpha}^{\mathrm{T}} \mathrm{Q} \boldsymbol{\alpha}-\mathbf{1}^{\mathrm{T}} \boldsymbol{\alpha} \quad \text {subject to } \quad \mathbf{y}^{\mathrm{T}} \boldsymbol{\alpha}=0 ; \quad \boldsymbol{\alpha} \geq \mathbf{0}$$</p>
      </li>
    </ol>
  </li>
</ol>

<hr />

<h1 id="soft-margin-svm">Soft-Margin SVM</h1>
<p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Soft-Margin SVM</button></p>
<ol hidden="">
  <li><strong style="color: red">Motivate the soft-margin SVM:</strong><br />
 The Hard-Margin SVM faces a few issues:
    <ol>
      <li>The Hard-Margin SVM fails if the data is not linearly separable.</li>
      <li>The Hard-Margin SVM is quite sensitive to outliers</li>
    </ol>

    <p>The Soft-Margin SVM aims to fix/reconcile these problems.</p>
  </li>
  <li><strong style="color: red">What is the main idea behind it?</strong><br />
 Allow some points to violate the margin, by introducing slack variables.</li>
  <li><strong style="color: red">Define the following wrt soft-margin SVM:</strong>
    <ol>
      <li><strong style="color: blue">Goal:</strong><br />
 Find weights ‘\(w\)’ and scalar ‘\(b\)’ that correctly classifies the data-points and, moreover, does so in the “<em>best</em>” possible way, but allow some points to violate the margin, by introducing slack variables.</li>
      <li><strong style="color: blue">Procedure:</strong><br />
 (1) Use a linear classifier<br />
 (2) But, Maximize the Margin<br />
 (3) Do so by Minimizing \(\|w\|\)<br />
 (4) But allow some points to penetrate the margin</li>
      <li><strong style="color: blue">Decision Function:</strong></li>
      <li><strong style="color: blue">Constraints:</strong>
        <p>$$y_i(wX_i + b) \geq 1 - \zeta_i, \forall i \in [1,n]$$</p>
        <p>where the \(\zeta_i\)s are slack variables.<br />
 We, also, enforce the non-negativity constraint on the slack variables:</p>
        <p>$$\zeta_i \geq 0, \:\:\: \forall i \in [1, n]$$</p>
        <ol>
          <li><strong style="color: blue">Why is there a non-negativity constraint?</strong>   <br />
 The non-negativity constraint forces the slack variables to be zero for all points that do not violate the original constraint:<br />
 i.e. are not inside the slab.</li>
        </ol>
      </li>
      <li><strong style="color: blue">Objective/Cost Function:</strong>
        <p>$$ R(w) = \dfrac{1}{2} w^Tw + C \sum_{i=1}^n \zeta_i$$</p>
      </li>
      <li><strong style="color: blue">The Optimization Problem:</strong><br />
 Find weights ‘\(w\)’, scalar ‘\(b\)’, and \(\zeta_i\)s that minimize
        <p>$$ \dfrac{1}{2} w^Tw + C \sum_{i=1}^n \zeta_i$$</p>
        <p>Subject to</p>
        <p>$$y_i(wX_i + b) \geq 1 - \zeta_i, \zeta_i \geq 0, \forall i \in [1,n]$$</p>
        <p>Formally,</p>
        <p>$$\min_w \dfrac{1}{2}w^Tw \:\:\: : \:\: y_i(wX_i + b) \geq 1 - \zeta_i, 
 \:\: \zeta_i \geq 0, \forall i \in [1,n]$$</p>
      </li>
      <li><strong style="color: blue">The Optimization Method:</strong><br />
 The SVM optimization problem reduces to a <a href="work_files/research/conv_opt/3_3">Quadratic Program</a> in \(d + n + 1\)-dimensions and \(2n\)-constraints.</li>
      <li><strong style="color: blue">Properties:</strong>
        <ol>
          <li>The Soft-Margin SVM will converge on non-linearly separable data.</li>
        </ol>
      </li>
    </ol>
  </li>
  <li>
    <p><strong style="color: red">Specify the effects of the regularization hyperparameter \(C\):</strong></p>

    <table>
      <tbody>
        <tr>
          <td> </td>
          <td><strong>Small C</strong></td>
          <td><strong>Large C</strong></td>
        </tr>
        <tr>
          <td><strong>Desire</strong></td>
          <td>Maximizing Margin = \(\dfrac{1}{|w|}\)</td>
          <td>keep most slack variables zero or small</td>
        </tr>
        <tr>
          <td><strong>Danger</strong></td>
          <td>underfitting (High Misclassification)</td>
          <td>overfitting (awesome training, awful test)</td>
        </tr>
        <tr>
          <td><strong>outliers</strong></td>
          <td>less sensitive</td>
          <td>very sensitive</td>
        </tr>
        <tr>
          <td><strong>boundary</strong></td>
          <td>more “flat”</td>
          <td>more sinuous</td>
        </tr>
      </tbody>
    </table>

    <ol>
      <li><strong style="color: blue">Describe the effect wrt over/under fitting:</strong> <br />
 Increase \(C\) hparam in SVM = causes overfitting</li>
    </ol>
  </li>
  <li><strong style="color: red">How do we choose \(C\)?</strong><br />
 We choose ‘\(C\)’ with cross-validation.</li>
  <li><strong style="color: red">Give an equivalent formulation in the standard form objective for function estimation (what should it minimize?)</strong><br />
 In function estimation we prefer the standard-form objective  to minimize (and trade-off); the loss + penalty form.<br />
 We introduce a loss function to moderate the use of the slack variables (i.e. to avoid abusing the slack variables), namely, <strong>Hinge Loss</strong>:
    <p>$${\displaystyle \max \left(0, 1-y_{i}({\vec {w}}\cdot {\vec {x}}_ {i}-b)\right)}$$</p>
    <p>The motivation: We motivate it by comparing it to the traditional \(0-1\) Loss function.<br />
 Notice that the \(0-1\) loss is actually non-convex. It has an infinite slope at \(0\). On the other hand, the hinge loss is actually convex.</p>

    <p>Analysis wrt maximum margin:<br />
 This function is zero if the constraint, \(y_{i}({\vec {w}}\cdot {\vec {x}}_ {i}-b)\geq 1\), is satisfied, in other words, if \({\displaystyle {\vec {x}}_{i}} {\vec {x}}_{i}\) lies on the correct side of the margin.<br />
 For data on the wrong side of the margin, the function’s value is proportional to the distance from the margin.</p>

    <p><strong>Modified Objective Function:</strong></p>
    <p>$$R(w) = \dfrac{\lambda}{2} w^Tw +  \sum_{i=1}^n {\displaystyle \max \left(0, 1-y_{i}({\vec {w}}\cdot {\vec {x}}_ {i}-b)\right)}$$</p>
    <p><strong>Proof of equivalence:</strong></p>
    <p>$$\begin{align}
         y_if\left(x_i\right) &amp; \ \geq 1-\zeta_i, &amp; \text{from 1st constraint } \\
         \implies \zeta_i &amp; \ \geq 1-y_if\left(x_i\right) \\
         \zeta_i &amp; \ \geq 1-y_if\left(x_i\right) \geq 0, &amp; \text{from 2nd positivity constraint on} \zeta_i \\
         \iff \zeta_i &amp; \ \geq \max \{0, 1-y_if\left(x_i\right)\} \\
         \zeta_i &amp; \ = \max \{0, 1-y_if\left(x_i\right)\}, &amp; \text{minimizing means } \zeta_i \text{reach lower bound}\\
         \implies R(w) &amp; \ = \dfrac{\lambda}{2} w^Tw +  \sum_{i=1}^n {\displaystyle \max \left(0, 1-y_{i}({\vec {w}}\cdot {\vec {x}}_ {i}-b)\right)}, &amp; \text{plugging in and multplying } \lambda = \dfrac{1}{C}
         \end{align}$$</p>
    <p><strong>The (reformulated) Optimization Problem:</strong></p>
    <p>$$\min_{w, b}\dfrac{\lambda}{2} w^Tw +  \sum_{i=1}^n {\displaystyle \max \left(0, 1-y_{i}({\vec {w}}\cdot {\vec {x}}_ {i}-b)\right)}$$</p>
  </li>
</ol>

<hr />

<h1 id="loss-functions">Loss Functions</h1>
<p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Loss Functions</button></p>
<ol hidden="">
  <li><strong style="color: red">Define:</strong>
    <ol>
      <li>
        <p><strong style="color: blue">Loss Functions - Abstractly and Mathematically:</strong><br />
 Abstractly, a <strong>loss function</strong> or <strong>cost function</strong> is a function that maps an event or values of one or more variables onto a real number, intuitively, representing some “cost” associated with the event.</p>

        <p>Formally, a <strong>loss function</strong> is a function \(L :(\hat{y}, y) \in \mathbb{R} \times Y \longmapsto L(\hat{y}, y) \in \mathbb{R}\)  that takes as inputs the predicted value \(\hat{y}\) corresponding to the real data value \(y\) and outputs how different they are.</p>
      </li>
      <li><strong style="color: blue">Distance-Based Loss Functions:</strong><br />
 A Loss function \(L(\hat{y}, y)\) is called <strong>distance-based</strong> if it:
        <ol>
          <li>Only depends on the <strong>residual</strong>:
            <p>$$L(\hat{y}, y) = \psi(y-\hat{y})  \:\: \text{for some } \psi : \mathbb{R} \longmapsto \mathbb{R}$$</p>
          </li>
          <li>Loss is \(0\) when residual is \(0\):
            <p>$$\psi(0) = 0$$</p>
          </li>
          <li><strong style="color: blue">Describe an important property of dist-based losses:</strong><br />
 <strong style="color: red">Translation Invariance:</strong><br />
 Distance-based losses are translation-invariant:
            <p>$$L(\hat{y}+a, y+a) = L(\hat{y}, y)$$</p>
          </li>
          <li><strong style="color: blue">What are they used for?</strong><br />
 Regression.</li>
        </ol>
      </li>
      <li><strong style="color: blue">Relative Error - What does it lack?</strong><br />
 <strong>Relative-Error</strong> \(\dfrac{\hat{y}-y}{y}\) is a more <em>natural</em> loss but it is NOT translation-invariant.</li>
    </ol>
  </li>
  <li><strong style="color: red">List 3 Regression Loss Functions</strong>
    <ol>
      <li>MSE</li>
      <li>MAE</li>
      <li>Huber</li>
    </ol>
  </li>
</ol>
<p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Show the rest of the questions</button></p>
<ol hidden="">
  <li><strong style="color: red">MSE</strong>
    <ol>
      <li><strong style="color: blue">What does it minimize:</strong><br />
 The <strong>MSE</strong> minimizes the sum of <em><strong>squared differences</strong></em> between the predicted values and the target values.</li>
      <li><strong style="color: blue">Formula:</strong>
        <p>$$L(\hat{y}, y) = \dfrac{1}{n} \sum_{i=1}^{n}\left(y_{i}-\hat{y}_ {i}\right)^{2}$$</p>
      </li>
      <li><strong style="color: blue">Graph:</strong><br />
 <img src="/main_files/dl/concepts/loss_funcs/1.png" alt="img" width="30%" class="center-image" /></li>
      <li><strong style="color: blue">Derivation:</strong><br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">Derivation</button>
 <img src="/main_files/dl/concepts/loss_funcs/5.png" alt="img" width="100%" hidden="" /></li>
    </ol>
  </li>
  <li><strong style="color: red">MAE</strong>
    <ol>
      <li><strong style="color: blue">What does it minimize:</strong><br />
 The <strong>MAE</strong> minimizes the sum of <em><strong>absolute differences</strong></em> between the predicted values and the target values.</li>
      <li><strong style="color: blue">Formula:</strong>
        <p>$$L(\hat{y}, y) = \dfrac{1}{n} \sum_{i=1}^{n}\vert y_{i}-\hat{y}_ {i}\vert$$</p>
      </li>
      <li><strong style="color: blue">Graph:</strong><br />
 <img src="/main_files/dl/concepts/loss_funcs/6.png" alt="img" width="40%" /></li>
      <li><strong style="color: blue">Derivation:</strong></li>
      <li><strong style="color: blue">List properties:</strong>
        <ol>
          <li>Solution may be <strong>Non-unique</strong></li>
          <li><strong>Robustness</strong> to outliers</li>
          <li><strong id="bodyContents22stability">Unstable Solutions:</strong>  <br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">Explanation</button>
 <em hidden="">The instability property of the method of least absolute deviations means that, for a small horizontal adjustment of a datum, the regression line may jump a large amount. The method has continuous solutions for some data configurations; however, by moving a datum a small amount, one could “jump past” a configuration which has multiple solutions that span a region. After passing this region of solutions, the least absolute deviations line has a slope that may differ greatly from that of the previous line. In contrast, the least squares solutions is stable in that, for any small adjustment of a data point, the regression line will always move only slightly; that is, the regression parameters are continuous functions of the data.</em></li>
          <li><strong>Data-points “Latching” <a href="/work_files/research/dl/concepts/loss_funcs#bodyContents22">ref</a>:</strong>
            <ol>
              <li><strong>Unique Solution</strong>:<br />
 If there are \(k\) <em><strong>features</strong></em> (including the constant), then at least one optimal regression surface will pass through \(k\) of the <em><strong>data points</strong></em>; unless there are multiple solutions.</li>
              <li><strong>Multiple Solutions</strong>:<br />
 The region of valid least absolute deviations solutions will be <strong>bounded by at least \(k\) lines</strong>, each of which <strong>passes through at least \(k\) data points</strong>.
                <blockquote>
                  <p><a href="https://en.wikipedia.org/wiki/Least_absolute_deviations#Other_properties">Wikipedia</a></p>
                </blockquote>
              </li>
            </ol>
          </li>
        </ol>
      </li>
    </ol>
  </li>
  <li><strong style="color: red">Huber Loss</strong>
    <ol>
      <li><strong style="color: blue">AKA:</strong><br />
 <strong>Smooth Mean Absolute Error</strong></li>
      <li><strong style="color: blue">Formula:</strong>
        <p>$$L(\hat{y}, y) = \left\{\begin{array}{cc}{\frac{1}{2}(y-\hat{y})^{2}} &amp; {\text {if }|(y-\hat{y})|&lt;\delta} \\ {\delta \vert y-\hat{y}\vert-\frac{1}{2} \delta^2} &amp; {\text {otherwise }}\end{array}\right.$$</p>
      </li>
      <li><strong style="color: blue">Graph:</strong><br />
 <img src="/main_files/dl/concepts/loss_funcs/6.png" alt="img" width="40%" /></li>
      <li><strong style="color: blue">List properties:</strong>
        <ol>
          <li>It’s <strong style="color: green">less sensitive</strong> to outliers than the <em>MSE</em> as it treats error as square only inside an interval.</li>
        </ol>
      </li>
    </ol>
  </li>
  <li>
    <p><strong style="color: red">Analyze MSE vs MAE <a href="/work_files/research/dl/concepts/loss_funcs#bodyContents26">ref</a>:</strong></p>

    <table>
      <tbody>
        <tr>
          <td><strong>MSE</strong></td>
          <td><strong>MAE</strong></td>
        </tr>
        <tr>
          <td>Sensitive to <em>outliers</em></td>
          <td>Robust to <em>outliers</em></td>
        </tr>
        <tr>
          <td>Differentiable Everywhere</td>
          <td>Non-Differentiable at \(0\)</td>
        </tr>
        <tr>
          <td>Stable Solutions</td>
          <td>Unstable Solutions</td>
        </tr>
        <tr>
          <td>Unique Solution</td>
          <td>Possibly multiple solutions</td>
        </tr>
      </tbody>
    </table>

    <ol>
      <li><strong>Statistical Efficiency</strong>:
        <ol>
          <li>“For normal observations MSE is about \(12\%\) more efficient than MAE” - Fisher</li>
          <li>\(1\%\) Error is enough to make MAE more efficient</li>
          <li>2/1000 bad observations, make the median more efficient than the mean</li>
        </ol>
      </li>
      <li>Subgradient methods are slower than gradient descent
        <ol>
          <li>you get a lot better convergence rate guarantees for MSE</li>
        </ol>
      </li>
    </ol>
  </li>
</ol>

<p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Show Questions on Classification</button></p>
<ol hidden="">
  <li><strong style="color: red">List 7 Classification Loss Functions</strong>
    <ol>
      <li>\(0-1\) Loss</li>
      <li>Square Loss</li>
      <li>Hinge Loss</li>
      <li>Logistic Loss</li>
      <li>Cross-Entropy</li>
      <li>Exponential Loss</li>
      <li>Perceptron Loss</li>
    </ol>
  </li>
  <li><strong style="color: red">\(0-1\) loss</strong>
    <ol>
      <li><strong style="color: blue">What does it minimize:</strong><br />
 It measures <strong>accuracy</strong>, and minimizes <strong>mis-classification error/rate</strong>.</li>
      <li><strong style="color: blue">Formula:</strong>
        <p>$$L(\hat{y}, y) = I(\hat{y} \neq y) = \left\{\begin{array}{ll}{0} &amp; {\hat{y}=y} \\ {1} &amp; {\hat{y} \neq y}\end{array}\right.$$</p>
      </li>
      <li><strong style="color: blue">Graph:</strong><br />
 <img src="/main_files/dl/concepts/loss_funcs/0.png" alt="img" width="40%" /></li>
    </ol>
  </li>
  <li><strong style="color: red">MSE</strong>
    <ol>
      <li><strong style="color: blue">Formula:</strong>
        <p>$$L(\hat{y}, y) = (1-y \hat{y})^{2}$$</p>
      </li>
      <li><strong style="color: blue">Graph:</strong><br />
 <img src="/main_files/dl/concepts/loss_funcs/0.png" alt="img" width="40%" /></li>
      <li><strong style="color: blue">Derivation (for classification) - give assumptions:</strong><br />
 We can write the loss in terms of the margin \(m = y\hat{y}\):<br />
 \(L(\hat{y}, y)=(y - \hat{y})^{2}=(1-y\hat{y})^{2}=(1-m)^{2}\)
        <blockquote>
          <p>Since \(y \in {-1,1} \implies y^2 = 1\)</p>
        </blockquote>
      </li>
      <li><strong style="color: blue">Properties:</strong>
        <ol>
          <li>Convex</li>
          <li>Smooth</li>
          <li>Sensitive to outliers: Penalizes outliers excessively</li>
          <li>^Slower Convergence Rate (wrt sample complexity) than logistic or hinge loss</li>
          <li>Functions which yield high values of \(f({\vec {x}})\) for some \(x\in X\) will perform poorly with the square loss function, since high values of \(yf({\vec {x}})\) will be penalized severely, regardless of whether the signs of \(y\) and \(f({\vec {x}})\) match.</li>
        </ol>
      </li>
    </ol>
  </li>
  <li><strong style="color: red">Hinge Loss</strong>
    <ol>
      <li><strong style="color: blue">What does it minimize:</strong><br />
 It minimizes missclassification wrt penetrating a margin \(\rightarrow\) maximizes a margin.</li>
      <li><strong style="color: blue">Formula:</strong>
        <p>$$L(\hat{y}, y) = \max (0,1-y \hat{y})=|1-y \hat{y}|_ {+}$$</p>
      </li>
      <li><strong style="color: blue">Graph:</strong><br />
 <img src="/main_files/dl/concepts/loss_funcs/3.png" alt="img" width="30%" class="center-image" /></li>
      <li><strong style="color: blue">Describe the Properties of the Hinge loss and why it is used?</strong>
        <ol>
          <li>Continuous, Convex, Non-Differentiable</li>
          <li>The hinge loss provides a relatively tight, convex upper bound on the \(0–1\) indicator function</li>
          <li>Hinge loss upper bounds 0-1 loss</li>
          <li>It is the tightest <em>convex</em> upper bound on the 0/1 loss</li>
          <li>Minimizing 0-1 loss is NP-hard in the worst-case</li>
        </ol>
      </li>
    </ol>
  </li>
  <li><strong style="color: red">Logistic Loss</strong>
    <ol>
      <li><strong style="color: blue">AKA:</strong>  <br />
 <strong>Log-Loss</strong>, <strong>Logarithmic Loss</strong></li>
      <li><strong style="color: blue">What does it minimize:</strong><br />
 Minimizes the Kullback-Leibler divergence between the empirical distribution and the predicted distribution.</li>
      <li><strong style="color: blue">Formula:</strong>
        <p>$$L(\hat{y}, y) = \log{\left(1+e^{-y \hat{y}}\right)}$$</p>
      </li>
      <li><strong style="color: blue">Graph:</strong><br />
 <img src="/main_files/dl/concepts/loss_funcs/2.png" alt="img" width="30%" class="center-image" /></li>
      <li><strong style="color: blue">Derivation:</strong><br />
 We get the <strong>likelihood</strong> of the dataset \(\mathcal{D}=\left(\mathbf{x}_{1}, y_{1}\right), \ldots,\left(\mathbf{x}_{N}, y_{N}\right)\):
        <p>$$\prod_{n=1}^{N} P\left(y_{n} | \mathbf{x}_{n}\right) =\prod_{n=1}^{N} \theta\left(y_{n} \mathbf{w}^{\mathrm{T}} \mathbf{x}_ {n}\right)$$</p>

        <p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Derivation</button></p>
        <ol hidden="">
          <li>Maximize:
            <p>$$\prod_{n=1}^{N} \theta\left(y_{n} \mathbf{w}^{\top} \mathbf{x}_ {n}\right)$$</p>
          </li>
          <li>Take the natural log to avoid products:
            <p>$$\ln \left(\prod_{n=1}^{N} \theta\left(y_{n} \mathbf{w}^{\top} \mathbf{x}_ {n}\right)\right)$$</p>
            <p>Motivation:</p>
            <ol>
              <li>The inner quantity is <strong>non-negative</strong> and non-zero.</li>
              <li>The natural log is <strong>monotonically increasing</strong> (its max, is the max of its argument)</li>
            </ol>
          </li>
          <li>Take the average (still monotonic):
            <p>$$\frac{1}{N} \ln \left(\prod_{n=1}^{N} \theta\left(y_{n} \mathbf{w}^{\top} \mathbf{x}_ {n}\right)\right)$$</p>
          </li>
          <li>Take the negative and <strong>Minimize</strong>:
            <p>$$-\frac{1}{N} \ln \left(\prod_{n=1}^{N} \theta\left(y_{n} \mathbf{w}^{\top} \mathbf{x}_ {n}\right)\right)$$</p>
          </li>
          <li>Simplify:
            <p>$$=\frac{1}{N} \sum_{n=1}^{N} \ln \left(\frac{1}{\theta\left(y_{n} \mathbf{w}^{\tau} \mathbf{x}_ {n}\right)}\right)$$</p>
          </li>
          <li>Substitute \(\left[\theta(s)=\frac{1}{1+e^{-s}}\right]\):
            <p>$$\frac{1}{N} \sum_{n=1}^{N} \underbrace{\ln \left(1+e^{-y_{n} \mathbf{w}^{\top} \mathbf{x}_{n}}\right)}_{e\left(h\left(\mathbf{x}_{n}\right), y_{n}\right)}$$</p>
          </li>
          <li>Use this as the <em><strong>Cross-Entropy</strong></em>  <strong>Error Measure</strong>:
            <p>$$E_{\mathrm{in}}(\mathrm{w})=\frac{1}{N} \sum_{n=1}^{N} \underbrace{\ln \left(1+e^{-y_{n} \mathrm{w}^{\top} \mathbf{x}_{n}}\right)}_{\mathrm{e}\left(h\left(\mathrm{x}_{n}\right), y_{n}\right)}$$</p>
          </li>
        </ol>
      </li>
      <li><strong style="color: blue">Properties:</strong>
        <ol>
          <li>Convex</li>
          <li>Grows linearly for negative values which make it less sensitive to outliers</li>
          <li>The logistic loss function does not assign zero penalty to any points. Instead, functions that correctly classify points with high confidence (i.e., with high values of \({\displaystyle \vert f({\vec {x}})\vert }\)) are penalized less. This structure leads the logistic loss function to be sensitive to outliers in the data.</li>
        </ol>
      </li>
    </ol>
  </li>
  <li><strong style="color: red">Cross-Entropy</strong>
    <ol>
      <li><strong style="color: blue">What does it minimize:</strong><br />
 It minimizes the Kullback-Leibler divergence between the empirical distribution and the predicted distribution.</li>
      <li><strong style="color: blue">Formula:</strong>
        <p>$$L(\hat{y}, y) = -\sum_{i} y_i \log \left(\hat{y}_ {i}\right)$$</p>
      </li>
      <li><strong style="color: blue">Binary Cross-Entropy:</strong>
        <p>$$L(\hat{y}, y) = -\left[y \log \hat{y}+\left(1-y\right) \log \left(1-\hat{y}_ {n}\right)\right]$$</p>
      </li>
      <li><strong style="color: blue">Graph:</strong><br />
 <img src="/main_files/dl/concepts/loss_funcs/4.png" alt="img" width="30%" class="center-image" /></li>
      <li><strong style="color: blue">CE and Negative-Log-Probability:</strong><br />
 The <strong>Cross-Entropy</strong> is equal to the <strong>Negative-Log-Probability</strong> (of predicting the true class) in the case that the true distribution that we are trying to match is <em><strong>peaked at a single point</strong></em> and is <em><strong>identically zero everywhere else</strong></em>; this is usually the case in ML when we are using a <em>one-hot encoded vector</em> with one class \(y = [0 \: 0 \: \ldots \: 0 \: 1 \: 0 \: \ldots \: 0]\) peaked at the \(j\)-th position <br />
 \(\implies\)
        <p>$$L(\hat{y}, y) = -\sum_{i} y_i \log \left(\hat{y}_ {i}\right) = - \log (\hat{y}_ {j})$$</p>
      </li>
      <li><strong style="color: blue">CE and Log-Loss:</strong><br />
 Given \(p \in\{y, 1-y\}\) and \(q \in\{\hat{y}, 1-\hat{y}\}\):
        <p>$$H(p,q)=-\sum_{x }p(x)\,\log q(x) = -y \log \hat{y}-(1-y) \log (1-\hat{y}) = L(\hat{y}, y)$$</p>
        <ol>
          <li>
            <p><strong style="color: blue">Derivation:</strong></p>

            <p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Derivation</button>
 <em hidden="">Given:</em></p>
            <ol hidden="">
              <li>\(\hat{y} = \sigma(yf(x))\),</li>
              <li>\(y \in \{-1, 1\}\),</li>
              <li>\(\hat{y}' = \sigma(f(x))\),</li>
              <li>
\[y' = (1+y)/2 = \left\{\begin{array}{ll}{1} &amp; {\text {for }} y' = 1 \\ {0} &amp; {\text {for }} y = -1\end{array}\right. \in \{0, 1\}\]
              </li>
              <li>We start with the modified binary cross-entropy<br />
 \(\begin{aligned} -y' \log \hat{y}'-(1-y') \log (1-\hat{y}') &amp;= \left\{\begin{array}{ll}{-\log\hat{y}'} &amp; {\text {for }} y' = 1 \\ {-\log(1-\hat{y}')} &amp; {\text {for }} y' = 0\end{array}\right. \\ \\
 &amp;= \left\{\begin{array}{ll}{-\log\sigma(f(x))} &amp; {\text {for }} y' = 1 \\ {-\log(1-\sigma(f(x)))} &amp; {\text {for }} y' = 0\end{array}\right. \\ \\
 &amp;= \left\{\begin{array}{ll}{-\log\sigma(1\times f(x))} &amp; {\text {for }} y' = 1 \\ {-\log(\sigma((-1)\times f(x)))} &amp; {\text {for }} y' = 0\end{array}\right. \\ \\
 &amp;= \left\{\begin{array}{ll}{-\log\sigma(yf(x))} &amp; {\text {for }} y' = 1 \\ {-\log(\sigma(yf(x)))} &amp; {\text {for }} y' = 0\end{array}\right. \\ \\
 &amp;= \left\{\begin{array}{ll}{-\log\hat{y}} &amp; {\text {for }} y' = 1 \\ {-\log\hat{y}} &amp; {\text {for }} y' = 0\end{array}\right. \\ \\
 &amp;= -\log\hat{y} \\ \\
 &amp;= \log\left[\dfrac{1}{\hat{y}}\right] \\ \\
 &amp;= \log\left[\hat{y}^{-1}\right] \\ \\
 &amp;= \log\left[\sigma(yf(x))^{-1}\right] \\ \\
 &amp;= \log\left[ \left(\dfrac{1}{1+e^{-yf(x)}}\right)^{-1}\right] \\ \\
 &amp;= \log \left(1+e^{-yf(x)}\right)\end{aligned}\)</li>
            </ol>
          </li>
        </ol>
      </li>
      <li><strong style="color: blue">CE and KL-Div:</strong><br />
 When comparing a distribution \({\displaystyle q}\) against a fixed reference distribution \({\displaystyle p}\), cross entropy and KL divergence are identical up to an additive constant (since \({\displaystyle p}\) is fixed): both take on their minimal values when \({\displaystyle p=q}\), which is \({\displaystyle 0}\) for KL divergence, and \({\displaystyle \mathrm {H} (p)}\) for cross entropy.
        <blockquote>
          <p>Basically, minimizing either will result in the same solution.</p>
        </blockquote>
      </li>
    </ol>
  </li>
  <li><strong style="color: red">Exponential Loss</strong>
    <ol>
      <li><strong style="color: blue">Formula:</strong>
        <p>$$L(\hat{y}, y) = e^{-\beta y \hat{y}}$$</p>
      </li>
      <li><strong style="color: blue">Properties:</strong>
        <ol>
          <li>Convex</li>
          <li>Grows Exponentially for negative values making it <strong>more sensitive to outliers</strong></li>
          <li>It penalizes incorrect predictions more than Hinge loss and has a larger gradient.</li>
          <li>Used in <strong>AdaBoost</strong> algorithm</li>
        </ol>
      </li>
    </ol>
  </li>
  <li><strong style="color: red">Perceptron Loss</strong>
    <ol>
      <li><strong style="color: blue">Formula:</strong>
        <p>$${\displaystyle L(z, y_i) = {\begin{cases}0&amp;{\text{if }}\ y_i\cdot z_i \geq 0\\-y_i z&amp;{\text{otherwise}}\end{cases}}}$$</p>
      </li>
    </ol>
  </li>
  <li><strong style="color: red">Analysis</strong>
    <ol>
      <li><strong style="color: blue">Logistic vs Hinge Loss:</strong><br />
 <strong>Logistic loss</strong> diverges faster than <strong>hinge loss</strong> <a href="#losses">(image)</a>. So, in general, it will be more sensitive to outliers. <a href="https://towardsdatascience.com/support-vector-machine-vs-logistic-regression-94cc2975433f">Reference. Bad info?</a></li>
      <li><strong style="color: blue">Cross-Entropy vs MSE:</strong><br />
 Basically, CE &gt; MSE because the gradient of MSE \(z(1-z)\) leads to saturation when then output \(z\) of a neuron is near \(0\) or \(1\) making the gradient very small and, thus, slowing down training.<br />
 CE &gt; Class-Loss because Class-Loss is binary and doesn’t take into account <em>“how well”</em> are we actually approximating the probabilities as opposed to just having the target class be slightly higher than the rest (e.g. \([c_1=0.3, c_2=0.3, c_3=0.4]\)).</li>
    </ol>
  </li>
</ol>

<hr />

<h1 id="information-theory">Information Theory</h1>
<p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Information Theory</button></p>
<ol hidden="">
  <li>
    <p><strong style="color: red">What is Information Theory? In the context of ML?</strong><br />
 <strong>Information theory</strong> is a branch of applied mathematics that revolves around quantifying how much information is present in a signal.</p>

    <p>In the context of machine learning, we can also apply information theory to continuous variables where some of these message length interpretations do not apply, instead, we mostly use a few key ideas from information theory to characterize probability distributions or to quantify similarity between probability distributions.</p>
  </li>
  <li>
    <p><strong style="color: red">Describe the Intuition for Information Theory. Intuitively, how does the theory quantify information (list)?</strong><br />
 The basic intuition behind information theory is that learning that an unlikely event has occurred is more informative than learning that a likely event has occurred. A message saying “the sun rose this morning” is so uninformative as to be unnecessary to send, but a message saying “there was a solar eclipse this morning” is very informative.</p>

    <p id="lst-p">Thus, information theory quantifies information in a way that formalizes this intuition:</p>
    <ol>
      <li>Likely events should have low information content - in the extreme case, guaranteed events have no information at all</li>
      <li>Less likely events should have higher information content</li>
      <li>Independent events should have additive information. For example, finding out that a tossed coin has come up as heads twice should convey twice as much information as finding out that a tossed coin has come up as heads once.</li>
    </ol>
  </li>
  <li><strong style="color: red">Measuring Information - Definitions and Formulas:</strong>
    <ol>
      <li><strong style="color: blue">In Shannons Theory, how do we quantify <em>“transmitting 1 bit of information”</em>?</strong><br />
 To <strong>transmit \(1\) bit of information</strong> means to <strong>divide the recipients <em>Uncertainty</em> by a factor of \(2\)</strong>.</li>
      <li><strong style="color: blue">What is <em>the amount of information transmitted</em>?</strong><br />
 The <strong>amount of information</strong> transmitted is the <strong>logarithm</strong> (base \(2\)) of the <strong>uncertainty reduction factor</strong>.</li>
      <li><strong style="color: blue">What is the <em>uncertainty reduction factor</em>?</strong><br />
 It is the <strong>inverse of the probability</strong> of the event being communicated.</li>
      <li><strong style="color: blue">What is the <em>amount of information in an event \(x\)</em>?</strong><br />
 The <strong>amount of information</strong> in an event \(\mathbf{x} = x\), called the <em><strong>Self-Information</strong></em>  is:
        <p>$$I(x) = \log (1/p(x)) = -\log(p(x))$$</p>
      </li>
    </ol>
  </li>
  <li><strong style="color: red">Define the <em>Self-Information</em> - Give the formula:</strong><br />
 The <strong>Self-Information</strong> or <strong>surprisal</strong> is a synonym for the surprise when a random variable is sampled.<br />
 The <strong>Self-Information</strong> of an event \(\mathrm{x} = x\):
    <p>$$I(x) = - \log P(x)$$</p>
    <ol>
      <li><strong style="color: blue">What is it defined with respect to?</strong><br />
 Self-information deals only with a single outcome.</li>
    </ol>
  </li>
  <li>
    <p><strong style="color: red">Define <em>Shannon Entropy</em> - What is it used for?</strong><br />
 <strong>Shannon Entropy</strong> is defined as the average amount of information produced by a stochastic source of data.<br />
 Equivalently, the amount of information that you get from one sample drawn from a given probability distribution \(p\).</p>

    <p>To quantify the amount of uncertainty in an entire probability distribution, we use <strong>Shannon Entropy</strong>.</p>
    <p>$$H(x) = {\displaystyle \operatorname {E}_{x \sim P} [I(x)]} = - {\displaystyle \operatorname {E}_{x \sim P} [\log P(X)] = -\sum_{i=1}^{n} p\left(x_{i}\right) \log p\left(x_{i}\right)}$$</p>

    <ol>
      <li><strong style="color: blue">Describe how Shannon Entropy relate to distributions with a graph:</strong><br />
 <img src="/main_files/math/prob/11.png" alt="img" width="100%" /></li>
    </ol>
  </li>
  <li><strong style="color: red">Define <em>Differential Entropy</em>:</strong><br />
 <strong>Differential Entropy</strong> is Shannons entropy of a <strong>continuous</strong> random variable \(x\)</li>
  <li><strong style="color: red">How does entropy characterize distributions?</strong><br />
 Distributions that are nearly deterministic (where the outcome is nearly certain) have low entropy; distributions that are closer to uniform have high entropy.</li>
  <li>
    <p><strong style="color: red">Define <em>Relative Entropy</em> - Give it’s formula:</strong><br />
 The <strong>Kullback–Leibler divergence</strong> (<strong>Relative Entropy</strong>) is a measure of how one probability distribution diverges from a second, expected probability distribution.</p>

    <p><strong>Mathematically:</strong></p>
    <p>$${\displaystyle D_{\text{KL}}(P\parallel Q)=\operatorname{E}_{x \sim P} \left[\log \dfrac{P(x)}{Q(x)}\right]=\operatorname{E}_{x \sim P} \left[\log P(x) - \log Q(x)\right]}$$</p>
    <ol>
      <li><strong>Discrete</strong>:</li>
    </ol>
    <p>$${\displaystyle D_{\text{KL}}(P\parallel Q)=\sum_{i}P(i)\log \left({\frac {P(i)}{Q(i)}}\right)}$$  </p>
    <ol>
      <li><strong>Continuous</strong>:</li>
    </ol>
    <p>$${\displaystyle D_{\text{KL}}(P\parallel Q)=\int_{-\infty }^{\infty }p(x)\log \left({\frac {p(x)}{q(x)}}\right)\,dx,}$$ </p>

    <ol>
      <li><strong style="color: blue">Give an interpretation:</strong>
        <ol>
          <li><strong>Discrete variables</strong>:<br />
 It is the extra amount of information needed to send a message containing symbols drawn from probability distribution \(P\), when we use a code that was designed to minimize the length of messages drawn from probability distribution \(Q\).</li>
        </ol>
      </li>
      <li><strong style="color: blue">List the properties:</strong>
        <ol>
          <li>Non-Negativity:<br />
     \({\displaystyle D_{\mathrm {KL} }(P\|Q) \geq 0}\)</li>
          <li>\({\displaystyle D_{\mathrm {KL} }(P\|Q) = 0 \iff}\) \(P\) and \(Q\) are:
            <ol>
              <li><em><strong>Discrete Variables</strong></em>:<br />
     the same distribution</li>
              <li><em><strong>Continuous Variables</strong></em>:<br />
     equal “almost everywhere”</li>
            </ol>
          </li>
          <li>Additivity of <em>Independent Distributions</em>:<br />
     \({\displaystyle D_{\text{KL}}(P\parallel Q)=D_{\text{KL}}(P_{1}\parallel Q_{1})+D_{\text{KL}}(P_{2}\parallel Q_{2}).}\)</li>
          <li>\({\displaystyle D_{\mathrm {KL} }(P\|Q) \neq D_{\mathrm {KL} }(Q\|P)}\)
            <blockquote>
              <p>This asymmetry means that there are important consequences to the choice of the ordering</p>
            </blockquote>
          </li>
          <li>Convexity in the pair of PMFs \((p, q)\) (i.e. \({\displaystyle (p_{1},q_{1})}\) and  \({\displaystyle (p_{2},q_{2})}\) are two pairs of PMFs):<br />
     \({\displaystyle D_{\text{KL}}(\lambda p_{1}+(1-\lambda )p_{2}\parallel \lambda q_{1}+(1-\lambda )q_{2})\leq \lambda D_{\text{KL}}(p_{1}\parallel q_{1})+(1-\lambda )D_{\text{KL}}(p_{2}\parallel q_{2}){\text{ for }}0\leq \lambda \leq 1.}\)</li>
        </ol>
      </li>
      <li><strong style="color: blue">Describe it as a distance:</strong><br />
 Because the KL divergence is non-negative and measures the difference between two distributions, it is often conceptualized as measuring some sort of distance between these distributions.<br />
 However, it is <strong>not</strong> a true distance measure because it is <strong><em>not symmetric</em></strong>.
        <blockquote>
          <p>KL-div is, however, a <em><strong>Quasi-Metric</strong></em>, since it satisfies all the properties of a distance-metric except symmetry</p>
        </blockquote>
      </li>
      <li><strong style="color: blue">List the applications of relative entropy:</strong><br />
 Characterizing:
        <ol>
          <li>Relative (Shannon) entropy in information systems</li>
          <li>Randomness in continuous time-series</li>
          <li>Information gain when comparing statistical models of inference</li>
        </ol>
      </li>
      <li><strong style="color: blue">How does the direction of minimization affect the optimization:</strong><br />
 Suppose we have a distribution \(p(x)\) and we wish to <em>approximate</em> it with another distribution \(q(x)\).<br />
 We have a choice of <em>minimizing</em> either:
        <ol>
          <li>\({\displaystyle D_{\text{KL}}(p\|q)} \implies q^\ast = \operatorname {arg\,min}_q {\displaystyle D_{\text{KL}}(p\|q)}\)<br />
 Produces an approximation that usually places high probability anywhere that the true distribution places high probability.</li>
          <li>\({\displaystyle D_{\text{KL}}(q\|p)} \implies q^\ast \operatorname {arg\,min}_q {\displaystyle D_{\text{KL}}(q\|p)}\)<br />
 Produces an approximation that rarely places high probability anywhere that the true distribution places low probability.
            <blockquote>
              <p>which are different due to the <em>asymmetry</em> of the KL-divergence</p>
            </blockquote>
          </li>
        </ol>

        <p><button class="showText" value="show" onclick="showTextPopHide(event);">Choice of KL-div Direction</button>
 <img src="/main_files/math/infothry/1.png" alt="img" width="100%" hidden="" /></p>
      </li>
    </ol>
  </li>
  <li><strong style="color: red">Define <em>Cross Entropy</em> - Give it’s formula:</strong><br />
 The <strong>Cross Entropy</strong> between two probability distributions \({\displaystyle p}\) and \({\displaystyle q}\) over the same underlying set of events measures the average number of bits needed to identify an event drawn from the set, if a coding scheme is used that is optimized for an “unnatural” probability distribution \({\displaystyle q}\), rather than the “true” distribution \({\displaystyle p}\).
    <p>$$H(p,q) = \operatorname{E}_{p}[-\log q]= H(p) + D_{\mathrm{KL}}(p\|q) =-\sum_{x }p(x)\,\log q(x)$$</p>
    <ol>
      <li><strong style="color: blue">What does it measure?</strong><br />
 The average number of bits that need to be transmitted using a different probability distribution \(q\) (for encoding) than the “true” distribution \(p\), to convey the information in \(p\).</li>
      <li><strong style="color: blue">How does it relate to <em>relative entropy</em>?</strong><br />
 It is similar to <strong>KL-Div</strong> but with an additional quantity - the entropy of \(p\).</li>
      <li><strong style="color: blue">When are they equivalent?</strong><br />
 Minimizing the cross-entropy with respect to \(Q\) is equivalent to minimizing the KL divergence, because \(Q\) does not participate in the omitted term.</li>
    </ol>
  </li>
  <li><strong style="color: red">Mutual Information:</strong>
    <ol>
      <li><strong style="color: blue">Definition:</strong><br />
 The <strong>Mutual Information (MI)</strong> of two random variables is a measure of the mutual dependence between the two variables.<br />
 More specifically, it quantifies the “amount of information” (in bits) obtained about one random variable through observing the other random variable.</li>
      <li><strong style="color: blue">What does it measure?</strong><br />
 It can be seen as a way of measuring the reduction in uncertainty (information content) of measuring a part of the system after observing the outcome of another parts of the system; given two R.Vs, knowing the value of one of the R.Vs in the system gives a corresponding reduction in (the uncertainty (information content) of) measuring the other one.</li>
      <li><strong style="color: blue">Intuitive Definitions:</strong>
        <ul>
          <li>Measures the information that \(X\) and \(Y\) share:<br />
  It measures how much knowing one of these variables reduces uncertainty about the other.
            <ul>
              <li><strong>\(X, Y\) Independent</strong>  \(\implies I(X; Y) = 0\): their MI is zero</li>
              <li><strong>\(X\) deterministic function of \(Y\) and vice versa</strong> \(\implies I(X; Y) = H(X) = H(Y)\) their MI is equal to entropy of each variable</li>
            </ul>
          </li>
          <li>It’s a Measure of the inherent dependence expressed in the joint distribution of  \(X\) and  \(Y\) relative to the joint distribution of \(X\) and \(Y\) under the assumption of independence.<br />
  i.e. The price for encoding \({\displaystyle (X,Y)}\) as a pair of independent random variables, when in reality they are not.</li>
        </ul>
      </li>
      <li><strong style="color: blue">Properties:</strong>
        <ul>
          <li>The KL-divergence shows that \(I(X; Y)\) is equal to zero precisely when <span style="color: goldenrod">the joint distribution conicides with the product of the marginals i.e. when </span> <strong style="color: goldenrod">\(X\) and \(Y\) are <em>independent</em></strong>.</li>
          <li>The MI is <strong>non-negative</strong>: \(I(X; Y) \geq 0\)
            <ul>
              <li>It is a measure of the price for encoding \({\displaystyle (X,Y)}\) as a pair of independent random variables, when in reality they are not.</li>
            </ul>
          </li>
          <li>It is <strong>symmetric</strong>: \(I(X; Y) = I(Y; X)\)</li>
          <li><strong>Related to conditional and joint entropies:</strong>
            <p>$${\displaystyle {\begin{aligned}\operatorname {I} (X;Y)&amp;{}\equiv \mathrm {H} (X)-\mathrm {H} (X|Y)\\&amp;{}\equiv \mathrm {H} (Y)-\mathrm {H} (Y|X)\\&amp;{}\equiv \mathrm {H} (X)+\mathrm {H} (Y)-\mathrm {H} (X,Y)\\&amp;{}\equiv \mathrm {H} (X,Y)-\mathrm {H} (X|Y)-\mathrm {H} (Y|X)\end{aligned}}}$$</p>
            <p>where \(\mathrm{H}(X)\) and \(\mathrm{H}(Y)\) are the marginal entropies, \(\mathrm{H}(X | Y)\) and \(\mathrm{H}(Y | X)\) are the conditional entopries, and \(\mathrm{H}(X, Y)\) is the joint entropy of \(X\) and \(Y\).</p>
            <ul>
              <li>Note the <em>analogy to the <strong>union, difference, and intersection of two sets</strong></em>:<br />
  <img src="https://cdn.mathpix.com/snip/images/aT2_JfK4TlRP9b5JawVqQigLD7dzxOrFjDIapoSF-F4.original.fullsize.png" alt="img" width="35%" class="center-image" /></li>
            </ul>
          </li>
          <li><strong>Related to KL-div of conditional distribution:</strong>
            <p>$$\mathrm{I}(X ; Y)=\mathbb{E}_{Y}\left[D_{\mathrm{KL}}\left(p_{X | Y} \| p_{X}\right)\right]$$</p>
          </li>
        </ul>
      </li>
      <li><strong style="color: blue">Applications:</strong><br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">Show Lists</button>
        <ul hidden="">
          <li>In search engine technology, mutual information between phrases and contexts is used as a feature for k-means clustering to discover semantic clusters (concepts)</li>
          <li>Discriminative training procedures for hidden Markov models have been proposed based on the maximum mutual information (MMI) criterion.</li>
          <li>Mutual information has been used as a criterion for feature selection and feature transformations in machine learning. It can be used to characterize both the relevance and redundancy of variables, such as the minimum redundancy feature selection.</li>
          <li>Mutual information is used in determining the similarity of two different clusterings of a dataset. As such, it provides some advantages over the traditional Rand index.</li>
          <li>Mutual information of words is often used as a significance function for the computation of collocations in corpus linguistics.</li>
          <li>Detection of phase synchronization in time series analysis</li>
          <li>The mutual information is used to learn the structure of Bayesian networks/dynamic Bayesian networks, which is thought to explain the causal relationship between random variables</li>
          <li>Popular cost function in decision tree learning.</li>
          <li>In the infomax method for neural-net and other machine learning, including the infomax-based Independent component analysis algorithm</li>
        </ul>
      </li>
      <li><strong style="color: blue">As KL-Divergence:</strong><br />
 Let \((X, Y)\) be a pair of random variables with values over the space \(\mathcal{X} \times \mathcal{Y}\) . If their joint distribution is \(P_{(X, Y)}\) and the marginal distributions are \(P_{X}\) and \(P_{Y},\) the mutual information is defined as:
        <p>$$I(X ; Y)=D_{\mathrm{KL}}\left(P_{(X, Y)} \| P_{X} \otimes P_{Y}\right)$$</p>
      </li>
      <li><strong style="color: blue">In-terms of PMFs for discrete distributions:</strong><br />
 The mutual information of two jointly discrete random variables \(X\) and \(Y\) is calculated as a double sum:
        <p>$$\mathrm{I}(X ; Y)=\sum_{y \in \mathcal{Y}} \sum_{x \in \mathcal{X}} p_{(X, Y)}(x, y) \log \left(\frac{p_{(X, Y)}(x, y)}{p_{X}(x) p_{Y}(y)}\right)$$</p>
        <p>where \({\displaystyle p_{(X,Y)}}\) is the joint probability mass function of \({\displaystyle X}\) X and \({\displaystyle Y}\), and \({\displaystyle p_{X}}\) and \({\displaystyle p_{Y}}\) are the marginal probability mass functions of \({\displaystyle X}\) and \({\displaystyle Y}\) respectively.</p>
      </li>
      <li><strong style="color: blue">In terms of PDFs for continuous distributions:</strong><br />
 In the case of jointly continuous random variables, the double sum is replaced by a double integral:
        <p>$$\mathrm{I}(X ; Y)=\int_{\mathcal{Y}} \int_{\mathcal{X}} p_{(X, Y)}(x, y) \log \left(\frac{p_{(X, Y)}(x, y)}{p_{X}(x) p_{Y}(y)}\right) d x d y$$</p>
        <p>where \(p_{(X, Y)}\) is now the joint probability density function of \(X\) and \(Y\) and \(p_{X}\) and \(p_{Y}\) are the marginal probability density functions of \(X\) and \(Y\) respectively.</p>
      </li>
      <li><strong style="color: blue">Relation to PMI:</strong><br />
 The mutual information (MI) of the random variables \(X\) and \(Y\) is the expected value of the PMI (over all possible outcomes).</li>
    </ol>
  </li>
  <li><strong style="color: red">Pointwise Mutual Information (PMI):</strong>
    <ol>
      <li><strong style="color: blue">Definition:</strong><br />
 The PMI of a pair of outcomes \(x\) and \(y\) belonging to discrete random variables \(X\) and \(Y\) quantifies the discrepancy between the probability of their coincidence given their joint distribution and their individual distributions, assuming independence. Mathematically:
        <p>$$\operatorname{pmi}(x ; y) \equiv \log \frac{p(x, y)}{p(x) p(y)}=\log \frac{p(x | y)}{p(x)}=\log \frac{p(y | x)}{p(y)}$$</p>
      </li>
      <li><strong style="color: blue">Relation to MI:</strong><br />
 In contrast to mutual information (MI) which builds upon PMI, it refers to single events, whereas MI refers to the average of all possible events.<br />
 The mutual information (MI) of the random variables \(X\) and \(Y\) is the expected value of the PMI (over all possible outcomes).</li>
    </ol>
  </li>
</ol>

<hr />

<h1 id="recommendation-systems">Recommendation Systems</h1>
<p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Recommendation Systems</button></p>
<ol hidden="">
  <li><strong style="color: red">Describe the different algorithms for recommendation systems:</strong></li>
</ol>

<hr />

<h1 id="ensemble-learning">Ensemble Learning</h1>
<p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Ensemble Learning</button></p>
<ol hidden="">
  <li><strong style="color: red">What are the two paradigms of ensemble methods?</strong>
    <ol>
      <li>Parallel</li>
      <li>Sequential</li>
    </ol>
  </li>
  <li><strong style="color: red">Random Forest VS GBM?</strong><br />
 The fundamental difference is, random forest uses bagging technique to make predictions. GBM uses boosting techniques to make predictions.</li>
</ol>

<hr />

<h1 id="data-processing-and-analysis">Data Processing and Analysis</h1>
<p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Data Processing and Analysis</button></p>
<ol hidden="">
  <li><strong style="color: red">What are 3 data preprocessing techniques to handle outliers?</strong>
    <ol>
      <li>Winsorizing/Winsorization (cap at threshold).</li>
      <li>Transform to reduce skew (using Box-Cox or similar).</li>
      <li>Remove outliers if you’re certain they are anomalies or measurement errors.</li>
    </ol>
  </li>
  <li><strong style="color: red">Describe the strategies to dimensionality reduction?</strong>
    <ol>
      <li>Feature Selection</li>
      <li>Feature Projection/Extraction</li>
    </ol>
  </li>
  <li><strong style="color: red">What are 3 ways of reducing dimensionality?</strong>
    <ol>
      <li>Removing Collinear Features</li>
      <li>Performing PCA, ICA, etc.</li>
      <li>Feature Engineering</li>
      <li>AutoEncoder</li>
      <li>Non-negative matrix factorization (NMF)</li>
      <li>LDA</li>
      <li>MSD</li>
    </ol>
  </li>
  <li><strong style="color: red">List methods for Feature Selection</strong>
    <ol>
      <li>Variance Threshold: normalize first (variance depends on scale)</li>
      <li>Correlation Threshold: remove the one with larger mean absolute correlation with other features.</li>
      <li>Genetic Algorithms</li>
      <li>Stepwise Search: bad performance, regularization much better, it’s a greedy algorithm (can’t account for future effects of each change)</li>
      <li>LASSO, Elastic-Net</li>
    </ol>
  </li>
  <li><strong style="color: red">List methods for Feature Extraction</strong>
    <ol>
      <li>PCA, ICA, CCA</li>
      <li>AutoEncoders</li>
      <li>LDA: LDA is a supervised linear transformation technique since the dependent variable (or the class label) is considered in the model. It Extracts the k new independent variables that <strong>maximize the separation between the classes of the dependent variable</strong>.
        <ol>
          <li>Linear discriminant analysis is used to find a linear combination of features that characterizes or separates two or more classes (or levels) of a categorical variable.</li>
          <li>Unlike PCA, LDA extracts the k new independent variables that <strong>maximize the separation between the classes of the dependent variable</strong>. LDA is a supervised linear transformation technique since the dependent variable (or the class label) is considered in the model.</li>
        </ol>
      </li>
      <li>Latent Semantic Analysis</li>
      <li>Isomap</li>
    </ol>
  </li>
  <li><strong style="color: red">How to detect correlation of “categorical variables”?</strong>
    <ol>
      <li>Chi-Squared test: it is a statistical test applied to the groups of categorical features to evaluate the likelihood of correlation or association between them using their frequency distribution.</li>
    </ol>
  </li>
  <li><strong style="color: red">Feature Importance</strong>
    <ol>
      <li>Use linear regression and select variables based on \(p\) values</li>
      <li>Use Random Forest, Xgboost and plot variable importance chart</li>
      <li>Lasso</li>
      <li>Measure information gain for the available set of features and select top \(n\) features accordingly.</li>
      <li>Use Forward Selection, Backward Selection, Stepwise Selection</li>
      <li>Remove the correlated variables prior to selecting important variables</li>
      <li>In linear models, feature importance can be calculated by the scale of the coefficients</li>
      <li>In tree-based methods (such as random forest), important features are likely to appear closer to the root of the tree. We can get a feature’s importance for random forest by computing the averaging depth at which it appears across all trees in the forest</li>
    </ol>
  </li>
  <li><strong style="color: red">Capturing the correlation between continuous and categorical variable? If yes, how?</strong><br />
 Yes, we can use ANCOVA (analysis of covariance) technique to capture association between continuous and categorical variables.<br />
 <a href="https://www.youtube.com/watch?v=a61mkzQRf6c&amp;t=2s">ANCOVA Explained</a></li>
  <li><strong style="color: red">What cross validation technique would you use on time series data set?</strong><br />
 <a href="https://en.wikipedia.org/wiki/Forward_chaining">Forward chaining strategy</a> with k folds.</li>
  <li><strong style="color: red">How to deal with missing features? (Imputation?)</strong>
    <ol>
      <li>Assign a unique category to missing values, who knows the missing values might decipher some trend.</li>
      <li>Remove them blatantly</li>
      <li>we can sensibly check their distribution with the target variable, and if found any pattern we’ll keep those missing values and assign them a new category while removing others.</li>
    </ol>
  </li>
  <li><strong style="color: red">Do you suggest that treating a categorical variable as continuous variable would result in a better predictive model?</strong><br />
 For better predictions, categorical variable can be considered as a continuous variable only when the variable is ordinal in nature.</li>
  <li><strong style="color: red">What are collinearity and multicollinearity?</strong>
    <ol>
      <li><strong>Collinearity</strong> occurs when two predictor variables (e.g., \(x_1\) and \(x_2\)) in a multiple regression have some correlation.</li>
      <li><strong>Multicollinearity</strong> occurs when more than two predictor variables (e.g., \(x_1, x_2, \text{ and } x_3\)) are inter-correlated.</li>
    </ol>
  </li>
  <li><strong style="color: red">What is data normalization and why do we need it?</strong><br />
 <img src="https://cdn.mathpix.com/snip/images/8aNuJetgTgCtv4pvqaI0dr96pDyUmfuX_d1aLK1lmaw.original.fullsize.png" alt="img" width="80%" /></li>
</ol>

<hr />

<h1 id="mlstatistical-models">ML/Statistical Models</h1>
<p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">ML/Statistical Models</button></p>
<ol hidden="">
  <li><strong style="color: red">What are parametric models?</strong><br />
 Parametric models are those with a finite number of parameters. To predict new data, you only need to know the parameters of the model. Examples include linear regression, logistic regression, and linear SVMs.</li>
  <li><strong style="color: red">What is a classifier?</strong><br />
 A function that maps…</li>
</ol>

<hr />

<h1 id="k-nn">K-NN</h1>
<p hidden=""><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">K-NN</button></p>

<hr />

<h1 id="pca">PCA</h1>
<p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">PCA</button></p>
<ol hidden="">
  <li><strong style="color: red">What is PCA?</strong><br />
 It is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components.</li>
  <li><strong style="color: red">What is the Goal of PCA?</strong><br />
 Given points \(\mathbf{x}_ i \in \mathbf{R}^d\), find k-directions that capture most of the variation.</li>
  <li><strong style="color: red">List the applications of PCA:</strong>
    <ol>
      <li>Find a small basis for representing variations in complex things.
        <blockquote>
          <p>e.g. faces, genes.</p>
        </blockquote>
      </li>
      <li>Reducing the number of dimensions makes some computations cheaper.</li>
      <li>Remove irrelevant dimensions to reduce over-fitting in learning algorithms.
        <blockquote>
          <p>Like “<em>subset selection</em>” but the features are <strong>not</strong> <em>axis aligned</em>.<br />
They are linear combinations of input features.</p>
        </blockquote>
      </li>
      <li>Represent the data with fewer parameters (dimensions)</li>
    </ol>
  </li>
  <li><strong style="color: red">Give formulas for the following:</strong>
    <ol>
      <li><strong style="color: blue">Assumptions on \(X\):</strong><br />
 The analysis above is valid only for (1) \(X\) w/ samples in rows and variables in columns  (2) \(X\) is centered (mean=0)</li>
      <li><strong style="color: blue">SVD of \(X\):</strong><br />
 \(X = USV^{T}\)</li>
      <li><strong style="color: blue">Principal Directions/Axes:</strong><br />
 \(V\)</li>
      <li><strong style="color: blue">Principal Components (scores):</strong><br />
 \(US\)</li>
      <li><strong style="color: blue">The \(j\)-th principal component:</strong><br />
 \(Xv_j = Us_j\)</li>
    </ol>
  </li>
  <li><strong style="color: red">Define the transformation, mathematically:</strong><br />
 Mathematically, the transformation is defined by a set of \(p\)-dimensional vectors of weights or coefficients \({\displaystyle \mathbf {v}_ {(k)}=(v_{1},\dots ,v_{p})_ {(k)}}\) that map each row vector \({\displaystyle \mathbf {x}_ {(i)}}\) of \(X\) to a new vector of principal component scores \({\displaystyle \mathbf {t} _{(i)}=(t_{1},\dots ,t_{l})_ {(i)}}\), given by:
    <p>$${\displaystyle {t_{k}}_{(i)}=\mathbf {x}_ {(i)}\cdot \mathbf {v}_ {(k)}\qquad \mathrm {for} \qquad i=1,\dots ,n\qquad k=1,\dots ,l}$$</p>
    <p>in such a way that the individual variables \({\displaystyle t_{1},\dots ,t_{l}}\)  of \(t\) considered over the data set successively inherit the maximum possible variance from \(X\), with each coefficient vector \(v\) constrained to be a unit vector (where \(l\) is usually selected to be less than \({\displaystyle p}\) to reduce dimensionality).</p>
  </li>
  <li><strong style="color: red">What does PCA produce/result in?</strong><br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">Show</button>
    <ol hidden="">
      <li><strong style="color: blue">Finds a lower dimensional subspace spanned by what?:</strong><br />
 Finds a lower dimensional subspace spanned by PCs.</li>
      <li><strong style="color: blue">Finds a lower dimensional subspace that minimizes what?:</strong><br />
 Finds a lower dimensional subspace (PCs) that Minimizes the RSS of projection errors.</li>
      <li><strong style="color: blue">What does each PC have (properties)?</strong><br />
 Produces a vector (1st PC) with the highest possible variance, each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components.</li>
      <li><strong style="color: blue">What does the procedure find in terms of a “basis”?</strong><br />
 Results in an <strong>uncorrelated orthogonal basis set</strong>.</li>
      <li><strong style="color: blue">What does the procedure find in terms of axes? (where do they point?):</strong><br />
 PCA constructs new axes that point to the directions of maximal variance (in the original variable space)</li>
    </ol>
  </li>
  <li><strong style="color: red">Describe the PCA algorithm:</strong>
    <ol>
      <li><strong>Data Preprocessing</strong>:
        <ol>
          <li>Training set: \(x^{(1)}, x^{(2)}, \ldots, x^{(m)}\)</li>
          <li>Preprocessing (<strong>feature scaling</strong> + <strong>mean normalization</strong>):
            <ol>
              <li><strong>mean normalization</strong>:<br />
 \(\mu_{j}=\frac{1}{m} \sum_{i=1}^{m} x_{j}^{(i)}\)<br />
 Replace each \(x_{j}^{(i)}\) with \(x_j^{(i)} - \mu_j\)</li>
              <li><strong>feature scaling</strong>:<br />
 If different features on different, scale features to have comparable range<br />
 \(s_j = S.D(X_j)\) (the standard deviation of feature \(j\))<br />
 Replace each \(x_{j}^{(i)}\) with \(\dfrac{x_j^{(i)} - \mu_j}{s_j}\)</li>
            </ol>
          </li>
        </ol>
      </li>
      <li><strong>Computing the Principal Components</strong>:
        <ol>
          <li>Compute the <strong>SVD</strong> of the matrix \(X = U S V^T\)</li>
          <li>Compute the Principal Components:
            <p>$$T = US = XV$$</p>
            <blockquote>
              <p>Note: The \(j\)-th principal component is: \(Xv_j\)</p>
            </blockquote>
          </li>
          <li>Choose the top \(k\) components singular values in \(S = S_k\)</li>
          <li>Compute the Truncated Principal Components:
            <p>$$T_k = US_k$$</p>
          </li>
        </ol>
      </li>
      <li><strong>Computing the Low-rank Approximation Matrix \(X_k\)</strong>:
        <ol>
          <li>Compute the reconstruction matrix:
            <p>$$X_k = T_kV^T = US_kV^T$$</p>
          </li>
        </ol>
      </li>
    </ol>

    <p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Show specifics</button></p>
    <ol hidden="">
      <li><strong style="color: blue">What Data Processing needs to be done?</strong>
        <ol>
          <li>Preprocessing (<strong>feature scaling</strong> + <strong>mean normalization</strong>):
            <ol>
              <li><strong>mean normalization</strong>:<br />
 \(\mu_{j}=\frac{1}{m} \sum_{i=1}^{m} x_{j}^{(i)}\)<br />
 Replace each \(x_{j}^{(i)}\) with \(x_j^{(i)} - \mu_j\)</li>
              <li><strong>feature scaling</strong>:<br />
 If different features on different, scale features to have comparable range<br />
 \(s_j = S.D(X_j)\) (the standard deviation of feature \(j\))<br />
 Replace each \(x_{j}^{(i)}\) with \(\dfrac{x_j^{(i)} - \mu_j}{s_j}\)</li>
            </ol>
          </li>
        </ol>
      </li>
      <li><strong style="color: blue">How to compute the Principal Components?</strong>
        <ol>
          <li>Compute the <strong>SVD</strong> of the matrix \(X = U S V^T\)</li>
          <li>Compute the Principal Components:
            <p>$$T = US = XV$$</p>
          </li>
        </ol>
      </li>
      <li><strong style="color: blue">How do you compute the Low-Rank Approximation Matrix \(X_k\)?</strong>
        <ol>
          <li>Choose the top \(k\) components singular values in \(S = S_k\)</li>
          <li>Compute the Truncated Principal Components:
            <p>$$T_k = US_k$$</p>
          </li>
          <li>Compute the reconstruction matrix:
            <p>$$X_k = T_kV^T = US_kV^T$$</p>
          </li>
        </ol>
      </li>
    </ol>
  </li>
  <li><strong style="color: red">Describe the Optimality of PCA:</strong><br />
 Optimal for Finding a lower dimensional subspace (PCs) that Minimizes the RSS of projection errors.</li>
  <li><strong style="color: red">List limitations of PCA:</strong>
    <ol>
      <li>PCA is highly sensitive to the (relative) scaling of the data; no consensus on best scaling.</li>
    </ol>
  </li>
  <li><strong style="color: red">Intuition:</strong>
    <ol>
      <li>PCA can be thought of as fitting a \(p\)-dimensional ellipsoid to the data, where each axis of the ellipsoid represents a principal component. If some axis of the ellipsoid is small, then the variance along that axis is also small, and by omitting that axis and its corresponding principal component from our representation of the dataset, we lose only a commensurately small amount of information.</li>
      <li>Its operation can be thought of as revealing the internal structure of the data in a way that best explains the variance in the data.</li>
    </ol>

    <p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Show specifics</button></p>
    <ol hidden="">
      <li><strong style="color: blue">What property of the internal structure of the data does PCA reveal/explain?</strong><br />
 The variance in the data.</li>
      <li><strong style="color: blue">What object does it fit to the data?:</strong>
        <ol>
          <li>PCA can be thought of as fitting a \(p\)-dimensional ellipsoid to the data, where each axis of the ellipsoid represents a principal component.</li>
        </ol>
      </li>
    </ol>
  </li>
  <li><strong style="color: red">Should you remove correlated features b4 PCA?</strong><br />
 Yes. Discarding correlated variables have a substantial effect on PCA because, in presence of correlated variables, the variance explained by a particular component gets inflated. <a href="https://stats.stackexchange.com/questions/50537/should-one-remove-highly-correlated-variables-before-doing-pca">Discussion</a></li>
  <li><strong style="color: red">How can we measure the “Total Variance” of the data?</strong><br />
 1.*The Total Variance** of the data can be expressed as the sum of all the eigenvalues:
    <p>$$\mathbf{Tr} \Sigma = \mathbf{Tr} (U \Lambda U^T) = \mathbf{Tr} (U^T U \Lambda) = \mathbf{Tr} \Lambda = \lambda_1 + \ldots + \lambda_n.$$</p>
  </li>
  <li><strong style="color: red">How can we measure the “Total Variance” of the <em>projected data</em>?</strong><br />
 1.*The Total Variance** of the <strong><em>Projected</em></strong> data is:
    <p>$$\mathbf{Tr} (P \Sigma P^T ) = \lambda_1 + \lambda_2 + \cdots + \lambda_k. $$</p>
  </li>
  <li><strong style="color: red">How can we measure the <em>“Error in the Projection”</em>?</strong><br />
 1.*The Error in the Projection** could be measured with respect to variance.
    <ol>
      <li>We define the <strong>ratio of variance</strong> “explained” by the projected data (equivalently, the ratio of information <em>“retained”</em>) as:</li>
    </ol>
    <p>$$\dfrac{\lambda_1 + \ldots + \lambda_k}{\lambda_1 + \ldots + \lambda_n}. $$</p>
    <ol>
      <li><strong style="color: blue">What does it mean when this ratio is high?</strong><br />
 If the ratio is <em>high</em>, we can say that much of the variation in the data can be observed on the projected plane.</li>
    </ol>
  </li>
  <li><strong style="color: red">How does PCA relate to CCA?</strong>
    <ol>
      <li><strong>CCA</strong> defines coordinate systems that optimally describe the cross-covariance between two datasets while</li>
      <li><strong>PCA</strong> defines a new orthogonal coordinate system that optimally describes variance in a single dataset.</li>
    </ol>
  </li>
  <li><strong style="color: red">How does PCA relate to ICA?</strong><br />
 <strong>Independent component analysis (ICA)</strong> is directed to similar problems as principal component analysis, but finds additively separable components rather than successive approximations.</li>
</ol>

<hr />

<h1 id="the-centroid-method">The Centroid Method</h1>
<p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">The Centroid Method</button></p>

<ol hidden="">
  <li>
    <p><strong style="color: red">Define “The Centroid”:</strong>  <br />
 In mathematics and physics, the centroid or geometric center of a plane figure is the arithmetic mean (“average”) position of all the points in the shape.</p>

    <p>The definition extends to any object in n-dimensional space: its centroid is the mean position of all the points in all of the coordinate directions.</p>
  </li>
  <li>
    <p><strong style="color: red">Describe the Procedure:</strong>  <br />
 Compute the mean (\(\mu_c\)) of all the vectors in class \(C\) and the mean (\(\mu_x\)) of all the vectors not in \(C\)</p>
  </li>
  <li><strong style="color: red">What is the Decision Function:</strong>
    <p>$$f(x) = (\mu_c - \mu_x) \cdot \vec{x} - (\mu_c - \mu_x) \cdot \dfrac{\mu_c + \mu_x}{2}$$</p>
  </li>
  <li><strong style="color: red">Describe the Decision Boundary:</strong>  <br />
 The decision boundary is a Hyperplane that bisects the line segment with endpoints \(&lt;\mu_c, \mu_x&gt;\)</li>
</ol>

<hr />

<h1 id="k-means">K-Means</h1>
<p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">K-Means</button></p>
<ol hidden="">
  <li><strong style="color: red">What is K-Means?</strong><br />
 It is a clustering algorithm. It aims to partition \(n\) observations into \(k\) clusters in which each observation belongs to the cluster with the nearest mean. It results in a partitioning of the data space into <strong>Voronoi Cells</strong>.</li>
  <li><strong style="color: red">What is the idea behind K-Means?</strong>
    <ol>
      <li>Minimize the <em>aggregate intra-cluster distance</em></li>
      <li>Equivalent to minimizing the <em>variance</em></li>
      <li>Thus, it finds \(k-\)clusters with <strong>minimum aggregate variance</strong></li>
    </ol>
  </li>
  <li><strong style="color: red">What does K-Mean find?</strong><br />
 It finds \(k-\)clusters with <strong>minimum aggregate variance</strong>.</li>
  <li><strong style="color: red">Formal Description of the Model:</strong><br />
 Given a set of observations \(\left(\mathbf{x}_{1}, \mathbf{x} _{2}, \ldots, \mathbf{x}_{n}\right)\), \(\mathbf{x}_ i \in \mathbb{R}^d\), the algorithm aims to partition the \(n\) observations into \(k\) sets \(\mathbf{S}=\left\{S_{1}, S_{2}, \ldots, S_{k}\right\}\) so as to minimize the <strong>intra-cluster Sum-of-Squares</strong> (i.e. <strong>variance</strong>).
    <ol>
      <li><strong style="color: blue">What is the Objective?</strong>
        <p>$$\underset{\mathbf{S}}{\arg \min } \sum_{i=1}^{k} \sum_{\mathbf{x} \in S_{i}}\left\|\mathbf{x}-\boldsymbol{\mu}_{i}\right\|^{2}=\underset{\mathbf{S}}{\arg \min } \sum_{i=1}^{k}\left|S_{i}\right| \operatorname{Var} S_{i}$$</p>
        <p>where \(\boldsymbol{\mu}_i\) is the mean of points in \(S_i\).</p>
      </li>
    </ol>
  </li>
  <li><strong style="color: red">Description of the Algorithm:</strong>
    <ol>
      <li>Choose two random points, call them <em>“Centroids”</em></li>
      <li>Assign the closest \(N/2\) points (Euclidean-wise) to each of the Centroids</li>
      <li>Compute the mean of each <em>“group”/class</em> of points</li>
      <li>Re-Assign the centroids to the newly computed Means ↑</li>
      <li>REPEAT!</li>
    </ol>
  </li>
  <li><strong style="color: red">What is the Optimization method used? What class does it belong to?</strong><br />
 Coordinate descent. Expectation-Maximization.<br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">Show</button>
    <ol hidden="">
      <li><strong style="color: blue">How does the optimization method relate to EM?</strong><br />
 The “assignment” step is referred to as the “expectation step”, while the “update step” is a maximization step, making this algorithm a variant of the generalized expectation-maximization algorithm.</li>
    </ol>
  </li>
  <li><strong style="color: red">What is the Complexity of the algorithm?</strong><br />
 The original formulation of the problem is <strong>NP-Hard</strong>; however, <strong>EM</strong> algorithms (specifically, Coordinate-Descent) can be used as efficient heuristic algorithms that converge quickly to good local minima.</li>
  <li><strong style="color: red">Describe the convergence and prove it:</strong><br />
 Guaranteed to converge after a finite number of iterations to a local minimum.
    <ol>
      <li>
        <p><strong>Proof:</strong><br />
 The Algorithm Minimizes a <strong>monotonically decreasing</strong>, <strong>Non-Negative</strong> <em>Energy function</em> on a finite Domain:<br />
 By <em><strong>Monotone Convergence Theorem</strong></em> the objective Value Converges.</p>

        <p><button class="showText" value="show" onclick="showTextPopHide(event);">Show Proof</button>
 <img src="/main_files/ml/kmeans/2.png" alt="img" hidden="" /></p>
      </li>
    </ol>
  </li>
  <li><strong style="color: red">Describe the Optimality of the Algorithm:</strong>
    <ol>
      <li><strong>Locally optimal</strong>:<br />
 due to convergence property</li>
      <li><strong>Non-Globally optimal:</strong>
        <ol>
          <li>The <em>objective function</em> is <em><strong>non-convex</strong></em></li>
          <li>Moreover, coordinate Descent doesn’t converge to global minimum on non-convex functions.</li>
        </ol>
      </li>
    </ol>
  </li>
  <li><strong style="color: red">Derive the estimated parameters of the algorithm:</strong>
    <ol>
      <li><strong style="color: blue">Objective Function:</strong>
        <p>$$J(S, \mu)= \sum_{i=1}^{k} \sum_{\mathbf{x} \in S_{i}} \| \mathbf{x} -\mu_i \|^{2}$$</p>
      </li>
      <li><strong style="color: blue">Optimization Objective:</strong>
        <p>$$\min _{\mu} \min _{S} \sum_{i=1}^{k} \sum_{\mathbf{x} \in S_{i}}\left\|\mathbf{x} -\mu_{i}\right\|^{2}$$</p>
      </li>
      <li><strong style="color: blue">Derivation:</strong></li>
      <li>Fix \(S = \hat{S}\), optimize \(\mu\):
        <p>$$\begin{aligned} &amp; \min _{\mu} \sum_{i=1}^{k} \sum_{\mathbf{x} \in \hat{S}_{i}}\left\|\mu_{i}-x_{j}\right\|^{2}\\
     =&amp;  \sum_{i=1}^{k} \min _{\mu_i} \sum_{\mathbf{x} \in \hat{S}_{i}}\left\|\mathbf{x} - \mu_{i}\right\|^{2}
 \end{aligned}$$</p>
        <ol>
          <li><strong>MLE</strong>:
            <p>$$\min _{\mu_i} \sum_{\mathbf{x} \in \hat{S}_{i}}\left\|\mathbf{x} - \mu_{i}\right\|^{2}$$</p>
            <p>\(\implies\)</p>
            <p>$${\displaystyle \hat{\mu_i} = \dfrac{\sum_{\mathbf{x} \in \hat{S}_ {i}} \mathbf{x}}{\vert\hat{S}_ i\vert}}$$</p>
            <p><button class="showText" value="show" onclick="showTextPopHide(event);">Show Derivation</button>
 <img src="/main_files/ml/kmeans/3.png" alt="img" width="75%" hidden="" /></p>
          </li>
        </ol>
      </li>
      <li>Fix \(\mu_i = \hat{\mu_i}, \forall i\), optimize \(S\):
        <p>$$\arg \min _{S} \sum_{i=1}^{k} \sum_{\mathbf{x} \in S_{i}}\left\|\mathbf{x} - \hat{\mu_{i}}\right\|^{2}$$</p>
        <p>\(\implies\)</p>
        <p>$$S_{i}^{(t)}=\left\{x_{p} :\left\|x_{p}-m_{i}^{(t)}\right\|^{2} \leq\left\|x_{p}-m_{j}^{(t)}\right\|^{2} \forall j, 1 \leq j \leq k\right\}$$</p>
        <ol>
          <li><strong>MLE</strong>:<br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">Show Derivation</button>
 <img src="/main_files/ml/kmeans/1.png" alt="img" width="75%" hidden="" /></li>
        </ol>
      </li>
    </ol>
  </li>
  <li><strong style="color: red">When does K-Means fail to give good results?</strong><br />
 K-means clustering algorithm fails to give good results when:
    <ol>
      <li>the data contains outliers</li>
      <li>the density spread of data points across the data space is different</li>
      <li>and the data points follow nonconvex shapes.</li>
    </ol>
  </li>
</ol>

<hr />

<h1 id="naive-bayes">Naive Bayes</h1>
<p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Naive Bayes</button></p>
<ol hidden="">
  <li><strong style="color: red">Define:</strong>
    <ol>
      <li><strong style="color: blue">Naive Bayes:</strong><br />
 It is a simple technique used for constructing classifiers.</li>
      <li><strong style="color: blue">Naive Bayes Classifiers:</strong><br />
 A family of <em>simple probabilistic classifiers</em> based on applying <em>bayes theorem</em> with <em>strong (naive) independence assumptions</em> <strong>between the features</strong>.</li>
      <li><strong style="color: blue">Bayes Theorem:</strong><br />
 \(p(x\vert y) = \dfrac{p(y\vert x) p(x)}{p(y)}\)</li>
    </ol>
  </li>
  <li><strong style="color: red">List the assumptions of Naive Bayes:</strong>
    <ol>
      <li><strong>Conditional Independence:</strong> the features are <em>conditionally independent</em> from each other given a class \(C_k\)</li>
      <li><strong>Bag-of-words:</strong> The relative importance (positions) of the features do not matter</li>
    </ol>
  </li>
  <li><strong style="color: red">List some properties of Naive Bayes:</strong>
    <ol>
      <li><strong>Not</strong> a <em>Bayesian method</em></li>
      <li>It’s a <strong>Bayes Classifier</strong>: minimizes the probability of misclassification</li>
    </ol>

    <p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Show</button></p>
    <ol hidden="">
      <li><strong style="color: blue">Is it a Bayesian Method or Frequentest Method?</strong><br />
 <strong>Not</strong> a <em>Bayesian method</em></li>
      <li><strong style="color: blue">Is it a Bayes Classifier? What does that mean?:</strong><br />
 It’s a <strong>Bayes Classifier</strong>: minimizes the probability of misclassification</li>
    </ol>
  </li>
  <li><strong style="color: red">Define the Probabilistic Model for the method:</strong> <br />
 Naive Bayes is a <strong>conditional probability model:</strong><br />
 given a problem instance to be classified represented by a vector \(\boldsymbol{x} = (x_1, \cdots, x_n)\) of \(n\) features/words (independent variables), it assigns to this instance probabilities:
    <p>$$P(C_k\vert \boldsymbol{x}) = p(C_k\vert x_1, \cdots, x_n)$$</p>
    <p>for each of the \(k\) classes.</p>

    <p>Using <strong>Bayes theorem</strong> to decompose the conditional probability:</p>
    <p>$$p(C_k\vert \boldsymbol{x}) = \dfrac{p(\boldsymbol{x}\vert C_k) p(C_k)}{p(\boldsymbol{x})}$$</p>

    <p>Notice that the <em><strong>numerator</strong></em> is equivalent to the <em><strong>joint probability distribution</strong></em>:</p>
    <p>$$p\left(C_{k}\right) p\left(\mathbf{x} | C_{k}\right) = p\left(C_{k}, x_{1}, \ldots, x_{n}\right)$$</p>
    <p>Using the <strong>Chain-Rule</strong> for repeated application of the conditional probability, the <em>joint probability</em> model can be rewritten as:</p>
    <p>$$p(C_{k},x_{1},\dots ,x_{n})\, = p(x_{1}\mid x_{2},\dots ,x_{n},C_{k})p(x_{2}\mid x_{3},\dots ,x_{n},C_{k})\dots p(x_{n-1}\mid x_{n},C_{k})p(x_{n}\mid C_{k})p(C_{k})$$</p>

    <p>Using the <strong>Naive Conditional Independence</strong> assumptions:</p>
    <p>$$p\left(x_{i} | x_{i+1}, \ldots, x_{n}, C_{k}\right)=p\left(x_{i} | C_{k}\right)$$</p>
    <p>Thus, we can write the <strong>joint model</strong> as:</p>
    <p>$${\displaystyle {\begin{aligned}p(C_{k}\mid x_{1},\dots ,x_{n})&amp;\varpropto p(C_{k},x_{1},\dots ,x_{n})\\&amp;=p(C_{k})\ p(x_{1}\mid C_{k})\ p(x_{2}\mid C_{k})\ p(x_{3}\mid C_{k})\ \cdots \\&amp;=p(C_{k})\prod _{i=1}^{n}p(x_{i}\mid C_{k})\,,\end{aligned}}}$$</p>

    <p>Finally, the <em><strong>conditional distribution over the class variable \(C\)</strong></em> is:</p>
    <p>$${\displaystyle p(C_{k}\mid x_{1},\dots ,x_{n})={\frac {1}{Z}}p(C_{k})\prod _{i=1}^{n}p(x_{i}\mid C_{k})}$$</p>
    <p>where, \({\displaystyle Z=p(\mathbf {x} )=\sum _{k}p(C_{k})\ p(\mathbf {x} \mid C_{k})}\) is a <strong>constant</strong> scaling factor, a <strong>dependent only</strong> on the, <em>known</em>, feature variables \(x_i\)s.</p>

    <p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Show</button></p>
    <ol hidden="">
      <li><strong style="color: blue">What kind of model is it?</strong><br />
 Conditional Probability Model.</li>
      <li><strong style="color: blue">What is a conditional probability model?</strong><br />
 A model that assigns probabilities to an input \(\boldsymbol{x}\) conditioned on being a member of each class in a set of \(k\) classes \(C_1, \cdots, C_k\).</li>
      <li><strong style="color: blue">Decompose the conditional probability w/ Bayes Theorem:</strong>
        <p>$$p(C_k\vert \boldsymbol{x}) \dfrac{p(\boldsymbol{x}\vert C_k) p(C_k)}{p(\boldsymbol{x})}= $$</p>
      </li>
      <li><strong style="color: blue">How does the new expression incorporate the joint probability model?</strong><br />
 We notice that the <em><strong>numerator</strong></em> is equivalent to the <em><strong>joint probability distribution</strong></em>:
        <p>$$p\left(C_{k}\right) p\left(\mathbf{x} | C_{k}\right) = p\left(C_{k}, x_{1}, \ldots, x_{n}\right)$$</p>
      </li>
      <li><strong style="color: blue">Use the chain rule to re-write the joint probability model:</strong>
        <p>$$p(C_{k},x_{1},\dots ,x_{n})\, = p(x_{1}\mid x_{2},\dots ,x_{n},C_{k})p(x_{2}\mid x_{3},\dots ,x_{n},C_{k})\dots p(x_{n-1}\mid x_{n},C_{k})p(x_{n}\mid C_{k})p(C_{k})$$</p>
      </li>
      <li><strong style="color: blue">Use the Naive Conditional Independence assumption to rewrite the joint model:</strong>
        <p>$${\displaystyle {\begin{aligned}p(C_{k}\mid x_{1},\dots ,x_{n})&amp;\varpropto p(C_{k},x_{1},\dots ,x_{n})\\&amp;=p(C_{k})\ p(x_{1}\mid C_{k})\ p(x_{2}\mid C_{k})\ p(x_{3}\mid C_{k})\ \cdots \\&amp;=p(C_{k})\prod _{i=1}^{n}p(x_{i}\mid C_{k})\,,\end{aligned}}}$$</p>
      </li>
      <li><strong style="color: blue">What is the conditional distribution over the class variable \(C_k\):</strong>
        <p>$${\displaystyle p(C_{k}\mid x_{1},\dots ,x_{n})={\frac {1}{Z}}p(C_{k})\prod _{i=1}^{n}p(x_{i}\mid C_{k})}$$</p>
        <p>where, \({\displaystyle Z=p(\mathbf {x} )=\sum _{k}p(C_{k})\ p(\mathbf {x} \mid C_{k})}\) is a <strong>constant</strong> scaling factor, a <strong>dependent only</strong> on the, <em>known</em>, feature variables \(x_i\)s.</p>
      </li>
    </ol>
  </li>
  <li>
    <p><strong style="color: red">Construct the classifier. What are its components? Formally define it.</strong><br />
 The classifier is made of (1) the conditional probability model (above) \(\:\)  and (2) A decision rule.</p>

    <p>The decision rule used is the <strong>MAP</strong> hypothesis: i.e. pick the hypothesis that is most probable (maximize the MAP estimate=posterior*prior).</p>

    <p>The classifier is the <strong>function that assigns a class label \(\hat{y} = C_k\)</strong> for some \(k\) as follows:</p>
    <p>$$\hat{y}=\underset{k \in\{1, \ldots, K\}}{\operatorname{argmax}} p\left(C_{k}\right) \prod_{i=1}^{n} p\left(x_{i} | C_{k}\right)$$</p>
    <p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Show</button></p>
    <ol hidden="">
      <li><strong style="color: blue">What’s the decision rule used?</strong><br />
 we commonly use the <a href="https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation"><strong>Maximum A Posteriori (MAP)</strong></a> hypothesis, as the decision rule; i.e. pick the hypothesis that is most probable.</li>
      <li><strong style="color: blue">List the difference between the Naive Bayes Estimate and the MAP Estimate:</strong>
        <ol>
          <li><strong>MAP Estimate</strong>:
            <p>$${\displaystyle {\hat {y}_{\text{MAP}}}={\underset {k\in \{1,\dots ,K\}}{\operatorname {argmax} }}\ p(C_{k})\ p(\mathbf {x} \mid C_{k})}$$</p>
          </li>
          <li><strong>Naive Bayes Estimate:</strong>
            <p>$${\displaystyle {\hat {y}_{\text{NB}}}={\underset {k\in \{1,\dots ,K\}}{\operatorname {argmax} }}\ p(C_{k})\displaystyle \prod _{i=1}^{n}p(x_{i}\mid C_{k})}$$</p>
          </li>
        </ol>
      </li>
    </ol>
  </li>
  <li><strong style="color: red">What are the parameters to be estimated for the classifier?:</strong>
    <ol>
      <li>The posterior probability of each feature/word given a class</li>
      <li>The prior probability of each class</li>
    </ol>
  </li>
  <li><strong style="color: red">What method do we use to estimate the parameters?:</strong><br />
 Maximum Likelihood Estimation (MLE).</li>
  <li><strong style="color: red">What are the estimates for each of the following parameters?:</strong><br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">Show</button>
    <ol hidden="">
      <li><strong style="color: blue">The prior probability of each class:</strong><br />
 \(\hat{P}(C_k) = \dfrac{\text{doc-count}(C=C_k)}{N_\text{doc}}\),</li>
      <li><strong style="color: blue">The conditional probability of each feature (word) given a class:</strong><br />
 \(\hat{P}(x_i | C_i) = \dfrac{\text{count}(x_i,C_j)}{\sum_{x \in V} \text{count}(x, C_j)}\)</li>
    </ol>
  </li>
</ol>

<hr />

<h1 id="cnns">CNNs</h1>
<p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">CNNs</button></p>
<ol hidden="">
  <li>
    <p><strong style="color: red">What is a CNN?</strong><br />
 A <strong>convolutional neural network (CNN, or ConvNet)</strong> is a class of deep, feed-forward artificial neural networks that has successfully been applied to analyzing visual imagery.</p>

    <p>Convolutional networks are simply neural networks that use convolution in place of general matrix multiplication in at least one of their layers.</p>
    <ol>
      <li><strong style="color: blue">What kind of data does it work on? What is the mathematical property?</strong><br />
 In general, it works on data that have <em>grid-like topology.</em></li>
    </ol>
  </li>
  <li><strong style="color: red">What are the layers of a CNN?</strong>
    <p>$$\text{Input} \longrightarrow \left[\text{Conv} \rightarrow \text{ReLU} \rightarrow \text{Pooling} \rightarrow \text{Norm}\right] \times N \longrightarrow \text{FC}$$</p>
  </li>
  <li><strong style="color: red">What are the four important ideas and their benefits that the convolution affords CNNs:</strong>
    <ol>
      <li>
        <p><strong>Sparse Interactions/Connectivity/Weights:</strong><br />
 Unlike FNNs, where every input unit is connected to every output unit, CNNs have sparse interactions. This is accomplished by making the kernel smaller than the input.</p>

        <p id="lst-p"><strong style="color: red">Benefits:</strong></p>
        <ol>
          <li>This means that we need to <em>store fewer parameters</em>, which both,
            <ol>
              <li><em>Reduces the memory requirements</em> of the model and</li>
              <li><em>Improves</em> its <em>statistical efficiency</em></li>
            </ol>
          </li>
          <li>Also, Computing the output requires fewer operations</li>
          <li>In deep CNNs, the units in the deeper layers interact indirectly with large subsets of the input which allows modelling of complex interactions through sparse connections.</li>
        </ol>
      </li>
      <li>
        <p><strong>Parameter Sharing:</strong> <br />
 refers to using the same parameter for more than one function in a model.</p>

        <p id="lst-p"><strong style="color: red">Benefits:</strong></p>
        <ol>
          <li>This means that rather than learning a separate set of parameters for every location, we <em>learn only one set of parameters</em>.
            <ol>
              <li>This does not affect the runtime of forward propagation—it is still \(\mathcal{O}(k \times n)\)</li>
              <li>But it does further reduce the storage requirements of the model to \(k\) parameters (\(k\) is usually several orders of magnitude smaller than \(m\))</li>
            </ol>
          </li>
        </ol>
      </li>
      <li>
        <p><strong>Equivariant Representations:</strong><br />
 For convolutions, the particular form of parameter sharing causes the layer to have a property called <strong>equivariance to translation</strong>.</p>

        <p>Thus, if we move the object in the input, its representation will move the same amount in the output.</p>

        <p id="lst-p"><strong style="color: red">Benefits:</strong></p>
        <ol>
          <li>It is most useful when we know that some function of a small number of neighboring pixels is useful when applied to multiple input locations (e.g. edge detection)</li>
          <li>Shifting the position of an object in the input doesn’t confuse the NN</li>
          <li>Robustness against translated inputs/images</li>
        </ol>
      </li>
      <li>
        <p><strong>Accepts inputs of variable sizes</strong></p>
      </li>
    </ol>
  </li>
  <li><strong style="color: red">What is the inspirational model for CNNs:</strong><br />
 Convolutional networks were inspired by biological processes in which the connectivity pattern between neurons is inspired by the organization of the animal visual cortex.<br />
 Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field.</li>
  <li><strong style="color: red">Describe the connectivity pattern of the neurons in a layer of a CNN:</strong> <br />
 The neurons in a layer will only be connected to a small region of the layer before it, instead of all of the neurons in a fully-connected manner.</li>
  <li><strong style="color: red">Describe the process of a ConvNet:</strong><br />
 ConvNets transform the original image layer by layer from the original pixel values to the final class scores.</li>
  <li><strong style="color: red">Convolution Operation:</strong>
    <ol>
      <li>
        <p><strong style="color: blue">Define:</strong></p>
      </li>
      <li><strong style="color: blue">Formula (continuous):</strong></li>
      <li><strong style="color: blue">Formula (discrete):</strong></li>
      <li><strong style="color: blue">Define the following:</strong>
        <ol>
          <li><strong style="color: blue">Feature Map:</strong></li>
        </ol>
      </li>
      <li><strong style="color: blue">Does the operation commute?</strong></li>
    </ol>
  </li>
  <li><strong style="color: red">Cross Correlation:</strong>
    <ol>
      <li><strong style="color: blue">Define:</strong></li>
      <li><strong style="color: blue">Formulae:</strong></li>
      <li><strong style="color: blue">What are the differences/similarities between convolution and cross-correlation:</strong></li>
    </ol>
  </li>
  <li><strong style="color: red">Write down the Convolution operation and the cross-correlation over two axes and:</strong>
    <ol>
      <li><strong style="color: blue">Convolution:</strong></li>
      <li><strong style="color: blue">Convolution (commutative):</strong></li>
      <li><strong style="color: blue">Cross-Correlation:</strong></li>
    </ol>
  </li>
  <li><strong style="color: red">The Convolutional Layer:</strong>
    <ol>
      <li><strong style="color: blue">What are the parameters and how do we choose them?</strong></li>
      <li><strong style="color: blue">Describe what happens in the forward pass:</strong></li>
      <li><strong style="color: blue">What is the output of the forward pass:</strong></li>
      <li><strong style="color: blue">How is the output configured?</strong></li>
    </ol>
  </li>
  <li><strong style="color: red">Spatial Arrangements:</strong>
    <ol>
      <li><strong style="color: blue">List the Three Hyperparameters that control the output volume:</strong></li>
      <li><strong style="color: blue">How to compute the spatial size of the output volume?</strong></li>
      <li><strong style="color: blue">How can you ensure that the input &amp; output volume are the same?</strong></li>
      <li><strong style="color: blue">In the output volume, how do you compute the \(d\)-th depth slice:</strong></li>
    </ol>
  </li>
  <li><strong style="color: red">Calculate the number of parameters for the following config:</strong>
    <blockquote>
      <p>Given:<br />
     1. <strong>Input Volume</strong>:  \(64\times64\times3\)<br />
     1. <strong>Filters</strong>:  \(15 7\times7\)<br />
     1. <strong>Stride</strong>:  \(2\)<br />
     1. <strong>Pad</strong>:  \(3\)</p>
    </blockquote>
  </li>
  <li><strong style="color: red">Definitions:</strong>
    <ol>
      <li><strong style="color: blue">Receptive Field:</strong></li>
    </ol>
  </li>
  <li><strong style="color: red">Suppose the input volume has size  \([ 32 × 32 × 3 ]\)  and the receptive field (or the filter size) is  \(5 × 5\) , then each neuron in the Conv Layer will have weights to a <em>__Blank__</em> region in the input volume, for a total of  <em>__Blank__</em> weights:</strong></li>
  <li><strong style="color: red">How can we achieve the greatest reduction in the spatial dims of the network (for classification):</strong></li>
  <li><strong style="color: red">Pooling Layer:</strong>
    <ol>
      <li><strong style="color: blue">Define:</strong></li>
      <li><strong style="color: blue">List key ideas/properties and benefits:</strong></li>
      <li><strong style="color: blue">List the different types of Pooling:</strong><br />
 <a href="http://localhost:8889/work_files/research/dl/nlp/cnnsNnlp#bodyContents12">Answer</a></li>
      <li><strong style="color: blue">List variations of pooling and their definitions:</strong><br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">Show Questions</button>
        <ol hidden="">
          <li><strong style="color: blue">What is “Learned Pooling”:</strong></li>
          <li><strong style="color: blue">What is “Dynamical Pooling”:</strong></li>
        </ol>
      </li>
      <li><strong style="color: blue">List the hyperparams of Pooling Layer:</strong></li>
      <li><strong style="color: blue">How to calculate the size of the output volume:</strong></li>
      <li><strong style="color: blue">How many parameters does the pooling layer have:</strong></li>
      <li><strong style="color: blue">What are other ways to perform downsampling:</strong></li>
    </ol>
  </li>
  <li><strong style="color: red">Weight Priors:</strong>
    <ol>
      <li><strong style="color: blue">Define “Prior Prob Distribution on the parameters”:</strong></li>
      <li><strong style="color: blue">Define “Weight Prior” and its types/classes:</strong><br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">Show Questions</button>
        <ol hidden="">
          <li><strong style="color: blue">Weak Prior:</strong></li>
          <li><strong style="color: blue">Strong Prior:</strong></li>
          <li><strong style="color: blue">Infinitely Strong Prior:</strong></li>
        </ol>
      </li>
      <li><strong style="color: blue">Describe the Conv Layer as a FC Layer using priors:</strong></li>
      <li><strong style="color: blue">What are the key insights of using this view:</strong><br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">Show Questions</button>
        <ol hidden="">
          <li><strong style="color: blue">When is the prior imposed by convolution INAPPROPRIATE:</strong></li>
          <li><strong style="color: blue">What happens when the priors imposed by convolution and pooling are not suitable for the task?</strong></li>
          <li><strong style="color: blue">What kind of other models should Convolutional models be compared to? Why?:</strong></li>
        </ol>
      </li>
    </ol>
  </li>
  <li><strong style="color: red">When do multi-channel convolutions commute?</strong><br />
<a href="/work_files/research/dl/archits/convnets#bodyContents61">Answer</a></li>
  <li><strong style="color: red">Why do we use several different kernels in a given conv-layer?</strong></li>
  <li><strong style="color: red">Strided Convolutions</strong>
    <ol>
      <li><strong style="color: blue">Define:</strong></li>
      <li><strong style="color: blue">What are they used for?</strong></li>
      <li><strong style="color: blue">What are they equivalent to?</strong></li>
      <li><strong style="color: blue">Formula:</strong></li>
    </ol>
  </li>
  <li><strong style="color: red">Zero-Padding:</strong>
    <ol>
      <li><strong style="color: blue">Definition/Usage:</strong></li>
      <li><strong style="color: blue">List the types of padding:</strong></li>
    </ol>
  </li>
  <li><strong style="color: red">Locally Connected Layers/Unshared Convolutions:</strong></li>
  <li><strong style="color: red">Bias Parameter:</strong>
    <ol>
      <li><strong style="color: blue">How many bias terms are used per output channel in the tradional convolution:</strong></li>
    </ol>
  </li>
  <li><strong style="color: red">Dilated Convolutions</strong>
    <ol>
      <li><strong style="color: blue">Define:</strong></li>
      <li><strong style="color: blue">What are they used for?</strong></li>
    </ol>
  </li>
  <li><strong style="color: red">Stacked Convolutions</strong>
    <ol>
      <li><strong style="color: blue">Define:</strong></li>
      <li><strong style="color: blue">What are they used for?</strong></li>
    </ol>
  </li>
  <li><strong style="color: red">What is the rule of Bias(es) in CNNs:</strong><br />
 Separating the biases may slightly reduce the statistical efficiency of the model, but it allows the model to correct for differences in the image statistics at different locations. For example, when using implicit zero padding, detector units at the edge of the image receive less total input and may need larger biases.
    <ul>
      <li><a href="http://localhost:8889/work_files/research/dl/arcts">Archits</a></li>
    </ul>
  </li>
</ol>

<hr />

<h1 id="theory">Theory</h1>
<p hidden=""><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Theory</button></p>

<hr />

<h1 id="rnns">RNNs</h1>
<p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">RNNs</button></p>
<ol hidden="">
  <li><strong style="color: red">What is an RNN?</strong>
    <ol>
      <li><strong style="color: blue">Definition:</strong><br />
 <strong>Recurrent Neural Networks (RNNs)</strong> are a family of neural networks for processing <strong>sequential data</strong>.</li>
      <li><strong style="color: blue">What machine-type is the standard RNN? What is its function?</strong><br />
 The standard RNN is a <strong>nonlinear dynamical system</strong> that maps sequences to sequences.</li>
      <li><strong style="color: blue">Describe the connections in an RNN? Why does it matter?</strong><br />
 In an RNN, the connections between units form a <em>directed cycle</em>, allowing it to exhibit dynamic temporal behavior.</li>
    </ol>
  </li>
  <li><strong style="color: red">What is the big idea behind RNNs?</strong><br />
 RNNs <strong>share parameters across different <em>time-steps</em></strong> of the sequence, which allows it to <em>generalize well to examples of <strong>different sequence length</strong></em>.<br />
 Such sharing is particularly important when a specific piece of information can occur at multiple positions within the sequence.</li>
  <li><strong style="color: red">Dynamical Systems:</strong>
    <ol>
      <li><strong style="color: blue">Standard Form (equation and graph):</strong>
        <p>$$\boldsymbol{s}^{(t)}=f\left(\boldsymbol{s}^{(t-1)} ; \boldsymbol{\theta}\right) \tag{10.1}$$</p>
        <p>where \(\boldsymbol{s}^{(t)}\)  is called the state of the system.</p>
      </li>
      <li><strong style="color: blue">Standard Form with an external signal:</strong>
        <p>$$\boldsymbol{s}^{(t)}=f\left(\boldsymbol{s}^{(t-1)}, \boldsymbol{x}^{(t)} ; \boldsymbol{\theta}\right) \tag{10.4}$$</p>
        <p>where \(\boldsymbol{x}^{(t)}\) is an external signal, and the state now contains information about the whole past sequence.</p>
      </li>
      <li><strong style="color: blue">RNN as a Dynamical System (eq+graph):</strong>
        <p>$$\boldsymbol{h}^{(t)}=f\left(\boldsymbol{h}^{(t-1)}, \boldsymbol{x}^{(t)} ; \boldsymbol{\theta}\right) \tag{10.5}$$</p>
        <p>where the variable \(\mathbf{h}\) represents the <strong>state</strong>.<br />
 <img src="/main_files/dl/archits/rnns/2.png" alt="img" width="100%" /></p>
      </li>
      <li><strong style="color: blue">What type of functions can be considered RNNs? (what property)</strong><br />
 Basically, any function containing <strong>recurrence</strong> can be considered an RNN.</li>
    </ol>
  </li>
  <li><strong style="color: red">Unfolding Computational Graphs</strong>
    <ol>
      <li><strong style="color: blue">Definition:</strong><br />
 <strong>Unfolding</strong> maps the left to the right in the figure below (from <em>figure 10.2</em>) (both are computational graphs of a RNN without output \(\mathbf{o}\)):<br />
 <img src="/main_files/dl/archits/rnns/3.png" alt="img" width="100%" /><br />
 where the black square indicates that an interaction takes place with a delay of \(1\) time step, from the state at time \(t\)  to the state at time \(t + 1\).</li>
      <li><strong style="color: blue">List the Advantages introduced by unfolding and the benefits:</strong>
        <ol id="lst-p">
          <li>Regardless of the sequence length, the learned model always has the same input size.<br />
 Because it is specified in terms of transition from one state to another state, rather than specified in terms of a variable-length history of states.</li>
          <li>It is possible to use the <em>same</em> transition function \(f\) with the same parameters at every time step.<br />
 Thus, we can learn a single shared model \(f\) that operates on all time steps and all sequence lengths, rather than needing to learn a separate model \(g^{(t)}\) for all possible time steps;<br />
 <strong>Benefits:</strong></li>
        </ol>
        <ul>
          <li>Allows generalization to sequence lengths that did <em>not</em> appear in the training set</li>
          <li>Enables the model to be estimated to be estimated with far fewer training examples than would be required without parameter sharing.</li>
        </ul>
      </li>
      <li><strong style="color: blue">Graph and write the equations of Unfolding hidden recurrence:</strong>  <br />
 We can represent the unfolded recurrence after \(t\) steps with a function \(g^{(t)}\):
        <p>$$\begin{aligned} \boldsymbol{h}^{(t)} &amp;=g^{(t)}\left(\boldsymbol{x}^{(t)}, \boldsymbol{x}^{(t-1)}, \boldsymbol{x}^{(t-2)}, \ldots, \boldsymbol{x}^{(2)}, \boldsymbol{x}^{(1)}\right) \\ &amp;=f\left(\boldsymbol{h}^{(t-1)}, \boldsymbol{x}^{(t)} ; \boldsymbol{\theta}\right) \end{aligned}$$</p>
        <p>The function \(g^{(t)}\) takes the whole past sequence \(\left(\boldsymbol{x}^{(t)}, \boldsymbol{x}^{(t-1)}, \boldsymbol{x}^{(t-2)}, \ldots, \boldsymbol{x}^{(2)}, \boldsymbol{x}^{(1)}\right)\) as input and produces the current state, but the unfolded recurrent structure allows us to factorize \(g^{(t)}\) into <em>repeated applications of a function \(f\)</em>.</p>
      </li>
    </ol>
  </li>
  <li>
    <p><strong style="color: red">Describe the State of the RNN, its usage, and extreme cases of the usage:</strong><br />
 The network typically learns to use \(\mathbf{h}^{(t)}\) as a kind of <em>lossy summary</em> of the task-relevant aspects of the past sequence of inputs up to \(t\).<br />
 This summary is, in general, <em>necessarily lossy</em>, since it maps an arbitrary length sequence \(\left(\boldsymbol{x}^{(t)}, \boldsymbol{x}^{(t-1)}, \boldsymbol{x}^{(t-2)}, \ldots, \boldsymbol{x}^{(2)}, \boldsymbol{x}^{(1)}\right)\)  to a fixed length vector \(h^{(t)}\).</p>

    <p>The most demanding situation (the extreme) is when we ask \(h^{(t)}\) to be rich enough to allow one to approximately recover/reconstruct the input sequence, as in <strong>AutoEncoders</strong>.</p>
  </li>
  <li><strong style="color: red">RNN Architectures:</strong>
    <ol>
      <li><strong style="color: blue">List the three standard architectures of RNNs:</strong>
        <ol>
          <li><strong style="color: blue">Graph:</strong></li>
          <li><strong style="color: blue">Architecture:</strong></li>
          <li><strong style="color: blue">Equations:</strong></li>
          <li><strong style="color: blue">Total Loss:</strong></li>
          <li><strong style="color: blue">Complexity:</strong></li>
          <li><strong style="color: blue">Properties:</strong></li>
        </ol>
      </li>
    </ol>
  </li>
  <li><strong style="color: red">Teacher Forcing:</strong>
    <ol>
      <li><strong style="color: blue">Definition:</strong><br />
 Teacher forcing is a procedure that emerges from the maximum likelihood criterion, in which during training the model receives the ground truth output \(y^{(t)}\) as input at time \(t + 1\).</li>
      <li><strong style="color: blue">Motivation:</strong><br />
 Maximum Likelihood.</li>
      <li><strong style="color: blue">Application:</strong><br />
 Models that have recurrent connections from their <em>outputs</em> leading <em>back into the model</em> may be trained with teacher forcing.<br />
 Teacher forcing may still be applied to models that have hidden-to-hidden connections as long as they have connections from the output at one time step to values computed in the next time step. As soon as the hidden units become a function of earlier time steps, however, the BPTT algorithm is necessary. Some models may thus be trained with both teacher forcing and BPTT.</li>
      <li><strong style="color: blue">Disadvantages:</strong><br />
 The <strong>disadvantage</strong> of strict teacher forcing arises if the network is going to be later used in an <strong>closed-loop</strong> mode, with the network outputs (or samples from the output distribution) fed back as input. In this case, the fed-back inputs that the network sees during training could be quite different from the kind of inputs that it will see at test time.</li>
      <li><strong style="color: blue">Possible Solutions (Methods) for Mitigation:</strong>
        <ol>
          <li>Train with both teacher-forced inputs and free-running inputs, for example by predicting the correct target a number of steps in the future through the unfolded recurrent output-to-input paths[^3].</li>
          <li>Another approach (<em>Bengio et al., 2015b</em>) to mitigate the gap between the inputs seen at training time and the inputs seen at test time randomly chooses to use generated values or actual data values as input. This approach exploits a curriculum learning strategy to gradually use more of the generated values as input.</li>
        </ol>
      </li>
    </ol>
  </li>
</ol>

<hr />

<h1 id="optimization">Optimization</h1>
<p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Optimization</button></p>
<ol hidden="">
  <li><strong style="color: red">Define the <em>sigmoid</em> function and some of its properties:</strong><br />
 \(\sigma(-x) = 1 - \sigma(x)\)</li>
  <li><strong style="color: red">Backpropagation:</strong>
    <ol>
      <li><strong style="color: blue">Definition:</strong><br />
 Backpropagation algorithms are a family of methods used to efficiently train artificial neural networks (ANNs) following a gradient descent approach that exploits the chain rule.</li>
      <li><strong style="color: blue">Derive Gradient Descent Update:</strong><br />
 <a href="/work_files/research/dl/concepts/grad_opt#bodyContents22">Answer</a></li>
      <li><strong style="color: blue">Explain the difference kinds of gradient-descent optimization procedures:</strong>
        <ol>
          <li><strong>Batch Gradient Descent</strong> AKA <strong>Vanilla Gradient Descent</strong>, computes the gradient of the objective wrt. the parameters \(\theta\) for the entire dataset:
            <p>$$\theta=\theta-\epsilon \cdot \nabla_{\theta} J(\theta)$$</p>
          </li>
          <li><strong>SGD</strong> performs a parameter update for each data-point:
            <p>$$\theta=\theta-\epsilon \cdot \nabla_{\theta} J\left(\theta ; x^{(i)} ; y^{(i)}\right)$$</p>
          </li>
          <li><strong>Mini-batch Gradient Descent</strong> a hybrid approach that perform updates for a, pre-specified, mini-batch of \(n\) training examples:
            <p>$$\theta=\theta-\epsilon \cdot \nabla_{\theta} J\left(\theta ; x^{(i : i+n)} ; y^{(i : i+n)}\right)$$</p>
          </li>
        </ol>
      </li>
      <li><strong style="color: blue">List the different optimizers and their properties:</strong><br />
 <a href="/work_files/research/dl/concepts/grad_opt#content4">Answer</a></li>
    </ol>
  </li>
  <li><strong style="color: red">Error-Measures:</strong>
    <ol>
      <li><strong style="color: blue">Define what an error measure is:</strong><br />
 <strong>Error Measures</strong> aim to answer the question:<br />
 “What does it mean for \(h\) to approximate \(f\) (\(h \approx f\))?”<br />
 The <strong>Error Measure</strong>: \(E(h, f)\)<br />
 It is almost always defined point-wise: \(\mathrm{e}(h(\mathbf{X}), f(\mathbf{X}))\).</li>
      <li>
        <p><strong style="color: blue">List the 5 most common error measures and where they are used:</strong></p>
      </li>
      <li><strong style="color: blue">Specific Questions:</strong><br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">Show Questions</button>
        <ol hidden="">
          <li><strong style="color: blue">Derive MSE carefully:</strong></li>
          <li><strong style="color: blue">Derive the Binary Cross-Entropy Loss function:</strong><br />
 It is the log-likelihood of a Bernoulli probability model:
            <p>$$\begin{array}{c}{L(p)=p^{y}(1-p)^{1-y}} \\ {\log (L(p))=y \log p+(1-y) \log (1-p)}\end{array}$$</p>
          </li>
          <li><strong style="color: blue">Explain the difference between Cross-Entropy and MSE and which is better (for what task)?</strong></li>
          <li><strong style="color: blue">Describe the properties of the Hinge loss and why it is used?</strong>
            <ol>
              <li>Hinge loss upper bounds 0-1 loss</li>
              <li>It is the tightest <em>convex</em> upper bound on the 0/1 loss</li>
              <li>Minimizing 0-1 loss is NP-hard in the worst-case</li>
            </ol>
          </li>
        </ol>
      </li>
    </ol>
  </li>
  <li><strong style="color: red">Show that the weight vector of a linear signal is orthogonal to the decision boundary?</strong><br />
 The weight vector \(\mathbf{w}\) is orthogonal to the separating-plane/decision-boundary, defined by \(\mathbf{w}^T\mathbf{x} + b = 0\), in the \(\mathcal{X}\) space; Reason:<br />
 Since if you take any two points \(\mathbf{x}^\prime\) and \(\mathbf{x}^{\prime \prime}\) on the plane, and create the vector \(\left(\mathbf{x}^{\prime}-\mathbf{x}^{\prime \prime}\right)\)  parallel to the plane by subtracting the two points, then the following equations must hold:
    <p>$$\mathbf{w}^{\top} \mathbf{x}^{\prime}+b=0 \wedge \mathbf{w}^{\top} \mathbf{x}^{\prime \prime}+b=0 \implies \mathbf{w}^{\top}\left(\mathbf{x}^{\prime}-\mathbf{x}^{\prime \prime}\right)=0$$</p>
  </li>
  <li><strong style="color: red">What does it mean for a function to be <em>well-behaved</em> from an optimization pov?</strong><br />
 The <strong>well-behaved</strong> property from an optimization standpoint, implies that \(f''(x)\) doesn’t change too much or too rapidly, leading to a nearly quadratic function that is easy to optimize by gradient methods.</li>
  <li><strong style="color: red">Write \(\|\mathrm{Xw}-\mathrm{y}\|^{2}\) as a summation</strong>
    <p>$$\frac{1}{N} \sum_{n=1}^{N}\left(\mathbf{w}^{\mathrm{T}} \mathbf{x}_{n}-y_{n}\right)^{2} = \frac{1}{N}\|\mathrm{Xw}-\mathrm{y}\|^{2}$$</p>
  </li>
  <li><strong style="color: red">Compute:</strong>
    <ol>
      <li><strong style="color: blue">\(\dfrac{\partial}{\partial y}\vert{x-y}\vert=\)</strong><br />
 \(\dfrac{\partial}{\partial y} \vert{x-y}\vert  = - \text{sign}(x-y)\)</li>
    </ol>
  </li>
  <li><strong style="color: red">State the difference between SGD and GD?</strong><br />
 <strong>Gradient Descent</strong>’s cost-function iterates over ALL training samples.<br />
 <strong>Stochastic Gradient Descent</strong>’s cost-function only accounts for ONE training sample, chosen at random.</li>
  <li><strong style="color: red">When would you use GD over SDG, and vice-versa?</strong><br />
 GD theoretically minimizes the error function better than SGD. However, SGD converges much faster once the dataset becomes large.<br />
 That means GD is preferable for small datasets while SGD is preferable for larger ones.</li>
  <li><strong style="color: red">What is convex hull ?</strong><br />
 In case of linearly separable data, convex hull represents the outer boundaries of the two group of data points. Once convex hull is created, we get maximum margin hyperplane (MMH) as a perpendicular bisector between two convex hulls. MMH is the line which attempts to create greatest separation between two groups.</li>
  <li><strong style="color: red">OLS vs MLE</strong><br />
 They both estimate parameters in the model. They are the same in the case of normal distribution.</li>
</ol>

<hr />

<h1 id="ml-theory">ML Theory</h1>
<p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">ML Theory</button></p>
<ol hidden="">
  <li><strong style="color: red">Explain intuitively why Deep Learning works?</strong><br />
 <strong>Circuit Theory:</strong> There are function you can compute with a “small” L-layer deep NN that shallower networks require exponentially more hidden units to compute. (comes from looking at networks as logic gates).
    <ol>
      <li><strong>Example</strong>:<br />
 Computing \(x_1 \text{XOR} x_2 \text{XOR} ... \text{XOR} x_n\)  takes:
        <ol>
          <li>\(\mathcal{O}(log(n))\) in a tree representation.<br />
 <img src="/main_files/concepts/7.png" alt="img" width="65%" /></li>
          <li>\(\mathcal{O}(2^n)\) in a one-hidden-layer network because you need to exhaustively enumerate all possible \(2^N\) configurations of the input bits that result in the \(\text{XOR}\) being \({1, 0}\). <br />
 <img src="/main_files/concepts/8.png" alt="img" width="65%" /></li>
        </ol>
      </li>
    </ol>
  </li>
  <li><strong style="color: red">List the different types of Learning Tasks and their definitions:</strong>
    <ol>
      <li><strong>Multi-Task Learning</strong>: general term for training on multiple tasks
        <ol>
          <li><em>Joint Learning:</em> by choosing mini-batches from two different tasks simultaneously/alternately</li>
          <li><em>Pre-Training:</em> first train on one task, then train on another
            <blockquote>
              <p>widely used for <strong>word embeddings</strong></p>
            </blockquote>
          </li>
        </ol>
      </li>
      <li><strong>Transfer Learning</strong>:<br />
 a type of multi-task learning where we are focused on one task; by learning on another task then applying those models to our main task</li>
      <li><strong>Domain Adaptation</strong>:<br />
 a type of transfer learning, where the output is the same, but we want to handle different inputs/topics/genres</li>
      <li><strong>Zero-Shot Learning</strong>: is a form of extending supervised learning to a setting of solving for example a classification problem when not enough labeled examples are available for all classes.
        <blockquote>
          <p>“Zero-shot learning is being able to solve a task despite not having received any training examples of that task.” - Goodfellow</p>
        </blockquote>
      </li>
    </ol>

    <p><a href="/concepts_#bodyContents64">answer</a></p>
  </li>
  <li><strong style="color: red">Describe the relationship between supervised and unsupervised learning?</strong><br />
 <a href="/concepts_#bodyContents64">answer</a><br />
 Many ml algorithms can be used to perform both tasks. E.g., the chain rule of probability states that for a vector \(x \in \mathbb{R}^n\), the joint distribution can be decomposed as:<br />
 \(p(\mathbf{x})=\prod_{i=1}^{n} p\left(\mathrm{x}_{i} | \mathrm{x}_{1}, \ldots, \mathrm{x}_{i-1}\right)\)<br />
 which implies that we can solve the Unsupervised problem of modeling \(p(x)\) by splitting it into \(n\) supervised learning problems.<br />
 Alternatively, we can solve the supervised learning problem of learning \(p(y \vert x)\) by using traditional unsupervised learning technologies to learn the joint distribution \(p(x, y)\), then inferring:<br />
 \(p(y | \mathbf{x})=\frac{p(\mathbf{x}, y)}{\sum_{y} p\left(\mathbf{x}, y^{\prime}\right)}\)</li>
  <li><strong style="color: red">Describe the differences between Discriminative and Generative Models?</strong>
    <ol>
      <li><strong>Generative Model</strong>: learns the <strong>joint</strong> probability distribution, “probability of \(\boldsymbol{x}\) and \(y\)”:
        <p>$$P(\mathbf{x}, y)$$</p>
      </li>
      <li><strong>Discriminative Model</strong>: learns the <strong>conditional</strong> probability distribution, “probability of \(y\) given \(\boldsymbol{x}\)”:
        <p>$$P(y \vert \mathbf{x})$$</p>
      </li>
    </ol>
  </li>
  <li>
    <p><strong style="color: red">Describe the curse of dimensionality and its effects on problem solving:</strong><br />
 It is a phenomena where many machine learning problems become exceedingly difficult when the number of dimensions in the data is high.</p>

    <p>The number of possible distinct configurations of a set of variables increases exponentially as the number of variables increases \(\rightarrow\) <span style="color: goldenrod">Sparsity</span>:<br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">Capacity and Bias/Variance</button>
 <img src="/main_files/dl_book/2.png" alt="img" hidden="" /></p>
    <ol>
      <li><strong>Common Theme - Sparsity:</strong> When the dimensionality increases, the <em>volume of the space increases so fast</em> that the available <em>data become sparse</em>.</li>
      <li><strong>Sparsity and Statistical Significance:</strong> This <strong>sparsity</strong> is problematic for any method that requires <strong>statistical significance</strong>. In order to obtain a statistically sound and reliable result, the <em>amount of data needed</em> to support the result often <em>grows exponentially with the dimensionality</em>.</li>
      <li><strong>Sparsity and Clustering:</strong> Also, organizing and searching data often relies on detecting areas where objects form groups with similar properties (<em>clustering</em>); in high dimensional data, however, all objects appear to be sparse and dissimilar in many ways, which prevents common data organization strategies from being efficient.</li>
      <li><strong>Statistical Challenge:</strong> the number of possible configurations of \(x\) is much larger than the number of training examples</li>
      <li><strong>Stastical Sampling</strong>: The sampling density is proportional to \(N^{1/p}\), where \(p\) is the dimension of the input space and \(N\) is the sample size. Thus, if \(N_1 = 100\) represents a dense sample for a single input problem, then \(N_{10} = 100^{10}\) is the sample size required for the same sampling density with \(10\) inputs. Thus in high dimensions all feasible training samples sparsely populate the input space.</li>
      <li><a href="/concepts_#bodyContents621">Further Reading</a></li>
    </ol>
  </li>
  <li><strong style="color: red">How to deal with curse of dimensionality?</strong>
    <ol>
      <li>Feature Selection</li>
      <li>Feature Extraction</li>
    </ol>
  </li>
  <li><strong style="color: red">Describe how to initialize a NN and any concerns w/ reasons:</strong>
    <ol>
      <li>Don’t initialize the weights to Zero. The symmetry of hidden units results in a similar computation for each hidden unit, making all the rows of the weight matrix to be equal (by induction).</li>
      <li>It’s OK to initialize the bias term to zero.</li>
    </ol>
  </li>
  <li><strong style="color: red">Describe the difference between Learning and Optimization in ML:</strong>
    <ol>
      <li>The problem of Reducing the <strong>training error</strong> on the <strong>training set</strong> is one of <em><strong>optimization</strong></em>.</li>
      <li>The problem of Reducing the <strong>training error</strong>, as well as, the <strong>generalization (test) error</strong> is one of <em><strong>learning</strong></em>.</li>
    </ol>
  </li>
  <li><strong style="color: red">List the 12 Standard Tasks in ML:</strong><br />
 <a href="/work_files/research/dl/theory/dl_book_pt1#bodyContents12">Answer</a>
    <ol>
      <li>Clustering</li>
      <li>Forecasting</li>
      <li>Dimension reduction</li>
      <li>Sequence Labeling</li>
    </ol>
  </li>
  <li><strong style="color: red">What is the difference between inductive and deductive learning?</strong>
    <ol>
      <li><strong>Inductive learning</strong> is the process of using observations to draw conclusions
        <ol>
          <li>It is a method of reasoning in which the <strong>premises are viewed as supplying <span style="color: goldenrod">some</span> evidence</strong> for the truth of the conclusion.</li>
          <li>It goes from <span style="color: goldenrod">specific</span> to <span style="color: goldenrod">general</span> (<em>“bottom-up logic”</em>).</li>
          <li>The truth of the conclusion of an inductive argument may be <strong style="color: goldenrod">probable</strong>, based upon the evidence given.</li>
        </ol>
      </li>
      <li><strong>Deductive learning</strong> is the process of using conclusions to form observations.
        <ol>
          <li>It is the process of reasoning from one or more statements (premises) to reach a logically certain conclusion.</li>
          <li>It goes from <span style="color: goldenrod">general</span> to <span style="color: goldenrod">specific</span> (<em>“top-down logic”</em>).</li>
          <li>The conclusions reached (“observations”) are necessarily <strong>True</strong>.</li>
        </ol>
      </li>
    </ol>
  </li>
</ol>

<hr />

<h1 id="statistical-learning-theory">Statistical Learning Theory</h1>
<p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Statistical Learning Theory</button></p>
<ol hidden="">
  <li><strong style="color: red">Define Statistical Learning Theory:</strong><br />
 It is a framework for machine learning drawing from the fields of <strong>statistics</strong> and <strong>functional analysis</strong> that allows us, under certain assumptions, to study the question:
    <blockquote>
      <p><strong style="color: blue">How can we affect performance on the test set when we can only observe the training set?</strong></p>
    </blockquote>

    <p>It is a statistical approach to <strong>Computational Learning Theory</strong>.</p>
  </li>
  <li><strong style="color: red">What assumptions are made by the theory?</strong>
    <ol>
      <li>The training and test data are generated by an <em><strong>unknown</strong></em> <strong>data generating distribution</strong> (over the product space \(Z = X \times Y\), denoted: \(p_{\text{data}}(z) = p(x,y)\)) called the <strong>data-generating process</strong>.</li>
      <li>The <strong>i.i.d</strong> assumptions:
        <ol>
          <li>The data-points in each dataset are <strong>independent</strong> from each other</li>
          <li>The training and testing are both <strong>identically distributed</strong> (drawn from the same distribution)</li>
        </ol>
      </li>
    </ol>

    <p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Show</button></p>
    <ol>
      <li><strong style="color: blue">Define the i.i.d assumptions?</strong><br />
 A collection of random variables is <strong>independent and identically distributed</strong> if each random variable has the same probability distribution as the others and all are mutually independent.</li>
      <li><strong style="color: blue">Why assume a <em>joint</em> probability distribution \(p(x,y)\)?</strong><br />
 Note that the assumption of a joint probability distribution allows us to model uncertainty in predictions (e.g. from noise in data) because \({\displaystyle y}\) is not a deterministic function of \({\displaystyle x}\), but rather a random variable with conditional distribution \({\displaystyle P(y|x)}\) for a fixed \({\displaystyle x}\).</li>
      <li><strong style="color: red">Why do we need to model \(y\) as a target-distribution and not a target-function?</strong><br />
 The ‘Target Function’ is not always a function because two ‘identical’ input points can be mapped to two different outputs (i.e. they have different labels).</li>
    </ol>
  </li>
  <li><strong style="color: red">Give the Formal Definition of SLT:</strong><br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">Show</button>
    <ol hidden="">
      <li><strong style="color: blue">The Definitions:</strong>
        <ol>
          <li>\(X\): \(\:\) Input (vector) Space</li>
          <li>\(Y\): \(\:\) Output (vector) Space</li>
          <li>\(Z = X \times Y\): \(\:\) Product space of (input, output) pairs</li>
          <li>\(n\): \(\:\) number of data points</li>
          <li>\(S = \left\{\left(\vec{x}_{1}, y_{1}\right), \ldots,\left(\vec{x}_{n}, y_{n}\right)\right\}=\left\{\vec{z}_{1}, \ldots, \vec{z}_{n}\right\}\): \(\:\) the <strong>training set</strong></li>
          <li>\(\mathcal{H} = f : X \rightarrow Y\): \(\:\) the <strong>hypothesis space</strong> of all functions</li>
          <li>\(V(f(\vec{x}), y)\): \(\:\) an <strong>error/loss function</strong></li>
        </ol>
      </li>
      <li><strong style="color: blue">The Assumptions:</strong>
        <ol>
          <li>The training and testing sets are generated by an <em><strong>unknown</strong></em> <strong>data-generating distribution function</strong> (over \(Z\), denoted: \(p_{\text{data}} = p(z) = p(x,y)\)) called the <strong>data-generation process</strong>.</li>
          <li>The <strong>i.i.d assumptions:</strong>
            <ol>
              <li>The examples in each dataset are generated independently from each other</li>
              <li>Both datasets are identically distributed</li>
            </ol>
          </li>
        </ol>
      </li>
      <li><strong style="color: blue">The Inference Problem:</strong><br />
 Find a function \(f : X \rightarrow Y\) such that \(f(\vec{x}) \sim y\).</li>
      <li><strong style="color: blue">The Expected Risk:</strong><br />
 It is the overall average risk over the entire (data) probability-distribution:
        <p>$$I[f] = \mathbf{E}[V(f(\vec{x}), y)]=\int_{X \times Y} V(f(\vec{x}), y) p(\vec{x}, y) d \vec{x} d y$$</p>
      </li>
      <li><strong style="color: blue">The Target Function:</strong><br />
 It is the best possible function \(f\) that can be chosen, and is given by:
        <p>$$f = \inf_{h \in \mathcal{H}} I[h]$$</p>
      </li>
      <li><strong style="color: blue">The Empirical Risk:</strong><br />
 It is a <strong>proxy measure</strong> to the <em>expected risk</em> based on the training set. It is <em>necessary</em> since the probability distribution \(p(\vec{x}, y)\) is <em>unknown</em>.
        <p>$$I_{S}[f]=\frac{1}{n} \sum_{i=1}^{n} V\left(f\left(\vec{x}_{i}\right), y_{i}\right)$$</p>
      </li>
    </ol>
  </li>
  <li>
    <p><strong style="color: red">Define Empirical Risk Minimization:</strong><br />
 It is a (<em>learning</em>) principle in <em>statistical learning theory</em> that is based on <em>approximating</em> the <strong>Expected/True Risk (Generalization Error)</strong> by measuring the <strong>Empirical Risk (Training Error)</strong>; i.e. the performance on the training-data.</p>

    <p>A <strong>learning algorithm</strong> that chooses the function \(f_S\) which <em>minimizes the empirical risk</em> is called <strong>Empirical Risk Minimization:</strong></p>
    <p>$$R_{\text{emp}} = I_S[f] = \dfrac{1}{n} \sum_{i=1}^n V(f(\vec{x}_ i, y_i))$$</p>
    <p>$$f_{S} = \hat{h} = \arg \min _{h \in \mathcal{H}} R_{\mathrm{emp}}(h)$$</p>
  </li>
  <li><strong style="color: red">What is the Complexity of ERM?</strong><br />
 <strong>NP-Hard</strong> for <em><strong>classification</strong></em> with \(0-1\) loss function, even for linear classifiers
    <ol>
      <li>It can be solved <em>efficiently</em> when the minimal <em>empirical risk</em> is ZERO; i.e. the data is linearly separable</li>
    </ol>

    <p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Show</button></p>
    <ol hidden="">
      <li><strong style="color: blue">How do you Cope with the Complexity?</strong>
        <ol>
          <li>Employ a <strong>convex approximation</strong> to the \(0-1\) loss: <em>Hinge</em>, <em>SVM</em> etc.</li>
          <li>Imposing <strong>assumptions on the data-generating distribution</strong> thus, stop being an <strong>agnostic learning algorithm</strong></li>
        </ol>
      </li>
    </ol>
  </li>
  <li><strong style="color: red">Definitions:</strong>
    <ol>
      <li><strong style="color: blue">Generalization:</strong><br />
 The ability to do well on previously unobserved data:
        <p>$$I[f] \approx I_S[f]$$</p>
        <p><em>“Good” generalization</em> is achieved when the <em>empirical risk</em> approximates the <em>expected risk</em> <strong>well</strong>.</p>
      </li>
      <li>
        <p><strong style="color: blue">Generalization Error:</strong><br />
 AKA: <strong>Expected Risk</strong>, <strong>Out-of-sample Error</strong>, <strong>\(E_{\text{out}}\)</strong><br />
 It is a measure of how accurately an algorithm is able to predict outcome values for previously unseen data defined as the <strong>expected value of the error</strong> on a new input, measured w.r.t. the data-generating probability distribution.</p>
      </li>
      <li><strong style="color: blue">Generalization Gap:</strong><br />
 It is a measure of how accurately an algorithm is able to predict outcome values for previously unseen data, defined as the difference between the <strong>expected risk</strong> and <strong>empirical risk</strong>:
        <p>$$G =I\left[f_{n}\right]-I_{S}\left[f_{n}\right]$$</p>
        <ol>
          <li><strong style="color: blue">Computing the Generalization Gap:</strong><br />
 Since the <em>empirical risk/generalization error \(I[f_n]\)</em> cannot be computed for an <em><strong>unknown</strong></em> <strong>distribution</strong>, the generalization gap cannot be computed either.</li>
          <li><strong style="color: blue">What is the goal of SLT in the context of the Generalization Gap given that it can’t be computed?</strong><br />
 Instead, the goal of SLT is to bound/characterize the <em>gap</em> in <strong>probability:</strong>
            <p>$$P_{G}=P\left(I\left[f_{n}\right]-I_{S}\left[f_{n}\right] \leq \epsilon\right) \geq 1-\delta_{n}$$</p>
            <p>I.E. goal is to characterize the probability \(1- \delta_n\) that the generalization gap is <em>less</em> than some <strong>error bound</strong> \(\epsilon\) (known as the <strong>learning rate</strong>)</p>
          </li>
        </ol>
      </li>
      <li><strong style="color: blue">Achieving (“good”) Generalization:</strong><br />
 An <em>algorithm</em> is said to <strong>generalize</strong> when the <em><strong>expected risk</strong></em> is <strong>well approximated</strong> by the <em><strong>empirical risk</strong></em>:
        <p>$$I\left[f_{n}\right] \approx I_{S}\left[f_{n}\right]$$</p>
        <p>Equivalently:</p>
        <p>$$E_{\text {out}}(g) \approx E_{\text {in}}(g)$$</p>
        <p>I.E. when the <strong>generalization gap</strong> approaches <em>zero</em> in the limit of data-points:</p>
        <p>$$\lim _{n \rightarrow \infty} G_{n}=\lim _{n \rightarrow \infty} I\left[f_{n}\right]-I_{S}\left[f_{n}\right]=0$$</p>
      </li>
      <li><strong style="color: blue">Empirical Distribution:</strong><br />
 AKA: <strong>Data-Generating Distribution</strong><br />
 is the <strong>discrete, uniform, joint</strong> distribution \(p_{\text{data}} = p(x,y)\) over the sample points.</li>
    </ol>
  </li>
  <li><strong style="color: red">Describe the difference between Learning and Optimization in ML:</strong>
    <ol>
      <li><strong>Optimization</strong>: is concerned with the problem of <em><strong>reducing</strong></em> the <strong>training error</strong> on the <strong>training set</strong></li>
      <li><strong>Learning</strong>: is concerned with the problem of <em><strong>reducing</strong></em> the <strong>training error</strong>, as well as, the <strong>generalization (test) error</strong></li>
    </ol>
  </li>
  <li><strong style="color: red">Describe the difference between Generalization and Learning in ML:</strong>
    <ol>
      <li><strong>Generalization</strong> guarantee: tells us that it is likely that the following condition holds:
        <p>$$E_{\mathrm{out}}(\hat{h}) \approx E_{\mathrm{in}}(\hat{h})$$</p>
        <p>I.E. that the <em>empirical error</em> <strong>tracks/approximates</strong> the <em>expected/generalization error</em> <strong>well</strong>.</p>
      </li>
      <li><strong>Learning</strong>: corresponds to the condition that \(\hat{h} \approx f\) the <em><strong>chosen hypothesis approximates the target function well</strong></em>, which in-turn corresponds to the condition:
        <p>$$E_{\mathrm{out}}(\hat{h}) \approx 0$$</p>
      </li>
    </ol>
  </li>
  <li><strong style="color: red">How to achieve Learning?</strong>        <br />
 To achieve learning we need to achieve the condition \(E_{\mathrm{out}}(\hat{h}) \approx 0\), which we do by:
    <ol>
      <li>\(E_{\mathrm{out}}(\hat{h}) \approx E_{\mathrm{in}}(\hat{h})\)<br />
 A <strong>theoretical</strong> result achieved through <em><strong>Hoeffding Inequality</strong></em></li>
      <li>\(E_{\mathrm{in}}(\hat{h}) \approx 0\)<br />
 A <strong>practical</strong> result of <em><strong>Empirical Error Minimization</strong></em></li>
    </ol>
  </li>
  <li><strong style="color: red">What does the (VC) Learning Theory Achieve?</strong>
    <ol>
      <li>Characterizing the <strong>feasibility of learning</strong> for <em><strong>infinite hypotheses</strong></em></li>
      <li>Characterizing the <strong>Approximation-Generalization Tradeoff</strong></li>
    </ol>
  </li>
  <li><strong style="color: red">Why do we need the probabilistic framework?</strong></li>
  <li><strong style="color: red">What is the <em>Approximation-Generalization Tradeoff</em>:</strong><br />
 It is a tradeoff between (1) How well we can approximate the target function \(f \:\:\) and (2) How well we can generalize to unseen data.<br />
 Given the <strong>goal:</strong> Small <strong>\(E_{\text{out}}\)</strong>, good approximation of \(f\) <em><strong>out of sample</strong></em> (not in-sample).<br />
 The tradeoff is characterized by the <strong>complexity</strong> of the <strong>hypothesis space \(\mathcal{H}\)</strong>:
    <ol>
      <li><strong>More Complex \(\mathcal{H}\)</strong>: Better chance of approximating \(f\)</li>
      <li><strong>Less Complex \(\mathcal{H}\)</strong>: Better chance of generalizing out-of-sample</li>
    </ol>
  </li>
  <li><strong style="color: red">What are the factors determining how well an ML-algo will perform?</strong>
    <ol>
      <li><em>Ability to</em> Approximate \(f\) well, in-sample | Make the training error small  &amp;</li>
      <li>Decrease the gap between \(E_{\text{in}}\) and \(E_{\text{out}}\) | Make gap between training and test error small</li>
    </ol>
  </li>
  <li><strong style="color: red">Define the following and their usage/application &amp; how they relate to each other:</strong>
    <ol>
      <li><strong style="color: blue">Underfitting:</strong><br />
 Occurs when the model cannot fit the training data well; high \(E_{\text{in}}\).</li>
      <li><strong style="color: blue">Overfitting:</strong><br />
 Occurs when the gap between the training error and test error is too large.</li>
      <li><strong style="color: blue">Capacity:</strong><br />
 a models ability to fit a high variety of functions (complexity).<br />
 It allows us to control the amount of overfitting and underfitting
        <ol>
          <li>Models with <strong style="color: blue">Low-Capacity:</strong><br />
 Underfitting. High-Bias. Struggle to fit training-data.</li>
          <li>Models with <strong style="color: blue">High-Capacity:</strong><br />
 Overfitting. High-Variance. Memorizes noise.</li>
        </ol>
      </li>
      <li><strong style="color: blue">Hypothesis Space:</strong><br />
 The set of functions that the learning algorithm is allowed to select as being the target function.<br />
 Allows us to control the capacity of a model.</li>
      <li><strong style="color: red">VC-Dimension:</strong><br />
 The largest possible value of \(m\) for which there exists a training set of \(m\) different points that the classifier can label arbitrarily.<br />
 Quantifies a models capacity.
        <ol>
          <li><strong style="color: blue">What does it measure?</strong><br />
 Measures the <strong>capacity of a binary classifier</strong>.</li>
        </ol>
      </li>
      <li><strong style="color: blue">Graph the relation between Error, and Capacity in the ctxt of (Underfitting, Overfitting, Training Error, Generalization Err, and Generalization Gap):</strong><br />
 <img src="/main_files/dl_book/10.png" alt="img" width="70%" /></li>
    </ol>
  </li>
  <li><strong style="color: red">What is the most important result in SLT that show that learning is feasible?</strong><br />
 Shows that the discrepancy between training error and generalization error is bounded above by a quantity that grows as the model capacity grows but shrinks as the number of training examples increases.</li>
</ol>

<hr />

<h1 id="bias-variance-decomposition-theory">Bias-Variance Decomposition Theory</h1>
<p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Bias-Variance Decomposition Theory</button></p>
<ol hidden="">
  <li><strong style="color: red">What is the Bias-Variance Decomposition Theory:</strong><br />
 It is an approach for the quantification of the <strong>Approximation-Generalization Tradeoff</strong>.</li>
  <li><strong style="color: red">What are the Assumptions made by the theory?</strong>
    <ol>
      <li>Analysis is done over the <strong>entire data-distribution</strong></li>
      <li><strong>Real-Valued</strong> inputs, targets (can be extended)</li>
      <li>Target function \(f\) is <strong>known</strong></li>
      <li>Uses <strong>MSE</strong> (can be extended)</li>
    </ol>
  </li>
  <li><strong style="color: red">What is the question that the theory tries to answer? What assumption is important? How do you achieve the answer/goal?</strong>
    <ol>
      <li>“How can \(\mathcal{H}\) approximate \(f\) overall? not just on our sample/training-data.”.</li>
      <li>We assume that the target function \(f\) is known.</li>
      <li>By taking the <strong>expectation over all possible realization of \(N\) data-points</strong>.</li>
    </ol>
  </li>
  <li><strong style="color: red">What is the Bias-Variance Decomposition:</strong><br />
 It is the decomposition of the <strong>expected error</strong> as a sum of three concepts: <strong>bias</strong>, <strong>variance</strong> and <strong>irreducible error</strong>, each quantifying an aspect of the error.</li>
  <li><strong style="color: red">Define each term w.r.t. source of the error:</strong>
    <ol>
      <li><strong>Bias</strong>: the error from the erroneous/<strong>simplifying</strong> assumptions in the learning algorithm</li>
      <li><strong>Variance</strong>: the error from sensitivity to small fluctuations in the training set</li>
      <li><strong>Irreducible Error</strong>: the error resulting from the noise in the problem itself</li>
    </ol>
  </li>
  <li><strong style="color: red">What does each of the following measure (error in)? Describe this measured quantity in words, mathematically. Describe Bias&amp;Variance in Words as a question statement. Give their AKA in statistics.</strong>
    <ol>
      <li><strong style="color: blue">Bias:</strong>
        <ol>
          <li>AKA: <strong>Approximation Error</strong></li>
          <li>Measures the error in approximating the target function with the best possible hypothesis in \(\mathcal{H}\)</li>
          <li>The expected deviation from the true value of the function (or parameter)</li>
          <li>How well can \(\mathcal{H}\) approximate the target function \(f\)</li>
        </ol>
      </li>
      <li><strong style="color: blue">Variance:</strong>
        <ol>
          <li>AKA: <strong>Estimation Error</strong></li>
          <li>Measures the error in estimating the best possible hypothesis in \(\mathcal{H}\) with a particular hypothesis resulting from a specific training-set</li>
          <li>The deviation from the expected estimator value that any particular sampling of the data is likely to cause</li>
          <li>How well can we zoom in on a good \(h \in \mathcal{H}\)</li>
        </ol>
      </li>
      <li><strong style="color: blue">Irreducible Error:</strong> measures the inherent noise in the target \(y\)</li>
    </ol>
  </li>
  <li><strong style="color: red">Give the Formal Definition of the Decomposition (Formula):</strong><br />
 Given any hypothesis \(\hat{f} = g^{\mathcal{D}}\) we select, we can decompose its <strong>expected risk</strong> on an <em>unseen sample</em> \(x\) as:
    <p>$$\mathbb{E}\left[(y-\hat{f}(x))^{2}\right]=(\operatorname{Bias}[\hat{f}(x)])^{2}+\operatorname{Var}[\hat{f}(x)]+\sigma^{2}$$</p>
    <p>Where:</p>
    <ol>
      <li><strong>Bias</strong>:
        <p>$$\operatorname{Bias}[\hat{f}(x)] = \mathbb{E}[\hat{f}(x)] - f(x)$$</p>
      </li>
      <li><strong>Variance</strong>:
        <p>$$\operatorname{Var}[\hat{f}(x)] = \mathbb{E}\left[(\hat{f}(x))^2\right] - \mathbb{E}\left[\hat{f}(x)\right]^2$$</p>
      </li>
      <li><strong style="color: blue">What is the Expectation over?</strong><br />
 The expectation is over all different samplings of the data \(\mathcal{D}\)</li>
    </ol>
  </li>
  <li><strong style="color: red">Define the <em>Bias-Variance Tradeoff</em>:</strong><br />
 It is the property of a set of predictive models whereby, models with a <em><strong>lower bias</strong></em> have a <em><strong>higher variance</strong></em> and vice-versa.
    <ol>
      <li><strong style="color: blue">Effects of Bias:</strong>
        <ol>
          <li><strong>High Bias</strong>: simple models \(\rightarrow\) <strong>underfitting</strong></li>
          <li><strong>Low Bias</strong>: complex models \(\rightarrow\) <strong>overfitting</strong></li>
        </ol>
      </li>
      <li><strong style="color: blue">Effects of Variance:</strong>
        <ol>
          <li><strong>High Variance</strong>: complex models \(\rightarrow\) <strong>overfitting</strong></li>
          <li><strong>Low Variance</strong>: simple models \(\rightarrow\) <strong>underfitting</strong></li>
        </ol>
      </li>
      <li><strong style="color: blue">Draw the Graph of the Tradeoff (wrt model capacity):</strong><br />
 <img src="/main_files/dl_book/1.png" alt="img" width="60%" /></li>
    </ol>
  </li>
  <li><strong style="color: red">Derive the Bias-Variance Decomposition with explanations:</strong>
    <p>$${\displaystyle {\begin{aligned}\mathbb{E}_{\mathcal{D}} {\big [}I[g^{(\mathcal{D})}]{\big ]}&amp;=\mathbb{E}_{\mathcal{D}} {\big [}\mathbb{E}_{x}{\big [}(g^{(\mathcal{D})}-y)^{2}{\big ]}{\big ]}\\
 &amp;=\mathbb{E}_{x} {\big [}\mathbb{E}_{\mathcal{D}}{\big [}(g^{(\mathcal{D})}-y)^{2}{\big ]}{\big ]}\\
 &amp;=\mathbb{E}_{x}{\big [}\mathbb{E}_{\mathcal{D}}{\big [}(g^{(\mathcal{D})}- f -\varepsilon)^{2}{\big ]}{\big ]}
 \\&amp;=\mathbb{E}_{x}{\big [}\mathbb{E}_{\mathcal{D}} {\big [}(f+\varepsilon -g^{(\mathcal{D})}+\bar{g}-\bar{g})^{2}{\big ]}{\big ]}\\
 &amp;=\mathbb{E}_{x}{\big [}\mathbb{E}_{\mathcal{D}} {\big [}(\bar{g}-f)^{2}{\big ]}+\mathbb{E}_{\mathcal{D}} [\varepsilon ^{2}]+\mathbb{E}_{\mathcal{D}} {\big [}(g^{(\mathcal{D})} - \bar{g})^{2}{\big ]}+2\: \mathbb{E}_{\mathcal{D}} {\big [}(\bar{g}-f)\varepsilon {\big ]}+2\: \mathbb{E}_{\mathcal{D}} {\big [}\varepsilon (g^{(\mathcal{D})}-\bar{g}){\big ]}+2\: \mathbb{E}_{\mathcal{D}} {\big [}(g^{(\mathcal{D})} - \bar{g})(\bar{g}-f){\big ]}{\big ]}\\
 &amp;=\mathbb{E}_{x}{\big [}(\bar{g}-f)^{2}+\mathbb{E}_{\mathcal{D}} [\varepsilon ^{2}]+\mathbb{E}_{\mathcal{D}} {\big [}(g^{(\mathcal{D})} - \bar{g})^{2}{\big ]}+2(\bar{g}-f)\mathbb{E}_{\mathcal{D}} [\varepsilon ]\: +2\: \mathbb{E}_{\mathcal{D}} [\varepsilon ]\: \mathbb{E}_{\mathcal{D}} {\big [}g^{(\mathcal{D})}-\bar{g}{\big ]}+2\: \mathbb{E}_{\mathcal{D}} {\big [}g^{(\mathcal{D})}-\bar{g}{\big ]}(\bar{g}-f){\big ]}\\
 &amp;=\mathbb{E}_{x}{\big [}(\bar{g}-f)^{2}+\mathbb{E}_{\mathcal{D}} [\varepsilon ^{2}]+\mathbb{E}_{\mathcal{D}} {\big [}(g^{(\mathcal{D})} - \bar{g})^{2}{\big ]}{\big ]}\\
 &amp;=\mathbb{E}_{x}{\big [}(\bar{g}-f)^{2}+\operatorname {Var} [y]+\operatorname {Var} {\big [}g^{(\mathcal{D})}{\big ]}{\big ]}\\
 &amp;=\mathbb{E}_{x}{\big [}\operatorname {Bias} [g^{(\mathcal{D})}]^{2}+\operatorname {Var} [y]+\operatorname {Var} {\big [}g^{(\mathcal{D})}{\big ]}{\big ]}\\
 &amp;=\mathbb{E}_{x}{\big [}\operatorname {Bias} [g^{(\mathcal{D})}]^{2}+\sigma ^{2}+\operatorname {Var} {\big [}g^{(\mathcal{D})}{\big ]}{\big ]}\\\end{aligned}}}$$</p>
    <p>where:<br />
 \(\overline{g}(\mathbf{x})=\mathbb{E}_{\mathcal{D}}\left[g^{(\mathcal{D})}(\mathbf{x})\right]\) is the <strong>average hypothesis</strong> over all realization of \(N\) data-points \(\mathcal{D}_ i\), and \({\displaystyle \varepsilon }\) and \({\displaystyle {\hat {f}}} = g^{(\mathcal{D})}\) are <strong>independent</strong>.</p>
  </li>
  <li><strong style="color: red">What are the key Takeaways from the Tradeoff?</strong><br />
 Match the “Model Capacity/Complexity” to the “Data Resources”, NOT to the Target Complexity.</li>
  <li><strong style="color: red">What are the most common ways to negotiate the Tradeoff?</strong>
    <ol>
      <li>Cross-Validation</li>
      <li>MSE of the Estimates</li>
    </ol>
  </li>
  <li><strong style="color: red">How does the decomposition relate to Classification?</strong><br />
 A similar decomposition exists for:
    <ol>
      <li>Classification with \(0-1\) loss</li>
      <li>Probabilistic Classification with MSE</li>
    </ol>
  </li>
  <li><strong style="color: red">Increasing/Decreasing Bias&amp;Variance:</strong>
    <ol>
      <li><strong>Adding Good Feature</strong>:
        <ol>
          <li>Decrease Bias</li>
        </ol>
      </li>
      <li><strong>Adding Bad Feature</strong>:
        <ol>
          <li>No effect</li>
        </ol>
      </li>
      <li><strong>Adding ANY Feature</strong>:
        <ol>
          <li>Increase Variance</li>
        </ol>
      </li>
      <li><strong>Adding more Data</strong>:
        <ol>
          <li>Decrease Variance</li>
          <li>May decrease Bias (if \(f \in \mathcal{H}\) | \(h\) can fit \(f\) exactly)</li>
        </ol>
      </li>
      <li><strong>Noise in Test Set</strong>:
        <ol>
          <li>Affects Only Irreducible Err</li>
        </ol>
      </li>
      <li><strong>Noise in Training Set</strong>:
        <ol>
          <li>Affects Bias and Variance</li>
        </ol>
      </li>
      <li><strong>Dimensionality Reduction</strong>:
        <ol>
          <li>Decrease Variance</li>
        </ol>
      </li>
      <li><strong>Feature Selection</strong>:
        <ol>
          <li>Decrease Variance</li>
        </ol>
      </li>
      <li><strong>Regularization</strong>:
        <ol>
          <li>Increase Bias</li>
          <li>Decrease Variance</li>
        </ol>
      </li>
      <li><strong>Increasing # of Hidden Units in ANNs</strong>:
        <ol>
          <li>Decrease Bias</li>
          <li>Increase Variance</li>
        </ol>
      </li>
      <li><strong>Increasing # of Hidden Layers in ANNs</strong>:
        <ol>
          <li>Decrease Bias</li>
          <li>Increase Variance</li>
        </ol>
      </li>
      <li><strong>Increasing \(k\) in K-NN</strong>:
        <ol>
          <li>Increase Bias</li>
          <li>Decrease Variance</li>
        </ol>
      </li>
      <li><strong>Increasing Depth in Decision-Trees</strong>:
        <ol>
          <li>Increase Variance</li>
        </ol>
      </li>
      <li><strong>Boosting</strong>:
        <ol>
          <li>Decrease Bias</li>
        </ol>
      </li>
      <li><strong>Bagging</strong>:
        <ol>
          <li>Decrease Variance</li>
        </ol>
      </li>
    </ol>
  </li>
</ol>

<hr />

<h1 id="activation-functions">Activation Functions</h1>
<p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Activation Functions</button></p>
<ol hidden="">
  <li><strong style="color: red">Describe the Desirable Properties for activation functions:</strong>
    <ol>
      <li><strong>Non-Linearity</strong>:<br />
 When the activation function is non-linear, then a two-layer neural network can be proven to be a universal function approximator. The identity activation function does not satisfy this property. When multiple layers use the identity activation function, the entire network is equivalent to a single-layer model.</li>
      <li><strong>Range</strong>:<br />
 When the range of the activation function is finite, gradient-based training methods tend to be more stable, because pattern presentations significantly affect only limited weights. When the range is infinite, training is generally more efficient because pattern presentations significantly affect most of the weights. In the latter case, smaller learning rates are typically necessary.</li>
      <li><strong>Continuously Differentiable</strong>:<br />
 This property is desirable for enabling gradient-based optimization methods. The binary step activation function is not differentiable at 0, and it differentiates to 0 for all other values, so gradient-based methods can make no progress with it.</li>
      <li><strong>Monotonicity</strong>:<br />
 When the activation function is monotonic, the error surface associated with a single-layer model is guaranteed to be convex.</li>
      <li><strong>Smoothness with Monotonic Derivatives</strong>:<br />
 These have been shown to generalize better in some cases.</li>
      <li><strong>Approximating Identity near Origin</strong>:<br />
 Equivalent to \({\displaystyle f(0)=0}\) and \({\displaystyle f'(0)=1}\), and \({\displaystyle f'}\) is continuous at \(0\).<br />
 When activation functions have this property, the neural network will learn efficiently when its weights are initialized with small random values. When the activation function does not approximate identity near the origin, special care must be used when initializing the weights.</li>
      <li><strong>Zero-Centered Range</strong>:<br />
 Has effects of centering the data (zero mean) by centering the activations. Makes learning easier.</li>
    </ol>

    <p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Explain the specifics of the desirability of each of the following</button></p>
    <ol hidden="">
      <li><strong style="color: blue">Non-Linearity:</strong></li>
      <li><strong style="color: blue">Range:</strong></li>
      <li><strong style="color: blue">Continuously Differentiable:</strong></li>
      <li><strong style="color: blue">Monotonicity:</strong></li>
      <li><strong style="color: blue">Smoothness with Monotonic Derivatives:</strong></li>
      <li><strong style="color: blue">Approximating Identity near Origin:</strong></li>
      <li><strong style="color: blue">Zero-Centered Range:</strong></li>
    </ol>
  </li>
  <li><strong style="color: red">Describe the NON-Desirable Properties for activation functions:</strong>
    <ol>
      <li><strong>Saturation</strong>:<br />
 An activation functions output, with finite range, may saturate near its tail or head (e.g. \(\{0, 1\}\) for sigmoid). This leads to a problem called <strong>vanishing gradient</strong>.</li>
      <li><strong>Vanishing Gradients</strong>:<br />
 Happens when the gradient of an activation function is very small/zero. This usually happens when the activation function <strong>saturates</strong> at either of its tails.<br />
 The chain-rule will <em><strong>multiply</strong></em> the local gradient (of activation function) with the whole objective. Thus, when gradient is small/zero, it will “kill” the gradient \(\rightarrow\) no signal will flow through the neuron to its weights or to its data.<br />
 <strong>Slows/Stops learning completely</strong>.</li>
      <li><strong>Range Not Zero-Centered</strong>:<br />
 This is undesirable since neurons in later layers of processing in a Neural Network would be receiving data that is not zero-centered. This has implications on the dynamics during gradient descent, because if the data coming into a neuron is always positive (e.g. \(x&gt;0\) elementwise in \(f=w^Tx+b\)), then the gradient on the weights \(w\) will during backpropagation become either all be positive, or all negative (depending on the gradient of the whole expression \(f\)). This could introduce undesirable zig-zagging dynamics in the gradient updates for the weights. However, notice that once these gradients are added up across a batch of data the final update for the weights can have variable signs, somewhat mitigating this issue. Therefore, this is an inconvenience but it has less severe consequences compared to the saturated activation problem above.<br />
 <strong>Makes optimization harder.</strong></li>
    </ol>

    <p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Explain the specifics of the non-desirability of each of the following</button></p>
    <ol hidden="">
      <li><strong style="color: blue">Saturation:</strong></li>
      <li><strong style="color: blue">Vanishing Gradients:</strong></li>
      <li><strong style="color: blue">Range Not Zero-Centered:</strong></li>
    </ol>
  </li>
  <li><strong style="color: red">List the different activation functions used in ML?</strong><br />
 Identity, Sigmoid, Tanh, ReLU, L-ReLU, ELU, SoftPlus<br />
 <img src="/main_files/concepts/16.png" alt="img" max-width="150%" /><br />
 <strong style="color: blue">Names, Definitions, Properties (pros&amp;cons), Derivatives, Applications, pros/cons:</strong>
    <ol>
      <li><strong>Sigmoid</strong>:
        <p>$$S(z)=\frac{1}{1+e^{-z}} \\ S^{\prime}(z)=S(z) \cdot(1-S(z))$$</p>
        <p><img src="/main_files/concepts/3.png" alt="img" width="68%" class="center-image" /></p>
        <ol>
          <li><strong>Properties</strong>:<br />
 Never use as activation, use as an output unit for binary classification.
            <ol>
              <li><strong>Pros</strong>:
                <ol>
                  <li>Has a nice interpretation as the firing rate of a neuron</li>
                </ol>
              </li>
              <li><strong>Cons</strong>:
                <ol>
                  <li>They Saturate and kill gradients \(\rightarrow\) Gives rise to <strong>vanishing gradients</strong> \(\rightarrow\) Stop Learning
                    <ol>
                      <li>Happens when initialization weights are too large</li>
                      <li>or sloppy with data preprocessing</li>
                      <li>Neurons Activation saturates at either tail of \(0\) or \(1\)</li>
                    </ol>
                  </li>
                  <li>Output NOT <strong>Zero-Centered</strong> \(\rightarrow\) Gradient updates go too far in different directions \(\rightarrow\) makes optimization harder</li>
                  <li>The local gradient \((z * (1-z))\) achieves maximum at \(0.25\), when \(z = 0.5\). \(\rightarrow\) very time the gradient signal flows through a sigmoid gate, its magnitude always diminishes by one quarter (or more) \(\rightarrow\) with basic SGD, the lower layers of a network train much slower than the higher one</li>
                </ol>
              </li>
            </ol>
          </li>
        </ol>
      </li>
      <li><strong>Tanh</strong>:
        <p>$$\tanh (z)=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}} \\ \tanh ^{\prime}(z)=1-\tanh (z)^{2}$$</p>
        <p><img src="/main_files/concepts/4.png" alt="img" width="68%" class="center-image" /><br />
 <strong>Properties:</strong><br />
 Strictly superior to Sigmoid (scaled version of sigmoid | stronger gradient). Good for activation.</p>
        <ol>
          <li><strong>Pros</strong>:
            <ol>
              <li>Zero Mean/Centered</li>
            </ol>
          </li>
          <li><strong>Cons</strong>:
            <ol>
              <li>They Saturate and kill gradients \(\rightarrow\) Gives rise to <strong>vanishing gradients</strong> \(\rightarrow\) Stop Learning</li>
            </ol>
          </li>
        </ol>
      </li>
      <li><strong>Relu</strong>:
        <p>$$R(z)=\left\{\begin{array}{cc}{z} &amp; {z&gt;0} \\ {0} &amp; {z&lt;=0}\end{array}\right\} \\  R^{\prime}(z)=\left\{\begin{array}{ll}{1} &amp; {z&gt;0} \\ {0} &amp; {z&lt;0}\end{array}\right\}$$</p>
        <p><img src="/main_files/concepts/5.png" alt="img" width="68%" class="center-image" /><br />
 <strong>Properties:</strong><br />
 The best for activation (Better gradients).</p>
        <ol>
          <li><strong>Pros</strong>:
            <ol>
              <li>Non-saturation of gradients which <em>accelerates convergence</em> of SGD</li>
              <li>Sparsity effects and induced regularization. <a href="https://stats.stackexchange.com/questions/176794/how-does-rectilinear-activation-function-solve-the-vanishing-gradient-problem-in/176905#176905">discussion</a></li>
              <li>Not computationally expensive</li>
            </ol>
          </li>
          <li><strong>Cons</strong>:
            <ol>
              <li><strong>ReLU not zero-centered problem</strong>:<br />
 The problem that ReLU is not zero-centered can be solved/mitigated by using <strong>batch normalization</strong>, which normalizes the signal before activation:
                <blockquote>
                  <p>From paper: We add the BN transform immediately before the nonlinearity, by normalizing \(x =  Wu + b\); normalizing it is likely to produce activations with a stable distribution.</p>
                  <ul>
                    <li><a href="https://www.youtube.com/watch?v=FDCfw-YqWTE&amp;list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&amp;index=10&amp;t=0s">WHY NORMALIZING THE SIGNAL IS IMPORTANT</a></li>
                  </ul>
                </blockquote>
              </li>
              <li><strong>Dying ReLUs (Dead Neurons):</strong><br />
 If a neuron gets clamped to zero in the forward pass (it doesn’t “fire” / \(x&lt;0\)), then its weights will get zero gradient. Thus, if a ReLU neuron is unfortunately initialized such that it never fires, or if a neuron’s weights ever get knocked off with a large update during training into this regime (usually as a symptom of aggressive learning rates), then this neuron will remain permanently dead.
                <ol>
                  <li><a href="https://www.youtube.com/embed/gYpoJMlgyXA?start=1249" value="show" onclick="iframePopA(event)"><strong>cs231n Explanation</strong></a>
 <a href="https://www.youtube.com/embed/gYpoJMlgyXA?start=1249"></a>
                    <div></div>
                  </li>
                </ol>
              </li>
              <li><strong>Infinite Range</strong>:<br />
 Can blow up the activation.</li>
            </ol>
          </li>
        </ol>
      </li>
      <li><strong>Leaky Relu</strong>:
        <p>$$R(z)=\left\{\begin{array}{cc}{z} &amp; {z&gt;0} \\ {\alpha z} &amp; {z&lt;=0}\end{array}\right\} \\ 
 R^{\prime}(z)=\left\{\begin{array}{ll}{1} &amp; {z&gt;0} \\ {\alpha} &amp; {z&lt;0}\end{array}\right\}$$</p>
        <p><img src="/main_files/concepts/6.png" alt="img" width="68%" class="center-image" /><br />
 <strong>Properties:</strong><br />
 Sometimes useful. Worth trying.</p>
        <ol>
          <li><strong>Pros</strong>:
            <ol>
              <li>Leaky ReLUs are one attempt to fix the “dying ReLU” problem by having a small negative slope (of 0.01, or so).</li>
            </ol>
          </li>
          <li><strong>Cons</strong>:<br />
 The consistency of the benefit across tasks is presently unclear.</li>
        </ol>
      </li>
    </ol>
  </li>
</ol>
<p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Show Questions</button></p>
<ol hidden="">
  <li><strong style="color: red">Fill in the following table:</strong><br />
 <img src="/main_files/dl/concepts/act_funcs/0.png" alt="img" width="100%" /></li>
  <li><strong style="color: red">Tanh VS Sigmoid for activation?</strong><br />
 Tanh &gt; Sigmoid</li>
  <li><strong style="color: red">ReLU:</strong>
    <p>$$R(z)=\left\{\begin{array}{cc}{z} &amp; {z&gt;0} \\ {0} &amp; {z&lt;=0}\end{array}\right\} \\  R^{\prime}(z)=\left\{\begin{array}{ll}{1} &amp; {z&gt;0} \\ {0} &amp; {z&lt;0}\end{array}\right\}$$</p>
    <p><img src="/main_files/concepts/5.png" alt="img" width="68%" class="center-image" /><br />
 <strong>Properties:</strong><br />
 The best for activation (Better gradients).</p>
    <ol>
      <li><strong>Pros</strong>:
        <ol>
          <li>Non-saturation of gradients which <em>accelerates convergence</em> of SGD</li>
          <li>Sparsity effects and induced regularization. <a href="https://stats.stackexchange.com/questions/176794/how-does-rectilinear-activation-function-solve-the-vanishing-gradient-problem-in/176905#176905">discussion</a></li>
          <li>Not computationally expensive</li>
        </ol>
      </li>
      <li><strong>Cons</strong>:
        <ol>
          <li><strong>ReLU not zero-centered problem</strong>:<br />
 The problem that ReLU is not zero-centered can be solved/mitigated by using <strong>batch normalization</strong>, which normalizes the signal before activation:
            <blockquote>
              <p>From paper: We add the BN transform immediately before the nonlinearity, by normalizing \(x =  Wu + b\); normalizing it is likely to produce activations with a stable distribution.</p>
              <ul>
                <li><a href="https://www.youtube.com/watch?v=FDCfw-YqWTE&amp;list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&amp;index=10&amp;t=0s">WHY NORMALIZING THE SIGNAL IS IMPORTANT</a></li>
              </ul>
            </blockquote>
          </li>
          <li><strong>Dying ReLUs (Dead Neurons):</strong><br />
 If a neuron gets clamped to zero in the forward pass (it doesn’t “fire” / \(x&lt;0\)), then its weights will get zero gradient. Thus, if a ReLU neuron is unfortunately initialized such that it never fires, or if a neuron’s weights ever get knocked off with a large update during training into this regime (usually as a symptom of aggressive learning rates), then this neuron will remain permanently dead.
            <ol>
              <li><a href="https://www.youtube.com/embed/gYpoJMlgyXA?start=1249" value="show" onclick="iframePopA(event)"><strong>cs231n Explanation</strong></a>
 <a href="https://www.youtube.com/embed/gYpoJMlgyXA?start=1249"></a>
                <div></div>
              </li>
            </ol>
          </li>
          <li><strong>Infinite Range</strong>:<br />
 Can blow up the activation.</li>
        </ol>
      </li>
      <li><strong style="color: blue">What makes it superior/advantageous?</strong><br />
 Larger Derivatives.</li>
      <li><strong style="color: blue">What problems does it have?</strong>
        <ol>
          <li><strong style="color: blue">What solution do we have to mitigate the problem?</strong><br />
 Batch Normalization.</li>
        </ol>
      </li>
    </ol>
  </li>
  <li><strong style="color: red">Compute the derivatives of all activation functions:</strong><br />
 <img src="/main_files/concepts/16.png" alt="img" max-width="150%" /></li>
  <li><strong style="color: red">Graph all activation functions and their derivatives:</strong><br />
 <img src="/main_files/concepts/16.png" alt="img" max-width="150%" /></li>
</ol>

<hr />

<h1 id="kernels">Kernels</h1>
<p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Kernels</button></p>
<ol hidden="">
  <li><strong style="color: red">Define “Local Kernel” and give an analogy to describe it:</strong></li>
  <li><strong style="color: red">Write the following kernels:</strong>
    <ol>
      <li><strong style="color: blue">Polynomial Kernel of degree, up to, \(d\):</strong></li>
      <li><strong style="color: blue">Gaussian Kernel:</strong></li>
      <li><strong style="color: blue">Sigmoid Kernel:</strong></li>
      <li><strong style="color: blue">Polynomial Kernel of degree, exactly, \(d\):</strong></li>
    </ol>
  </li>
</ol>

<hr />

<h1 id="math">Math</h1>
<p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Math</button></p>
<ol hidden="">
  <li><strong style="color: red">What is a metric?</strong><br />
 A <strong>Metric (distance function)</strong> \(d\)  is a function that defines a distance between each pair of elements of a set \(X\).<br />
 A Metric induces a <em>topology</em> on a set; BUT, not all topologies can be generated by a metric.<br />
 Mathematically, it is a function:<br />
 \({\displaystyle d:X\times X\to [0,\infty )},\)<br />
 that must satisfy the following properties:
    <ol>
      <li>\({\displaystyle d(x,y)\geq 0}\) \(\:\:\:\:\:\:\:\)   non-negativity or separation axiom</li>
      <li>\({\displaystyle d(x,y)=0\Leftrightarrow x=y}\) \(\:\:\:\:\:\:\:\)  identity of indiscernibles</li>
      <li>\({\displaystyle d(x,y)=d(y,x)}\) \(\:\:\:\:\:\:\:\)  symmetry</li>
      <li>\({\displaystyle d(x,z)\leq d(x,y)+d(y,z)}\) \(\:\:\:\:\:\:\:\)  subadditivity or triangle inequality
        <blockquote>
          <p>The first condition is implied by the others.<br />
 <a href="http://localhost:8889/concepts_#bodyContents31">Metric</a></p>
        </blockquote>
      </li>
    </ol>
  </li>
  <li><strong style="color: red">Describe Binary Relations and their Properties?</strong><br />
 A <strong>binary relation</strong> on a set \(A\) is a set of ordered pairs of elements of \(A\). In other words, it is a subset of the Cartesian product \(A^2 = A ×A\).<br />
 The number of binary relations on a set of \(N\) elements is \(= 2^{N^2}\)
 <strong>Examples:</strong>
    <ol>
      <li>“is greater than”</li>
      <li>“is equal to”</li>
      <li>A function \(f(x)\)<br />
 <strong>Properties:</strong>  (for a relation \(R\) and set \(X\))</li>
      <li><em>Reflexive:</em> for all \(x\) in \(X\) it holds that \(xRx\)</li>
      <li><em>Symmetric:</em> for all \(x\) and \(y\) in \(X\) it holds that if \(xRy\) then \(yRx\)</li>
      <li><em>Transitive:</em> for all \(x\), \(y\) and \(z\) in \(X\) it holds that if \(xRy\) and \(yRz\) then \(xRz\)<br />
 <a href="/concepts_#bodyContents32">answer</a></li>
    </ol>
  </li>
  <li><strong style="color: red">Formulas:</strong>
    <ol>
      <li><strong style="color: blue">Set theory:</strong>
        <ol>
          <li><strong style="color: blue">Number of subsets of a set of \(N\) elements:</strong><br />
 \(= 2^N\)</li>
          <li><strong style="color: blue">Number of pairs \((a,b)\) of a set of N elements:</strong><br />
 \(= N^2\)</li>
        </ol>
      </li>
      <li><strong style="color: blue">Binomial Theorem:</strong>
        <p>$$(x+y)^{n}=\sum_{k=0}^{n}{n \choose k}x^{n-k}y^{k}=\sum_{k=0}^{n}{n \choose k}x^{k}y^{n-k} \\={n \choose 0}x^{n}y^{0}+{n \choose 1}x^{n-1}y^{1}+{n \choose 2}x^{n-2}y^{2}+\cdots +{n \choose n-1}x^{1}y^{n-1}+{n \choose n}x^{0}y^{n},$$</p>
      </li>
      <li><strong style="color: blue">Binomial Coefficient:</strong>
        <p>$${\binom {n}{k}}={\frac {n!}{k!(n-k)!}} = N \text{choose} k = N \text{choose} (n-k)$$</p>
      </li>
      <li><strong style="color: blue">Expansion of \(x^n - y^n =\)</strong>
        <p>$$x^n - y^n = (x-y)(x^{n-1} + x^{n-2} y + ... + x y^{n-2} + y^{n-1})$$</p>
      </li>
      <li><strong style="color: blue">Number of ways to partition \(N\) data points into \(k\) clusters:</strong>
        <ol>
          <li><strong>Number of pairs (e.g. \((a,b)\)) of a set of \(N\) elements</strong> \(= N^2\)</li>
          <li>There are at most \(k^N\) ways to partition \(N\) data points into \(k\) clusters - there are \(N\) choose \(k\) clusters, precisely</li>
        </ol>
      </li>
      <li><strong style="color: blue">\(\log_x(y) =\)</strong>
        <p>$$\log_x(y) = \dfrac{\ln(y)}{\ln(x)}$$</p>
      </li>
      <li><strong style="color: blue">The length of a vector \(\mathbf{x}\)  along a direction (projection):</strong>
        <ol>
          <li>Along a unit-length vector \(\hat{\mathbf{w}}\):<br />
 \(\text{comp}_ {\hat{\mathbf{w}}}(\mathbf{x}) = \hat{\mathbf{w}}^T\mathbf{x}\)</li>
          <li>Along an unnormalized vector \(\mathbf{w}\):<br />
 \(\text{comp}_ {\mathbf{w}}(\mathbf{x}) = \dfrac{1}{\|\mathbf{w}\|} \mathbf{w}^T\mathbf{x}\)</li>
        </ol>
      </li>
      <li><strong style="color: blue">\(\sum_{i=1}^{n} 2^{i}=\)</strong><br />
 \(\sum_{i=1}^{n} 2^{i}=2^{n+1}-2\)</li>
    </ol>
  </li>
  <li><strong style="color: red">List 6 proof methods:</strong>
    <ol>
      <li>Direct Proof</li>
      <li>Mathematical Induction
        <ol>
          <li>Strong Induction</li>
          <li>Infinite Descent</li>
        </ol>
      </li>
      <li>Contradiction</li>
      <li>Contraposition (\((p \implies q) \iff (!q \implies !p)\))</li>
      <li>Construction</li>
      <li>Combinatorial</li>
      <li>Exhaustion</li>
      <li>Non-Constructive proof (existence proofs)
<a href="/concepts_#bodyContents34">answer</a></li>
    </ol>
  </li>
  <li><strong style="color: red">Important Formulas</strong>
    <ol>
      <li><strong style="color: blue">Projection \(\tilde{\mathbf{x}}\) of a vector \(\mathbf{x}\) onto another vector \(\mathbf{u}\):</strong>
        <p>$$\tilde{\mathbf{x}} = \mathbf{x}\cdot \dfrac{\mathbf{u}}{\|\mathbf{u}\|_2} \mathbf{u}$$</p>
      </li>
    </ol>
  </li>
</ol>

<hr />

<h1 id="statistics">Statistics</h1>
<p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Statistics</button></p>
<ol hidden="">
  <li><strong style="color: red">ROC curve:</strong>
    <ol>
      <li><strong style="color: blue">Definition:</strong><br />
 A <strong>receiver operating characteristic curve</strong>, or <strong>ROC curve</strong>, is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied.</li>
      <li><strong style="color: blue">Purpose:</strong><br />
 A way to quantify how good a <strong>binary classifier</strong> separates two classes.</li>
      <li><strong style="color: blue">How do you create the plot?</strong><br />
 The ROC curve is created by plotting the <strong>true positive rate (TPR)</strong> against the <strong>false positive rate (FPR)</strong> at various threshold settings.<br />
 - \(\mathrm{TPR}=\frac{\mathrm{TP}}{\mathrm{P}}=\frac{\mathrm{TP}}{\mathrm{TP}+\mathrm{FN}}\)<br />
 - \(\mathrm{FPR}=\frac{\mathrm{FP}}{\mathrm{N}}=\frac{\mathrm{FP}}{\mathrm{FP}+\mathrm{TN}}\)</li>
      <li><strong style="color: blue">How to identify a good classifier:</strong><br />
 A Good classifier has a ROC curve that is near the top-left diagonal (hugging it).</li>
      <li><strong style="color: blue">How to identify a bad classifier:</strong><br />
 A Bad Classifier has a ROC curve that is close to the diagonal line.</li>
      <li><strong style="color: blue">What is its application in tuning the model?</strong><br />
 It allows you to set the <strong>classification threshold</strong>:
        <ol>
          <li>You can minimize False-positive rate or maximize the True-Positive Rate</li>
        </ol>
      </li>
    </ol>
  </li>
  <li><strong style="color: red">AUC - AUROC:</strong>
    <ol>
      <li><strong style="color: blue">Definition:</strong><br />
 When using normalized units, the area under the curve (often referred to as simply the AUC) is equal to the probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one (assuming ‘positive’ ranks higher than ‘negative’).</li>
      <li><strong style="color: blue">Range:</strong><br />
 Range \(= 0.5 - 1.0\), from poor to perfect, with an uninformative classifier yielding \(0.5\)</li>
      <li><strong style="color: blue">What does it measure:</strong><br />
 It is a measure of aggregated classification performance.</li>
      <li><strong style="color: blue">Usage in ML:</strong><br />
 For model comparison.</li>
    </ol>
  </li>
  <li>
    <p><strong style="color: red">Define Statistical Efficiency (of an estimator)?</strong><br />
 Essentially, a more efficient estimator, experiment, or test needs fewer observations than a less efficient one to achieve a given performance.<br />
 Efficiencies are often defined using the <em>variance</em> or <em>mean square error</em> as the measure of desirability.<br />
 An efficient estimator is also the minimum variance unbiased estimator (MVUE).</p>

    <ol>
      <li>An Efficient Estimator has lower variance than an inefficient one</li>
      <li>The use of an inefficient estimator gives results equivalent to those obtainable from a subset of data; and is therefor, wasteful of data</li>
    </ol>
  </li>
  <li>
    <p><strong style="color: red">Whats the difference between <em>Errors</em> and <em>Residuals</em>:</strong><br />
 The <strong>Error</strong> of an observed value is the deviation of the observed value from the (unobservable) <strong><em>true</em></strong> value of a quantity of interest.</p>

    <p>The <strong>Residual</strong> of an observed value is the difference between the observed value and the <em><strong>estimated</strong></em> value of the quantity of interest.</p>

    <ol>
      <li><strong style="color: blue">Compute the statistical errors and residuals of the univariate, normal distribution defined as \(X_{1}, \ldots, X_{n} \sim N\left(\mu, \sigma^{2}\right)\):</strong>
        <ol>
          <li><strong>Statistical Errors</strong>:
            <p>$$e_{i}=X_{i}-\mu$$</p>
          </li>
          <li><strong>Residuals</strong>:
            <p>$$r_{i}=X_{i}-\overline {X}$$</p>
          </li>
          <li><a href="https://en.wikipedia.org/wiki/Errors_and_residuals#In_univariate_distributions" value="show" onclick="iframePopA(event)"><strong>Example in Univariate Distributions</strong></a>
 <a href="https://en.wikipedia.org/wiki/Errors_and_residuals#In_univariate_distributions"></a>
            <div></div>
          </li>
        </ol>
      </li>
    </ol>
  </li>
  <li><strong style="color: red">What is a biased estimator?</strong><br />
 We define the <strong>Bias</strong> of an estimator as:
    <p>$$ \operatorname{bias}\left(\hat{\boldsymbol{\theta}}_{m}\right)=\mathbb{E}\left(\hat{\boldsymbol{\theta}}_{m}\right)-\boldsymbol{\theta} $$</p>
    <p>A <strong>Biased Estimator</strong> is an estimator \(\hat{\boldsymbol{\theta}}_ {m}\) such that:</p>
    <p>$$ \operatorname{bias}\left(\hat{\boldsymbol{\theta}}_ {m}\right) \geq 0$$</p>
    <ol>
      <li><strong style="color: blue">Why would we prefer biased estimators in some cases?</strong><br />
 Mainly, due to the <em><strong>Bias-Variance Decomposition</strong></em>. The <strong>MSE</strong> takes into account both the <em>bias</em> and the <em>variance</em> and sometimes the biased estimator might have a lower variance than the unbiased one, which results in a total <em>decrease</em> in the MSE.</li>
    </ol>
  </li>
  <li><strong style="color: red">What is the difference between “Probability” and “Likelihood”:</strong><br />
 <strong>Probabilities</strong> are the areas under a fixed distribution<br />
 \(pr(\)data\(|\)distribution\()\)<br />
 i.e. probability of some <em>data</em> (left hand side) given a distribution (described by the right hand side)<br />
 <strong>Likelihoods</strong> are the y-axis values for fixed data points with distributions that can be moved..<br />
 \(L(\)distribution\(|\)observation/data\()\)<br />
 It is the likelihood of the parameter \(\theta\) for the data \(\mathcal{D}\).
    <blockquote>
      <p>Likelihood is, basically, a specific probability that can only be calculated after the fact (of observing some outcomes). It is not normalized to \(1\) (it is <strong>not</strong> a probability). It is just a way to quantify how likely a set of observation is to occur given some distribution with some parameters; then you can manipulate the parameters to make the realization of the data more <em>“likely”</em> (it is precisely meant for that purpose of estimating the parameters); it is a <em>function</em> of the <strong>parameters</strong>.<br />
 Probability, on the other hand, is absolute for all possible outcomes. It is a function of the <strong>Data</strong>.</p>
    </blockquote>
  </li>
  <li><strong style="color: red">Estimators:</strong>
    <ol>
      <li><strong style="color: blue">Define:</strong><br />
 A <strong>Point Estimator</strong> or <strong>statistic</strong> is any function of the data.</li>
      <li><strong style="color: blue">Formula:</strong>
        <p>$$\hat{\boldsymbol{\theta}}_{m}=g\left(\boldsymbol{x}^{(1)}, \ldots, \boldsymbol{x}^{(m)}\right)$$</p>
      </li>
      <li><strong style="color: blue">Whats a good estimator?</strong><br />
 A good estimator is a function whose output is close to the true underlying \(\theta\) that generated the training data.</li>
      <li><strong style="color: blue">What are the Assumptions made regarding the estimated parameter:</strong><br />
 We assume that the true \(\boldsymbol{\theta}\) is fixed, and that \(\hat{\boldsymbol{\theta}}\) is a function of the data, which is drawn from a random process, making \(\hat{\boldsymbol{\theta}}\) a <strong>random variable</strong>.</li>
    </ol>
  </li>
  <li><strong style="color: red">What is Function Estimation:</strong><br />
 <strong>Function Estimation/Approximation</strong> refers to estimation of the relationship between <em>input</em> and <em>target data</em>.<br />
 I.E. We are trying to predict a variable \(y\) given an input vector \(x\), and we assume that there is a function \(f(x)\) that describes the approximate relationship between \(y\) and \(x\).<br />
 If we assume that: \(y = f(x) + \epsilon\), where \(\epsilon\) is the part of \(y\) that is not predictable from \(x\); then we are interested in approximating \(f\) with a model or estimate \(\hat{f}\).
    <ol>
      <li><strong style="color: blue">Whats the relation between the Function Estimator \(\hat{f}\) and Point Estimator:</strong><br />
 Function estimation is really just the same as estimating a parameter \(\boldsymbol{\theta}\); the function estimator \(\hat{f}\) is simply a point estimator in function space.</li>
    </ol>
  </li>
  <li><strong style="color: red">Define “marginal likelihood” (wrt naive bayes):</strong><br />
 Marginal likelihood is, the probability that the word ‘FREE’ is used in any message (not given any other condition?).</li>
</ol>

<hr />

<h1 id="statistics---mle">(Statistics) - MLE</h1>
<p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">(Statistics) - MLE</button></p>

<ol hidden="">
  <li><strong style="color: red">Clearly Define MLE and derive the final formula:</strong><br />
 <strong>MLE</strong> is a method/principle from which we can derive specific functions that are <em><strong>good estimators</strong></em> for different models.<br />
 <strong style="color: silver">Likelihood in Parametric Models:</strong><br />
 Suppose we have a parametric model \(\{p(y ; \theta) | \theta \in \Theta\}\) and a sample \(D=\left\{y_{1}, \ldots, y_{n}\right\}\):
    <ol>
      <li>The likelihood of parameter estimate \(\hat{\theta} \in \Theta\) for sample \(\mathcal{D}\) is:
        <p>$$\begin{aligned}  {\displaystyle {\mathcal {L}}(\theta \,;\mathcal{D} )} &amp;= p(\mathcal{D} ; \theta) \\&amp;=\prod_{i=1}^{n} p\left(y_{i} ; \theta\right)\end{aligned}$$</p>
      </li>
      <li>In practice, we prefer to work with the <strong>log-likelihood</strong>.  Same maximum but
        <p>$$\log {\displaystyle {\mathcal {L}}(\theta \,;\mathcal{D} )}=\sum_{i=1}^{n} \log p\left(y_{i} ; \theta\right)$$</p>
        <p>and sums are easier to work with than products.</p>
      </li>
    </ol>

    <p><strong style="color: silver">MLE for Parametric Models:</strong><br />
 The <strong>maximum likelihood estimator (MLE)</strong> for \(\theta\) in the (parametric) model \(\{p(y, \theta) | \theta \in \Theta\}\) is:</p>
    <p>$$\begin{aligned} \hat{\theta} &amp;=\underset{\theta \in \Theta}{\arg \max } \log p(\mathcal{D}, \hat{\theta}) \\ &amp;=\underset{\theta \in \Theta}{\arg \max } \sum_{i=1}^{n} \log p\left(y_{i} ; \theta\right) \end{aligned}$$</p>
    <ol>
      <li><strong style="color: blue">Write MLE as an expectation wrt the Empirical Distribution:</strong><br />
 Because the \(\text {arg max }\) does not change when we rescale the cost function, we can divide by \(m\) to obtain a version of the criterion that is expressed as an <strong>expectation with respect to the empirical distribution \(\hat{p}_ {\text {data}}\)</strong>  defined by the training data:
        <p>$${\displaystyle \boldsymbol{\theta}_{\mathrm{ML}}=\underset{\boldsymbol{\theta}}{\arg \max } \:\: \mathbb{E}_{\mathbf{x} \sim \hat{p} \text {data}} \log p_{\text {model}}(\boldsymbol{x} ; \boldsymbol{\theta})} \tag{5.59}$$</p>
      </li>
      <li><strong style="color: blue">Describe formally the relationship between MLE and the KL-divergence:</strong><br />
 <strong style="color: silver">MLE as Minimizing KL-Divergence between the Empirical dist. and the model dist.:</strong><br />
 We can interpret maximum likelihood estimation as <em>minimizing the dissimilarity</em> between the <strong>empirical distribution \(\hat{p}_ {\text {data}}\)</strong>, defined by the training set, and the <strong>model distribution</strong>, with the degree of dissimilarity between the two measured by the <strong>KL divergence</strong>.
        <ol>
          <li>The <strong>KL-divergence</strong> is given by:
            <p>$${\displaystyle D_{\mathrm{KL}}\left(\hat{p}_{\text {data}} \| p_{\text {model}}\right)=\mathbb{E}_{\mathbf{x} \sim \hat{p}_{\text {data}}}\left[\log \hat{p}_{\text {data}}(\boldsymbol{x})-\log p_{\text {model}}(\boldsymbol{x})\right]} \tag{5.60}$$</p>
            <p>The term on the left is a function only of the data-generating process, not the model. This means when we train the model to minimize the KL divergence, we need only minimize:</p>
          </li>
        </ol>
        <p>$${\displaystyle -\mathbb{E}_{\mathbf{x} \sim \hat{p}_{\text {data}}}\left[\log p_{\text {model}}(\boldsymbol{x})\right]} \tag{5.61}$$</p>
        <p>which is of course the same as the <em>maximization</em> in equation \(5.59\).</p>
      </li>
      <li>
        <p><strong style="color: blue">Extend the argument to show the link between MLE and Cross-Entropy. Give an example:</strong><br />
 Minimizing this KL-divergence corresponds exactly to <strong>minimizing the cross-entropy between the distributions</strong>.<br />
 Any loss consisting of a negative log-likelihood is a cross-entropy between the empirical distribution defined by the training set and the probability distribution defined by model.<br />
 E.g. <strong>MSE</strong> is the <em>cross-entropy</em> between the <strong>empirical distribution</strong> and a <strong>Gaussian model</strong>.</p>

        <p>Maximum likelihood thus becomes minimization of the negative log-likelihood(NLL), or equivalently, minimization of the cross-entropy.</p>
      </li>
      <li><strong style="color: blue">How does the form of the model (model family) affect the MLE Estimate?</strong><br />
 The form of the model (e.g. Gaussian Model) determines the resulting loss function (e.g. MSE).</li>
      <li><strong style="color: blue">How does MLE relate to the model distribution and the empirical distribution?</strong><br />
 We can thus see maximum likelihood as an attempt to <em>make the model distribution match the empirical distribution \(\hat{p} _ {\text {data}}\)</em>.</li>
      <li><strong style="color: blue">What is the intuition behind using MLE?</strong><br />
 If I choose a <em>hypothesis</em> \(h\) underwhich the <em>observed data</em> is very <em><strong>plausible</strong></em> then the <em>hypothesis</em> is very <em><strong>likely</strong></em>.</li>
      <li><strong style="color: blue">What does MLE find/result in?</strong><br />
 It finds the value of the parameter \(\theta\) that, if used (in the model) to generate the probability of the data, would make the data most <em>“likely”</em> to occur.</li>
      <li><strong style="color: blue">What kind of problem is MLE and how to solve for it?</strong><br />
 It is an optimization problem that is solved by calculus for problems with closed-form solutions or with numerical methods (e.g. SGD).</li>
      <li><strong style="color: blue">How does it relate to SLT:</strong> <br />
 It corresponds to <strong>Empirical Risk Minimization</strong>.</li>
      <li><strong style="color: blue">Explain clearly why we maximize the natural log of the likelihood</strong>
        <ol>
          <li>Numerical Stability: change products to sums</li>
          <li>The logarithm of a member of the family of exponential probability distributions (which includes the ubiquitous normal) is polynomial in the parameters (i.e. max-likelihood reduces to least-squares for normal distributions)<br />
 \(\log\left(\exp\left(-\frac{1}{2}x^2\right)\right) = -\frac{1}{2}x^2\)</li>
          <li>The latter form is both more numerically stable and symbolically easier to differentiate than the former. It increases the dynamic range of the optimization algorithm (allowing it to work with extremely large or small values in the same way).</li>
          <li>The logarithm is a monotonic transformation that preserves the locations of the extrema (in particular, the estimated parameters in max-likelihood are identical for the original and the log-transformed formulation)</li>
          <li>Gradient methods generally work better optimizing \(log_p(x)\) than \(p(x)\) because the gradient of \(log_p(x)\) is generally more <strong>well-scaled</strong>. <a href="https://stats.stackexchange.com/questions/174481/why-to-optimize-max-log-probability-instead-of-probability">link</a><br />
 <strong>Justification:</strong> the gradient of the original term will include a \(e^{\vec{x}}\) multiplicative term that scales very quickly one way or another, requiring the step-size to equally scale/stretch in the opposite direction.</li>
        </ol>
      </li>
    </ol>
  </li>
</ol>

<hr />

<h1 id="text-classification--classical">Text-Classification | Classical</h1>
<p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Text-Classification | Classical</button></p>
<ol hidden="">
  <li><strong style="color: red">List some Classification Methods:</strong>
    <ol>
      <li><strong>(Hand-Coded)Rules-Based Algorithms</strong>: use rules based on combinations of words or other features.
        <ol>
          <li>Can have high accuracy if the rules are carefully refined and maintained by experts.</li>
          <li>However, building and maintaining these rules is very hard.</li>
        </ol>
      </li>
      <li><strong>Supervised Machine Learning</strong>: using an ML algorithm that trains on a training set of (document, class) elements to train a classifier.
        <ol>
          <li><em>Types of Classifiers</em>:
            <ol>
              <li>Naive Bayes</li>
              <li>Logistic Regression</li>
              <li>SVMs</li>
              <li>K-NNs</li>
            </ol>
          </li>
        </ol>
      </li>
    </ol>
  </li>
  <li><strong style="color: red">List some Applications of Txt Classification:</strong>
    <ol>
      <li><strong>Spam Filtering</strong>: discerning spam emails form legitimate emails.</li>
      <li><strong>Email Routing</strong>: sending an email sento to a genral address to a specfic affress based on the topic.</li>
      <li><strong>Language Identification</strong>: automatiacally determining the genre of a piece of text.</li>
      <li>Readibility Assessment__: determining the degree of readability of a piece of text.</li>
      <li><strong>Sentiment Analysis</strong>: determining the general emotion/feeling/attitude of the author of a piece of text.</li>
      <li><strong>Authorship Attribution</strong>: determining which author wrote which piece of text.</li>
      <li><strong>Age/Gender Identification</strong>: determining the age and/or gender of the author of a piece of text.</li>
    </ol>
  </li>
</ol>

<hr />

<h1 id="nlp">NLP</h1>
<p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">NLP</button></p>
<ol hidden="">
  <li><strong style="color: red">List some problems in NLP:</strong>
    <ol>
      <li>Question Answering (QA)</li>
      <li>Information Extraction (IE)</li>
      <li>Sentiment Analysis</li>
      <li>Machine Translation (MT)</li>
      <li>Spam Detection</li>
      <li>Parts-of-Speech (POS) Tagging</li>
      <li>Named Entity Recognition (NER)</li>
      <li>Conference Resolution</li>
      <li>Word Sense Disambugation (WSD)</li>
      <li>Parsing</li>
      <li>Paraphrasing</li>
      <li>Summarization</li>
      <li>Dialog</li>
    </ol>
  </li>
  <li><strong style="color: red">List the Solved Problems in NLP:</strong>
    <ol>
      <li>Spam Detection</li>
      <li>Parts-of-Speech (POS) Tagging</li>
      <li>Named Entity Recognition (NER)</li>
    </ol>
  </li>
  <li><strong style="color: red">List the “within reach” problems in NLP:</strong>
    <ol>
      <li>Sentiment Analysis</li>
      <li>Conference Resolution</li>
      <li>Word Sense Disambugation (WSD)</li>
      <li>Parsing</li>
      <li>Machine Translation (MT)</li>
      <li>Information Extraction (IE)</li>
    </ol>
  </li>
  <li><strong style="color: red">List the Open Problems in NLP:</strong>
    <ol>
      <li>Question Answering (QA)</li>
      <li>Paraphrasing</li>
      <li>Summarization</li>
      <li>Dialog</li>
    </ol>
  </li>
  <li><strong style="color: red">Why is NLP hard? List Issues:</strong>
    <ol>
      <li><strong>Non-Standard English</strong>: “Great Job @ahmed_badary! I luv u 2!! were SOO PROUD of dis.”</li>
      <li><strong>Segmentation Issues</strong>: “New York-New Haven” vs “New-York New-Haven”</li>
      <li><strong>Idioms</strong>: “dark horse”, “getting cold feet”, “losing face”</li>
      <li><strong>Neologisms</strong>: “unfriend”, “retweet”, “google”, “bromance”</li>
      <li><strong>World Knowledge</strong>: “Ahmed and Zach are brothers”, “Ahmed and Zach are fathers”</li>
      <li><strong>Tricky Entity Names</strong>: “Where is <em>Life of Pie</em> playing tonight?”, “<em>Let it be</em> was a hit song!”</li>
    </ol>
  </li>
  <li><strong style="color: red">Define:</strong>
    <ol>
      <li><strong style="color: blue">Morphology:</strong><br />
 The study of words, how they are formed, and their relationship to other words in the same language.</li>
      <li><strong style="color: blue">Morphemes:</strong><br />
 the small meaningful units that make up words.</li>
      <li><strong style="color: blue">Stems:</strong><br />
 the core meaning-bearing units of words.</li>
      <li><strong style="color: blue">Affixes:</strong><br />
 the bits and pieces that adhere to stems (often with grammatical functions).</li>
      <li><strong style="color: blue">Stemming:</strong><br />
 is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form.<br />
 The stem need <strong>not</strong> map to a valid root in the language.</li>
      <li><strong style="color: blue">Lemmatization:</strong><br />
 reducing inflections or variant forms to base form.</li>
    </ol>
  </li>
  <li><strong style="color: red">Topic Modeling vs Document Classification:</strong>
    <ul>
      <li><strong>Text Classification</strong> is a form of supervised learning, hence the set of possible classes are known/defined in advance, and won’t change.</li>
      <li><strong>Topic Modeling</strong> is a form of unsupervised learning (akin to clustering), so the set of possible topics are unknown apriori. They’re defined as part of generating the topic models.</li>
    </ul>
  </li>
</ol>

<hr />

<h1 id="language-modeling">Language Modeling</h1>
<p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Language Modeling</button></p>
<ol hidden="">
  <li><strong style="color: red">What is a Language Model?</strong></li>
  <li><strong style="color: red">List some Applications of LMs:</strong></li>
  <li><strong style="color: red">Traditional LMs:</strong>
    <ol>
      <li><strong style="color: blue">How are they setup?</strong></li>
      <li><strong style="color: blue">What do they depend on?</strong></li>
      <li><strong style="color: blue">What is the Goal of the LM task? (in the ctxt of the problem setup)</strong></li>
      <li><strong style="color: blue">What assumptions are made by the problem setup? Why?</strong></li>
      <li><strong style="color: blue">What are the MLE Estimates for probabilities of the following:</strong>
        <ol>
          <li><strong style="color: blue">Bi-Grams:</strong>
            <p>$$p(w_2\vert w_1) = $$</p>
          </li>
          <li><strong style="color: blue">Tri-Grams:</strong>
            <p>$$p(w_3\vert w_1, w_2) = $$</p>
          </li>
        </ol>
      </li>
      <li><strong style="color: red">What are the issues w/ Traditional Approaches?</strong></li>
    </ol>
  </li>
  <li><strong style="color: red">What+How can we setup some NLP tasks as LM tasks:</strong></li>
  <li><strong style="color: red">How does the LM task relate to Reasoning/AGI:</strong></li>
  <li><strong style="color: red">Evaluating LM models:</strong>
    <ol hidden="">
      <li><strong style="color: blue">List the Loss Functions (+formula) used to evaluate LM models? Motivate each:</strong></li>
      <li><strong style="color: blue">Which application of LM modeling does each loss work best for?</strong><br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">Show Questions</button></li>
      <li><strong style="color: blue">Why Cross-Entropy:</strong></li>
      <li><strong style="color: blue">Which setting it used for?</strong></li>
      <li><strong style="color: blue">Why Perplexity:</strong></li>
      <li><strong style="color: blue">Which setting used for?</strong></li>
      <li><strong style="color: blue">If no surprise, what is the perplexity?</strong></li>
      <li><strong style="color: blue">How does having a good LM relate to Information Theory?</strong></li>
    </ol>
  </li>
  <li><strong style="color: red">LM DATA:</strong>
    <ol>
      <li><strong style="color: blue">How does the fact that LM is a time-series prediction problem affect the way we need to train/test:</strong></li>
      <li><strong style="color: blue">How should we choose a subset of articles for testing:</strong></li>
    </ol>
  </li>
  <li><strong style="color: red">List three approaches to Parametrizing LMs:</strong><br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">Show Questions</button>
    <ol hidden="">
      <li><strong style="color: blue">Describe “Count-Based N-gram Models”:</strong></li>
      <li><strong style="color: blue">What distributions do they capture?:</strong></li>
      <li><strong style="color: blue">Describe “Neural N-gram Models”:</strong></li>
      <li><strong style="color: blue">What do they replace the captured distribution with?</strong></li>
      <li><strong style="color: blue">What are they better at capturing:</strong></li>
      <li><strong style="color: blue">Describe “RNNs”:</strong></li>
      <li><strong style="color: blue">What do they replace/capture?</strong></li>
      <li><strong style="color: blue">How do they capture it?</strong></li>
      <li><strong style="color: blue">What are they best at capturing:</strong></li>
    </ol>
  </li>
  <li><strong style="color: red">What’s the main issue in LM modeling?</strong><br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">Show Questions</button>
    <ol hidden="">
      <li><strong style="color: blue">How do N-gram models capture/approximate the history?:</strong></li>
      <li><strong style="color: blue">How do RNNs models capture/approximate the history?:</strong></li>
    </ol>
    <ol>
      <li><strong style="color: blue">The Bias-Variance Tradeoff of the following:</strong>
        <ol>
          <li><strong style="color: blue">N-Gram Models:</strong></li>
          <li><strong style="color: blue">RNNs:</strong></li>
          <li><strong style="color: blue">An Estimate s.t. it predicts the probability of a sentence by how many times it has seen it before:</strong>
            <ol>
              <li><strong style="color: blue">What happens in the limit of infinite data?</strong></li>
            </ol>
          </li>
        </ol>
      </li>
    </ol>
  </li>
  <li><strong style="color: red">What are the advantages of sub-word level LMs:</strong></li>
  <li><strong style="color: red">What are the disadvantages of sub-word level LMs:</strong></li>
  <li><strong style="color: red">What is a “Conditional LM”?</strong></li>
  <li><strong style="color: red">Write the decomposition of the probability for the Conditional LM:</strong></li>
  <li><strong style="color: red">Describe the Computational Bottleneck for Language Models:</strong></li>
  <li><strong style="color: red">Describe/List some solutions to the Bottleneck:</strong></li>
  <li><strong style="color: red">Complexity Comparison of the different solutions:</strong><br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">Show Questions</button>
 <img src="/main_files/qs/1.png" alt="img" width="100%" hidden="" /></li>
</ol>

<hr />

<h1 id="regularization">Regularization</h1>
<p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Regularization</button></p>
<ol hidden="">
  <li>
    <p><strong style="color: red">Define Regularization both intuitively and formally:</strong><br />
 <strong>Regularization</strong> can be, loosely, defined as: any modification we make to a learning algorithm that is intended to <em>reduce</em> its <em>generalization error</em> but not its <em>training error</em>.</p>

    <p>Formally, it is a set of techniques that impose certain restrictions on the hypothesis space (by adding information) in order to solve an <strong>ill-posed</strong> problem or to prevent <strong>overfitting</strong>.</p>
  </li>
  <li>
    <p><strong style="color: red">Define “well-posedness”:</strong><br />
 Hadamard defines <strong>Well-Posed Problems</strong> as having the properties (1) A Solution Exists (2) It is Unique (3) It’s behavior changes continuously with the initial conditions.</p>
  </li>
  <li><strong style="color: red">Give four aspects of justification for regularization (theoretical):</strong><br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">Show</button>
    <ol hidden="">
      <li><strong style="color: blue">From a philosophical pov:</strong><br />
 It attempts to impose Occam’s razor on the solution.</li>
      <li><strong style="color: blue">From a probabilistic pov:</strong><br />
 From a Bayesian point of view, many regularization techniques correspond to imposing certain prior distributions on model parameters. This is equivalent to making a <strong>MAP Estimate</strong> of the function we are trying to learn:
        <p>$$\hat{\boldsymbol{\theta}}_{\mathrm{MAP}}=\underset{\boldsymbol{\theta}}{\arg \min }-\left(\sum_{i=1}^{n} \log \left[p\left(y_{i} | \mathbf{x}_ {i}, \boldsymbol{\theta}\right)\right]\right)-\log [p(\boldsymbol{\theta})]$$</p>
      </li>
      <li><strong style="color: blue">From an SLT pov:</strong><br />
 Regularization can be motivated as imposing a restriction on the complexity of a hypothesis space; called, the “capacity”. In SLT, this is called the <strong>Approximation-Generalization Tradeoff</strong>. Regularization, effectively, works as a way to maneuver that tradeoff by giving preference to certain solutions. This can be described in two ways:
        <ol>
          <li><strong>VC-Theory</strong>:<br />
 The theory results in a very simple, yet informative, inequality that captures the tradeoff, called <strong>The Generalization Bound</strong>:
            <p>$$E_{\text {out}}(h) \leq E_{\text {in}}(h)+\Omega(\mathcal{H})$$</p>
            <p>But, if we formalize regularization as adding a regularizer (from the MAP estimate above): \(\Omega = \Omega(h)\) a function of the hypothesis \(h\), to the in-sample error \(E_{\text {in}}\), then we get a new <em><strong>“Augmented”</strong></em>  <strong>Error</strong>:</p>
            <p>$$E_{\text {aug}}(h) = E_{\text {in}} + \lambda \Omega(h)$$</p>
            <p>Notice the correspondence between the form of the new <em>augmented error</em> \(E_{\text {aug}}\) and the <em>VC Inequality</em>:</p>
            <p>$$\begin{aligned}E_{\text {aug}}(h) &amp;= E_{\text {in}}(h) + \lambda \Omega(h) \\
     &amp;\downarrow \\
 E_{\text {out}}(h) &amp;\leq E_{\text {in}}(h)+\Omega(\mathcal{H})\end{aligned}$$</p>
            <p>(we can relate the complexity of a single object to the complexity of a set of objects)<br />
 <strong>Interpreting the correspondence:</strong><br />
 Consider the goal of ML: “to find an <em><strong>in-sample estimate</strong></em> of the <em><strong>out-sample error</strong></em>”. From that perspective, the Augmented Error \(E_{\text {aug}}(h)\) is a better <em>proxy</em> to (estimate of) the out-sample error. Thus, minimizing \(E_{\text {aug}}(h)\) corresponds, better, to minimizing \(E_{\text {out}}(h)\). Where \(\Omega(h)\) is an estimate of \(\Omega(\mathcal{H})\); the regularization term a minimizer of the complexity term.</p>
          </li>
          <li><strong>Bias-Variance Decomposition:</strong><br />
 If the variance term \(\operatorname{Var}[h]\) corresponds to the complexity term \(\Omega(\mathcal{H})\), and the regularization term \(\Omega(h)\), also, corresponds to \(\Omega(\mathcal{H})\); then, the regularization term must also correspond to the variance term.<br />
 From that perspective, adding the regularization term to the augmented error accounts for the variance of the hypothesis and acts as a measure for it. Thus, minimizing the regularization term \(\Omega(h)\) corresponds to minimizing the variance \(\operatorname{Var}[h]\). However, when the variance of a model goes down the <strong>bias</strong> tends to go up. Since this inverse-relationship is <strong>not linear</strong>, this analysis/view gives us a way to apply regularization effectively, by allowing us to measure the effect of regularization.<br />
 Basically, we would like our regularization term to <em>reduce the variance</em> more than it <em>increases the bias</em>. We can control that tradeoff by the hyperparameter \(\lambda\).</li>
        </ol>
      </li>
      <li>
        <p><strong style="color: blue">From a practical pov (relating to the real-world):</strong><br />
 Most applications of DL are to domains where the true data-generating process is almost certainly outside the model family (hypothesis space). Deep learning algorithms are typically applied to extremely complicated domains such as images, audio sequences and text, for which the true generation process essentially involves simulating the entire universe.</p>

        <p>Thus, controlling the complexity of the mdoel is not a simple matter of finding the model of the right size, with the right number of parameters; instead, the best fitting model (wrt. generalization error) is a large model that has been regularized appropriately.</p>
      </li>
    </ol>
  </li>
  <li><strong style="color: red">Describe an overview of regularization in DL. How does it usually work?</strong><br />
 In the context of DL, most regularization strategies are based on <strong>regularizing estimators</strong>, which usually works by <em>trading increased bias for reduced variance</em>.
    <ol>
      <li><strong style="color: blue">Intuitively, how can a regularizer be effective?</strong><br />
 An effective regularizer is one that makes a profitable trade, reducing variance significantly while not overly increasing the bias.</li>
    </ol>
  </li>
  <li><strong style="color: red">Describe the relationship between regularization and capacity:</strong><br />
 Regularization is a (more general) way of controlling a models capacity by allowing us to express preference for one function over another in the same hypothesis space; instead of including or excluding members from the hypothesis space completely.</li>
  <li><strong style="color: red">Describe the different approaches to regularization:</strong>
    <ol>
      <li>Parameter Norm Penalties: \(L^p\) norms, early stopping</li>
      <li>Data Augmentation: noise robustness/injection, dropout</li>
      <li>Semi-supervised Learning</li>
      <li>Multi-task Learning</li>
      <li>Ensemble Learning: bagging, etc.</li>
      <li>Adversarial Training</li>
      <li>Infinite Priors: parameter tying and sharing</li>
    </ol>
  </li>
  <li><strong style="color: red">List 9 regularization techniques:</strong>
    <ol>
      <li>\(L^2\) regularization</li>
      <li>\(L^1\) regularization</li>
      <li>Dataset Augmentation</li>
      <li>Noise Injection</li>
      <li>Semi-supervised Learning</li>
      <li>Multi-task Learning</li>
      <li>Early Stopping</li>
      <li>Parameter Tying, Sharing</li>
      <li>Sparse Representations</li>
      <li>Ensemble Methods, Bagging etc.
 1* Dropout</li>
      <li>Adversarial Training</li>
      <li>Tangent prop and Manifold Tangent Classifier</li>
    </ol>
  </li>
</ol>

<p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Show the rest of the questions</button></p>
<ol hidden="">
  <li><strong style="color: red">Describe Parameter Norm Penalties (PNPs):</strong><br />
 Many regularization approaches are based on limiting the capacity of models by adding a parameter norm penalty \(\Omega(\boldsymbol{\theta})\) to the objective function \(J\). We denote the regularized objective function by \(\tilde{J}\):
    <p>$$\tilde{J}(\boldsymbol{\theta} ; \boldsymbol{X}, \boldsymbol{y})=J(\boldsymbol{\theta} ; \boldsymbol{X}, \boldsymbol{y})+\alpha \Omega(\boldsymbol{\theta}) \tag{7.1}$$</p>
    <p>where \(\alpha \in[0, \infty)\) is a HP that weights the relative contribution of the norml penalty term, \(\Omega\), relative to the standard objective function \(J\).<br />
 <button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Show</button></p>
    <ol hidden="">
      <li><strong style="color: blue">Define the regularized objective:</strong>
        <p>$$\tilde{J}(\boldsymbol{\theta} ; \boldsymbol{X}, \boldsymbol{y})=J(\boldsymbol{\theta} ; \boldsymbol{X}, \boldsymbol{y})+\alpha \Omega(\boldsymbol{\theta}) \tag{7.1}$$</p>
      </li>
      <li><strong style="color: blue">Describe the parameter \(\alpha\):</strong><br />
 \(\alpha \in[0, \infty)\) is a HP that weights the relative contribution of the norm penalty term, \(\Omega\), relative to the standard objective function \(J\).</li>
      <li><strong style="color: blue">How does it influence the regularization:</strong>
        <ol>
          <li><strong>Effects of \(\alpha\)</strong>:
            <ol>
              <li>\(\alpha = 0\) results in NO regularization</li>
              <li>Larger values of \(\alpha\) correspond to MORE regularization</li>
            </ol>
          </li>
        </ol>
      </li>
      <li><strong style="color: blue">What is the effect of minimizing the regularized objective?</strong><br />
 The <strong>effect of minimizing the regularized objective function</strong> is that it will <em><strong>decrease</strong></em>, both, <em>the original objective \(J\)</em> on the training data and some <em>measure of the size of the parameters \(\boldsymbol{\theta}\)</em>.</li>
    </ol>
  </li>
  <li><strong style="color: red">How do we deal with the Bias parameter in PNPs? Explain.</strong><br />
 In NN, we usually penalize <strong>only the weights</strong> of the affine transformation at each layer and we leave the <strong>biases unregularized</strong>.<br />
 Biases typically require less data than the weights to fit accurately. The reason is that <em>each weight specifies how TWO variables interact</em> so fitting the weights well, requires observing both variables in a variety of conditions. However, <em>each bias controls only a single variable</em>, thus, we don’t induce too much <em>variance</em> by leaving the biases unregularized. If anything, regularizing the bias can introduce a significant amount of <em>underfitting</em>.</li>
  <li><strong style="color: red">Describe the tuning of the \(\alpha\) HP in NNs for different hidden layers:</strong><br />
 In the context of neural networks, it is sometimes desirable to use a separate penalty with a different \(\alpha\) coefficient for each layer of the network. Because it can be expensive to search for the correct value of multiple hyperparameters, it is still reasonable to use the same weight decay at all layers just to reduce the size of search space.</li>
  <li><strong style="color: red">Formally describe the \(L^2\) parameter regularization:</strong><br />
 It is a regularization strategy that <em>drives the weights closer to the origin</em> by adding a regularization term:
    <p>$$\Omega(\mathbf{\theta}) = \frac{1}{2}\|\boldsymbol{w}\|_ {2}^{2}$$</p>
    <p>to the objective function.</p>
    <ol>
      <li><strong style="color: blue">AKA:</strong><br />
 In statistics, \(L^2\) regularization is also known as <strong>Ridge Regression</strong> or <strong>Tikhonov Regularization</strong>.<br />
 In ML, it is known as <strong>weight decay</strong>.</li>
      <li><strong style="color: blue">Describe the regularization contribution to the gradient in a single step.</strong><br />
 The addition of the weight decay term has modified the learning rule to <strong>multiplicatively shrink the weight vector by  a constant factor on each step</strong>, just before performing the usual gradient update.
        <p>$$\boldsymbol{w} \leftarrow(1-\epsilon \alpha) \boldsymbol{w}-\epsilon \nabla_{\boldsymbol{w}} J(\boldsymbol{w} ; \boldsymbol{X}, \boldsymbol{y}) \tag{7.5}$$</p>
      </li>
      <li><strong style="color: blue">Describe the regularization contribution to the gradient. How does it scale?</strong><br />
 The effect of weight decay is to rescale \(\boldsymbol{w}^{\ast}\) along the axes defined by the eigenvector of \(\boldsymbol{H}\). Specifically, the component of \(\boldsymbol{w}^{\ast}\) that is aligned with the \(i\)-th eigenvector of \(\boldsymbol{H}\) is rescaled by a factor of \(\frac{\lambda_{i}}{\lambda_{i}+\alpha}\).</li>
      <li>
        <p><strong style="color: blue">How does weight decay relate to shrinking the individual weight wrt their size? What is the measure/comparison used?</strong><br />
 Only directions along which the parameters contribute significantly to reducing the objective function are preserved relatively intact. In directions that do not contribute to reducing the objective function, a small eigenvalue of the Hessian tells us that movement in this direction will not significantly increase the gradient. Components of the weight vector corresponding to such unimportant directions are decayed away through the use of the regularization throughout training.</p>

        <table>
          <tbody>
            <tr>
              <td><strong>Condition</strong></td>
              <td><strong>Effect of Regularization</strong></td>
            </tr>
            <tr>
              <td>\(\lambda_{i}&gt;&gt;\alpha\)</td>
              <td>Not much</td>
            </tr>
            <tr>
              <td>\(\lambda_{i}&lt;&lt;\alpha\)</td>
              <td>The weight value almost shrunk to \(0\)</td>
            </tr>
          </tbody>
        </table>
      </li>
    </ol>
  </li>
  <li><strong style="color: red">Draw a graph describing the effects of \(L^2\) regularization on the weights:</strong><br />
 <img src="/main_files/dl_book/regularization/1.png" alt="img" width="80%" /></li>
  <li><strong style="color: red">Describe the effects of applying weight decay to linear regression</strong><br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">Application to Linear Regression</button>
 <img src="/main_files/dl_book/regularization/2.png" alt="img" width="100%" hidden="" /></li>
  <li><strong style="color: red">Derivation:</strong><br />
 We place a <strong>Gaussian Prior</strong> on the weights, with <strong>zero mean</strong> and <strong>equal variance \(\tau^2\)</strong>:
    <p>$$\begin{aligned} \hat{\theta}_ {\mathrm{MAP}} &amp;=\arg \max_{\theta} \log P(y | \theta)+\log P(\theta) \\ &amp;=\arg \max _{\boldsymbol{w}}\left[\log \prod_{i=1}^{n} \dfrac{1}{\sigma \sqrt{2 \pi}} e^{-\dfrac{\left(y_{i}-\boldsymbol{w}^T\boldsymbol{x}_i\right)^{2}}{2 \sigma^{2}}}+\log \prod_{j=0}^{p} \dfrac{1}{\tau \sqrt{2 \pi}} e^{-\dfrac{w_{j}^{2}}{2 \tau^{2}}} \right] \\ &amp;=\arg \max _{\boldsymbol{w}} \left[-\sum_{i=1}^{n} \dfrac{\left(y_{i}-\boldsymbol{w}^T\boldsymbol{x}_i\right)^{2}}{2 \sigma^{2}}-\sum_{j=0}^{p} \dfrac{w_{j}^{2}}{2 \tau^{2}}\right] \\ &amp;=\arg \min_{\boldsymbol{w}} \dfrac{1}{2 \sigma^{2}}\left[\sum_{i=1}^{n}\left(y_{i}-\boldsymbol{w}^T\boldsymbol{x}_i\right)^{2}+\dfrac{\sigma^{2}}{\tau^{2}} \sum_{j=0}^{p} w_{j}^{2}\right] \\ &amp;=\arg \min_{\boldsymbol{w}} \left[\sum_{i=1}^{n}\left(y_{i}-\boldsymbol{w}^T\boldsymbol{x}_i\right)^{2}+\lambda \sum_{j=0}^{p} w_{j}^{2}\right] \\ &amp;= \arg \min_{\boldsymbol{w}} \left[ \|XW - \boldsymbol{y}\|^2 + \lambda {\|\boldsymbol{w}\|_ 2}^2\right]\end{aligned}$$</p>
    <p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Show</button></p>
    <ol hidden="">
      <li><strong style="color: blue">What is \(L^2\) regularization equivalent to?</strong><br />
 \(L^2\) regularization is equivalent to <strong>MAP Bayesian inference with a Gaussian prior on the weights</strong>.</li>
      <li><strong style="color: blue">What are we maximizing?</strong><br />
 The MAP Estimate of the data.</li>
      <li><strong style="color: blue">Derive the MAP Estimate:</strong>
        <p>$$\begin{aligned} \hat{\theta}_ {\mathrm{MAP}} &amp;=\arg \max_{\theta} P(\theta | y) \\ &amp;=\arg \max_{\theta} \frac{P(y | \theta) P(\theta)}{P(y)} \\ &amp;=\arg \max_{\theta} P(y | \theta) P(\theta) \\ &amp;=\arg \max_{\theta} \log (P(y | \theta) P(\theta)) \\ &amp;=\arg \max_{\theta} \log P(y | \theta)+\log P(\theta) \end{aligned}$$</p>
      </li>
      <li><strong style="color: blue">What kind of prior do we place on the weights? What are its parameters?</strong><br />
 We place a <strong>Gaussian Prior</strong> on the weights, with <strong>zero mean</strong> and <strong>equal variance \(\tau^2\)</strong>.</li>
    </ol>
  </li>
  <li><strong style="color: red">List the properties of \(L^2\) regularization:</strong>
    <ol>
      <li>Notice that L2-regularization has a rotational invariance. This actually makes it more sensitive to irrelevant features.</li>
      <li>Adding L2-regularization to a convex function gives a strongly-convex function. So L2-regularization can make gradient descent converge much faster.</li>
    </ol>
  </li>
  <li><strong style="color: red">Formally describe the \(L^1\) parameter regularization:</strong><br />
 \(L^1\) Regularization is another way to regulate the model by <em>penalizing the size of its parameters</em>; the technique adds a regularization term:
    <p>$$\Omega(\boldsymbol{\theta})=\|\boldsymbol{w}\|_{1}=\sum_{i}\left|w_{i}\right| \tag{7.18}$$</p>
    <p>which is a sum of absolute values of the individual parameters.</p>
    <ol>
      <li><strong style="color: blue">AKA:</strong><br />
 <strong>LASSO</strong>.</li>
      <li><strong style="color: blue">Whats the regularized objective function?</strong>
        <p>$$\tilde{J}(\boldsymbol{w} ; \boldsymbol{X}, \boldsymbol{y})=\alpha\|\boldsymbol{w}\|_ {1}+J(\boldsymbol{w} ; \boldsymbol{X}, \boldsymbol{y}) \tag{7.19}$$</p>
      </li>
      <li><strong style="color: blue">What is its gradient?</strong>
        <p>$$\nabla_{\boldsymbol{w}} \tilde{J}(\boldsymbol{w} ; \boldsymbol{X}, \boldsymbol{y})=\alpha \operatorname{sign}(\boldsymbol{w})+\nabla_{\boldsymbol{w}} J(\boldsymbol{X}, \boldsymbol{y} ; \boldsymbol{w}) \tag{7.20}$$</p>
      </li>
      <li><strong style="color: blue">Describe the regularization contribution to the gradient compared to L2. How does it scale?</strong><br />
 The regularization contribution to the gradient <strong>no longer scales linearly with each \(w_i\)</strong>; instead it is a <strong>constant factor with a sign = \(\text{sign}(w_i)\)</strong>.</li>
    </ol>
  </li>
  <li><strong style="color: red">List the properties and applications of \(L^1\) regularization:</strong>
    <ol>
      <li>Induces Sparser Solutions</li>
      <li>Solutions may be non-unique</li>
    </ol>

    <p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Show</button></p>
    <ol hidden="">
      <li><strong style="color: blue">How is it used as a feature selection mechanism?</strong><br />
 <strong>LASSO</strong>: The Least Absolute Shrinkage and Selection Operator integrates an \(L^1\) penalty with a <em>linear model</em> and a <em>least-squares cost function</em>.<br />
 The \(L^1\) penalty causes a subset of the weights to become <strong>zero</strong>, suggesting that the corresponding features may safely be discarded.</li>
    </ol>
  </li>
  <li><strong style="color: red">Derivation:</strong>
    <p>$$\begin{aligned} \hat{\theta}_ {\mathrm{MAP}} &amp;=\arg \max_{\theta} \log P(y | \theta)+\log P(\theta) \\  &amp;=\arg \max _{\boldsymbol{w}}\left[\log \prod_{i=1}^{n} \dfrac{1}{\sigma \sqrt{2 \pi}} e^{-\dfrac{\left(y_{i}-\boldsymbol{w}^T\boldsymbol{x}_i\right)^{2}}{2 \sigma^{2}}}+\log \prod_{j=0}^{p} \dfrac{1}{2 b} e^{-\dfrac{\left|\theta_{j}\right|}{2 b}} \right] \\    &amp;=\arg \max _{\boldsymbol{w}} \left[-\sum_{i=1}^{n} \dfrac{\left(y_{i}-\boldsymbol{w}^T\boldsymbol{x}_i\right)^{2}}{2 \sigma^{2}}-\sum_{j=0}^{p} \dfrac{\left|w_{j}\right|}{2 b}\right] \\    &amp;=\arg \min_{\boldsymbol{w}} \dfrac{1}{2 \sigma^{2}}\left[\sum_{i=1}^{n}\left(y_{i}-\boldsymbol{w}^T\boldsymbol{x}_i\right)^{2}+\dfrac{\sigma^{2}}{b} \sum_{j=0}^{p}\left|w_{j}\right|\right] \\    &amp;=\arg \min_{\boldsymbol{w}} \left[\sum_{i=1}^{n}\left(y_{i}-\boldsymbol{w}^T\boldsymbol{x}_i\right)^{2}+\lambda \sum_{j=0}^{p}\left|w_{j}\right|\right] \\    &amp;= \arg \min_{\boldsymbol{w}} \left[ \|XW - \boldsymbol{y}\|^2 + \lambda \|\boldsymbol{w}\|_ 1\right]\end{aligned}$$</p>
    <p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Show</button></p>
    <ol hidden="">
      <li><strong style="color: blue">What is \(L^1\) regularization equivalent to?</strong><br />
 MAP Bayesian inference with an isotropic Laplace distribution prior on the weights.</li>
      <li><strong style="color: blue">What kind of prior do we place on the weights? What are its parameters?</strong><br />
 Isotropic Laplace distribution prior on the weights: \(\operatorname{Laplace}\left(w_{i} ; 0, \frac{1}{\alpha}\right)\).</li>
    </ol>
  </li>
  <li>
    <p><strong style="color: red">Analyze \(L^1\) vs \(L^2\) regularization:</strong><br />
 They both shrink the weights towards zero.</p>

    <table>
      <tbody>
        <tr>
          <td><strong>\(L^2\)</strong></td>
          <td><strong>\(L^1\)</strong></td>
        </tr>
        <tr>
          <td>Sensitive to irrelevant features</td>
          <td>Robust to irrelevant features</td>
        </tr>
        <tr>
          <td>Computationally Efficient</td>
          <td>Non-Differentiable at \(0\)</td>
        </tr>
        <tr>
          <td>No Sparse Solutions</td>
          <td>Produces Sparse Solutions</td>
        </tr>
        <tr>
          <td>No Feature Selection</td>
          <td>Built-in Feature Selection</td>
        </tr>
        <tr>
          <td>Unique Solution</td>
          <td>Possibly multiple solutions</td>
        </tr>
      </tbody>
    </table>

    <p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Show</button></p>
    <ol hidden="">
      <li><strong style="color: blue">For Sparsity:</strong><br />
 The analysis above shows that \(L^1\) produces sparse solutions by killing certain coefficients/weights. While \(L^2\) does not have this property.</li>
      <li><strong style="color: blue">For correlated features:</strong>
        <ol>
          <li><strong>Identical features</strong>:
            <ol>
              <li>\(L^1\) regularization spreads weight arbitrarily (all weights same sign)</li>
              <li>\(L^2\) regularization spreads weight evenly</li>
            </ol>
          </li>
          <li><strong>Linearly related features</strong>:
            <ol>
              <li>\(L^1\) regularization chooses variable with larger scale, \(0\) weight to others</li>
              <li>\(L^2\) prefers variables with larger scale — spreads weight proportional to scale</li>
            </ol>
          </li>
        </ol>
      </li>
      <li><strong style="color: blue">For optimization:</strong><br />
 Adding \(L^2\) regularization to a convex function gives a strongly-convex function. So L2-regularization can make gradient descent converge much faster. Moreover, it has analytic solutions and is differentiable everywhere, making it more computationally efficient. \(L^1\) being Non-Differentiable makes it harder to optimize with gradient-based methods and requires approximations.</li>
      <li><strong style="color: blue">Give an example that shows the difference wrt sparsity:</strong><br />
 Let’s imagine we are estimating two coefficients in a regression. In \(L^2\) regularization, the solution \(\boldsymbol{w} =(0,1)\) has the same weight as \(\boldsymbol{w}=(\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}})\)  so they are both treated equally. In \(L^1\) regularization, the same two solutions favor the sparse one:
        <p>$$\|(1,0)\|_{1}=1&lt;\left\|\left(\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}}\right)\right\|_{1}=\sqrt{2}$$</p>
        <p>So \(L^2\) regularization doesn’t have any specific built in mechanisms to favor zeroed out coefficients, while \(L^1\) regularization actually favors these sparser solutions.</p>
      </li>
      <li><strong style="color: blue">For sensitivity:</strong><br />
 The rotational invariance of the \(L^2\) regularizer makes it more sensitive to irrelevant features.</li>
    </ol>
  </li>
  <li><strong style="color: red">Describe Elastic Net Regularization. Why was it devised? Any properties?</strong>
    <p>$$\Omega = \lambda\left(\alpha\|w\|_{1}+(1-\alpha)\|w\|_{2}^{2}\right), \alpha \in[0,1]$$</p>
    <ol>
      <li>Combines both \(L^1\) and \(L^2\)</li>
      <li>Used to <strong>produce sparse solutions</strong>, but to avoid the problem of \(L^1\) solutions being sometimes <strong>Non-Unique</strong>
        <ol>
          <li>The problem mainly arises with <strong>correlated features</strong></li>
        </ol>
      </li>
      <li>Elastic net regularization tends to have a grouping effect, where correlated input features are assigned equal weights.</li>
    </ol>
  </li>
  <li><strong style="color: red">Motivate Regularization for ill-posed problems:</strong>
    <ol>
      <li><strong style="color: blue">What is the property that needs attention?</strong><br />
 Under-Constrained Problems. Under-determined systems. Specifically, when \(X^TX\) is <em><strong>singular</strong></em>.</li>
      <li><strong style="color: blue">What would the regularized solution correspond to in this case?</strong><br />
 Many forms of regularization correspond to solving inverting \(\boldsymbol{X}^{\top} \boldsymbol{X}+\alpha \boldsymbol{I}\) instead.</li>
      <li><strong style="color: blue">Are there any guarantees for the solution to be well-posed? How/Why?</strong><br />
 This regularized matrix, \(\boldsymbol{X}^{\top} \boldsymbol{X}+\alpha \boldsymbol{I}\), is <strong>guaranteed to be invertible</strong>.</li>
    </ol>

    <p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Show</button></p>
    <ol hidden="">
      <li><strong style="color: blue">What is the Linear Algebraic property that needs attention?</strong><br />
 \(X^{\top}X\) being singular.</li>
      <li><strong style="color: blue">What models are affected by this?</strong><br />
 Many linear models (e.g. Linear Regression, PCA) depend on <strong>inverting \(\boldsymbol{X}^{\top}\boldsymbol{X}\)</strong>.</li>
      <li><strong style="color: blue">What would the sol correspond to in terms of inverting \(X^{\top}X\):</strong><br />
 Many forms of regularization correspond to solving inverting \(\boldsymbol{X}^{\top} \boldsymbol{X}+\alpha \boldsymbol{I}\) instead.</li>
      <li><strong style="color: blue">When would \(X^{\top}X\) be singular?</strong>
        <ol>
          <li>The data-generating function truly has no variance in some direction.</li>
          <li>No Variance is <em>observed</em> in some direction because there are fewer examples (rows of \(\boldsymbol{X}\)) than input features (columns).</li>
        </ol>
      </li>
      <li><strong style="color: blue">Describe the Linear Algebraic Perspective. What does it correspond to? [LAP]</strong><br />
 Given that the <strong>Moore-Penrose pseudoinverse</strong> \(\boldsymbol{X}^{+}\) of a matrix \(\boldsymbol{X}\) can solve underdetermined linear equations:
        <p>$$\boldsymbol{X}^{+}=\lim_{\alpha \searrow 0}\left(\boldsymbol{X}^{\top} \boldsymbol{X}+\alpha \boldsymbol{I}\right)^{-1} \boldsymbol{X}^{\top} \tag{7.29}$$</p>
        <p>The equation corresponds to <strong>performing linear regression with weight-decay</strong>.</p>
      </li>
      <li><strong style="color: blue">Can models with no closed-form solution be underdetermined? Explain. [CFS]</strong><br />
 Models with no closed-form solution can, also, be <em>underdetermined</em>:<br />
 Take <strong>logistic regression on a linearly separable dataset</strong>, if a weight vector \(\boldsymbol{w}\) is able to achieve perfect classification, then so does \(2\boldsymbol{w}\) but with even <strong>higher likelihood</strong>. Thus, an iterative optimization procedure (sgd) will continually increase the magnitude of \(\boldsymbol{w}\) and, in theory, will <strong>never halt</strong>.</li>
      <li><strong style="color: blue">What models are affected by this? [CFS]</strong><br />
 Many models that use MLE, log-likelihood/cross-entropy. Namely, <strong>Logistic Regression</strong>.</li>
    </ol>

    <p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Show [LAP] Problems</button></p>
    <ol hidden="">
      <li><strong style="color: blue">Define the Moore-Penrose Pseudoinverse:</strong>
        <p>$$\boldsymbol{X}^{+}=\lim_{\alpha \searrow 0}\left(\boldsymbol{X}^{\top} \boldsymbol{X}+\alpha \boldsymbol{I}\right)^{-1} \boldsymbol{X}^{\top} \tag{7.29}$$</p>
      </li>
      <li><strong style="color: blue">What can it solve? How?</strong><br />
 It can solve <em><strong>underdetermined linear equations</strong></em>.<br />
 When applied to underdetermined systems w/ non-unique solutions; It finds the minimum norm solution to a linear system.</li>
      <li><strong style="color: blue">What does it correspond to in terms of regularization?</strong><br />
 The equation corresponds to <strong>performing linear regression with weight-decay</strong>.</li>
      <li><strong style="color: blue">What is the limit wrt?</strong><br />
 \(7.29\) is the limit of eq \(7.17\) as the <em>regularization coefficient \(\alpha\) shrinks to zero</em>.</li>
      <li><strong style="color: blue">How can we interpret the pseudoinverse wrt regularization?</strong><br />
 We can thus interpret the pseudoinverse as <strong>stabilizing underdetermined problems using regularization</strong>.</li>
    </ol>

    <p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Show [CFS] Problems</button></p>
    <ol hidden="">
      <li><strong style="color: blue">Explain the problem with Logistic Regression:</strong><br />
 <strong>Logistic regression on a linearly separable dataset</strong>, if a weight vector \(\boldsymbol{w}\) is able to achieve perfect classification, then so does \(2\boldsymbol{w}\) but with even <strong>higher likelihood</strong>. Thus, an iterative optimization procedure (sgd) will continually increase the magnitude of \(\boldsymbol{w}\) and, in theory, will <strong>never halt</strong>.</li>
      <li><strong style="color: blue">What are the possible solutions?</strong><br />
 Regularization: e.g. <strong>weight decay</strong>.</li>
      <li><strong style="color: blue">Are there any guarantees that we achieve with regularization? How?</strong><br />
 We can use regularization to guarantee the convergence of iterative methods applied to underdetermined problems.<br />
 Weight decay will cause gradient descent to <em>quit increasing the magnitude of the weights when the <strong>slope of the likelihood is equal to the weight decay coefficient</strong></em>.</li>
    </ol>
  </li>
  <li><strong style="color: red">Describe dataset augmentation and its techniques:</strong><br />
 Having more data is the most desirable thing to improving a machine learning model’s performance. In many cases, it is relatively easy to artificially generate data.</li>
  <li><strong style="color: red">When is dataset augmentation applicable?</strong><br />
 For certain problems like <strong>classification</strong> this approach is readily usable. E.g. for a classification task, we require the model to be <em>invariant to certain types of transformations</em>, of which we can generate data by applying them on our current dataset.</li>
  <li><strong style="color: red">When is it not?</strong><br />
 This approach is not applicable to many problems, especially those that require us to learn the true data-distribution first E.g. Density Estimation.</li>
  <li><strong style="color: red">Motivate the Noise Robustness property:</strong><br />
 Noise Robustness is an important property for ML models:
    <ol>
      <li>For many classification and (some) regression tasks: the task should be possible to solve even if small random noise is added to the input <a href="/work_files/research/dl/theory/dl_book_pt1#bodyContents32">(Local Constancy)</a></li>
      <li>Moreover, NNs prove not to be very robust to noise.</li>
    </ol>
  </li>
  <li><strong style="color: red">How can Noise Robustness motivate a regularization technique?</strong><br />
 To increase noise robustness, we will need to somehow teach our models to ignore noise. That can be done by teaching it to model data that is, perhaps, more noisy than what we actually have; thus, regularizing it from purely and completely fitting the training data.</li>
  <li>
    <p><strong style="color: red">How can we enhance noise robustness in NN?</strong><br />
 By <strong>Noise Injection</strong> in different parts of the network.</p>

    <p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Show</button></p>
    <ol hidden="">
      <li><strong style="color: blue">Give a motivation for Noise Injection:</strong><br />
 It can be seen as a form of <strong>data augmentation</strong>.</li>
      <li><strong style="color: blue">Where can noise be injected?</strong>
        <ol>
          <li>Input Layer</li>
          <li>Hidden Layer</li>
          <li>Weight Matrices</li>
          <li>Output Layer</li>
        </ol>
      </li>
      <li><strong style="color: blue">Give Motivation, Interpretation and Applications of injecting noise in the different components (from above):</strong><br />
 <strong style="color: red">Injecting Noise in the Input Layer:</strong>
        <ol>
          <li><strong>Motivation</strong>:<br />
 We have motivated the injection of noise, to the inputs, as a dataset augmentation strategy.</li>
          <li><strong>Interpretation</strong>:<br />
 For some models, the addition of noise with infinitesimal variance at the input of the model is equivalent to <strong>imposing a penalty on the norm of the weights</strong> <em>(Bishop, 1995a,b)</em>.</li>
        </ol>

        <p id="lst-p"><strong style="color: red">Injecting Noise in the Hidden Layers:</strong></p>
        <ol>
          <li><strong>Motivation</strong>:<br />
 We can motivate it as a variation of data augmentation.</li>
          <li><strong>Interpretation</strong>:<br />
 It can be seen as doing <strong>data-augmentation</strong> at <em><strong>multiple levels of abstraction</strong></em>.</li>
          <li><strong>Applications</strong>:<br />
 The most successful application of this type of noise injection is <strong>Dropout</strong>.<br />
 It can be seen as a process of constructing new inputs by <em>multiplying</em> by noise.</li>
        </ol>

        <p id="lst-p"><strong style="color: red">Injecting Noise in the Weight Matrices:</strong></p>
        <ol>
          <li><strong>Interpretation</strong>:
            <ol>
              <li>It can be interpreted as a stochastic implementation of Bayesian inference over the weights.
                <ol>
                  <li><strong>The Bayesian View</strong>:<br />
 The Bayesian treatment of learning would consider the model weights to be <em>uncertain and representable via a probability distribution that reflects this uncertainty</em>. Adding noise to the weights is a practical, stochastic way to reflect this uncertainty.</li>
                </ol>
              </li>
              <li>It can, also, be interpreted as equivalent a more traditional form of regularization, <em>encouraging stability of the function to be learned</em>.
                <ol>
                  <li><button class="showText" value="show" onclick="showTextPopHide(event);">Analysis</button>
 <img src="/main_files/dl_book/regularization/3.png" alt="img" width="100%" hidden="" /></li>
                </ol>
              </li>
            </ol>
          </li>
          <li><strong>Applications</strong>:<br />
 This technique has been used primarily in the context of <strong>recurrent neural networks</strong> <em>(Jim et al., 1996; Graves, 2011)</em>.</li>
        </ol>

        <p id="lst-p"><strong style="color: red">Injecting Noise in the Output Layer:</strong></p>
        <ol>
          <li><strong>Motivation</strong>:
            <ol>
              <li>Most datasets have some number of mistakes in the \(y\) labels. It can be harmful to maximize \(\log p(y | \boldsymbol{x})\) when \(y\) is a mistake. One way to prevent this is to explicitly model the noise on the labels.<br />
 One can assume that for some small constant \(\epsilon\), the training set label \(y\) is correct with probability \(1-\epsilon\).<br />
 This assumption is easy to incorporate into the cost function analytically, rather than by explicitly drawing noise samples (e.g. <strong>label smoothing</strong>).</li>
              <li>MLE with a softmax classifier and hard targets may never converge - the softmax can never predict a probability of exactly \(0\) or \(1\), so it will continue to learn larger and larger weights, making more extreme predictions forever.{: #bodyContents33mle}</li>
            </ol>
          </li>
          <li><strong>Interpretation</strong>:<br />
 For some models, the addition of noise with infinitesimal variance at the input of the</li>
          <li><strong>Applications</strong>:<br />
 <strong>Label Smoothing</strong> regularizes a model based on a softmax with \(k\) output values by replacing the hard \(0\) and \(1\) classification targets with targets of \(\dfrac{\epsilon}{k-1}\) and \(1-\epsilon\), respectively.
            <ol>
              <li><a href="#bodyContents33mle"><strong>Applied to MLE problem:</strong></a> Label smoothing, compared to weight-decay, has the advantage of preventing the pursuit of hard probabilities without discouraging correct classification.</li>
              <li>Application in modern NN: <em>(Szegedy et al. 2015)</em></li>
            </ol>
          </li>
        </ol>
      </li>
    </ol>

    <p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Show further questions</button></p>
    <ol hidden="">
      <li><strong style="color: blue">Give an interpretation for injecting noise in the Input layer:</strong><br />
 For some models, the addition of noise with infinitesimal variance at the input of the model is equivalent to <strong>imposing a penalty on the norm of the weights</strong> <em>(Bishop, 1995a,b)</em>.</li>
      <li><strong style="color: blue">Give an interpretation for injecting noise in the Hidden layers:</strong><br />
 It can be seen as doing <strong>data-augmentation</strong> at <em><strong>multiple levels of abstraction</strong></em>.</li>
      <li><strong style="color: blue">What is the most successful application of this technique:</strong><br />
 The most successful application of this type of noise injection is <strong>Dropout</strong>.</li>
      <li><strong style="color: blue">Describe the Bayesian View of learning:</strong><br />
 The Bayesian treatment of learning would consider the model weights to be <em>uncertain and representable via a probability distribution that reflects this uncertainty</em>. Adding noise to the weights is a practical, stochastic way to reflect this uncertainty.</li>
      <li><strong style="color: blue">How does it motivate injecting noise in the weight matrices?</strong><br />
 It can be interpreted as a stochastic implementation of Bayesian inference over the weights.</li>
      <li><strong style="color: blue">Describe a different, more traditional, interpretation of injecting noise to matrices. What are its effects on the function to be learned?</strong><br />
 It can, also, be interpreted as equivalent a more traditional form of regularization, <em>encouraging stability of the function to be learned</em>.</li>
      <li><strong style="color: blue">Whats the biggest application for this kind of regularization?</strong><br />
 This technique has been used primarily in the context of <strong>recurrent neural networks</strong> <em>(Jim et al., 1996; Graves, 2011)</em>.</li>
      <li><strong style="color: blue">Motivate injecting noise in the Output layer:</strong>
        <ol>
          <li>Most datasets have some number of mistakes in the \(y\) labels. It can be harmful to maximize \(\log p(y | \boldsymbol{x})\) when \(y\) is a mistake. One way to prevent this is to explicitly model the noise on the labels.<br />
 One can assume that for some small constant \(\epsilon\), the training set label \(y\) is correct with probability \(1-\epsilon\).<br />
 This assumption is easy to incorporate into the cost function analytically, rather than by explicitly drawing noise samples (e.g. <strong>label smoothing</strong>).</li>
          <li>MLE with a softmax classifier and hard targets may never converge - the softmax can never predict a probability of exactly \(0\) or \(1\), so it will continue to learn larger and larger weights, making more extreme predictions forever.{: #bodyContents33mle}</li>
        </ol>
      </li>
      <li><strong style="color: blue">What is the biggest application of this technique?</strong><br />
 <strong>Label Smoothing</strong> regularizes a model based on a softmax with \(k\) output values by replacing the hard \(0\) and \(1\) classification targets with targets of \(\dfrac{\epsilon}{k-1}\) and \(1-\epsilon\), respectively.</li>
      <li><strong style="color: blue">How does it compare to weight-decay when applied to MLE problems?</strong><br />
 <a href="#bodyContents33mle"><strong>Applied to MLE problem:</strong></a> Label smoothing, compared to weight-decay, has the advantage of preventing the pursuit of hard probabilities without discouraging correct classification.</li>
    </ol>
  </li>
  <li><strong style="color: red">Define “Semi-Supervised Learning”:</strong>   <br />
 <strong>Semi-Supervised Learning</strong> is a class of ML tasks and techniques that makes use of both unlabeled examples from \(P(\mathbf{x})\) and labeled examples from \(P(\mathbf{x}, \mathbf{y})\) to estimate \(P(\mathbf{y} | \mathbf{x})\) or predict \(\mathbf{y}\) from \(\mathbf{x}\).
    <ol>
      <li><strong style="color: blue">What does it refer to in the context of DL:</strong><br />
 In the context of Deep Learning, Semi-Supervised Learning usually refers to <em>learning a representation \(\boldsymbol{h}=f(\boldsymbol{x})\)</em>.</li>
      <li><strong style="color: blue">What is its goal?</strong><br />
 The goal is to learn a representation such that <strong>examples from the same class have similar representations</strong>.</li>
      <li><strong style="color: blue">Give an example in classical ML:</strong><br />
 Using <strong>unsupervised learning</strong> to build representations: <strong>PCA</strong>, as a preprocessing step before applying a classifier, is a long-standing variant of this approach.</li>
    </ol>
  </li>
  <li><strong style="color: red">Describe an approach to applying semi-supervised learning:</strong><br />
 Instead of separating the supervised and unsupervised criteria, we can instead have a generative model of \(P(\mathbf{x})\) (or \(P(\mathbf{x}, \mathbf{y})\)) which shares parameters with a discriminative model \(P(\mathbf{y} \vert \mathbf{x})\).<br />
 The idea is to share the unsupervised/generative criterion with the supervised criterion to <em>express a prior belief that the structure of \(P(\mathbf{x})\) (or \(P(\mathbf{x}, \mathbf{y})\)) is connected to the structure of \(P(\mathbf{y} \vert \mathbf{x})\)</em>, which is captured by the <em>shared parameters</em>.<br />
 By controlling how much of the generative criterion is included in the total criterion, one can find a better trade-off than with a purely generative or a purely discriminative training criterion <em>(Lasserre et al., 2006; Larochelle and Bengio, 2008)</em>.</li>
  <li>
    <p><strong style="color: red">How can we interpret dropout wrt data augmentation?</strong><br />
 Dropout can be seen as a process of constructing new inputs by multiplying by noise.</p>
  </li>
  <li><strong style="color: red">Describe Multitask Learning as regularization:</strong><br />
 The idea is to improve the generalization error by pooling together examples from multiple tasks. Similar to how more data leads to more generalization, using a part of the model for different tasks constrains that part to learn good values.</li>
  <li><strong style="color: red">List the types:</strong></li>
</ol>

<ol hidden="">
  <li><strong style="color: red">Add Answers from link below for L2 applied to linear regression and how it reduces variance:</strong><br />
 <a href="http://cs229.stanford.edu/notes-spring2019/addendum_bias_variance.pdf">Link</a></li>
  <li>
    <p><strong style="color: red">When is Ridge regression favorable over Lasso regression? for correlated features?</strong><br />
 If there exists a subset consisting of few variables that have medium / large sized effect, use lasso regression. In presence of many variables with small / medium sized effect, use ridge regression.?? (ESL authors)</p>

    <p>If features are correlated, it is so hard to determine which variables to drop, it is often better not to drop variables. Thus, use <strong>ridge</strong> over <strong>lasso</strong> since the latter produces non-unique solutions and might drop random features; while former, spreads weight more evenly.</p>
  </li>
</ol>

<hr />

<h1 id="misc">Misc.</h1>
<p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Misc.</button></p>
<ol hidden="">
  <li><strong style="color: red">Explain Latent Dirichlet Allocation (LDA)</strong><br />
 Latent Dirichlet Allocation (LDA) is a common method of topic modeling, or classifying documents by subject matter.<br />
 LDA is a generative model that represents documents as a mixture of topics that each have their own probability distribution of possible words.<br />
 The “Dirichlet” distribution is simply a distribution of distributions. In LDA, documents are distributions of topics that are distributions of words.</li>
  <li><strong style="color: red">How to deal with curse of dimensionality</strong>
    <ol>
      <li>Feature Selection</li>
      <li>Feature Extraction</li>
    </ol>
  </li>
  <li><strong style="color: red">How to detect correlation of “categorical variables”?</strong>
    <ol>
      <li>Chi-Squared test</li>
    </ol>
  </li>
  <li><strong style="color: red">Define “marginal likelihood” (wrt naive bayes):</strong><br />
 Marginal likelihood is, the probability that the word ‘FREE’ is used in any message (not given any other condition?).</li>
  <li><strong style="color: red">KNN VS K-Means</strong>
    <ol>
      <li><strong>KNN</strong>: Supervised, Classification/Regression Algorithm</li>
      <li><strong>K-Means</strong>: Unsupervised Clustering Algorithm</li>
    </ol>
  </li>
  <li>
    <p><strong style="color: red">When is Ridge regression favorable over Lasso regression for correlated features?</strong><br />
 If there are exists a subset consisting of few variables that have medium / large sized effect, use lasso regression. In presence of many variables with small / medium sized effect, use ridge regression.?? (ESL authors)</p>

    <p>If features are correlated; it is so hard to determine which variables to drop, it is often better not to drop variables. Thus, use <strong>ridge</strong> over <strong>lasso</strong> since the latter produces non-unique solutions and might drop random features; while former, spreads weight more evenly.</p>
  </li>
  <li><strong style="color: red">What is convex hull ?</strong><br />
 In case of linearly separable data, convex hull represents the outer boundaries of the two group of data points. Once convex hull is created, we get maximum margin hyperplane (MMH) as a perpendicular bisector between two convex hulls. MMH is the line which attempts to create greatest separation between two groups.</li>
  <li><strong style="color: red">Do you suggest that treating a categorical variable as continuous variable would result in a better predictive model?</strong><br />
 For better predictions, categorical variable can be considered as a continuous variable only when the variable is ordinal in nature.</li>
  <li><strong style="color: red">OLS vs MLE</strong><br />
 They both estimate parameters in the model. They are the same in the case of normal distribution.</li>
  <li><strong style="color: red">What are collinearity and multicollinearity?</strong>
    <ol>
      <li><strong>Collinearity</strong> occurs when two predictor variables (e.g., x1 and x2) in a multiple regression have some correlation.</li>
      <li><strong>Multicollinearity</strong> occurs when more than two predictor variables (e.g., x1, x2, and x3) are inter-correlated.</li>
    </ol>
  </li>
  <li><strong style="color: red">Describe ways to overcome scaling (scalability) issues:</strong><br />
 nystrom methods/low-rank kernel matrix approximations, random features, local by query/near neighbors
    <ul>
      <li><strong style="color: blue">Topic Modeling vs Document Classification:</strong>
        <ul>
          <li><strong>Text Classification</strong> is a form of supervised learning, hence the set of possible classes are known/defined in advance, and won’t change.</li>
          <li><strong>Topic Modeling</strong> is a form of unsupervised learning (akin to clustering), so the set of possible topics are unknown apriori. They’re defined as part of generating the topic models.  <br />
            <ul>
              <li><strong style="color: red">Why are deep networks better than shallow ones?</strong><br />
 Both shallow and deep networks are capable of approximating any function. For the same level of accuracy, deeper networks can be much more efficient in terms of computation and number of parameters. Deeper networks are able to create deep representations, at every layer, the network learns a new, more abstract representation of the input.</li>
              <li><strong style="color: red">What is backpropagation?</strong><br />
 Backpropagation is a training algorithm used for a multilayer neural networks. It moves the error information from the end of the network to all the weights inside the network and thus allows for efficient computation of the gradient.<br />
 <a href="https://www.mladdict.com/neural-network-simulator">Neural Network Simulator</a><br />
 The backpropagation algorithm can be divided into several steps:
                <ol>
                  <li>Forward propagation of training data through the network in order to generate output.</li>
                  <li>Use target value and output value to compute error derivative with respect to output activations.</li>
                  <li>Backpropagate to compute the derivative of the error with respect to output activations in the previous layer and continue for all hidden layers.</li>
                  <li>Use the previously calculated derivatives for output and all hidden layers to calculate the error derivative with respect to weights.</li>
                  <li>Update the weights.</li>
                </ol>
              </li>
              <li><strong style="color: red">How do you avoid Overfitting?</strong></li>
              <li><strong style="color: red">How do you avoid Underfitting?</strong></li>
              <li><strong style="color: red">What is the difference between covariance and correlation?</strong><br />
 Correlation is the standardized form of covariance.<br />
 Covariances are difficult to compare. For example: if we calculate the covariances of salary ($) and age (years), we’ll get different covariances which can’t be compared because of having unequal scales. To combat such situation, we calculate correlation to get a value between -1 and 1, irrespective of their respective scale.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<hr />

<h1 id="feedforward-neural-network">FeedForward Neural Network</h1>
<p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">FeedForward Neural Network</button></p>
<ul hidden="">
  <li><strong style="color: red">Define “FeedForward Neural Network”:</strong><br />
  The <strong>FeedForward Neural Network (FNN)</strong> is an <em>artificial neural network</em> wherein the connections between the nodes do <em>not</em> form a <em>cycle</em>, allowing the information to move only in one direction, forward, from the input layer to the subsequent layers.</li>
  <li><strong style="color: red">What is the Architecture of an FFN:</strong><br />
  An FNN consists of one or more layers, each consisting of nodes (simulating biological neurons) that hold a certain <em>wight value \(w _ {ij}\).</em> Those weights are usually multiplied by the input values (in the input layer) in each node and, then, summed; finally, one can apply some sort of activation function on the multiplied values to simulate a response (e.g. 1-0 classification).</li>
  <li><strong style="color: red">List the Classes of FNNs:</strong><br />
  There are many variations of FNNs. As long as they utilize FeedForward control signals and have a layered structure (described above) they are a type of FNN:
    <ul>
      <li>Single-Layer Perceptron</li>
      <li>Multi-Layer Perceptron</li>
    </ul>

    <p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Further Questions</button></p>
    <ul hidden="">
      <li><strong style="color: blue">Describe the “Single-Layer Perceptron”:</strong><br />
  A <span style="color: purple">linear binary classifier</span>, the <strong>single-layer perceptron</strong> is the simplest feedforward neural network.<br />
  It consists of a single layer of output nodes; the inputs are multiplied by a series of weights, effectively, being fed directly to the outputs where they values are summed in each node, and if the value is above some threshold (typically 0) the neuron fires and takes the activated value (typically 1); otherwise it takes the deactivated value (typically 0).
        <p>$$f(\mathbf{x})=\left\{\begin{array}{ll}{1} &amp; {\text { if } \mathbf{w} \cdot \mathbf{x}+b&gt;0} \\ {0} &amp; {\text { otherwise }}\end{array}\right.$$</p>
        <p>In the context of neural networks, a perceptron is an artificial neuron using the <a href="https://en.wikipedia.org/wiki/Heaviside_step_function"><strong>Heaviside step function</strong></a> as the activation function.</p>
      </li>
      <li><strong style="color: blue">Describe the “Multi-Layer Perceptron”:</strong><br />
  This class of networks consists of multiple layers of computational units, usually interconnected in a feed-forward way. Each neuron in one layer has directed connections to the neurons of the subsequent layer.<br />
  In many applications the units of these networks apply a sigmoid function as an activation function.</li>
    </ul>
  </li>
</ul>

<hr />

<h1 id="multilayer-perceptron">Multilayer Perceptron</h1>
<p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Multilayer Perceptron</button></p>
<ul hidden="">
  <li><strong style="color: red">Define the “Multi-Layer Perceptron”:</strong><br />
  The <strong>Multilayer Perceptron (MLP)</strong> is a class of <em>FeedForward Neural Networks</em> that is used for learning from data.</li>
  <li><strong style="color: red">What is the Architecture of an MLP:</strong><br />
  <button class="showText" value="show" onclick="showTextPopHide(event);">Further Questions</button>
    <ul hidden="">
      <li><strong style="color: blue">What are the Layers:</strong><br />
  The MLP consists of at least three layers of nodes: <em><strong>input layer</strong></em>, <em><strong>hidden layer</strong></em>, and an <em><strong>output layer</strong></em>.</li>
      <li><strong style="color: blue">What are the Connections:</strong><br />
  The layers in a neural network are connected by certain weights and the MLP is known as a <strong>fully-connected network</strong> where each neuron in one layer is connected with a weight \(w_{ij}\) to every node in the following layer.</li>
      <li><strong style="color: blue">What else is important to make it multi-layer? why?</strong><br />
  Each node (except for the input nodes) uses a <strong>non-linear activation function</strong> that were developed to <em>model the frequency of <strong>action potential</strong> (firing) of biological neurons</em>.</li>
    </ul>
  </li>
  <li><strong style="color: red">Describe “Learning” of an MLP:</strong><br />
  The MLP employs a <strong>supervised learning</strong> technique called <strong>backpropagation</strong>.<br />
  Learning occurs by changing the weights, connecting the layers, based on the amount of error in the output compared to the expected result. Those weights are changed by using <em>gradient-methods</em> to optimize a, given, objective function (called the <strong>loss function</strong>).</li>
  <li><strong style="color: red">List the properties of the MLP:</strong>
    <ul>
      <li>Due to their <em>non-linearity</em>, MLPs can distinguish and model non-linearly-separable data</li>
      <li>According to <a href="https://pdfs.semanticscholar.org/05ce/b32839c26c8d2cb38d5529cf7720a68c3fab.pdf"><strong>Cybenko’s Theorem</strong></a>, MLPs are <em><strong>universal function approximators</strong></em>; thus, they can be used for <em>regression analysis</em> and, by extension, <em>classification</em></li>
      <li>Without the <em>non-linear activation functions</em>, MLPs will be identical to <strong>Perceptrons</strong>, since Linear Algebra shows that the linear transformations in many hidden layers can be collapsed into one linear-transformation</li>
    </ul>
  </li>
</ul>

<hr />

<h1 id="deep-feedforward-neural-networks">Deep Feedforward Neural Networks</h1>
<p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Deep Feedforward Neural Networks</button></p>
<ul hidden="">
  <li><strong style="color: red">Describe the Deep Feedforward Neural Networks:</strong><br />
  <button class="showText" value="show" onclick="showTextPopHide(event);">Further Questions</button>
    <ul hidden="">
      <li><strong style="color: blue">As a “Classifier”:</strong><br />
  Given \(y=f^{\ast}(\boldsymbol{x})\), a DNN maps an input \(\boldsymbol{x}\) to a category \(y\).</li>
      <li><strong style="color: blue">How does it solve the Classification/Regression Problems?</strong><br />
  An <strong>FNN</strong> defines a mapping \(\boldsymbol{y}=f(\boldsymbol{x} ; \boldsymbol{\theta})\) and learns the value of the parameters \(\boldsymbol{\theta}\)  that result in the best function approximation.</li>
      <li><strong style="color: blue">How does it model the targets?</strong><br />
  It models the targets \(\boldsymbol{y}\) as a function \(\boldsymbol{y}=f(\boldsymbol{x} ; \boldsymbol{\theta})\).</li>
      <li><strong style="color: blue">What does it learn?</strong><br />
  It learns parameters \(\boldsymbol{\theta}\) that result would best approximate the data.</li>
      <li><strong style="color: blue">What is its goal?</strong><br />
  A balance between:
        <ol>
          <li>Function Approximation</li>
          <li>Statistical Generalization</li>
        </ol>
      </li>
      <li><strong style="color: blue">Why are they called “networks”/how are they represented?:</strong>
        <ul>
          <li>FNNs are called <strong>networks</strong> because they are typically represented by composing together many different functions.</li>
          <li>The model is associated with a <strong>DAG</strong> describing how the functions are composed together.</li>
          <li>Functions connected in a <strong>chain structure</strong> are the most commonly used structure of neural networks.<br />
  E.g. we might have three functions \(f^{(1)}, f^{(2)},\) and \(f^{(3)}\) connected in a chain, to form \(f(\boldsymbol{x})=f^{(3)}\left(f^{(2)}\left(f^{(1)}(\boldsymbol{x})\right)\right)\); being called the \(n\)-th <strong>Layer</strong> respectively.</li>
        </ul>
      </li>
    </ul>

    <p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Further Further Questions</button></p>
    <ul hidden="">
      <li><strong style="color: blue">How are the Functions composed together?</strong>  <br />
  Functions connected in a <strong>chain structure</strong> are the most commonly used structure of neural networks.<br />
  E.g. we might have three functions \(f^{(1)}, f^{(2)},\) and \(f^{(3)}\) connected in a chain, to form \(f(\boldsymbol{x})=f^{(3)}\left(f^{(2)}\left(f^{(1)}(\boldsymbol{x})\right)\right)\); being called the \(n\)-th <strong>Layer</strong> respectively.</li>
      <li><strong style="color: blue">How is this composition described?</strong><br />
  The model is associated with a <strong>DAG</strong> describing how the functions are composed together.</li>
      <li><strong style="color: blue">How do we define Depth:</strong><br />
  The overall length of the chain is the <strong>depth</strong> of the model.</li>
      <li><strong style="color: blue">What does the Training Data provide? and how do we learn from it?</strong><br />
  During training, we drive \(f(\boldsymbol{x})\) to match \(f^{\ast}(\boldsymbol{x}) = \boldsymbol{y}\). <br />
  The training data provides us with noisy, approximate examples of \(f^{\ast}(\boldsymbol{x})\) evaluated at different training points.</li>
    </ul>
  </li>
  <li><strong style="color: red">Describe the Motivation for Deep FFNs:</strong><br />
  We can motivate Deep FFNs as an <span style="color: purple">extension of <strong>linear models</strong></span>.<br />
  The biggest <em><strong>limitation</strong></em> of linear models is that <span style="color: purple"><em><strong>model capacity</strong></em> is limited to <strong>linear functions</strong></span>.<br />
  To <strong>extend</strong> linear models to represent non-linear functions of \(\boldsymbol{x}\) we can:
    <ul>
      <li>Apply the linear model not to \(\boldsymbol{x}\) itself but <span style="color: purple"><em>to a transformed input</em> \(\phi(\boldsymbol{x})\)</span>, where \(\phi\) is a <span style="color: purple"><strong>nonlinear transformation</strong></span>.</li>
      <li>Equivalently, apply the <strong>kernel trick</strong> to obtain nonlinear learning algorithm based on implicitly applying the \(\phi\) mapping.</li>
    </ul>

    <p><span style="color: goldenrod">We can think of \(\phi\) as providing a <strong>set of features <em>describing</em></strong> \(\boldsymbol{x}\), or as providing a <strong>new representation</strong> for \(\boldsymbol{x}\).</span></p>

    <p id="lst-p"><strong>Choosing the mapping \(\phi\):</strong></p>
    <ol>
      <li>Use a very generic \(\phi\), s.a. infinite-dimensional (RBF) kernel.<br />
 <strong>Problems:</strong>
        <ul>
          <li>If \(\phi(\boldsymbol{x})\) is of <em>high enough dimension</em>, we can <em>always have enough capacity</em> to fit the training set, but <em>generalization</em> to the test set often <em>remains poor</em>.</li>
          <li>Very generic feature mappings are usually <em>based only</em> on the <em>principle of local smoothness</em> and do not encode enough prior information to solve advanced problems.</li>
        </ul>
      </li>
      <li>Manually Engineer \(\phi\).<br />
 <strong>Problems:</strong>
        <ul>
          <li>Requires decades of human effort and the results are usually poor and non-scalable.</li>
        </ul>
      </li>
      <li>The <em>strategy</em> of <em>deep learning</em> is to <strong>learn \(\phi\)</strong>. We have a model:
        <ul>
          <li>We have a <strong>model</strong>:
            <p>$$y=f(\boldsymbol{x} ; \boldsymbol{\theta}, \boldsymbol{w})=\phi(\boldsymbol{x} ; \boldsymbol{\theta})^{\top} \boldsymbol{w}$$</p>
            <p>We now have <strong>parameters \(\theta\)</strong> that we <span style="color: purple">use to <strong>learn \(\phi\)</strong> from a <span style="color: purple"><em><strong>broad class of functions</strong></em></span></span>, and <strong>parameters \(\boldsymbol{w}\)</strong> that <span style="color: purple"><strong>map</strong> from <em>\(\phi(\boldsymbol{x})\)</em> to the <em>desired output</em></span>.<br />
  This is an example of a <strong>deep FNN</strong>, with \(\phi\) defining a <strong>hidden layer</strong>.</p>
          </li>
          <li>This approach is the <em>only one</em> of the three that <em>gives up</em> on the <em>convexity</em> of the training problem, but the <em>benefits outweigh the harms</em>.</li>
          <li>In this approach, we parametrize the representation as \(\phi(\boldsymbol{x}; \theta)\) and use the optimization algorithm to find the \(\theta\) that corresponds to a good representation.</li>
          <li><strong>Advantages:</strong>
            <ul>
              <li><strong>Capturing the benefit of the <em>first</em> approach</strong>:<br />
  by being highly generic — we do so by using a very broad family \(\phi(\boldsymbol{x};\theta)\).</li>
              <li><strong>Capturing the benefit of the <em>second</em> approach</strong>:<br />
  Human practitioners can encode their knowledge to help generalization by designing families \(\phi(\boldsymbol{x}; \theta)\) that they expect will perform well.<br />
  The <strong>advantage</strong> is that the human designer only needs to find the right general function family rather than finding precisely the right function.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ol>

    <p>Thus, we can <em>motivate</em> <strong>Deep NNs</strong> as a way to do <span style="color: goldenrod"><strong>automatic, <em>non-linear</em> feature extraction</strong></span> from the <strong>inputs</strong>.</p>

    <p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Further Questions</button></p>
    <ul hidden="">
      <li><strong style="color: blue">topic:</strong></li>
    </ul>
  </li>
  <li><strong style="color: red">Interpretation of Neural Networks</strong><br />
  It is best to think of feedforward networks as <strong>function approximation machines</strong> that are designed to achieve <span style="color: purple"><em>statistical generalization</em></span>, occasionally drawing some insights from what we know about the brain, rather than as models of brain function.</li>
</ul>

<hr />

<h1 id="autoencoders">AutoEncoders</h1>
<p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">AutoEncoders</button></p>
<ol hidden="">
  <li><strong style="color: red">What is an AutoEncoder? What is its goal? (draw a diagram)</strong><br />
 An <strong>Autoencoder</strong> is an ANN used for unsupervised learning of efficient codings/representations.<br />
 It aims to learn a <em>good</em> representation (encoding) for a set of data, typically lower-dimensional.<br />
 <img src="/main_files/cs231n/aencdrs/1.png" alt="img" width="25%" /></li>
  <li><strong style="color: red">What type of NN is the Autoencoder?</strong><br />
 A Feedforward NN.</li>
  <li><strong style="color: red">Give Motivation for AutoEncoders:</strong><br />
 <a href="/work_files/research/dl/archits/aencdrs#bodyContents10">Autoencoder are a generalization of <strong>PCA</strong> to non-linear projections</a>.<br />
 <strong>Auto-Encoders</strong> allows us to deal with <em>curved manifolds</em> in the input space by using deep layers, where the <span style="color: purple"><strong>code</strong> is a <em>non-linear function</em> of the <strong>input</strong></span>, and the <span style="color: purple"><strong><em>reconstruction</em> of the data</strong> from the code is, also, a <em>non-linear function</em> of the <strong>code</strong></span>.</li>
  <li><strong style="color: red">Why Deep AutoEncoders? What do they allow us to do?</strong><br />
 They provide a really nice way to do <span style="color: goldenrod"><strong><em>non-linear</em> dimensionality reduction</strong></span>:
    <ul>
      <li>They provide <strong>flexible mappings</strong> <strong><em>both</em></strong> ways</li>
      <li>The <span style="color: goldenrod">learning time is linear</span> (or better) in the number of training examples</li>
      <li>The final encoding model/<strong>Encoder</strong> is fairly <em><strong>compact</strong></em> and <em><strong>fast</strong></em></li>
    </ul>
  </li>
  <li><strong style="color: red">List the Advantages of Depth in AutoEncoders:</strong>
    <ul>
      <li><strong>Computational Efficiency:</strong> Depth can <strong>exponentially reduce the computational cost</strong> of representing some functions</li>
      <li><strong>Statistical Efficiency:</strong> Depth can <strong>exponentially decrease the amount of training data</strong> needed to learn some functions</li>
      <li><strong>Representational Efficiency:</strong> Experimentally, deep Autoencoders yield <strong>better compression</strong> compared to shallow or linear Autoencoders</li>
    </ul>
  </li>
  <li><strong style="color: red">List the Applications of AutoEncoders (and historical information):</strong><br />
 The applications of auto-encoders have changed overtime.<br />
 This is due to the advances in the fields that auto-encoders were applied in, or to the incompetency of the auto-encoders.
    <ul>
      <li>Historically:
        <ul>
          <li>Information Retrieval</li>
          <li>Anomaly Detection</li>
        </ul>
      </li>
      <li>Recently, auto-encoders are applied to:
        <ul>
          <li><strong>Data-Denoising</strong></li>
          <li><strong>Dimensionality Reduction</strong> (for data visualization)</li>
          <li>Drug discovery</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong style="color: red">Describe the Training of Deep AutoEncoders:</strong><br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">Extra</button>
    <ol hidden="">
      <li><strong style="color: blue">What are the challenges if any?</strong><br />
 Training Deep Autoencoders is very challenging:
        <ul>
          <li>It is difficult to optimize deep Autoencoders using backpropagation</li>
          <li>With small initial weights the backpropagated gradient dies</li>
        </ul>
      </li>
      <li><strong style="color: blue">What are the main methods for training Deep AutoEncoders?</strong>
        <ul>
          <li>Just initialize the weights carefully as in Echo-State Nets. (No longer used)</li>
          <li>Use unsupervised layer-by-layer pre-training. (<em>Hinton</em>)<br />
  This method involves treating each neighbouring set of two layers as a restricted Boltzmann machine so that the pretraining approximates a good solution, then using a backpropagation technique to fine-tune the results. This model takes the name of <strong>deep belief network</strong>.</li>
          <li>Joint Training (most common)<br />
  This method involves training the whole architecture together with a single global reconstruction objective to optimize.</li>
        </ul>
      </li>
      <li><strong style="color: blue">Which one is the superior method?</strong><br />
 A study published in 2015 empirically showed that the joint training method not only learns better data models, but also learned more representative features for classification as compared to the layerwise method.<br />
 The success of joint training, however, is mostly attributed (depends heavily) on the <strong>regularization strategies</strong> adopted in the modern variants of the model.</li>
    </ol>

    <p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Extra Extra</button></p>
    <ol hidden="">
      <li><strong style="color: blue">How is Joint Training better:</strong><br />
 It not only learns better data models, but also learned more representative features for classification as compared to the layerwise method.</li>
      <li><strong style="color: blue">Why is Joint Training better:</strong><br />
 The success of joint training is mostly attributed (depends heavily) on the <strong>regularization strategies</strong> adopted in the modern variants of the model.</li>
    </ol>
  </li>
  <li><strong style="color: red">Describe the Architecture of AutoEncoders:</strong>
    <ul>
      <li>An auto-encoder consists of:
        <ul>
          <li>An Encoding Function</li>
          <li>A Decoding Function</li>
          <li>A Distance Function</li>
        </ul>
      </li>
      <li>We choose the <strong>encoder</strong> and <strong>decoder</strong> to be <span style="color: purple">parametric functions</span> (typically <span style="color: purple">neural networks</span>), and to be <span style="color: purple">differentiable</span> with respect to the distance function, so the parameters of the encoding/decoding functions can be optimized to minimize the reconstruction loss, using Stochastic Gradient Descent.</li>
    </ul>

    <ol>
      <li><strong style="color: blue">What is the simplest form of an AE:</strong><br />
 The simplest form of an Autoencoder is a <strong>feedforward neural network</strong> (similar to the multilayer perceptron (MLP)) – having an input layer, an output layer and one or more hidden layers connecting them – but with the output layer having the same number of nodes as the input layer, and with the purpose of reconstructing its own inputs (instead of predicting the target value \({\displaystyle Y}\) given inputs \({\displaystyle X}\)).</li>
      <li><strong style="color: blue">What realm of “Learning” is employed for AEs?</strong><br />
 Self-Supervised Learning.</li>
    </ol>
  </li>
  <li><strong style="color: red">Mathematical Description of the Structure of AutoEncoders:</strong><br />
 The <em>encoder</em> and the <em>decoder</em> in an auto-encoder can be defined as <a href="https://en.wikipedia.org/wiki/Atlas_(topology)#Transition_maps">transitions</a> \(\phi\) and \({\displaystyle \psi ,}\) such that:
    <p>$$ {\displaystyle \phi :{\mathcal {X}}\rightarrow {\mathcal {F}}} \\ 
     {\displaystyle \psi :{\mathcal {F}}\rightarrow {\mathcal {X}}} \\ 
     {\displaystyle \phi ,\psi =\arg \min_{\phi ,\psi }\|X-(\psi \circ \phi )X\|^{2}}$$
 </p>
    <p>where \({\mathcal {X} = \mathbf{R}^d}\) is the input space, and \({\mathcal {F} = \mathbf{R}^p}\) is the latent (feature) space, and \(p &lt; d\).</p>

    <p>The encoder takes the input \({\displaystyle \mathbf {x} \in \mathbb {R} ^{d}={\mathcal {X}}}\) and maps it to \({\displaystyle \mathbf {z} \in \mathbb {R} ^{p}={\mathcal {F}}}\):</p>
    <p>$${\displaystyle \mathbf {z} =\sigma (\mathbf {Wx} +\mathbf {b} )}$$</p>
    <ul>
      <li>The image \(\mathbf{z}\) is referred to as <em>code</em>, <em>latent variables</em>, or <em>latent representation</em>.</li>
      <li>\({\displaystyle \sigma }\) is an element-wise activation function such as a sigmoid function or a rectified linear unit.</li>
      <li>\({\displaystyle \mathbf {W} }\) is a weight matrix</li>
      <li>\({\displaystyle \mathbf {b} }\) is the bias.</li>
    </ul>

    <p>The Decoder maps \({\displaystyle \mathbf {z} }\) to the reconstruction \({\displaystyle \mathbf {x'} }\)  of the same shape as \({\displaystyle \mathbf {x} }\):</p>
    <p>$${\displaystyle \mathbf {x'} =\sigma '(\mathbf {W'z} +\mathbf {b'} )}$$</p>
    <p>where \({\displaystyle \mathbf {\sigma '} ,\mathbf {W'} ,{\text{ and }}\mathbf {b'} }\) for the decoder may differ in general from those of the encoder.</p>

    <p>Autoencoders minimize  reconstruction errors, such as the L-2 loss:</p>
    <p>$${\displaystyle {\mathcal {L}}(\mathbf {x} ,\psi ( \phi (\mathbf {x} ) ) ) =  {\mathcal {L}}(\mathbf {x} ,\mathbf {x'} )=\|\mathbf {x} -\mathbf {x'} \|^{2}=\|\mathbf {x} -\sigma '(\mathbf {W'} (\sigma (\mathbf {Wx} +\mathbf {b} ))+\mathbf {b'} )\|^{2}}$$</p>
    <p>where \({\displaystyle \mathbf {x} }\) is usually averaged over some input training set.</p>

    <p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Extra Extra</button></p>
    <ol hidden="">
      <li><strong style="color: blue">How do we define the “Encoder” and “Decoder”?</strong><br />
 Transition Maps.</li>
      <li><strong style="color: blue">The Encoder maps what to what?</strong><br />
 The encoder takes the input \({\displaystyle \mathbf {x} \in \mathbb {R} ^{d}={\mathcal {X}}}\) and maps it to \({\displaystyle \mathbf {z} \in \mathbb {R} ^{p}={\mathcal {F}}}\):
        <p>$${\displaystyle \mathbf {z} =\sigma (\mathbf {Wx} +\mathbf {b} )}$$</p>
      </li>
      <li><strong style="color: blue">The Decoder maps what to what?</strong><br />
 The Decoder maps \({\displaystyle \mathbf {z} }\) to the reconstruction \({\displaystyle \mathbf {x'} }\)  of the same shape as \({\displaystyle \mathbf {x} }\):
        <p>$${\displaystyle \mathbf {x'} =\sigma '(\mathbf {W'z} +\mathbf {b'} )}$$</p>
        <p>where \({\displaystyle \mathbf {\sigma '} ,\mathbf {W'} ,{\text{ and }}\mathbf {b'} }\) for the decoder may differ in general from those of the encoder.</p>
      </li>
      <li><strong style="color: blue">What is the type of loss?</strong><br />
 Autoencoders minimize <strong>reconstruction errors</strong>, such as the <strong>L2 loss</strong>:
        <p>$${\displaystyle {\mathcal {L}}(\mathbf {x} ,\psi ( \phi (\mathbf {x} ) ) ) =  {\mathcal {L}}(\mathbf {x} ,\mathbf {x'} )=\|\mathbf {x} -\mathbf {x'} \|^{2}=\|\mathbf {x} -\sigma '(\mathbf {W'} (\sigma (\mathbf {Wx} +\mathbf {b} ))+\mathbf {b'} )\|^{2}}$$</p>
        <p>where \({\displaystyle \mathbf {x} }\) is usually averaged over some input training set.</p>
      </li>
    </ol>

    <p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Extra Extra</button></p>
    <ol hidden="">
      <li><strong style="color: blue">What are “Transition Functions”?</strong></li>
    </ol>
  </li>
  <li><strong style="color: red">Compare AutoEncoders and PCA (wrt what they learn):</strong></li>
  <li><strong style="color: red">List the different Types of AEs</strong>
    <ul>
      <li>Vanilla Auto-Encoder</li>
      <li>Sparse Auto-Encoder</li>
      <li>Denoising Auto-Encoder</li>
      <li>Variational Auto-Encoder (VAE)</li>
      <li>Contractive Auto-Encoder.</li>
    </ul>
  </li>
  <li><strong style="color: red">How can we use AEs for Initialization?</strong><br />
 After training an auto-encoder, we can use the <em>encoder</em> to compress the input data into it’s latent representation (which we can view as <em>features</em>) and input those to the neural-net (e.g. a classifier) for prediction.<br />
 <img src="/main_files/cs231n/aencdrs/2.png" alt="img" width="40%" /></li>
  <li><strong style="color: red">Describe the Representational Power of AEs:</strong>
    <ul>
      <li>The universal approximator theorem guarantees that a feedforward neural network with at least one hidden layer can represent an approximation of any function (within a broad class) to an arbitrary degree of accuracy, provided that it has enough hidden units. This means that an Autoencoder with a single hidden layer is able to represent the identity function along the domain of the data arbitrarily well.</li>
      <li>However, <strong>the mapping from input to code is shallow</strong>. This means that we are not able to enforce arbitrary constraints, such as that the code should be sparse.</li>
      <li>A deep Autoencoder, with at least one additional hidden layer inside the encoder itself, can approximate any mapping from input to code arbitrarily well, given enough hidden units.</li>
    </ul>

    <p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Extra</button></p>
    <ol hidden="">
      <li><strong style="color: red">(wrt Layer Size and Depth):</strong></li>
      <li><strong style="color: blue">Why is Depth Important?</strong></li>
    </ol>
  </li>
  <li><strong style="color: red">Describe the progression (stages) of AE Architectures in CV:</strong>
    <ul>
      <li>Originally: Linear + nonlinearity (sigmoid).</li>
      <li>Later: Deep, fully-connected.</li>
      <li>Later: ReLU CNN (UpConv).</li>
    </ul>
  </li>
  <li><strong style="color: red">List the Types of AEs wrt the Bottleneck/Code-Length:</strong>
    <ul>
      <li><strong>Undercomplete</strong>:</li>
      <li><strong>Overcomplete</strong>:</li>
      <li><strong>Complete</strong>:</li>
    </ul>
  </li>
  <li><strong style="color: red">What are <em>Undercomplete</em> AutoEncoders?</strong><br />
 An <strong>Undercomplete Autoencoder</strong> is one whose code dimension is less than the input dimension.</li>
  <li><strong style="color: red">What’s the motivation behind <em>Undercomplete</em> AEs?</strong><br />
 Learning an undercomplete representation forces the autoencoder to capture the most salient features of the training data.</li>
  <li><strong style="color: red">List the Challenges of Training AEs using the Auto-Encoding Objective (case-by-case):</strong>
    <ul>
      <li>If an Autoencoder succeeds in simply learning to set \(\psi(\phi (x)) = x\) everywhere, then it is not especially useful.<br />
  Instead, Autoencoders are designed to be unable to learn to copy perfectly. Usually they are restricted in ways that allow them to copy only approximately, and to copy only input that resembles the training data.</li>
      <li>In <a href="#bodyContents21"><strong>Undercomplete Autoencoders</strong></a> If the encoder and decoder are allowed too much capacity, the Autoencoder can learn to perform the copying task without extracting useful information about the distribution of the data.<br />
  Theoretically, one could imagine that an Autoencoder with a one-dimensional code but a very powerful nonlinear encoder could learn to represent each training example \(x^{(i)}\) with the code \(i\). This specific scenario does not occur in practice, but it illustrates clearly that an Autoencoder trained to perform the copying task can fail to learn anything useful about the dataset if the capacity of the Autoencoder is allowed to become too great.</li>
      <li>A similar problem occurs in <strong>Complete AEs</strong></li>
      <li>As well as in the <strong>Overcomplete</strong> case, in which the hidden code has dimension greater than the input.<br />
  In complete and overcomplete cases, even a linear encoder and linear decoder can learn to copy the input to the output without learning anything useful about the data distribution.</li>
    </ul>
  </li>
  <li><strong style="color: red">What is the Main Method/Approach of addressing the Challenges above (Training AEs)?</strong><br />
 <strong>Regularized</strong> Autoencoders.</li>
</ol>

<p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Further Questions</button></p>
<ol hidden="">
  <li>
    <p><strong style="color: red">Define Regularized Autoencoders:</strong><br />
 Regularized Autoencoders are a variant of Autoencoders that modifies the loss function with regularization techniques to allow us to train any architecture of AEs successfully.</p>

    <ol>
      <li>
        <p><strong style="color: red">What’s the Big-Idea/Main-Point behind R-AEs and AEs in general?</strong><br />
 A regularized Autoencoder can be nonlinear and overcomplete but still learn something useful about the data distribution even if the model capacity is great enough to learn a trivial identity function.</p>
      </li>
      <li><strong style="color: blue">What does it allow us to do? How*?</strong><br />
 It allows us to train any architecture of Autoencoder successfully, choosing the code dimension and the capacity of the encoder and decoder based on the complexity of distribution to be modeled.</li>
      <li><strong style="color: blue">How does it address the Challenges? (Compare)</strong><br />
 Rather than limiting the model capacity by keeping the Encoder and Decoder <em>shallow</em> and the Code-Size <em>small</em>, they use a <strong>loss function</strong> that <em>encourages</em> the model to have other (useful) properties besides the ability to copy its input to it output:</li>
    </ol>

    <p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Extra</button></p>
    <ol hidden="">
      <li><strong style="color: blue">List if any the restrictions/guidelines for setting the capacity of the R-AEs:</strong><br />
 We should choose the code dimension and the capacity of the encoder and decoder based on the complexity of distribution to be modeled.</li>
      <li><strong style="color: blue">What other Properties does it encourage to be learned?</strong>
        <ul>
          <li><strong>Sparsity</strong> of the Representation</li>
          <li><strong>Smallness</strong> of the <strong>Derivative</strong> of the Representation</li>
          <li><strong>Robustness</strong> to <strong>Noise</strong> or to <strong>Missing Inputs</strong></li>
        </ul>
      </li>
      <li><strong style="color: blue">What kind of technique (also type) of AEs can be “non-linear” and still learn useful codes?</strong><br />
 Regularized AEs.</li>
      <li><strong style="color: blue">What kind of technique-needed for (also type-of) AEs can be “overcomplete” and still learn useful codes?</strong><br />
 Regularized AEs.</li>
      <li><strong style="color: blue">What kind of technique-needed for (also type-of) AEs can be “nonlinear” AND “overcomplete” and still learn useful codes?</strong><br />
 Regularized AEs.</li>
      <li><strong style="color: red">What are the ways to learn useful encodings/representations?</strong><br />
 Defining an appropriate <span style="color: purple"><strong>Objective and Objective Function</strong></span>.
        <ol>
          <li><strong style="color: red">What types of objectives help learn useful encodings/representations?</strong><br />
 <strong>(regularized/approximate) Auto-Encoding</strong> - <strong>Maximizing the Probability of training Data (NLL)</strong><br />
 <a href="/work_files/research/dl/archits/aencdrs#bodyContents24">Further Info</a></li>
        </ol>
      </li>
    </ol>
  </li>
  <li>
    <p><strong style="color: red">Describe the Relationship between Generative Models and AEs:</strong><br />
 In addition to the traditional AEs described here, nearly any <strong>generative model</strong> with <strong>latent variables</strong> and equipped with an <strong>inference procedure</strong> (for computing latent representations given input) may be viewed as a particular form of Autoencoder.</p>

    <p><button class="showText" value="show" onclick="showTextPopHide(event);">Extra</button></p>
    <ol hidden="">
      <li><strong style="color: blue">What are the components needed for the Generative Model?:</strong>
        <ul>
          <li><strong>generative model</strong></li>
          <li><strong>latent variables</strong></li>
          <li><strong>inference procedure</strong> (for computing latent representations given input)</li>
        </ul>
      </li>
      <li><strong style="color: blue">What notable types? List:</strong>. 
 The descendants of the <strong>Helmholtz machine</strong> <em>(Hinton et al., 1995b)</em>, such as:
        <ul>
          <li>Variational Autoencoders</li>
          <li>Generative Stochastic Networks</li>
        </ul>
      </li>
      <li><strong style="color: blue">Compare Generative Models &amp; AEs in how they learn codings/representations:</strong><br />
 These models naturally learn <em>high-capacity</em>, <em>overcomplete encodings</em> of the input and do NOT require regularization for these encodings to be useful.</li>
    </ol>

    <p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Extra Extra</button></p>
    <ol hidden="">
      <li><strong style="color: blue">Why are their representations “naturally” useful?</strong><br />
 Their <span style="color: goldenrod">encodings are naturally useful</span> because the models were <span style="color: goldenrod">trained to <em>approximately maximize the probability of the training data</em> rather than to <em>copy the input to the output</em></span>.</li>
    </ol>
  </li>
  <li>
    <p><strong style="color: red">List the Different Types of Regularized Autoencoders:</strong></p>
    <ul>
      <li><strong>Sparse</strong> Autoencoders</li>
      <li><strong>Denoising</strong> Autoencoders</li>
      <li>Regularizing by <strong>Penalizing Derivatives</strong></li>
      <li><strong>Contractive</strong> Autoencoders</li>
      <li><strong>Predictive Sparse Decomposition</strong></li>
    </ul>
  </li>
</ol>

<p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Further Further Questions (Regularized AEs)</button></p>
<ol hidden="">
  <li><strong style="color: red">Define Sparse Autoencoders (w/ equation):</strong><br />
 <strong>Sparse Autoencoders</strong> are simply Autoencoders whose training criterion involves a sparsity penalty \(\Omega(\boldsymbol{h})\) on the code layer \(\boldsymbol{h},\) in addition to the reconstruction error:
    <p>$${\displaystyle {\mathcal {L}}(\mathbf {x} ,\psi ( \phi (\mathbf {x} ) ) ) + \Omega(\boldsymbol{h})}$$</p>
    <p>where typically we have \(\boldsymbol{h}=\phi(\boldsymbol{x})\), the encoder output.</p>
  </li>
  <li><strong style="color: red">How can we interpret Sparse AEs? (Hint: 3 interpretations)</strong>
    <ul hidden="">
      <li><strong>Regularization Interpretation</strong></li>
      <li><strong>Bayesian Interpretation</strong></li>
      <li><strong>Latent-Variable Interpretation</strong><br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">Extra</button>
        <ol>
          <li><strong style="color: red">Give the “Regularization” Interpretation of Sparse AEs:</strong><br />
  We can think of the <span style="color: purple">penalty \(\Omega(\boldsymbol{h})\) simply as a regularizer term added to a feedforward network</span> whose primary task is to copy the input to the output (unsupervised learning objective) and possibly also perform some supervised task (with a supervised learning objective) that depends on these sparse features.</li>
          <li><strong style="color: red">Give the “Bayesian” Interpretation of Regularized AEs:</strong><br />
  Unlike other regularizers such as weight decay, there is not a straightforward Bayesian interpretation to this regularizer.<br />
  Regularized Autoencoders defy such an interpretation because <strong>the regularizer depends on the data</strong> and is therefore by definition not a prior in the formal sense of the word.<br />
  We can still think of these regularization terms as <span style="color: purple"><em>implicitly</em> expressing a preference over functions</span>.</li>
          <li><strong style="color: red">Give the “Latent Variable” Interpretation of Sparse AEs:</strong><br />
  Rather than thinking of the sparsity penalty as a regularizer for the copying task, we can think of the entire sparse Autoencoder framework as <span style="color: goldenrod">approximating maximum likelihood training of a generative model that has latent variables</span>.<br />
  <button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Extra Extra</button></li>
        </ol>
        <ol hidden="">
          <li><strong style="color: blue">What do Sparse AEs approximate?</strong><br />
 They approximate <strong>Latent Variable Models</strong>.<br />
 Specifically, <strong>Generative Models</strong> with <strong>Latent Variables</strong> <span style="color: purple">Trained with <strong>Maximum Likelihood</strong></span>.</li>
          <li><strong style="color: blue">How do they (does that) relate to MLE?</strong><br />
 They approximate <strong>Generative Models</strong> with <strong>Latent Variables</strong> <span style="color: purple">Trained with <strong>Maximum Likelihood</strong></span>.</li>
        </ol>
      </li>
    </ul>
  </li>
  <li><strong style="color: red">Define Denoising Autoencoders:</strong><br />
 <strong>Denoising Autoencoders (DAEs)</strong> is an Autoencoder that receives a corrupted data point as input and is trained to predict the original, uncorrupted data point as its output.</li>
  <li><strong style="color: red">What do they minimize? (canonical loss)</strong>
    <p>$$L(\boldsymbol{x}, g(f(\tilde{\boldsymbol{x}})))$$</p>
    <p>where \(\tilde{\boldsymbol{x}}\) is a copy of \(\boldsymbol{x}\) that has been corrupted by some form of noise.</p>
  </li>
  <li><strong style="color: red">What do they learn? How? (compare)</strong><br />
 They learn to <strong>undo the corruption</strong> rather than simply copying their input.<br />
 The <strong>denoising training</strong> forces \(\psi\) and \(\phi\) to implicitly learn the structure of \(p_{\text {data}}(\boldsymbol{x})\).</li>
  <li><strong style="color: red">How do we generate the inputs?</strong><br />
 We introduce a <strong>Corruption Process</strong> \(C(\tilde{\mathbf{x}} \vert \mathbf{x})\) which represents a <strong>conditional distribution over corrupted samples \(\tilde{\boldsymbol{x}}\)</strong>, given a data sample \(\boldsymbol{x}\).<br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">Extra</button>
    <ol hidden="">
      <li><strong style="color: blue">What does the “Corruption Process” represent/define?</strong><br />
 It represents a <span style="color: purple"><strong>Conditional Distribution</strong></span> <strong>over corrupted samples \(\tilde{\boldsymbol{x}}\)</strong>, given a data sample \(\boldsymbol{x}\).</li>
    </ol>
  </li>
  <li><strong style="color: red">How do we generate the training examples (input-output pair)? (process)</strong>
    <ul>
      <li>Sample a training example \(\boldsymbol{x}\) from the training data.</li>
      <li>Sample a corrupted version \(\tilde{\boldsymbol{x}}\) from \(C(\tilde{\mathbf{x}} \vert \mathbf{x}=\boldsymbol{x})\)</li>
      <li>Use \((\boldsymbol{x}, \tilde{\boldsymbol{x}})\) as a training example for estimating the Autoencoder reconstruction distribution \(p_{\text {reconstruct }}(\boldsymbol{x} \vert \tilde{\boldsymbol{x}})=p_{\text {decoder }}(\boldsymbol{x} \vert \boldsymbol{h})\) with \(\boldsymbol{h}\) the output of encoder \(f(\tilde{\boldsymbol{x}})\) and \(p_{\text {decoder}}\) typically defined by a decoder \(g(\boldsymbol{h})\).</li>
    </ul>
  </li>
  <li><strong style="color: red">What does the Denoising AE learn specifically? (mathematically)</strong><br />
 The DAE learns a <strong>Reconstruction Distribution</strong> \(p_{\text {reconstruct }}(\boldsymbol{x} \vert \tilde{\boldsymbol{x}})=p_{\text {decoder }}(\boldsymbol{x} \vert \boldsymbol{h})\) with \(\boldsymbol{h}\) the output of encoder \(f(\tilde{\boldsymbol{x}})\) and \(p_{\text {decoder}}\) typically defined by a decoder \(g(\boldsymbol{h})\).<br />
 <button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Extra</button>
    <ol hidden="">
      <li><strong style="color: blue">What do we use as an estimate for the “Reconstruction Distribution”?</strong><br />
 We use \((\boldsymbol{x}_ i, \tilde{\boldsymbol{x}}_ i)\) as Training Data for estimating the Autoencoder reconstruction distribution \(p_{\text {reconstruct }}(\boldsymbol{x} \vert \tilde{\boldsymbol{x}})\).</li>
      <li><strong style="color: blue">What is the output of the encoder \(f\)?</strong><br />
 \(f(\tilde{\boldsymbol{x}}) = \boldsymbol{h}\).</li>
      <li><strong style="color: blue">What is the output of the decoder \(g\)?</strong><br />
 \(g(\boldsymbol{h})=p_{\text {decoder}}(\boldsymbol{x} \vert \boldsymbol{h})=\boldsymbol{x}\).</li>
      <li><strong style="color: blue">What is the “Reconstruction Distribution” equal to?</strong><br />
 \(p_{\text {reconstruct}}(\boldsymbol{x} \vert \tilde{\boldsymbol{x}})=p_{\text {decoder}}(\boldsymbol{x} \vert \boldsymbol{h})\).</li>
    </ol>
  </li>
  <li><strong style="color: red">How do we Train the Denoising AE?</strong><br />
 Typically we can simply perform gradient-based approximate minimization (such as minibatch gradient descent) on the negative log-likelihood \(-\log p_{\text {decoder}}(\boldsymbol{x} \vert \boldsymbol{h})\).<br />
 So long as the encoder is deterministic, the denoising Autoencoder is a feedforward network and may be trained with exactly the same techniques as any other FFN.
    <ol>
      <li><strong style="color: blue">What is the Loss?</strong><br />
 Negative Log-Likelihood (NLL): \(-\log p_{\text {decoder}}(\boldsymbol{x} \vert \boldsymbol{h})\).</li>
      <li><strong style="color: blue">What is the Optimization Method?</strong><br />
 <strong>SGD</strong>.</li>
      <li><strong style="color: blue">What is the Training similar to?</strong><br />
 So long as the encoder is deterministic, the DAE is a <strong>feedforward network (FFN)</strong> and may be trained with exactly the same techniques as any other FFN.</li>
      <li><strong style="color: blue">Is the Encoder Deterministic?</strong><br />
 Yes.
        <ol>
          <li><strong style="color: blue">Would change if it was one or the other?</strong><br />
 If it were <strong>“stochastic”</strong> then it can’t be learned w/ max-likelihood training.</li>
        </ol>
      </li>
    </ol>
  </li>
  <li><strong style="color: red">How can we view the function of DAEs (wrt learning/training) from a Probabilistic pov?</strong><br />
 Given above, we can therefore view the DAE as <strong>performing stochastic gradient descent on the following expectation</strong>:
    <p>$$-\mathbb{E}_{\mathbf{x} \sim \hat{p}_{\text {data }}(\mathbf{x})} \mathbb{E}_{\tilde{\mathbf{x}} \sim C(\tilde{\mathbf{x}} \vert \boldsymbol{x})} \log p_{\text {decoder}}(\boldsymbol{x} \vert \boldsymbol{h}=f(\tilde{\boldsymbol{x}}))$$</p>
    <p>where \(\hat{p}_ {\text {data}}(\mathrm{x})\) is the training distribution.<br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">Extra</button></p>
    <ol hidden="">
      <li><strong style="color: blue">What Expectation is it minimizing? Over what?</strong>
        <p>$$-\mathbb{E}_{\mathbf{x} \sim \hat{p}_{\text {data }}(\mathbf{x})} \mathbb{E}_{\tilde{\mathbf{x}} \sim C(\tilde{\mathbf{x}} \vert \boldsymbol{x})} \log p_{\text {decoder}}(\boldsymbol{x} \vert \boldsymbol{h}=f(\tilde{\boldsymbol{x}}))$$</p>
        <p>where \(\hat{p}_ {\text {data}}(\mathrm{x})\) is the training distribution.</p>
      </li>
      <li><strong style="color: blue">Can we re-write the Objective/Loss wrt the Empirical Distribution?</strong>
        <p>$$-\mathbb{E}_{\mathbf{x} \sim \hat{p}_{\text {data }}(\mathbf{x})} \mathbb{E}_{\tilde{\mathbf{x}} \sim C(\tilde{\mathbf{x}} \vert \boldsymbol{x})} \log p_{\text {decoder}}(\boldsymbol{x} \vert \boldsymbol{h}=f(\tilde{\boldsymbol{x}}))$$</p>
        <p>where \(\hat{p}_ {\text {data}}(\mathrm{x})\) is the training distribution.</p>
      </li>
    </ol>
  </li>
  <li><strong style="color: red">What other ways exist for learning/training DAEs?</strong><br />
 <strong>Score Matching</strong>.</li>
  <li>
    <p><strong style="color: red">How do DAEs and VAEs relate to each other?</strong><br />
 DAEs learn to represent a <strong>probability distribution</strong>.<br />
 To use the AE as a <strong>generative model</strong> to <em><strong>draw samples</strong></em> from this distribution, we need to make the <span style="color: purple"><strong>Encoder</strong> <em><strong>Stochastic</strong></em></span>: this is known as the <strong>Variational Autoencoder</strong>.</p>
  </li>
  <li><strong style="color: red">Define Contractive Autoencoders</strong><br />
 <strong>Contractive Autoencoders (CAEs)</strong> <em>(Rifai et al., 2011a,b)</em> introduces an explicit regularizer on the code \(\boldsymbol{h}=\phi(\boldsymbol{x}),\) encouraging the derivatives of \(\phi\) to be as small as possible:
    <p>$$\Omega(\boldsymbol{h})=\lambda\left\|\frac{\partial \phi(\boldsymbol{x})}{\partial \boldsymbol{x}}\right\|_ {F}^{2}$$</p>
    <p>The <strong>penalty</strong> \(\Omega(\boldsymbol{h})\) is the <em><strong>squared Frobenius norm</strong></em>  (sum of squared elements) of <strong>the Jacobian matrix</strong> of partial derivatives associated with the encoder function.<br />
 <button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Extra</button></p>
    <ol hidden="">
      <li><strong style="color: blue">What is the regularizer/penalty used?</strong>  <br />
 The <strong>penalty</strong> \(\Omega(\boldsymbol{h})\) is the <em><strong>squared Frobenius norm</strong></em>  (sum of squared elements) of <strong>the Jacobian matrix</strong> of partial derivatives associated with the encoder function.</li>
      <li><strong style="color: blue">What does it encourage the system to do?</strong><br />
 It encouraging the <span style="color: purple"><strong>derivatives</strong></span> of the <strong>Encoder</strong> \(\phi\) to be <span style="color: purple">as small as possible</span>.</li>
    </ol>
  </li>
  <li><strong style="color: red">How is the Contractive AE connected to the DAE:</strong>
    <ul>
      <li>In the limit of small Gaussian input noise, the denoising reconstruction error is equivalent to a contractive penalty on the reconstruction function that maps \(\boldsymbol{x}\) to \(\boldsymbol{r}=\psi(\phi(\boldsymbol{x}))\). I.e.:<br />
  - <strong>Denoising Autoencoders:</strong> make the reconstruction function resist small but finite-sized perturbations of the input<br />
  - <strong>Contractive Autoencoders:</strong> make the feature extraction function resist infinitesimal perturbations of the input.</li>
      <li>When using the Jacobian-based contractive penalty to pretrain features \(\phi(\boldsymbol{x})\) for use with a classifier, the best classification accuracy usually results from applying the contractive penalty to \(\phi(\boldsymbol{x})\) rather than to \(\psi(\phi(\boldsymbol{x}))\).</li>
      <li>A contractive penalty on \(\phi(\boldsymbol{x})\) also has close <a href="#bodyContents32sm">connections to <strong>score matching</strong></a>.</li>
    </ul>
  </li>
  <li><strong style="color: red">Why is the CAE called “Contractive”?</strong><br />
 The name <strong>contractive</strong> arises from the way that the CAE <em>warps space</em>.<br />
 Specifically, because the CAE is trained to resist perturbations of its input, it is encouraged to map a neighborhood of input points to a smaller neighborhood of output points. We can think of this as contracting the input neighborhood to a smaller output neighborhood.<br />
 <button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Extra</button>
    <ol hidden="">
      <li><strong style="color: blue">Is it contractive locally or globally or both?</strong><br />
 The CAE is contractive only <em><strong>locally</strong></em>-all perturbations of a training point \(\boldsymbol{x}\) are mapped near to \(\phi(\boldsymbol{x})\). <em><strong>Globally</strong></em>, two different points \(\boldsymbol{x}\) and \(\boldsymbol{x}^{\prime}\) may be mapped to \(\phi(\boldsymbol{x})\) and \(\psi\left(\boldsymbol{x}^{\prime}\right)\) points that are farther apart than the original points.</li>
      <li><strong style="color: blue">Give the Interpretation of the CAE as a Linear Operator:</strong><br />
 We can think of the <em>Jacobian matrix</em> \(J\) at a point \(x\) as <span style="color: goldenrod"><em>approximating</em> the <strong>nonlinear encoder \(\phi(x)\)</strong> as being a <strong>linear operator</strong></span>. This allows us to use the word <em>“contractive”</em> more formally.<br />
 In the theory of linear operators, a linear operator is said to be <em>contractive</em> if the norm of \(J x\) remains less than or equal to 1 for all unit-norm \(x\). In other words, <strong>\(J\) is contractive if it shrinks the unit sphere</strong>.<br />
 We can think of the CAE as <em>penalizing the Frobenius norm of the local linear approximation of \(\phi(x)\) at every training point \(x\)</em> in order <em>to encourage each of these local linear operator to become a <strong>contraction</strong></em>.</li>
    </ol>
  </li>
  <li><strong style="color: red">List the Issues associated with using a “Contractive Penalty”:</strong>
    <ul>
      <li><a href="/work_files/research/dl/archits/aencdrs#bodyContents35issues_ctrctv_pnlt">Answer</a></li>
    </ul>
  </li>
</ol>

<hr />
<hr />

<p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Show Questions on “Extra”</button></p>
<ol hidden="">
  <li><strong style="color: red">What is an AutoEncoder? Why do we “Auto-Encode”? (Hint: it’s really a misnomer) Why is “AutoEncoder” a misnomer?</strong><br />
 AutoEncoders are FFNs with a narrow, low-dimensional bottleneck layer in the middle that learn are used to learn efficient data codings/representations.<br />
 The objective for an AE is to learn to copy its input to its output. 
 However, performing the copying task per se would be meaningless, and this is why usually autoencoders are restricted in ways that force them to reconstruct the input only approximately, prioritizing the most relevant aspects of the data to be copied.<br />
 i.e. It’s not really a misnomer since the general objective is in the form of <em>“auto-encoding”</em> however, the main goal is not just to learn to reconstruct the data from the code but to learn useful information about the distribution of the data.<br />
 The goal is not perform the copying task, but to <span style="color: purple">perform the copying task for the purpose of __ extracting useful information about the distribution of the data__.</span><br />
 The Goal: <span style="color: purple">capture important information and learn richer representations.</span>.</li>
  <li><strong style="color: red">What’s the Big-Idea/Main-Point behind R-AEs and AEs in general?</strong><br />
     A regularized Autoencoder can be nonlinear and overcomplete but still learn something useful about the data distribution even if the model capacity is great enough to learn a trivial identity function.
    <ul>
      <li><strong style="color: red">What do DAEs learn to represent?</strong><br />
 They learn to represent a <strong>probability distribution</strong>.</li>
      <li><strong style="color: red">How do Regularized AEs learn Manifolds?</strong><br />
 Regularized Autoencoders learn manifolds by balancing two opposing forces.</li>
      <li><strong style="color: red">How can we Learn Manifolds w/ AEs?</strong></li>
    </ul>
  </li>
</ol>


      <footer class="site-footer">
    <!--   <span class="site-footer-owner"><a href="http://localhost:8889">Ahmad Badary</a> is maintained by <a href="https://ahmedbadary.github.io/">Ahmad Badary</a>.</span> -->
    
<!--  -->
    <div class="wrapper">
        <div class="footer-col-wrapper">
            <div class="footer-col footer-col-1">
            <span class="site-footer-owner"><a href="http://localhost:8889">Site</a> maintained by <a href="https://ahmedbadary.github.io/">Ahmad Badary</a>.</span>
    <span class="site-footer-credits">
        <p>
            &copy; 2017. All rights reserved.
        </p> 
    </span>
            </div>
            <div class="footer-col footer-col-2">
            <div><p>         </p></div>
            </div>
            <div class="footer-col footer-col-3">
                <ul class="social-media-list">
                    
                      <li>
                        <a href="https://github.com/AhmedBadary">
                          <i class="fa fa-github"></i> GitHub
                        </a>
                      </li>
                    
                    
                      <li>
                        <a href="https://linkedin.com/in/ahmad-badary-656098121/">
                          <i class="fa fa-linkedin"></i> LinkedIn
                        </a>
                      </li>
                    
                    
                      <li>
                        <a href="https://www.facebook.com/ahmed.thabet.94">
                          <i class="fa fa-facebook"></i> Facebook
                        </a>
                      </li>
                    
                </ul>
            </div>
        </div>
    </div>
<!--  -->
</footer>


    </section>

  </body>

<!-- Table of Content Script -->
<script type="text/javascript">
var bodyContents = $(".bodyContents1");
$("<ol>").addClass("TOC1ul").appendTo(".TOC1");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC1ul");
     });
// 
var bodyContents = $(".bodyContents2");
$("<ol>").addClass("TOC2ul").appendTo(".TOC2");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC2ul");
     });
// 
var bodyContents = $(".bodyContents3");
$("<ol>").addClass("TOC3ul").appendTo(".TOC3");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC3ul");
     });
//
var bodyContents = $(".bodyContents4");
$("<ol>").addClass("TOC4ul").appendTo(".TOC4");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC4ul");
     });
//
var bodyContents = $(".bodyContents5");
$("<ol>").addClass("TOC5ul").appendTo(".TOC5");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC5ul");
     });
//
var bodyContents = $(".bodyContents6");
$("<ol>").addClass("TOC6ul").appendTo(".TOC6");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC6ul");
     });
//
var bodyContents = $(".bodyContents7");
$("<ol>").addClass("TOC7ul").appendTo(".TOC7");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC7ul");
     });
//
var bodyContents = $(".bodyContents8");
$("<ol>").addClass("TOC8ul").appendTo(".TOC8");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC8ul");
     });
//
var bodyContents = $(".bodyContents9");
$("<ol>").addClass("TOC9ul").appendTo(".TOC9");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC9ul");
     });

</script>

<!-- VIDEO BUTTONS SCRIPT -->
<script type="text/javascript">
  function iframePopInject(event) {
    var $button = $(event.target);
    // console.log($button.parent().next());
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $figure = $("<div>").addClass("video_container");
        $iframe = $("<iframe>").appendTo($figure);
        $iframe.attr("src", $button.attr("src"));
        // $iframe.attr("frameborder", "0");
        $iframe.attr("allowfullscreen", "true");
        $iframe.css("padding", "4px 6px");
        $button.next().css("display", "block");
        $figure.appendTo($button.next());
        $button.text("Hide Video")
    } else {
        $button.attr('value', 'show');
        $button.next().html("");
        $button.text("Show Video")
    }
}
</script>

<!-- BUTTON TRY -->
<script type="text/javascript">
  function iframePopA(event) {
    event.preventDefault();
    var $a = $(event.target).parent();
    console.log($a);
    if ($a.attr('value') == 'show') {
        $a.attr('value', 'hide');
        $figure = $("<div>");
        $iframe = $("<iframe>").addClass("popup_website_container").appendTo($figure);
        $iframe.attr("src", $a.attr("href"));
        $iframe.attr("frameborder", "1");
        $iframe.attr("allowfullscreen", "true");
        $iframe.css("padding", "4px 6px");
        $a.next().css("display", "block");
        $figure.appendTo($a.next().next());
        // $a.text("Hide Content")
        $('html, body').animate({
            scrollTop: $a.offset().top
        }, 1000);
    } else {
        $a.attr('value', 'show');
        $a.next().next().html("");
        // $a.text("Show Content")
    }

    $a.next().css("display", "inline");
}
</script>


<!-- TEXT BUTTON SCRIPT - INJECT -->
<script type="text/javascript">
  function showTextPopInject(event) {
    var $button = $(event.target);
    var txt = $button.attr("input");
    console.log(txt);
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $p = $("<p>");
        $p.html(txt);
        $button.next().css("display", "block");
        $p.appendTo($button.next());
        $button.text("Hide Content")
    } else {
        $button.attr('value', 'show');
        $button.next().html("");
        $button.text("Show Content")
    }

}
</script>

<!-- TEXT BUTTON SCRIPT - HIDDEN / HIDE / SHOW / HIDE/SHOW -->
<script type="text/javascript">
  function showTextPopHide(event) {
    var $button = $(event.target);
    // var txt = $button.attr("input");
    var txt = $button.text();
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $button.next().removeAttr("hidden");
        $button.text(txt + " - Hide Content");
    } else {
        $button.attr('value', 'show');
        $button.next().attr("hidden", "");
        $button.text(txt.replace(" - Hide Content",""));
    }
}
</script>

<!-- TEXT BUTTON SCRIPT - HIDDEN / HIDE / SHOW / HIDE/SHOW -->
<script type="text/javascript">
  function showText_withParent_PopHide(event) {
    var $button = $(event.target);
    var $parent = $button.parent();
    var txt = $button.text();
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $parent.next().removeAttr("hidden");
        $button.text(txt + " - Hide Content");
    } else {
        $button.attr('value', 'show');
        $parent.next().attr("hidden", "");
        $button.text(txt.replace(" - Hide Content",""));
    }
}
</script>

<!-- Print / Printing / printme -->
<!-- <script type="text/javascript">
i = 0

for (var i = 1; i < 6; i++) {
    var bodyContents = $(".bodyContents" + i);
    $("<p>").addClass("TOC1ul")  .appendTo(".TOC1");
    bodyContents.each(function(index, element) {
        var paragraph = $(element);
        $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC1ul");
         });
} 
</script>
 -->
 
</html>

