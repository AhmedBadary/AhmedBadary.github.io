<!DOCTYPE html>
<html lang="en-us">
  <head>
  <meta charset="UTF-8">
  <title>Ahmad Badary</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <link rel="stylesheet" href="/css/normalize.css">
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="/css/cayman.css">
  <link rel="stylesheet" href="/css/style.css">
  <link rel="shortcut icon" href="/main_files/favicon.ico" />
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="/css/customStyle.css">
  <title> Â» Ahmad Badary</title>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
</head>

  <body>
    <nav class="main-nav">
    <a href="https://ahmedbadary.github.io/" class="main-nav-logo">
        <img src="/main_files/logo.png">
    </a>
    <ul id="menu-main" class="main-nav-items">
        <li id="menu-item-1859" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-home menu-item-1859">
            <a href="/">Home</a>
        </li>
        <li id="menu-item-2869" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-2869">
            <a href="/work">Work</a>
        </li>
        <li id="menu-item-1892" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1892">
            <a href="/projects">Projects</a>
        </li>
        <li id="menu-item-1858" class="menu-item menu-item-type-post_type menu-item-object-page current_page_parent menu-item-1858">
            <a href="/blog">Blog</a>
        </li>
        <li id="menu-item-1862" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1862">
            <a href="/about">About</a>
        </li>
    </ul>
</nav>


<section class="page-header">
  <h1 class="project-name">Classes of Matrices</h1>
  <h2 class="project-tagline"></h2>
  <a href="/#" class="btn">Home</a>
  <a href="/work" class="btn">Work-Space</a>
  <a href= /work_files/research/la.html class="btn">Previous</a>
</section>

<!-- <div>
  <ul class="posts">
    
      <li><span>02 Jan 2014</span> &raquo; <a href="/2014/01/02/introducing-Ahmad/">Introducing Ahmad</a></li>
    
  </ul>
</div> -->


    <section class="main-content">
      
      <div class="TOC">
  <h1 id="table-of-contents">Table of Contents</h1>

  <ul class="TOC1">
    <li><a href="#content1">Diagonal Matrices</a></li>
  </ul>
  <ul class="TOC2">
    <li><a href="#content2">Symmetric Matrices</a></li>
  </ul>
  <ul class="TOC3">
    <li><a href="#content3">Skew-Symmetric Matrices</a></li>
  </ul>
  <ul class="TOC4">
    <li><a href="#content4">Covariance Matrices</a></li>
  </ul>
  <ul class="TOC5">
    <li><a href="#content5">Positive Semi-Definite Matrices</a></li>
  </ul>
  <ul class="TOC6">
    <li><a href="#content6">Positive Definite Matrices</a></li>
  </ul>
  <ul class="TOC7">
    <li><a href="#content7">Orthogonal Matrices</a></li>
  </ul>
  <ul class="TOC8">
    <li><a href="#content8">Dyads</a></li>
  </ul>
  <ul class="TOC9">
    <li><a href="#content9">Correlation Matrices</a></li>
  </ul>
</div>

<hr />
<hr />

<p><a href="https://medium.com/@jonathan_hui/machine-learning-linear-algebra-special-matrices-c750cd742dfe">Special Matrices in ML</a> <br />
<a href="https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf">Matrix Cookbook</a></p>

<h2 id="content1">Diagonal Matrices</h2>

<ol>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents11">Definition.</strong><br />
 <strong>Diagonal matrices</strong> are square matrices <script type="math/tex">A</script> with <script type="math/tex">A_{ij} = 0 \text{, when } i \ne j.</script><br />
 <br /></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents12">Properties:</strong></p>
    <ol>
      <li>Diagonal matrices correspond to quadratic functions that are simple sums of squares, of the form:
        <p>$$q(x) = \sum_{i=1}^n \lambda_i x_i^2 = x^T \mathbf{diag}(\lambda) x.$$</p>
      </li>
      <li>They are easy to invert <script type="math/tex">A^{-1}_ {ii} = 1/A_ {ii} \: \forall i \in [1, n]</script></li>
      <li>The <strong>pseudo-inverse</strong> is easy to compute: keep zero diagonal elements as zero</li>
      <li>Allow easier matrix multiplication and powers<br />
 <br /></li>
    </ol>
  </li>
</ol>

<hr />

<h2 id="content2">Symmetric Matrices</h2>

<ol>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents21">Definition:</strong>  <br />
 <strong>Symmetric matrices</strong> are square matrices that satisfy <script type="math/tex">A_{ij} = A_{ji}</script> for every pair <script type="math/tex">(i,j).</script></p>

    <p><strong>The set of symmetric</strong> <script type="math/tex">(n \times n)</script> matrices is denoted <script type="math/tex">\mathbf{S}^n</script>. This set is a subspace of <script type="math/tex">\mathbf{R}^{n \times n}</script>.<br />
 <br /></p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents22">Properties:</strong>
    <ul>
      <li>Has <strong>orthonormal</strong> eigenvectors (even with repeated eigenvalues)</li>
      <li>Its inverse is also symmetric</li>
      <li>All eigenvalues of a symmetric matrix are <strong>real</strong></li>
      <li><script type="math/tex">A^TA</script> is <strong>invertible</strong> iff columns of <script type="math/tex">A</script> are linearly independent</li>
      <li>Every symmetric matrix <script type="math/tex">S</script> can be diagonalized (factorized) with <script type="math/tex">Q</script> formed by the orthonormal eigenvectors <script type="math/tex">v_i</script> of <script type="math/tex">S</script> and <script type="math/tex">\Lambda</script> is a diagonal matrix holding all the eigenvalues:
        <p>$$\begin{aligned} S&amp;=Q \Lambda Q^{T} \\
      &amp;= \lambda_{1} v_{1} v_{1}^{T}+\ldots+\lambda_{n} v_{n} v_{h}^{T} = \sum_{i=1}^n \lambda_i v_i v_i^T
      \end{aligned}$$</p>
      </li>
      <li>One can create a symmetric matrix from any matrix by:
        <p>$$M = {\displaystyle {\tfrac {1}{2}}\left(A+A^{\textsf {T}}\right)}$$</p>
        <p><br /></p>
      </li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents23">Examples:</strong>
    <ol>
      <li><a href="http://livebooklabs.com/keeppies/c5a5868ce26b8125/3c5245ebb8a556da" value="show" onclick="iframePopA(event)"><strong>Representation of a weighted, undirected graph.</strong></a>
 <a href="http://livebooklabs.com/keeppies/c5a5868ce26b8125/73a4ae787085d554"><code class="highlighter-rouge"> Visit the Book</code></a>
        <div></div>
      </li>
      <li><a href="http://livebooklabs.com/keeppies/c5a5868ce26b8125/0e696ef8a78e090c" value="show" onclick="iframePopA(event)"><strong>Laplacian matrix of a graph.</strong></a>
 <a href="http://livebooklabs.com/keeppies/c5a5868ce26b8125/73a4ae787085d554"><code class="highlighter-rouge"> Visit the Book</code></a>
        <div></div>
      </li>
      <li><a href="http://livebooklabs.com/keeppies/c5a5868ce26b8125/6c0afdfbf11892c6" value="show" onclick="iframePopA(event)"><strong>Hessian of a function.</strong></a>
 <a href="http://livebooklabs.com/keeppies/c5a5868ce26b8125/73a4ae787085d554"><code class="highlighter-rouge"> Visit the Book</code></a>
        <div></div>
      </li>
      <li><a href="http://livebooklabs.com/keeppies/c5a5868ce26b8125/e236418f4e2d6b3b" value="show" onclick="iframePopA(event)"><strong>Gram matrix of data points.</strong></a>
 <a href="http://livebooklabs.com/keeppies/c5a5868ce26b8125/73a4ae787085d554"><code class="highlighter-rouge"> Visit the Book</code></a>
        <div></div>
        <p><br /></p>
      </li>
    </ol>
  </li>
</ol>

<hr />

<h2 id="content3">Skew-Symmetric Matrices</h2>

<ol>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents31">Definition.</strong></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents32">Properties:</strong></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents33">Asynchronous:</strong></p>
  </li>
</ol>

<hr />

<h2 id="content4">Covariance Matrices</h2>

<ol>
  <li><strong style="color: SteelBlue" class="bodyContents4" id="bodyContents41">Definition.</strong> <br />
 <strong>Standard Form:</strong>
    <p>$$\Sigma :=\mathrm {E} \left[\left(\mathbf {X} -\mathrm {E} [\mathbf {X} ]\right)\left(\mathbf {X} -\mathrm {E} [\mathbf {X} ]\right)^{\rm {T}}\right]$$</p>
    <p>$$ \Sigma := \dfrac{1}{m} \sum_{k=1}^m (x_k - \hat{x})(x_k - \hat{x})^T. $$</p>

    <p><strong>Matrix Form:</strong></p>
    <p>$$ \Sigma := \dfrac{X^TX}{n} $$</p>

    <blockquote>
      <p>valid only for (1) <script type="math/tex">X</script> w/ samples in rows and variables in columns  (2) <script type="math/tex">X</script> is centered (mean=0)</p>
    </blockquote>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents4" id="bodyContents42">Properties:</strong>
    <ol>
      <li>
        <p>The sample covariance matrix allows to find the variance along any direction in data space.</p>
      </li>
      <li>
        <p>The diagonal elements of <script type="math/tex">\Sigma</script> give the variances of each vector in the data.</p>
      </li>
      <li>
        <p>The trace of <script type="math/tex">\Sigma</script> gives the sum of all the variances.</p>
      </li>
      <li>
        <p>The matrix <script type="math/tex">\Sigma</script> is positive semi-definite, since the associated quadratic form <script type="math/tex">u \rightarrow u^T \Sigma u</script> is non-negative everywhere.</p>
      </li>
      <li>
        <p>It is Symmetric.</p>
      </li>
      <li>Every symmetric positive semi-definite matrix is a covariance matrix.<br />
 <a href="http://ahmedbadary.ml/work_files/research/opt_probs#bodyContents12" value="show" onclick="iframePopA(event)"><strong>Proof.</strong></a><br />
 <a href="http://ahmedbadary.ml/work_files/research/opt_probs#bodyContents12"><code class="highlighter-rouge"> OR, Visit the website</code></a>
        <div></div>
      </li>
      <li>The sample variance along direction <script type="math/tex">u</script> can be expressed as a quadratic form in <script type="math/tex">u</script>:<br />
 <script type="math/tex">\sigma^2(u) = \dfrac{1}{n} \sum_{k=1}^n [u^T(x_k-\hat{x})]^2 = u^T \Sigma u,</script></li>
      <li>
        <p>The diminsion of the matrix is <script type="math/tex">(n \times n)</script>, where <script type="math/tex">n</script> is the number of variables/features/columns.</p>
      </li>
      <li>
        <p>The inverse of this matrix, <script type="math/tex">{\displaystyle \Sigma ^{-1},}</script> if it exists, is the inverse covariance matrix, also known as the <em>concentration matrix</em> or <em>precision matrix</em>.</p>
      </li>
      <li>
        <p>If a vector of <script type="math/tex">n</script> possibly correlated random variables is jointly normally distributed, or more generally elliptically distributed, then its probability density function can be expressed in terms of the covariance matrix.</p>
      </li>
      <li>
        <p><script type="math/tex">\Sigma =\mathrm {E} (\mathbf {XX^{\rm {T}}} )-{\boldsymbol {\mu }}{\boldsymbol {\mu }}^{\rm {T}}</script>.</p>
      </li>
      <li>
        <p><script type="math/tex">{\displaystyle \operatorname {var} (\mathbf {AX} +\mathbf {a} )=\mathbf {A} \,\operatorname {var} (\mathbf {X} )\,\mathbf {A^{\rm {T}}} }</script>.</p>
      </li>
      <li>
        <p><script type="math/tex">\operatorname {cov} (\mathbf {X} ,\mathbf {Y} )=\operatorname {cov} (\mathbf {Y} ,\mathbf {X} )^{\rm {T}}</script>.</p>
      </li>
      <li>
        <p><script type="math/tex">\operatorname {cov} (\mathbf {X}_ {1}+\mathbf {X}_ {2},\mathbf {Y} )=\operatorname {cov} (\mathbf {X}_ {1},\mathbf {Y} )+\operatorname {cov} (\mathbf {X}_ {2},\mathbf {Y} )</script>.</p>
      </li>
      <li>
        <p>If <script type="math/tex">(p = q)</script>, then <script type="math/tex">\operatorname {var} (\mathbf {X} +\mathbf {Y} )=\operatorname {var} (\mathbf {X} )+\operatorname {cov} (\mathbf {X} ,\mathbf {Y} )+\operatorname {cov} (\mathbf {Y} ,\mathbf {X} )+\operatorname {var} (\mathbf {Y} )</script>.</p>
      </li>
      <li>
        <p><script type="math/tex">\operatorname {cov} (\mathbf {AX} +\mathbf {a} ,\mathbf {B} ^{\rm {T}}\mathbf {Y} +\mathbf {b} )=\mathbf {A} \,\operatorname {cov} (\mathbf {X} ,\mathbf {Y} )\,\mathbf {B}</script>.</p>
      </li>
      <li>
        <p>If <script type="math/tex">{\displaystyle \mathbf {X} }</script>  and <script type="math/tex">{\displaystyle \mathbf {Y} }</script>  are independent (or somewhat less restrictedly, if every random variable in <script type="math/tex">{\displaystyle \mathbf {X} }</script> is uncorrelated with every random variable in <script type="math/tex">{\displaystyle \mathbf {Y} }</script>), then <script type="math/tex">{\displaystyle \operatorname {cov} (\mathbf {X} ,\mathbf {Y} )=\mathbf {0} }</script>.</p>
      </li>
      <li><script type="math/tex">\operatorname {var} (\mathbf {b} ^{\rm {T}}\mathbf {X} )= \mathbf {b} ^{\rm {T}}\operatorname {cov} (\mathbf {X} )\mathbf {b} = \operatorname {cov} (\mathbf{b}^T\mathbf {X}, \mathbf{b}^T\mathbf {X} ) \geq 0,\,</script>.
        <blockquote>
          <p>This quantity is NON-Negative because itâs variance.<br />
Maybe <script type="math/tex">= \mathbf {b} ^{\rm {T}}\operatorname {var} (\mathbf {X} )\mathbf {b}</script>??</p>
        </blockquote>
      </li>
      <li>
        <p>An identity covariance matrix, <script type="math/tex">\Sigma = I</script> has variance <script type="math/tex">= 1</script> for all variables.</p>
      </li>
      <li>
        <p>A covariance matrix of the form, <script type="math/tex">\Sigma=\sigma^2I</script> has variance <script type="math/tex">= \sigma^2</script> for all variables.</p>
      </li>
      <li>
        <p>A diagonal covariance matrix has variance <script type="math/tex">\sigma_i^2</script> for the <script type="math/tex">i-th</script>  variable.</p>
      </li>
      <li>When the mean <script type="math/tex">\hat{x}</script> is not known the denominator of the âSAMPLE COVARIANCE MATRIXâ should be <script type="math/tex">(n-1)</script> and not <script type="math/tex">n</script>.</li>
    </ol>

    <blockquote>
      <p>where,
  <script type="math/tex">{\displaystyle \mathbf {X} ,\mathbf {X} _{1}},</script> and <script type="math/tex">{\displaystyle \mathbf {X} _{2}}</script> are random <script type="math/tex">(p\times 1)</script> vectors, <script type="math/tex">{\displaystyle \mathbf {Y} }</script>  is a random <script type="math/tex">(q\times 1)</script> vector, <script type="math/tex">{\displaystyle \mathbf {a} }</script>  is a <script type="math/tex">(q\times 1)</script> vector, <script type="math/tex">{\displaystyle \mathbf {b} }</script> is a <script type="math/tex">(p\times 1)</script> vector, and <script type="math/tex">{\displaystyle \mathbf {A} }</script> and <script type="math/tex">{\displaystyle \mathbf {B} }</script>  are <script type="math/tex">(q\times p)</script> matrices of constants.</p>
    </blockquote>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents4" id="bodyContents43"><script type="math/tex">\Sigma</script> as a Linear Operator:</strong>
    <ul>
      <li><strong>Applied to one vector</strong>, the covariance matrix <em>maps a linear combination</em>, <script type="math/tex">c</script>, of the random variables, <script type="math/tex">X</script>, onto a vector of covariances with those variables:</li>
    </ul>

    <script type="math/tex; mode=display">{\displaystyle \mathbf {c} ^{\rm {T}}\Sigma =\operatorname {cov} (\mathbf {c} ^{\rm {T}}\mathbf {X} ,\mathbf {X} )}</script>

    <ul>
      <li><strong>Treated as a bilinear form</strong>, it yields the covariance between the two linear combinations:</li>
    </ul>

    <script type="math/tex; mode=display">{\displaystyle \mathbf {d} ^{\rm {T}}\Sigma \mathbf {c} =\operatorname {cov} (\mathbf {d} ^{\rm {T}}\mathbf {X} ,\mathbf {c} ^{\rm {T}}\mathbf {X} )}</script>

    <ul>
      <li><strong>The variance of a linear combination</strong> is then (its covariance with itself)</li>
    </ul>

    <script type="math/tex; mode=display">{\displaystyle \mathbf {c} ^{\rm {T}}\Sigma \mathbf {c} }</script>

    <ul>
      <li><strong>The (pseudo-)inverse covariance matrix</strong> provides an <em>inner product</em>,  <script type="math/tex">{\displaystyle \langle c-\mu \|\Sigma ^{+}\| c-\mu \rangle }</script>  which induces the <em>Mahalanobis distance</em>, a measure of the âunlikelihoodâ of <script type="math/tex">c</script>.</li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents4" id="bodyContents44">Applications [Examples]:</strong>
    <ol>
      <li>
        <p><a href="https://en.wikipedia.org/wiki/Whitening_transformation"><strong>The Whitening Transformation:</strong></a> allows one to completely decorrelate the data,  Equivalently,<br />
 allows one to find an optimal basis for representing the data in a compact way.</p>
      </li>
      <li>
        <p><a href="https://en.wikipedia.org/wiki/Rayleigh_quotient"><strong>Rayleigh Quotient:</strong></a></p>
      </li>
      <li>
        <p><a href="https://en.wikipedia.org/wiki/Principal_components_analysis"><strong>Principle Component Analysis [PCA]</strong></a></p>
      </li>
      <li>
        <p><a href="https://en.wikipedia.org/wiki/Karhunen-Lo%C3%A8ve_transform"><strong>The Karhunen-LoÃ¨ve transform (KL-transform)</strong></a></p>
      </li>
      <li>
        <p><a href="https://en.wikipedia.org/wiki/Mutual_fund_separation_theorem"><strong>Mutual fund separation theorem</strong></a></p>
      </li>
      <li>
        <p><a href="https://en.wikipedia.org/wiki/Capital_asset_pricing_model"><strong>Capital asset pricing model</strong></a></p>
      </li>
      <li>
        <p><a href="https://en.wikipedia.org/wiki/Modern_portfolio_theory"><strong>Portfolio Theory:</strong></a> The matrix of covariances among various assetsâ returns is used to determine, under certain assumptions, the relative amounts of different assets that investors should (in a normative analysis) or are predicted to (in a positive analysis) choose to hold in a context of diversification.
 <br /></p>
      </li>
    </ol>
  </li>
</ol>

<hr />

<h2 id="content5">Positive Semi-Definite Matrices</h2>

<ol>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents5" id="bodyContents51">Definition:</strong><br />
 A <em><strong>symmetric</strong></em> <script type="math/tex">{\displaystyle n\times n}</script> <em><strong>real</strong></em> matrix <script type="math/tex">{\displaystyle M}</script> is said to be <strong>positive semi-definite</strong> if the scalar <script type="math/tex">{\displaystyle z^{\textsf {T}}Mz}</script> is <em><strong>non-negative</strong></em> for <em><strong>every non-zero</strong></em> column vector <script type="math/tex">{\displaystyle z}</script>  of <script type="math/tex">n</script> real numbers.</p>

    <p><strong>Mathematically:</strong></p>
    <p>$$M \text { positive semi-definite } \Longleftrightarrow \quad x^{\top} M x \geq 0 \text { for all } x \in \mathbb{R}^{n}$$</p>
    <p><br /></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents5" id="bodyContents52">Properties:</strong></p>
    <ul>
      <li><script type="math/tex">AA^T</script> and <script type="math/tex">A^TA</script> are PSD</li>
      <li><script type="math/tex">M</script> is positive definite if All pivots &gt; 0</li>
      <li><script type="math/tex">M</script> is positive definite if and only if <strong>all of its eigenvalues are non-negative</strong>.</li>
      <li><strong>Covariance Matrices <script type="math/tex">\Sigma</script></strong> are PSD<br />
 <br /></li>
    </ul>
  </li>
</ol>

<!-- 3. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents53}    -->

<hr />

<h2 id="content6">Positive Definite Matrices</h2>

<ol>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents6" id="bodyContents61">Definition:</strong>  <br />
 A <em><strong>symmetric</strong></em> <script type="math/tex">{\displaystyle n\times n}</script> <em><strong>real</strong></em> matrix <script type="math/tex">{\displaystyle M}</script> is said to be <strong>positive definite</strong> if the scalar <script type="math/tex">{\displaystyle z^{\textsf {T}}Mz}</script> is strictly <em><strong>positive</strong></em> for <em><strong>every non-zero</strong></em> column vector <script type="math/tex">{\displaystyle z}</script>  of <script type="math/tex">n</script> real numbers.</p>

    <p><strong>Mathematically:</strong></p>
    <p>$$M \text { positive definite } \Longleftrightarrow x^{\top} M x&gt;0 \text { for all } x \in \mathbb{R}^{n} \backslash \mathbf{0}$$</p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents6" id="bodyContents62">Properties:</strong></p>
    <ul>
      <li><script type="math/tex">M</script> is positive definite if and only if <strong>all of its eigenvalues are positive</strong></li>
      <li>The matrix <script type="math/tex">{\displaystyle M}</script> is positive definite if and only if the bilinear form <script type="math/tex">{\displaystyle \langle z,w\rangle =z^{\textsf {T}}Mw}</script> is positive definite</li>
      <li>A symmetric matrix <script type="math/tex">{\displaystyle M}</script> is positive definite if and only if its <strong>quadratic form is a strictly convex function</strong></li>
      <li>Every positive definite matrix is <strong>invertible</strong> and its inverse is also positive definite.</li>
      <li><script type="math/tex">M</script> is positive definite if All pivots &gt; 0</li>
      <li><strong>Covariance Matrices <script type="math/tex">\Sigma</script></strong> are positive definite unless one variable is an exact linear function of the others. Conversely, every positive semi-definite matrix is the covariance matrix of some multivariate distribution</li>
      <li>Any quadratic function from <script type="math/tex">{\displaystyle \mathbb {R} ^{n}}</script> to <script type="math/tex">{\displaystyle \mathbb {R} }</script> can be written as <script type="math/tex">{\displaystyle x^{\textsf {T}}Mx+x^{\textsf {T}}b+c}</script> where <script type="math/tex">{\displaystyle M}</script> is a symmetric <script type="math/tex">{\displaystyle n\times n}</script> matrix, <script type="math/tex">b</script> is a real <script type="math/tex">n</script>-vector, and <script type="math/tex">c</script> a real constant. This quadratic function is strictly convex, and hence has a unique finite global minimum, if and only if <script type="math/tex">{\displaystyle M}</script> is positive definite.<br />
 <br /></li>
    </ul>
  </li>
</ol>

<!-- 3. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents6 #bodyContents63}  
 -->

<hr />

<h2 id="content7">Orthogonal Matrices</h2>

<ol>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents7" id="bodyContents71">Definition.</strong><br />
 Orthogonal (or, unitary (complex)) matrices are square matrices, such that the columns form an orthonormal basis.<br />
 <br /></p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents7" id="bodyContents72">Properties:</strong>
    <ol>
      <li>If <script type="math/tex">U = [u_1, \ldots, u_n]</script> is an orthogonal matrix, then
        <p>$$u_i^Tu_j = \left\{ \begin{array}{ll} 1 &amp; \mbox{if } i=j, \\  0 &amp; \mbox{otherwise.} \end{array} \right. $$</p>
      </li>
      <li>
        <p><script type="math/tex">U^TU = UU^T = I_n</script>.</p>
      </li>
      <li>Easy inverse:
        <p>$$U^{-1} = U^T$$</p>
      </li>
      <li>Geometrically, orthogonal matrices correspond to rotations (around a point) or reflections (around a line passing through the origin).
        <blockquote>
          <p>i.e. they preserve length and angles!<br />
Proof. Part 5 and 6.</p>
        </blockquote>
      </li>
      <li>For all vectors <script type="math/tex">\vec{x}</script>,<br />
 <script type="math/tex">\|Ux\|_2^2 = (Ux)^T(Ux) = x^TU^TUx = x^Tx = \|x\|_2^2 .</script>
        <blockquote>
          <p>Known as, <em>the rotational invariance</em> of the Euclidean norm.<br />
Thus, If we multiply x with an orthogonal matrix, the errors present in x will not be magnified. This behavior is very desirable for maintaining numerical stability.</p>
        </blockquote>
      </li>
      <li>If <script type="math/tex">x, y</script> are two vectors with unit norm, then the angle <script type="math/tex">\theta</script> between them satisfies <script type="math/tex">\cos \theta = x^Ty</script><br />
 while the angle <script type="math/tex">\theta'</script> between the rotated vectors <script type="math/tex">x' = Ux, y' = Uy</script> satisfies <script type="math/tex">\cos \theta' = (x')^Ty'</script>.<br />
 Since, <script type="math/tex">(Ux)^T(Uy) = x^T U^TU y = x^Ty,</script> we obtain that the angles are the same.<br />
 <br /></li>
    </ol>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents7" id="bodyContents73">Examples:</strong>
    <ul>
      <li><a href="http://livebooklabs.com/keeppies/c5a5868ce26b8125/72793237f1b79da9" value="show" onclick="iframePopA(event)"><strong>Permutation Matrices</strong></a>
  <a href="http://livebooklabs.com/keeppies/c5a5868ce26b8125/72793237f1b79da9"><code class="highlighter-rouge"> Visit the Book</code></a>
        <div></div>
        <p><br /></p>
      </li>
    </ul>
  </li>
</ol>

<hr />

<h2 id="content8">Dyads</h2>

<ol>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents8" id="bodyContents81">Definition.</strong><br />
 A matrix <script type="math/tex">A \in \mathbf{R}^{m \times n}</script> is a dyad if it is of the form <script type="math/tex">A = uv^T</script> for some vectors <script type="math/tex">u \in \mathbf{R}^m, v \in \mathbf{R}^n</script>.</p>

    <p>The dyad acts on an input vector <script type="math/tex">x \in \mathbf{R}^n</script> as follows:</p>
    <p>$$ Ax = (uv^T) x = (v^Tx) u.$$</p>
    <p><br /></p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents8" id="bodyContents82">Properties:</strong>
    <ol>
      <li>
        <p>The output always points in the same direction <script type="math/tex">u</script> in output space (<script type="math/tex">\mathbf{R}^m</script>), no matter what the input <script type="math/tex">x</script> is.</p>
      </li>
      <li>
        <p>The output is always a simple scaled version of <script type="math/tex">u</script>.</p>
      </li>
      <li>
        <p>The amount of scaling depends on the vector <script type="math/tex">v</script>, via the linear function <script type="math/tex">x \rightarrow v^Tx</script>.<br />
 <br /></p>
      </li>
    </ol>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents8" id="bodyContents83">Examples:</strong>
    <ul>
      <li><a href="http://livebooklabs.com/keeppies/c5a5868ce26b8125/438c7a0d0fc50d09" value="show" onclick="iframePopA(event)"><strong>Single factor model of financial price data</strong></a>
  <a href="http://livebooklabs.com/keeppies/c5a5868ce26b8125/438c7a0d0fc50d09"><code class="highlighter-rouge"> Visit the Book</code></a>
        <div></div>
      </li>
    </ul>

    <p><br /></p>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents8" id="bodyContents84">Normalized dyads:</strong></dt>
      <dd>We can always normalize the dyad, by assuming that both u,v are of unit (Euclidean) norm, and using a factor to capture their scale.</dd>
      <dd>That is, any dyad can be written in normalized form:</dd>
      <dd>
        <script type="math/tex; mode=display">A = uv^T = (\|u\|_2 \cdot |v|_2 ) \cdot (\dfrac{u}{\|u\|_2}) ( \dfrac{v}{\|v\|_2}) ^T = \sigma \tilde{u}\tilde{v}^T,</script>
      </dd>
      <dd>where <script type="math/tex">\sigma > 0</script>, and <script type="math/tex">\|\tilde{u}\|_2 = \|\tilde{v}\|_2 = 1.</script></dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents8" id="bodyContents85">Symmetric dyads:</strong></dt>
      <dd>Another important class of symmetric matrices is that of the form <script type="math/tex">uu^T</script>, where <script type="math/tex">u \in \mathbf{R}^n</script>.
 The matrix has elements <script type="math/tex">u_iu_j</script>, and is symmetric.
        <blockquote>
          <p>If <script type="math/tex">\|u\|_2 = 1</script>, then the dyad is said to be normalized.</p>
        </blockquote>
      </dd>
      <dd>
        <script type="math/tex; mode=display">% <![CDATA[
uu^T = \left(\begin{array}{ccc} u_1^2  & u_1u_2  & u_1u_3  \\
u_1u_2 & u_2^2   & u_2u_3  \\
u_1u_3 & u_2u_3  & u_3^2  \end{array} \right) %]]></script>
      </dd>
      <dd>
        <ul>
          <li><strong>Properties:</strong>
            <ol>
              <li>Symmetric dyads corresponds to quadratic functions that are simply squared linear forms:<br />
  <script type="math/tex">q(x) = (u^Tx)^2</script></li>
              <li>When the vector <script type="math/tex">u</script> is normalized (unit), then:<br />
  <script type="math/tex">\mathbf{Tr}(uu^T) = \|u\|_2^2 = 1^2 = 1</script>
                <blockquote>
                  <p>This follows from the fact that the diagonal entries of a symmetric dyad are just <script type="math/tex">u_i^2, \forall i \in [1, n]</script></p>
                </blockquote>
              </li>
              <li></li>
            </ol>
          </li>
        </ul>
      </dd>
    </dl>
  </li>
</ol>

<hr />

<h2 id="content9">Correlation matrix</h2>

<ol>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents9" id="bodyContents91">Definition.</strong></dt>
      <dd>
        <script type="math/tex; mode=display">{\text{corr}}(\mathbf {X} )=\left({\text{diag}}(\Sigma )\right)^{-{\frac {1}{2}}}\,\Sigma \,\left({\text{diag}}(\Sigma )\right)^{-{\frac {1}{2}}}</script>
      </dd>
    </dl>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents9" id="bodyContents92">Properties:</strong>
    <ol>
      <li>
        <p>It is the matrix of âPearson product-moment correlation coefficientsâ between each of the random variables in the random vector <script type="math/tex">{\displaystyle \mathbf {X} }</script>.</p>
      </li>
      <li>
        <p>The correlation matrix can be seen as the covariance matrix of the standardized random variables <script type="math/tex">{\displaystyle X_{i}/\sigma (X_{i})}</script> for <script type="math/tex">{\displaystyle i=1,\dots ,n}</script>.</p>
      </li>
      <li>
        <p>Each element on the principal diagonal of a correlation matrix is the correlation of a random variable with itself, which always equals 1.</p>
      </li>
      <li>
        <p>Each off-diagonal element is between 1 and â1 inclusive.</p>
      </li>
    </ol>
  </li>
</ol>

<!-- 3. **Examples:**{: style="color: SteelBlue"}{: .bodyContents9 #bodyContents93}  

***

## Ten
{: #content10}

1. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents10 #bodyContents101}  

2. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents10 #bodyContents102}  

3. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents10 #bodyContents103}  

4. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents10 #bodyContents104}  

5. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents10 #bodyContents105}  

6. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents10 #bodyContents106}  

7. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents10 #bodyContents107}  

8. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents10 #bodyContents108}  
 -->


      <footer class="site-footer">
    <!--   <span class="site-footer-owner"><a href="http://localhost:8889">Ahmad Badary</a> is maintained by <a href="http://ahmedbadary.ml">Ahmad Badary</a>.</span> -->
    
<!--  -->
    <div class="wrapper">
        <div class="footer-col-wrapper">
            <div class="footer-col footer-col-1">
            <span class="site-footer-owner"><a href="http://localhost:8889">Site</a> maintained by <a href="http://ahmedbadary.ml">Ahmad Badary</a>.</span>
    <span class="site-footer-credits">
        <p>
            &copy; 2017. All rights reserved.
        </p> 
    </span>
            </div>
            <div class="footer-col footer-col-2">
            <div><p>         </p></div>
            </div>
            <div class="footer-col footer-col-3">
                <ul class="social-media-list">
                    
                      <li>
                        <a href="https://github.com/AhmedBadary">
                          <i class="fa fa-github"></i> GitHub
                        </a>
                      </li>
                    
                    
                      <li>
                        <a href="https://linkedin.com/in/ahmad-badary-656098121/">
                          <i class="fa fa-linkedin"></i> LinkedIn
                        </a>
                      </li>
                    
                    
                      <li>
                        <a href="https://www.facebook.com/ahmed.thabet.94">
                          <i class="fa fa-facebook"></i> Facebook
                        </a>
                      </li>
                    
                </ul>
            </div>
        </div>
    </div>
<!--  -->
</footer>


    </section>

  </body>

<!-- Table of Content Script -->
<script type="text/javascript">
var bodyContents = $(".bodyContents1");
$("<ol>").addClass("TOC1ul").appendTo(".TOC1");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC1ul");
     });
// 
var bodyContents = $(".bodyContents2");
$("<ol>").addClass("TOC2ul").appendTo(".TOC2");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC2ul");
     });
// 
var bodyContents = $(".bodyContents3");
$("<ol>").addClass("TOC3ul").appendTo(".TOC3");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC3ul");
     });
//
var bodyContents = $(".bodyContents4");
$("<ol>").addClass("TOC4ul").appendTo(".TOC4");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC4ul");
     });
//
var bodyContents = $(".bodyContents5");
$("<ol>").addClass("TOC5ul").appendTo(".TOC5");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC5ul");
     });
//
var bodyContents = $(".bodyContents6");
$("<ol>").addClass("TOC6ul").appendTo(".TOC6");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC6ul");
     });
//
var bodyContents = $(".bodyContents7");
$("<ol>").addClass("TOC7ul").appendTo(".TOC7");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC7ul");
     });
//
var bodyContents = $(".bodyContents8");
$("<ol>").addClass("TOC8ul").appendTo(".TOC8");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC8ul");
     });
//
var bodyContents = $(".bodyContents9");
$("<ol>").addClass("TOC9ul").appendTo(".TOC9");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC9ul");
     });

</script>

<!-- VIDEO BUTTONS SCRIPT -->
<script type="text/javascript">
  function iframePopInject(event) {
    var $button = $(event.target);
    // console.log($button.parent().next());
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $figure = $("<div>").addClass("video_container");
        $iframe = $("<iframe>").appendTo($figure);
        $iframe.attr("src", $button.attr("src"));
        // $iframe.attr("frameborder", "0");
        $iframe.attr("allowfullscreen", "true");
        $iframe.css("padding", "4px 6px");
        $button.next().css("display", "block");
        $figure.appendTo($button.next());
        $button.text("Hide Video")
    } else {
        $button.attr('value', 'show');
        $button.next().html("");
        $button.text("Show Video")
    }
}
</script>

<!-- BUTTON TRY -->
<script type="text/javascript">
  function iframePopA(event) {
    event.preventDefault();
    var $a = $(event.target).parent();
    console.log($a);
    if ($a.attr('value') == 'show') {
        $a.attr('value', 'hide');
        $figure = $("<div>");
        $iframe = $("<iframe>").addClass("popup_website_container").appendTo($figure);
        $iframe.attr("src", $a.attr("href"));
        $iframe.attr("frameborder", "1");
        $iframe.attr("allowfullscreen", "true");
        $iframe.css("padding", "4px 6px");
        $a.next().css("display", "block");
        $figure.appendTo($a.next().next());
        // $a.text("Hide Content")
        $('html, body').animate({
            scrollTop: $a.offset().top
        }, 1000);
    } else {
        $a.attr('value', 'show');
        $a.next().next().html("");
        // $a.text("Show Content")
    }

    $a.next().css("display", "inline");
}
</script>


<!-- TEXT BUTTON SCRIPT - INJECT -->
<script type="text/javascript">
  function showTextPopInject(event) {
    var $button = $(event.target);
    var txt = $button.attr("input");
    console.log(txt);
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $p = $("<p>");
        $p.html(txt);
        $button.next().css("display", "block");
        $p.appendTo($button.next());
        $button.text("Hide Content")
    } else {
        $button.attr('value', 'show');
        $button.next().html("");
        $button.text("Show Content")
    }

}
</script>

<!-- TEXT BUTTON SCRIPT - HIDDEN / HIDE / SHOW / HIDE/SHOW -->
<script type="text/javascript">
  function showTextPopHide(event) {
    var $button = $(event.target);
    // var txt = $button.attr("input");
    var txt = $button.text();
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $button.next().removeAttr("hidden");
        $button.text(txt + " - Hide Content");
    } else {
        $button.attr('value', 'show');
        $button.next().attr("hidden", "");
        $button.text(txt.replace(" - Hide Content",""));
    }
}
</script>

<!-- TEXT BUTTON SCRIPT - HIDDEN / HIDE / SHOW / HIDE/SHOW -->
<script type="text/javascript">
  function showText_withParent_PopHide(event) {
    var $button = $(event.target);
    var $parent = $button.parent();
    var txt = $button.text();
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $parent.next().removeAttr("hidden");
        $button.text(txt + " - Hide Content");
    } else {
        $button.attr('value', 'show');
        $parent.next().attr("hidden", "");
        $button.text(txt.replace(" - Hide Content",""));
    }
}
</script>

<!-- Print / Printing / printme -->
<!-- <script type="text/javascript">
i = 0

for (var i = 1; i < 6; i++) {
    var bodyContents = $(".bodyContents" + i);
    $("<p>").addClass("TOC1ul")  .appendTo(".TOC1");
    bodyContents.each(function(index, element) {
        var paragraph = $(element);
        $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC1ul");
         });
} 
</script>
 -->
 
</html>

