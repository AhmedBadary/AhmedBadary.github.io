<!DOCTYPE html>
<html lang="en-us">
  <head>
  <meta charset="UTF-8">
  <title>Ahmad Badary</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <link rel="stylesheet" href="/css/normalize.css">
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="/css/cayman.css">
  <link rel="stylesheet" href="/css/style.css">
  <link rel="shortcut icon" href="/main_files/favicon.ico" />
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="/css/customStyle.css">
  <title> » Ahmad Badary</title>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
</head>

  <body>
    <nav class="main-nav">
    <a href="https://ahmedbadary.github.io/" class="main-nav-logo">
        <img src="/main_files/logo.png">
    </a>
    <ul id="menu-main" class="main-nav-items">
        <li id="menu-item-1859" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-home menu-item-1859">
            <a href="/">Home</a>
        </li>
        <li id="menu-item-2869" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-2869">
            <a href="/work">Work</a>
        </li>
        <li id="menu-item-1892" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1892">
            <a href="/projects">Projects</a>
        </li>
        <li id="menu-item-1858" class="menu-item menu-item-type-post_type menu-item-object-page current_page_parent menu-item-1858">
            <a href="/blog">Blog</a>
        </li>
        <li id="menu-item-1862" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1862">
            <a href="/about">About</a>
        </li>
    </ul>
</nav>


<section class="page-header">
  <h1 class="project-name">Generative Compression</h1>
  <h2 class="project-tagline"></h2>
  <a href="/#" class="btn">Home</a>
  <a href="/work" class="btn">Work-Space</a>
  <a href= /work_files/research/dl/cv.html class="btn">Previous</a>
</section>

<!-- <div>
  <ul class="posts">
    
      <li><span>02 Jan 2014</span> &raquo; <a href="/2014/01/02/introducing-Ahmad/">Introducing Ahmad</a></li>
    
  </ul>
</div> -->


    <section class="main-content">
      
      <div class="TOC">
  <h1 id="table-of-contents">Table of Contents</h1>

  <ul class="TOC1">
    <li><a href="#content1">Introduction</a></li>
  </ul>
  <ul class="TOC2">
    <li><a href="#content2">WaveOne</a></li>
  </ul>
  <ul class="TOC3">
    <li><a href="#content3">Proposed Changes</a></li>
  </ul>
</div>

<hr />
<hr />

<p><a href="https://docs.google.com/document/d/12yb9bhZfr84e6tPJwwJrKXpNKVEhhbmoMgOGl_gGbBY/edit">Feature Extraction (Notes - Docs)</a><br />
<a href="https://docs.google.com/document/d/1TUHWxU3TPR1mRCDF1kUM9xgNmvVHe5f9KxvBb4sPu_Q/edit">Audio Compression</a></p>
<ul>
  <li><a href="https://bair.berkeley.edu/blog/2019/09/19/bit-swap/">A Deep Learning Approach to Data Compression (Blog+Paper!)</a></li>
</ul>

<h2 id="content1">Introduction</h2>

<ol>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents15">ML-based Compression:</strong></dt>
      <dd>The main idea behind ML-based compression is that structure is <strong>automatically discovered</strong> instead of <strong>manually engineered</strong>.</dd>
      <dd>
        <ul>
          <li><strong>Examples</strong>:
            <ul>
              <li><em><strong>DjVu</strong></em>: employs segmentation and K-means clustering to separate foreground from background and analyze the documents contents.</li>
            </ul>
          </li>
        </ul>
      </dd>
    </dl>
  </li>
</ol>

<hr />

<h2 id="content2">WaveOne</h2>

<p><a href="https://arxiv.org/pdf/1705.05823.pdf">Further Reading</a></p>

<p>WaveOne is a machine learning-based approach to lossy image compression.</p>

<ol>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents21">Main Idea:</strong></dt>
      <dd>An ML-based approach to compression that utilizes the older techniques for quantization but with an encoder-decoder model that depends on adversarial training for a higher quality reconstruction.</dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents22">Model:</strong></dt>
      <dd>The model includes three main steps that are layered together in one pipeline:
        <ul>
          <li><strong>Feature Extraction</strong>: an approach that aims to recognize the different types of structures in an image.
            <ul>
              <li><strong>Structures</strong>:
                <ul>
                  <li>Across input channels</li>
                  <li>Within individual scales</li>
                  <li>Across Scales</li>
                </ul>
              </li>
              <li><strong>Methods</strong>:
                <ul>
                  <li><em><strong>Pyramidal Decomposition</strong></em>: for analyzing individual scales</li>
                  <li><em><strong>Interscale Alignment Procedure</strong></em>:  for exploiting structure shared across scales</li>
                </ul>
              </li>
            </ul>
          </li>
          <li><strong>Code Computation and Regularization</strong>: a module responsible for further compressing the extracted features by *quantizing the features and encoding them via two methods.
            <ul>
              <li><strong>Methods</strong>:
                <ul>
                  <li><em><strong>Adaptive Arithmetic Coding Scheme</strong></em>: applied on the features binary expansions</li>
                  <li><em><strong>Adaptive Codelength Regularization</strong></em>:  to penalize the entropy of the features to achieve better compression</li>
                </ul>
              </li>
            </ul>
          </li>
          <li><strong>Adversarial Training (Discriminator Loss)</strong>: a module responsible for enforcing realistic reconstructions.
            <ul>
              <li><strong>Methods</strong>:
                <ul>
                  <li><em><strong>Adaptive Arithmetic Coding Scheme</strong></em>: applied on the features binary expansions</li>
                  <li><em><strong>Adaptive Codelength Regularization</strong></em>:  to penalize the entropy of the features to achieve better compression</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents23">Feature Extraction:</strong></dt>
      <dd>
        <ul>
          <li><strong>Pyramidal Decomposition</strong>: <br />
  Inspired by <em>the use of wavelets for multiresolution analysis, in which an input is analyzed recursively via feature extraction and downsampling operators</em>, the pyramidal decomposition encoder generalizes the wavelet decomposition idea to <em>learn optimal, nonlinear extractors individually for each scale</em>.<br />
  For each input <script type="math/tex">\mathbf{x}</script> to the model, and a total of <script type="math/tex">M</script> scales, denote the input to scale <script type="math/tex">m</script> by <script type="math/tex">\mathbf{x}_m</script>.
            <ul>
              <li><em><strong>Algorithm</strong></em>:
                <ul>
                  <li>Set input to first scale <script type="math/tex">\mathbf{x}_1 = \mathbf{x}</script></li>
                  <li>For each scale <script type="math/tex">m</script>:
                    <ul>
                      <li>Extract coefficients <script type="math/tex">\mathbf{c}_m = \mathbf{f}_m(\mathbf{x}_m) \in \mathbb{R}^{C_m \times H_m \times W_m}</script> via some parametrized function <script type="math/tex">\mathbf{f}_m(\dot)</script> for output channels <script type="math/tex">C_m</script>, height <script type="math/tex">H_m</script> and width <script type="math/tex">W_m</script></li>
                      <li>Compute the input to the next scale as <script type="math/tex">\mathbf{x}_{m+1} = \mathbf{D}_m(\mathbf{x}_m)</script>, where <script type="math/tex">\mathbf{D}_m(\dot)</script> is some <em>downsampling operator</em> (either fixed or learned)</li>
                    </ul>
                  </li>
                </ul>
              </li>
            </ul>

            <p>Typically, <script type="math/tex">M</script> is chosen to be <script type="math/tex">= 6</script> scales.<br />
  The <strong>feature extractors for the individual scales</strong> are composed of <em><strong>a sequence of convolutions with kernels <script type="math/tex">3 \times 3</script> or <script type="math/tex">1 \times 1</script> and ReLUs with a leak of <script type="math/tex">0.2</script></strong></em>.<br />
  All <em>downsamplers</em> are learned as <script type="math/tex">4 \times 4</script> convolutions with a stride of <script type="math/tex">2</script>.</p>
          </li>
        </ul>
      </dd>
      <dd><img src="/main_files/cv/compression/1.png" alt="img" width="80%" /></dd>
      <dd>
        <ul>
          <li><strong>Interscale Alignment</strong>: <br />
  Designed to leverage information shared across different scales — a benefit not offered by the classic wavelet analysis.
            <ul>
              <li><em><strong>Structure</strong></em>:
                <ul>
                  <li><strong>Input</strong>: the set of coefficients extracted from the different scales <script type="math/tex">\{\mathbf{c}_m\}_{m=1}^M \subset \mathbb{R}^{C_m \times H_m \times W_m}</script></li>
                  <li><strong>Output</strong>: a tensor <script type="math/tex">\mathbf{y} \in \mathbb{R}^{C \times H \times W}</script></li>
                </ul>
              </li>
              <li><em><strong>Algorithm</strong></em>:
                <ul>
                  <li>Map each input tensor <script type="math/tex">\mathbf{c}_m</script> to the target dimensionality via some parametrized function <script type="math/tex">\mathbf{g}_m(·)</script>:  this involves ensuring that this function spatially resamples <script type="math/tex">\mathbf{c}_m</script> to the appropriate output map size <script type="math/tex">H \times W</script>, and ouputs the appropriate number of channels <script type="math/tex">C</script></li>
                  <li>Sum <script type="math/tex">\mathbf{g}_m(\mathbf{c}_m) = 1, \ldots, M</script>, and apply another parameterized non-linear transformation <script type="math/tex">\mathbf{g}(·)</script> for <em>joint processing</em><br />
<script type="math/tex">\mathbf{g}_m(·)</script> is chosen as a <strong>convolution</strong> or a <strong>deconvolution</strong> with an appropriate stride to produce the target spatial map size <script type="math/tex">H \times W</script>.<br />
<script type="math/tex">\mathbf{g}(·)</script> is choses as a <strong>sequence of <script type="math/tex">3 \times 3</script> convolutions</strong>.</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents24">Code Computation and Regularization:</strong></dt>
      <dd>Given the output tensor <script type="math/tex">\mathbf{y} \in \mathbb{R}^{C \times H \times W}</script> of the <em>feature extraction step</em> (namely alignment), we proceed to <strong>quantize and encode</strong> it.</dd>
      <dd>
        <ul>
          <li><strong>Quantization</strong>: the tensor <script type="math/tex">\mathbf{y}</script> is quantized to bit precision <script type="math/tex">B</script>:  <br />
  <img src="/main_files/cv/compression/2.png" alt="img" width="50%" /><br />
  Given a desired precision of <script type="math/tex">B</script> bits, we quantize the feature tensor into <script type="math/tex">2^B</script> equal-sized bins as:<br />
  <img src="/main_files/cv/compression/3.png" alt="img" width="80%" /><br />
  For the special case <script type="math/tex">B = 1</script>, this reduces exactly to a binary quantization scheme.<br />
  In-practice <script type="math/tex">B = 6</script> is chosen as a smoother quantization method.
            <ul>
              <li><strong>Reason</strong>: <br />
  Mapping the <em>continuous input values</em> representing the image signal to a <em>smaller countable set</em> to achieve a desired precision of <script type="math/tex">B</script> bits</li>
            </ul>
          </li>
        </ul>
      </dd>
      <dd>
        <ul>
          <li>
            <p><strong>Bitplane Decomposition</strong>: we transform the <em>quantized tensor</em> <script type="math/tex">\mathbf{\hat{y}}</script> into a <em>binary tensor</em> suitable for encoding via a lossless bitplane decomposition:<br />
  <img src="/main_files/cv/compression/4.png" alt="img" width="80%" /><br />
  <script type="math/tex">\mathbf{\hat{y}}</script> is decomposed into <em>bitplanes</em> by a transformation that maps each value <script type="math/tex">\hat{y}_{chw}</script> into its <em>binary expansion</em> of <script type="math/tex">B</script> bits.<br />
  Hence, each of the <script type="math/tex">C</script> spatial maps <script type="math/tex">\mathbf{\hat{y}}_c \in \mathbb{R}^{H \times W}</script> of <script type="math/tex">\mathbf{\hat{y}}</script> expands into <script type="math/tex">B</script> <em>binary bitplanes</em>.</p>

            <ul>
              <li><strong>Reason</strong>: <br />
  This decomposition enables the entropy coder to exploit structure in the distribution of the activations in <script type="math/tex">\mathbf{y}</script> to achieve a compact representation.</li>
            </ul>
          </li>
        </ul>
      </dd>
      <dd>
        <ul>
          <li><strong>Adaptive Arithmetic Encoding</strong>: encodes <script type="math/tex">\mathbf{b}</script> into its final <em>variable-length binary sequence</em> <script type="math/tex">\mathbf{s}</script> of length <script type="math/tex">\mathcal{l}(\mathbf{s})</script>:<br />
  <img src="/main_files/cv/compression/5.png" alt="img" width="65%" />
            <ul>
              <li>The <em>binary tensor</em> <script type="math/tex">\mathbf{b}</script> that is produced by the bitplane decomposition contains significant structure (e.g. higher bitplanes are sparser, and spatially neighboring bits often have the same value).<br />
  This structure can be exploited by using Adaptive Arithmetic Encoding.</li>
              <li><strong>Method</strong>:
                <ul>
                  <li><em><strong>Encoding</strong></em>:   <br />
  Associate each bit location in the <em>binary tensor</em> <script type="math/tex">\mathbf{b}</script> with a <em>context</em>, which comprises a set of features indicative of the bit value.<br />
  The <em>features</em> are based on the <em>position of the bit</em> and the <em>values of neighboring bits.<br />
  To predict the value of each bit from its context features, we _train a classifier</em> and use <em>its output probabilities</em> to compress <script type="math/tex">\mathbf{b}</script> via <em>arithmetic coding</em>.</li>
                  <li><em><strong>Decoding</strong></em>:  <br />
  At decoding time, we perform the <em>inverse operation</em> to <em>decompress the code</em>.<br />
  We interleave between:
                    <ul>
                      <li>Computing the context of a particular bit using the values of previously decoded bits</li>
                      <li>Using this context to retrieve the activation probability of the bit and decode it
                        <blockquote>
                          <p>This operation constrains the context of each bit to only include features composed of bits already decoded</p>
                        </blockquote>
                      </li>
                    </ul>
                  </li>
                </ul>
              </li>
              <li><strong>Reason</strong>: <br />
  We aim to leverage the structure in the data, specifically in the <em>binary tensor</em> <script type="math/tex">\mathbf{b}</script> produced by the <em>bitplane decomposition</em> which has <em>low entropy</em></li>
            </ul>
          </li>
        </ul>
      </dd>
      <dd>
        <ul>
          <li><strong>Adaptive Codelength Regularization</strong>: modulates the distribution of the quantized representation <script type="math/tex">\mathbf{\hat{y}}</script> to achieve a target expected bit count across inputs:<br />
  <img src="/main_files/cv/compression/6.png" alt="img" width="60%" />
            <ul>
              <li><strong>Goal</strong>: regulate the expected codelength <script type="math/tex">\mathbb{E}_x[\mathcal{l}(\mathbf{s})]</script> to a target value <script type="math/tex">\mathcal{l}_{\text{target}}</script>.</li>
              <li><strong>Method</strong>:<br />
  We design a penalty that encourages a structure that the <strong>AAC</strong> is able to encode.<br />
  Namely, we <em><strong>regularize</strong></em> the <em>quantized tensor</em> <script type="math/tex">\mathbf{\hat{y}}</script> with:<br />
  <img src="/main_files/cv/compression/7.png" alt="img" width="60%" /><br />
  for iteration <script type="math/tex">t</script> and difference index set <script type="math/tex">S = \{(0,1), (1,0), (1,1), (-1,1)\}</script>.
                <blockquote>
                  <p>The <strong>first term</strong> <em>penalizes the magnitude of each tensor element</em><br />
The <strong>Second Term</strong> <em>penalizes deviations between spatial neighbors</em></p>
                </blockquote>
              </li>
              <li><strong>Reason</strong>: <br />
  The <em>Adaptive Codelength Regularization</em> is designed to solve one problem; the non-variability of the latent space code, which is what controls (defines) the bitrate.<br />
  It, essentially, allows us to have latent-space codes with different lengths, depending on the complexity of the input, <em>by enabling better prediction by the <strong>AAX</strong></em><br />
  In practice, a <strong>total-to-target ratio</strong> <script type="math/tex">= BCHW/\mathcal{l}_{\text{target}} = 4</script> works well.</li>
            </ul>
          </li>
        </ul>
      </dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents25">Adversarial Train:</strong></dt>
      <dd>
        <ul>
          <li><strong>GAN Architecture</strong>:
            <ul>
              <li><em><strong>Generator</strong></em>: Encoder-Decoder Pipeline</li>
              <li><em><strong>Discriminator</strong></em>: Classification ConvNet</li>
            </ul>
          </li>
        </ul>
      </dd>
      <dd>
        <ul>
          <li><strong>Discriminator Design</strong>:</li>
        </ul>
      </dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents27">Architecture:</strong></dt>
      <dd>
        <ul>
          <li><strong>Problem</strong>: Regression Problem</li>
          <li><strong>Goal</strong>: Learn a function <script type="math/tex">\psi(x;\theta)</script> that is trained and used to regress to a pose vector.</li>
          <li><strong>Estimation</strong>: <script type="math/tex">\psi</script> is based on (learned through) Deep Neural Net</li>
          <li><strong>Deep Neural Net</strong>: is a Convolutional Neural Network; namely, <strong>AlexNet</strong>
            <ul>
              <li><em><strong>Input</strong></em>: image with pre-defined size <script type="math/tex">= \:</script> #-pixels <script type="math/tex">\times 3</script>-color channels
                <blockquote>
                  <p><script type="math/tex">(220 \times 220)</script> with a stride of <script type="math/tex">4</script></p>
                </blockquote>
              </li>
              <li><em><strong>Output</strong></em>: target value of the regression<script type="math/tex">= 2k</script> joint coordinates</li>
            </ul>
          </li>
        </ul>
      </dd>
      <dd>
        <blockquote>
          <p>Denote by <script type="math/tex">\mathbf{C}</script> a convolutional layer, by <script type="math/tex">\mathbf{LRN}</script> a local response normalization layer, <script type="math/tex">\mathbf{P}</script> a pooling layer and by <script type="math/tex">\mathbf{F}</script> a fully connected layer</p>
        </blockquote>
      </dd>
      <dd>
        <blockquote>
          <p>For <script type="math/tex">\mathbf{C}</script> layers, the size is defined as width <script type="math/tex">\times</script> height <script type="math/tex">\times</script> depth, where the first two dimensions have a spatial meaning while the depth defines the number of filters.</p>
        </blockquote>
      </dd>
      <dd>
        <ul>
          <li><strong>Alex-Net</strong>:
            <ul>
              <li><em><strong>Architecture</strong></em>:     <script type="math/tex">\mathbf{C}(55 \times 55 \times 96) − \mathbf{LRN} − \mathbf{P} − \mathbf{C}(27 \times 27 \times 256) − \mathbf{LRN} − \mathbf{P} − \\\mathbf{C}(13 \times 13 \times 384) − \mathbf{C}(13 \times 13 \times 384) − \mathbf{C}(13 \times 13 \times 256) − \mathbf{P} − \mathbf{F}(4096) − \mathbf{F}(4096)</script></li>
              <li><em><strong>Filters</strong></em>:
                <ul>
                  <li><script type="math/tex">\mathbf{C}_{1} = 11 \times 11</script>,</li>
                  <li><script type="math/tex">\mathbf{C}_{2} = 5 \times 5</script>,</li>
                  <li><script type="math/tex">\mathbf{C}_{3-5} = 3 \times 3</script>.</li>
                </ul>
              </li>
              <li><em><strong>Total Number of Parameters</strong></em> <script type="math/tex">= 40</script>M</li>
              <li><em><strong>Training Dataset</strong></em>:<br />
  Denote by <script type="math/tex">D</script> the training set and <script type="math/tex">D_N</script> the normalized training set: <br />
  <script type="math/tex">\ \ \ \ \ \ \ \ \ \ \ \ \ \</script> <script type="math/tex">\ \ \ \ \ \ \ \ \ \ \ \ \ \</script>  <script type="math/tex">D_N = \{(N(x),N(\mathbf{y}))\vert (x,\mathbf{y}) \in D\}</script></li>
              <li><em><strong>Loss</strong></em>: the Loss is modified; instead of a <em>classification loss</em>, we train a linear regression on top of the last network layer to predict a pose vector by minimizing <script type="math/tex">L_2</script> distance between the prediction and the true pose vector,</li>
            </ul>
          </li>
        </ul>
      </dd>
      <dd>
        <script type="math/tex; mode=display">\arg \min_\theta \sum_{(x,y) \in D_N} \sum_{i=1}^k \|\mathbf{y}_i - \psi_i(x;\theta)\|_2^2</script>
      </dd>
      <dd>
        <ul>
          <li><strong>Optimization</strong>:
            <ul>
              <li><em><strong>BackPropagation</strong></em> in a distributed online implementation</li>
              <li><em><strong>Adaptive Gradient Updates</strong></em></li>
              <li><em><strong>Learning Rate</strong></em> <script type="math/tex">= 0.0005 = 5\times 10^{-4}</script></li>
              <li><em><strong>Data Augmentation</strong></em>: randomly translated image crops, left/right flips</li>
              <li><em><strong>DropOut Regularization</strong></em> for the <script type="math/tex">\mathbf{F}</script> layers <script type="math/tex">= 0.6</script></li>
            </ul>
          </li>
        </ul>
      </dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents29">Architecture:</strong></dt>
      <dd>
        <ul>
          <li><strong>Motivation</strong>: <br />
  Although, the pose formulation of the DNN has the advantage that the joint estimation is based on the full image and thus relies on context, due its fixed input size of <script type="math/tex">220 \times 220</script>, the network has <em>limited capacity to look at detail</em> - it <em>learns filters capturing pose properties at coarse scale</em>.<br />
  The <em>pose properties</em> are necessary to <em>estimate rough pose</em> but <strong>insufficient</strong> to always <em>precisely localize the body joints</em>.<br />
  Increasing the input size is infeasible since it will increase the already large number of parameters.<br />
  Thus, a <em>cascade of pose regressors</em> is used to achieve better precision.</li>
          <li><strong>Structure and Training</strong>: <br />
  At the first stage:
            <ul>
              <li>The cascade starts off by estimating an initial pose as outlined in the previous section.<br />
  At subsequent stages:</li>
              <li>Additional DNN regressors are trained to predict a displacement of the joint locations from previous stage to the true location.
                <blockquote>
                  <p>Thus, each subsequent stage can be thought of as a refinement of the currently predicted pose.</p>
                </blockquote>
              </li>
              <li>Each subsequent stage uses the predicted joint locations to focus on the relevant parts of the image – subimages are cropped around the predicted joint location from previous stage and the pose displacement regressor for this joint is applied on this sub-image.
                <blockquote>
                  <p>Thus, subsequent pose regressors see higher resolution images and thus learn features for finer scales which ultimately leads to higher precision</p>
                </blockquote>
              </li>
            </ul>
          </li>
          <li><strong>Method and Architecture</strong>:
            <ul>
              <li>The same network architecture is used for all stages of the cascade but learn different parameters.</li>
              <li>Start with a bounding box <script type="math/tex">b^0</script>: which either encloses the full image or is obtained by a person detector</li>
              <li>Obtain an initial pose:<br />
  Stage 1: <script type="math/tex">\mathbf{y}^1 \leftarrow N^{-1}(\psi(N(x;b^0);\theta_1);b^0)</script></li>
              <li>At stages <script type="math/tex">s \geq 2</script>, for all joints:
                <ul>
                  <li>Regress first towards a refinement displacement <script type="math/tex">\mathbf{y}_i^s - \mathbf{y}_i^{(s-1)}</script> by applying a regressor on the sub image defined by <script type="math/tex">b_i^{(s-1)}</script></li>
                  <li>Estimate new joint boxes <script type="math/tex">b_i^s</script>:<br />
  Stage <script type="math/tex">s</script>: <script type="math/tex">\ \ \ \ \ \ \ \ \ \ \ \ \ \ \mathbf{y}_i^s \leftarrow \mathbf{y}_i^{(2-1)} + N^{-1}(\psi(N(x;b^0);\theta_s);b)  \:\: (6)  \\
 \ \ \ \ \ \ \ \ \ \ \ \ \ \                  \:\:\:\: \text{for } b = b_i^(s-1) \\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ 
  b_i^s \leftarrow (\mathbf{y}_i^s, \sigma diam(\mathbf{y}^s), \sigma diam(\mathbf{y}^s))) \:\: (7)</script><br />
  where we considered a joint bounding box <script type="math/tex">b_i</script> capturing the sub-image around <script type="math/tex">\mathbf{y}_i: b_i(\mathbf{y}; \sigma) = (\mathbf{y}_i, \sigma diam(\mathbf{y}), \sigma diam(\mathbf{y}))</script> having as center the i-th joint and as dimension the pose diameter scaled by <script type="math/tex">\sigma</script>, to refine a given joint location <script type="math/tex">\mathbf{y}_i</script>.</li>
                </ul>
              </li>
              <li>Apply the cascade for a fixed number of stages <script type="math/tex">= S</script></li>
            </ul>
          </li>
          <li><strong>Loss</strong>: (at each stage <script type="math/tex">s</script>)</li>
        </ul>
      </dd>
      <dd>
        <script type="math/tex; mode=display">\theta_s = \arg \min_\theta \sum_{(x,\mathbf{y}_i) \in D_A^s} \|\mathbf{y}_i - \psi_i(x;\theta)\|_2^2 \:\:\:\:\: (8)</script>
      </dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents26">Advantages:</strong></dt>
      <dd>
        <ul>
          <li>The DNN is capable of capturing the full context of each body joint</li>
          <li>The approach is simpler to formulate than graphical-models methods - no need to explicitly design feature representations and detectors for parts or to explicitly design a model topology and interactions between joints.
            <blockquote>
              <p>Instead a generic ConvNet learns these representations</p>
            </blockquote>
          </li>
        </ul>
      </dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents28">Notes:</strong></dt>
      <dd>
        <ul>
          <li>The use of a generic DNN architecture is motivated by its outstanding results on both classification and localization problems and translates well to pose estimation</li>
          <li>Such a model is a truly holistic one — the final joint location estimate is based on a complex nonlinear transformation of the full image</li>
          <li>The use of a DNN obviates the need to design a domain specific pose model</li>
          <li>Although the regression loss does not model explicit interactions between joints, such are implicitly captured by all of the 7 hidden layers – all the internal features are shared by all joint regressors</li>
        </ul>
      </dd>
    </dl>
  </li>
</ol>

<hr />

<h2 id="content3">Proposed Changes</h2>

<ol>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents31">Gated-Matrix Selection for Latent Space dimensionality estimation and Dynamic bit-rate modification:</strong></dt>
      <dd></dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents32">Conditional Generative-Adversarial Training with Random-Forests for Generalizable domain-compression:</strong></dt>
      <dd></dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents33">Adversarial Feature Learning for Induced Natural Representation and Artifact Removal:</strong></dt>
      <dd></dd>
    </dl>
  </li>
</ol>

<hr />

<p><strong>Papers:</strong></p>
<ul>
  <li><strong>Generative Compression</strong>
    <ul>
      <li>WaveOne: https://arxiv.org/pdf/1705.05823.pdf</li>
      <li>MIT Generative Compression: https://arxiv.org/pdf/1703.01467.pdf</li>
    </ul>
  </li>
  <li><strong>Current Standards</strong>
    <ul>
      <li>An overview of the JPEG2000 still image compression standard
        <ul>
          <li>Paper:http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.128.9040&amp;rep=rep1&amp;type=pdf</li>
          <li>Notes: Pretty in-depth, by Eastman Kodak Company, from early 2000s (maybe improvements since then?)</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<ol>
  <li>WaveOne
 Paper: https://arxiv.org/pdf/1705.05823.pdf
 Site: http://www.wave.one/
 Post: http://www.wave.one/icml2017</li>
  <li>Generative Compression – Santurker, Budden, Shavit (MIT)
 Paper: https://arxiv.org/pdf/1703.01467.pdf</li>
  <li>Toward Conceptual Compression – DeepMind
 Paper: https://papers.nips.cc/paper/6542-towards-conceptual-compression.pdf</li>
</ol>

<ul>
  <li>Generative Compression:<br />
  Generative Compression (https://arxiv.org/pdf/1703.01467.pdf and http://www.wave.one/icml2017/ ), think about streaming videos with orders of magnitude better compression. The results are pretty insane, and this could possibly be the key to bringing AR/VR into the everyday market. If we can figure out how to integrate this into real-time systems, like lets say a phone, you could take hidef video, buffer it and encode it to compress it (the above waveone model can compress 100 img/sec from the Kodak dataset – not shabby at all), we could save massive amounts of data with order of magnitude less storage. We could easily create a mobile app as a proof of concept, but this shit could be huge. These can be also trained to be domain specific, because they are learned not hardcoded. We could create an API allowing any device to connect to it and dynamically compress data, think drones, etc. We can also build in encryption into the system, which adds a layer of security.</li>
</ul>



      <footer class="site-footer">
    <!--   <span class="site-footer-owner"><a href="http://localhost:8889">Ahmad Badary</a> is maintained by <a href="http://ahmedbadary.ml">Ahmad Badary</a>.</span> -->
    
<!--  -->
    <div class="wrapper">
        <div class="footer-col-wrapper">
            <div class="footer-col footer-col-1">
            <span class="site-footer-owner"><a href="http://localhost:8889">Site</a> maintained by <a href="http://ahmedbadary.ml">Ahmad Badary</a>.</span>
    <span class="site-footer-credits">
        <p>
            &copy; 2017. All rights reserved.
        </p> 
    </span>
            </div>
            <div class="footer-col footer-col-2">
            <div><p>         </p></div>
            </div>
            <div class="footer-col footer-col-3">
                <ul class="social-media-list">
                    
                      <li>
                        <a href="https://github.com/AhmedBadary">
                          <i class="fa fa-github"></i> GitHub
                        </a>
                      </li>
                    
                    
                      <li>
                        <a href="https://linkedin.com/in/ahmad-badary-656098121/">
                          <i class="fa fa-linkedin"></i> LinkedIn
                        </a>
                      </li>
                    
                    
                      <li>
                        <a href="https://www.facebook.com/ahmed.thabet.94">
                          <i class="fa fa-facebook"></i> Facebook
                        </a>
                      </li>
                    
                </ul>
            </div>
        </div>
    </div>
<!--  -->
</footer>


    </section>

  </body>

<!-- Table of Content Script -->
<script type="text/javascript">
var bodyContents = $(".bodyContents1");
$("<ol>").addClass("TOC1ul").appendTo(".TOC1");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC1ul");
     });
// 
var bodyContents = $(".bodyContents2");
$("<ol>").addClass("TOC2ul").appendTo(".TOC2");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC2ul");
     });
// 
var bodyContents = $(".bodyContents3");
$("<ol>").addClass("TOC3ul").appendTo(".TOC3");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC3ul");
     });
//
var bodyContents = $(".bodyContents4");
$("<ol>").addClass("TOC4ul").appendTo(".TOC4");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC4ul");
     });
//
var bodyContents = $(".bodyContents5");
$("<ol>").addClass("TOC5ul").appendTo(".TOC5");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC5ul");
     });
//
var bodyContents = $(".bodyContents6");
$("<ol>").addClass("TOC6ul").appendTo(".TOC6");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC6ul");
     });
//
var bodyContents = $(".bodyContents7");
$("<ol>").addClass("TOC7ul").appendTo(".TOC7");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC7ul");
     });
//
var bodyContents = $(".bodyContents8");
$("<ol>").addClass("TOC8ul").appendTo(".TOC8");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC8ul");
     });
//
var bodyContents = $(".bodyContents9");
$("<ol>").addClass("TOC9ul").appendTo(".TOC9");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC9ul");
     });

</script>

<!-- VIDEO BUTTONS SCRIPT -->
<script type="text/javascript">
  function iframePopInject(event) {
    var $button = $(event.target);
    // console.log($button.parent().next());
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $figure = $("<div>").addClass("video_container");
        $iframe = $("<iframe>").appendTo($figure);
        $iframe.attr("src", $button.attr("src"));
        // $iframe.attr("frameborder", "0");
        $iframe.attr("allowfullscreen", "true");
        $iframe.css("padding", "4px 6px");
        $button.next().css("display", "block");
        $figure.appendTo($button.next());
        $button.text("Hide Video")
    } else {
        $button.attr('value', 'show');
        $button.next().html("");
        $button.text("Show Video")
    }
}
</script>

<!-- BUTTON TRY -->
<script type="text/javascript">
  function iframePopA(event) {
    event.preventDefault();
    var $a = $(event.target).parent();
    console.log($a);
    if ($a.attr('value') == 'show') {
        $a.attr('value', 'hide');
        $figure = $("<div>");
        $iframe = $("<iframe>").addClass("popup_website_container").appendTo($figure);
        $iframe.attr("src", $a.attr("href"));
        $iframe.attr("frameborder", "1");
        $iframe.attr("allowfullscreen", "true");
        $iframe.css("padding", "4px 6px");
        $a.next().css("display", "block");
        $figure.appendTo($a.next().next());
        // $a.text("Hide Content")
        $('html, body').animate({
            scrollTop: $a.offset().top
        }, 1000);
    } else {
        $a.attr('value', 'show');
        $a.next().next().html("");
        // $a.text("Show Content")
    }

    $a.next().css("display", "inline");
}
</script>


<!-- TEXT BUTTON SCRIPT - INJECT -->
<script type="text/javascript">
  function showTextPopInject(event) {
    var $button = $(event.target);
    var txt = $button.attr("input");
    console.log(txt);
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $p = $("<p>");
        $p.html(txt);
        $button.next().css("display", "block");
        $p.appendTo($button.next());
        $button.text("Hide Content")
    } else {
        $button.attr('value', 'show');
        $button.next().html("");
        $button.text("Show Content")
    }

}
</script>

<!-- TEXT BUTTON SCRIPT - HIDDEN / HIDE / SHOW / HIDE/SHOW -->
<script type="text/javascript">
  function showTextPopHide(event) {
    var $button = $(event.target);
    // var txt = $button.attr("input");
    var txt = $button.text();
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $button.next().removeAttr("hidden");
        $button.text(txt + " - Hide Content");
    } else {
        $button.attr('value', 'show');
        $button.next().attr("hidden", "");
        $button.text(txt.replace(" - Hide Content",""));
    }
}
</script>

<!-- TEXT BUTTON SCRIPT - HIDDEN / HIDE / SHOW / HIDE/SHOW -->
<script type="text/javascript">
  function showText_withParent_PopHide(event) {
    var $button = $(event.target);
    var $parent = $button.parent();
    var txt = $button.text();
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $parent.next().removeAttr("hidden");
        $button.text(txt + " - Hide Content");
    } else {
        $button.attr('value', 'show');
        $parent.next().attr("hidden", "");
        $button.text(txt.replace(" - Hide Content",""));
    }
}
</script>

<!-- Print / Printing / printme -->
<!-- <script type="text/javascript">
i = 0

for (var i = 1; i < 6; i++) {
    var bodyContents = $(".bodyContents" + i);
    $("<p>").addClass("TOC1ul")  .appendTo(".TOC1");
    bodyContents.each(function(index, element) {
        var paragraph = $(element);
        $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC1ul");
         });
} 
</script>
 -->
 
</html>

