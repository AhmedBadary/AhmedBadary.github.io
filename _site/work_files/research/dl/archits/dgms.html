<!DOCTYPE html>
<html lang="en-us">
  <head>
  <meta charset="UTF-8">
  <title>Ahmad Badary</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <link rel="stylesheet" href="/css/normalize.css">
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="/css/cayman.css">
  <link rel="stylesheet" href="/css/style.css">
  <link rel="shortcut icon" href="/main_files/favicon.ico" />
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="/css/customStyle.css">
  <title> » Ahmad Badary</title>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
</head>

  <body>
    <nav class="main-nav">
    <a href="https://ahmedbadary.github.io/" class="main-nav-logo">
        <img src="/main_files/logo.png">
    </a>
    <ul id="menu-main" class="main-nav-items">
        <li id="menu-item-1859" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-home menu-item-1859">
            <a href="/">Home</a>
        </li>
        <li id="menu-item-2869" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-2869">
            <a href="/work">Work</a>
        </li>
        <li id="menu-item-1892" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1892">
            <a href="/projects">Projects</a>
        </li>
        <li id="menu-item-1858" class="menu-item menu-item-type-post_type menu-item-object-page current_page_parent menu-item-1858">
            <a href="/blog">Blog</a>
        </li>
        <li id="menu-item-1862" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1862">
            <a href="/about">About</a>
        </li>
    </ul>
</nav>


<section class="page-header">
  <h1 class="project-name">Deep Generative Models</h1>
  <h2 class="project-tagline"></h2>
  <a href="/#" class="btn">Home</a>
  <a href="/work" class="btn">Work-Space</a>
  <a href= /work_files/research/dl.html class="btn">Previous</a>
</section>

<!-- <div>
  <ul class="posts">
    
      <li><span>02 Jan 2014</span> &raquo; <a href="/2014/01/02/introducing-Ahmad/">Introducing Ahmad</a></li>
    
  </ul>
</div> -->


    <section class="main-content">
      
      <div class="TOC">
  <h1 id="table-of-contents">Table of Contents</h1>

  <ul class="TOC9">
    <li><a href="#content9">Introduction and Preliminaries</a></li>
  </ul>
  <ul class="TOC1">
    <li><a href="#content1">Deep Generative Models</a></li>
  </ul>
  <ul class="TOC2">
    <li><a href="#content2">SECOND</a></li>
  </ul>
  <ul class="TOC3">
    <li><a href="#content3">THIRD</a></li>
  </ul>
  <!--   * [FOURTH](#content4)
  {: .TOC4}
  * [FIFTH](#content5)
  {: .TOC5}
  * [SIXTH](#content6)
  {: .TOC6} -->
</div>

<hr />
<hr />

<ul>
  <li><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4943066/">Probabilistic Models and Generative Neural Networks (paper)</a></li>
  <li><a href="http://mlvis2016.hiit.fi/latentVariableGenerativeModels.pdf">Latent Variable Model Intuition (slides!)</a></li>
  <li><a href="https://openai.com/blog/generative-models/">Generative Models (OpenAI Blog!)</a></li>
</ul>

<p id="lst-p"><strong style="color: red">In situations respecting the following assumptions, Semi-Supervised Learning should <em>improve performance</em>:</strong>  :</p>
<ul>
  <li>Semi-supervised learning works <span style="color: goldenrod">when <script type="math/tex">p(\mathbf{y} \vert \mathbf{x})</script> and <script type="math/tex">p(\mathbf{x})</script> are <strong>tied</strong> together</span>.
    <ul>
      <li>This happens when <script type="math/tex">\mathbf{y}</script> is closely associated with one of the causal factors of <script type="math/tex">\mathbf{x}</script>.</li>
    </ul>
  </li>
  <li>The <span style="color: goldenrod"><strong>best possible model</strong> of <script type="math/tex">\mathbf{x}</script> (wrt. <strong>generalization</strong>)</span> is the one that <span style="color: goldenrod"><em><strong>uncovers</strong></em> the above <strong>“true” structure</strong></span>, with <span style="color: goldenrod"><script type="math/tex">\boldsymbol{h}</script> as a <strong>latent variable</strong> that <em><strong>explains</strong></em> the <strong>observed variations</strong> in <script type="math/tex">\boldsymbol{x}</script></span>.
    <ul>
      <li>Since we can write the <strong>Marginal Probability of Data</strong> as:
        <p>$$p(\boldsymbol{x})=\mathbb{E}_ {\mathbf{h}} p(\boldsymbol{x} \vert \boldsymbol{h})$$</p>
        <ul>
          <li>Because the <strong>“true” generative process</strong> can be conceived as <span style="color: purple"><em><strong>structured</strong></em> according to this <strong>directed graphical model</strong></span>, with <script type="math/tex">\mathbf{h}</script> as the <strong>parent</strong> of <script type="math/tex">\mathbf{x}</script>:
            <p>$$p(\mathbf{h}, \mathbf{x})=p(\mathbf{x} \vert \mathbf{h}) p(\mathbf{h})$$</p>
          </li>
        </ul>
      </li>
      <li>Thus, <strong>The “ideal” representation learning discussed above should recover these latent factors</strong>.</li>
    </ul>
  </li>
  <li>The <span style="color: purple"><strong>marginal</strong> <script type="math/tex">p(\mathbf{x})</script> is <em><strong>intimately tied</strong></em> to the <strong>conditional</strong> <script type="math/tex">p(\mathbf{y} \vert \mathbf{x})</script>, and knowledge of the structure of the former should be helpful to learn the latter</span>.
    <ul>
      <li>Since the <strong>conditional distribution</strong> of <script type="math/tex">\mathbf{y}</script> given <script type="math/tex">\mathbf{x}</script> is <span style="color: purple">tied by <em>Bayes’ rule</em> to the <strong>components in the above equation</strong></span>:
        <p>$$p(\mathbf{y} \vert \mathbf{x})=\frac{p(\mathbf{x} \vert \mathbf{y}) p(\mathbf{y})}{p(\mathbf{x})}$$</p>
      </li>
    </ul>
  </li>
</ul>

<h2 id="content9">Introduction and Preliminaries</h2>

<ol>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents9" id="bodyContents91">Unsupervised Learning:</strong><br />
 <strong>Unsupervised Learning</strong> is the task of making inferences, by learning a better representation from some datapoints that do not have any labels associated with them.<br />
 It intends to learn/infer an <strong><em>a priori</em> probability distribution</strong> <script type="math/tex">p_{X}(x)</script>; I.E. it solves a <strong>density estimation problem</strong>.<br />
 It is a type of <em><strong>self-organized</strong></em> <strong>Hebbian learning</strong> that helps find previously unknown patterns in data set without pre-existing labels. <br />
 <img src="/main_files/dl/archits/dgms/1.png" alt="img" width="100%" /></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents9" id="bodyContents92">Density Estimation:</strong><br />
 <strong>Density Estimation</strong> is a problem in Machine Learning that requires learning a function <script type="math/tex">p_{\text {model}} : \mathbb{R}^{n} \rightarrow \mathbb{R}</script>, where <script type="math/tex">p_{\text {model}}(x)</script> can be interpreted as a <strong>probability  density function</strong> (if <script type="math/tex">x</script> is continuous) or a <strong>probability mass function</strong> (if <script type="math/tex">x</script> is discrete) on the space that the examples were drawn from.</p>

    <p>To perform such a task well, an algorithm needs to <span style="color: purple">learn the <strong>structure of the data</strong></span> it has seen. It must know where examples cluster tightly and where they are unlikely to occur.</p>

    <ul>
      <li><strong>Types</strong> of Density Estimation:
        <ul>
          <li><em><strong>Explicit</strong></em>: Explicitly define and solve for <script type="math/tex">p_\text{model}(x)</script></li>
          <li><em><strong>Implicit</strong></em>: Learn model that can sample from <script type="math/tex">p_\text{model}(x)</script> without explicitly defining it</li>
        </ul>
      </li>
    </ul>

    <p><br /></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents9" id="bodyContents93">Generative Models (GMs):</strong><br />
 A <strong>Generative Model</strong> is a <em>statistical model</em> of the <span style="color: purple"><strong>joint</strong> probability distribution</span> on <script type="math/tex">X \times Y</script>:</p>
    <p>$${\displaystyle P(X,Y)}$$</p>
    <p>where <script type="math/tex">X</script> is an <em>observable</em> variable and <script type="math/tex">Y</script> is a <em>target</em> variable.</p>

    <p>In <strong>supervised settings</strong>, a <strong>Generative Model</strong> is a model of the <span style="color: purple"><strong>conditional</strong> probability</span> of the observable <script type="math/tex">X,</script> given a target <script type="math/tex">y,</script>:</p>
    <p>$$P(X | Y=y)$$</p>

    <p><strong style="color: red">Application - Density Estimation:</strong><br />
 Generative Models address the <strong>Density Estimation</strong> problem, a core problem in unsupervised learning, since they model <br />
 Given training data, GMs will generate new samples from the same distribution.</p>
    <ul>
      <li><strong>Types</strong> of Density Estimation:
        <ul>
          <li><em><strong>Explicit</strong></em>: Explicitly define and solve for <script type="math/tex">p_\text{model}(x)</script></li>
          <li><em><strong>Implicit</strong></em>: Learn model that can sample from <script type="math/tex">p_\text{model}(x)</script> without explicitly defining it</li>
        </ul>
      </li>
    </ul>

    <p id="lst-p"><strong style="color: red">Examples of Generative Models:</strong></p>
    <ul>
      <li>Gaussian Mixture Model (and other types of mixture model)</li>
      <li>Hidden Markov Model</li>
      <li>Probabilistic context-free grammar</li>
      <li>Bayesian network (e.g. Naive Bayes, Autoregressive Model)</li>
      <li>Averaged one-dependence estimators</li>
      <li>Latent Dirichlet allocation (LDA)</li>
      <li>Boltzmann machine (e.g. Restricted Boltzmann machine, Deep belief network)</li>
      <li>Variational autoencoder</li>
      <li>Generative Adversarial Networks</li>
      <li>Flow-based Generative Model</li>
    </ul>

    <p><button class="showText" value="show" onclick="showTextPopHide(event);">A generative model for generative (graphical) models - Diagram</button>
 <img src="https://cdn.mathpix.com/snip/images/ZIHj9KE3aa7i-o5jSaxLD8fP-tDPzFtBf7ymVT8g2JQ.original.fullsize.png" alt="img" width="100%" hidden="" /></p>

    <p id="lst-p"><strong style="color: red">Notes:</strong></p>
    <ul>
      <li>Generative Models are <strong>Joint Models</strong>.</li>
      <li>Latent Variables are <strong style="color: goldenrod">Random Variables</strong>.<br />
 <br /></li>
    </ul>
  </li>
</ol>

<!-- 3. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents9 #bodyContents93}
4. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents9 #bodyContents94} -->

<hr />

<h2 id="content1">Deep Generative Models</h2>

<ol>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents11">Generative Models (GMs):</strong></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents12">Deep Generative Models (DGMs):</strong></p>

    <ul>
      <li>DGMs <strong>represent <em>probability distributions</em> over multiple variables</strong> in some way:
        <ul>
          <li>Some allow the probability distribution function to be evaluated explicitly.</li>
          <li>Others do not allow the evaluation of the probability distribution function but support operations that implicitly require knowledge of it, such as drawing samples from the distribution.</li>
        </ul>
      </li>
      <li><strong>Structure/Representation:</strong>
        <ul>
          <li>Some of these models are structured probabilistic models described in terms of graphs and factors, using the language of (probabilistic) graphical models.</li>
          <li>Others cannot be easily described in terms of factors but represent probability distributions nonetheless.</li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<!-- 3. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13} -->
<!-- 4. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents14} -->

<!-- 5. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents15}
6. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents16}
7. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents17}
8. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents18}
 -->

<hr />

<h2 id="content2">Likelihood-based Models</h2>

<ol>
  <li><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents21">Likelihood-based Models:</strong><br />
 <strong>Likelihood-based Model</strong>: is a statistical model of a <span style="color: purple">joint distribution over data</span>.<br />
 It estimates <script type="math/tex">\mathbf{p}_ {\text {data}}</script> from samples <script type="math/tex">\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(n)} \sim \mathbf{p}_ {\text {data}}(\mathbf{x})</script>.<br />
 It Learns a distribution <script type="math/tex">p</script> that allows:
    <ul>
      <li><span style="color: purple">Computing probability of a sample</span> <script type="math/tex">p(x)</script> for arbitrary <script type="math/tex">x</script></li>
      <li><span style="color: purple">Sampling</span> <script type="math/tex">x \sim p(x)</script><br />
  Sampling is a computable efficient process that generates an RV <script type="math/tex">x</script> that has the same distribution as a <script type="math/tex">p_{\text{data}}</script>.<br />
 The distribution <script type="math/tex">\mathbf{p}_ {\text {data}}</script> is just a <span style="color: purple"><strong>function</strong></span> that takes as an <span style="color: purple">input a sample <script type="math/tex">x</script></span> and <span style="color: purple">outputs the probability of <script type="math/tex">x</script></span> under the learned distribution.</li>
    </ul>

    <p><strong style="color: red">The Goal for Learning Likelihood-based Models:</strong><br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">Show Goal</button></p>
    <ul hidden="">
      <li><strong>Original Goal</strong>: estimate <script type="math/tex">\mathbf{p}_{\text {data}}</script> from samples <script type="math/tex">\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(n)} \sim \mathbf{p}_{\text {data}}(\mathbf{x})</script>.</li>
      <li><strong>Revised Goal - Function Approximation</strong>: Find <script type="math/tex">\theta</script> (the parameter vector indexing into the distribution space) so that you approximately get the data distribution.<br />
  I.E. Learn <script type="math/tex">\theta</script> so that <script type="math/tex">p_{\theta}(x) \approx p_{\text {data}}(x)</script>.</li>
    </ul>

    <p id="lst-p"><strong style="color: red">Motivation:</strong></p>
    <ul>
      <li><strong>Solving Hard Problems</strong>:
        <ul>
          <li><strong>Generating Data</strong>: synthesizing images, videos, speech, text</li>
          <li><strong>Compressing Data</strong>: constructing efficient codes</li>
          <li><strong>Anomaly Detection</strong><br />
 <br /></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents22">The Histogram Model:</strong><br />
 The <strong>Histogram</strong> Model is a very simple <strong>likelihood-based</strong> model.<br />
 It is a model of <strong>discrete data</strong> where the samples can take on values in a finite set <script type="math/tex">\{1, \ldots, \mathrm{k}\}</script>.</p>

    <p>The <strong>Goal:</strong> estimate <script type="math/tex">\mathbf{p}_{\text {data}}</script> from samples <script type="math/tex">\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(n)} \sim \mathbf{p}_{\text {data}}(\mathbf{x})</script>.</p>

    <p id="lst-p">The <strong>Model</strong>: a <span style="color: purple"><strong>Histogram</strong></span></p>
    <ul>
      <li><strong>Described</strong> by <script type="math/tex">k</script> nonnegative numbers: <script type="math/tex">\mathrm{p}_{1}, \ldots, \mathrm{p}_{\mathrm{k}}</script></li>
      <li><strong>Trained</strong> by <span style="color: purple">counting frequencies</span>
        <p>$$\mathrm{p}_ {\mathrm{i}}=(\# \text { times } i \text { appears in the dataset) } /(\#\text { points in the dataset) }$$</p>
      </li>
      <li><strong>At Runtime</strong>:
        <ul>
          <li><strong>Inference</strong> (querying <script type="math/tex">p_i</script> for arbitrary <script type="math/tex">i</script>): simply a lookup into the array <script type="math/tex">\mathrm{p}_{1}, \ldots, \mathrm{p}_{\mathrm{k}}</script></li>
          <li><strong>Sampling</strong> (lookup into the inverse cumulative distribution function):
            <ol>
              <li>From the model probabilities <script type="math/tex">p_{1}, \ldots, p_{k},</script> compute the cumulative distribution:
                <p>$$\mathrm{F}_{\mathrm{i}}=\mathrm{p}_ {1}+\cdots+\mathrm{p}_ {\mathrm{i}} \quad$ for all $\mathrm{i} \in\{1, \ldots, \mathrm{k}\}$$</p>
              </li>
              <li>Draw a uniform random number  <script type="math/tex">u \sim[0,1]</script></li>
              <li>Return the smallest <script type="math/tex">i</script> such that <script type="math/tex">u \leq F_{i}</script></li>
            </ol>
          </li>
        </ul>
      </li>
    </ul>

    <p id="lst-p"><strong style="color: red">Generalization Problem:</strong></p>
    <ul>
      <li><strong>The Curse of Dimensionality</strong>: Counting fails when there are too many bins and <strong>generalization</strong> is not achieved.<br />
  <button class="showText" value="show" onclick="showTextPopHide(event);">Analysis</button>
        <ul hidden="">
          <li>(Binary) MNIST: <script type="math/tex">28 \times 28</script> images, each pixel in <script type="math/tex">\{0,1\}</script></li>
          <li>There are <script type="math/tex">2^{784} \approx 10^{236} \approx 10^{236}</script> probabilities to estimate</li>
          <li>Any reasonable training set covers only a tiny fraction of this</li>
          <li>Each image influences only one parameter and there is only <script type="math/tex">60,000</script> MNIST images:<br />
  <span style="color: purple"><strong>No generalization</strong> whatsoever!</span></li>
        </ul>
      </li>
      <li><strong>Solution</strong>: <span style="color: goldenrod"><strong>Function Approximation</strong></span><br />
  Instead of storing each probability store a <span style="color: purple"><strong><em>parameterized</em> function</strong></span> <script type="math/tex">p_{\theta}(x)</script>.<br />
 <br /></li>
    </ul>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents23">Achieving Generalization via Function Approximation:</strong><br />
 <strong>Function Approximation</strong>: Defines a <em><strong>mapping <script type="math/tex">p_{\theta}</script></strong></em> from a <strong>parameter space</strong> to a <strong>space</strong> of <strong>probability distributions</strong>.<br />
 E.g. <script type="math/tex">\theta</script> are the weights of a NN, <script type="math/tex">p_{\theta}</script> some NN architecture with those weights set.</p>

    <p>Instead of storing each probability store a <span style="color: purple"><strong><em>parameterized</em> function</strong></span> <script type="math/tex">p_{\theta}(x)</script>.<br />
 i.e. Instead of treating the probabilities <script type="math/tex">p_1, ..., p_k</script> themselves as <strong>parameters</strong>, we define them to be <span style="color: purple"><strong>functions</strong> of other parameters <script type="math/tex">\theta</script></span>.<br />
 The probability of every data point <script type="math/tex">p_i = p_{\theta}(x_i)</script> will be a function of <script type="math/tex">\theta</script>.<br />
 The mapping is defined such that whenever we update <script type="math/tex">\theta</script> for any one particular data point, its likely to influence <script type="math/tex">p_i</script> for other data points that are similar to it.<br />
 We <strong>constraint</strong> the <span style="color: purple"><strong>dimension</strong> <script type="math/tex">d</script> of <script type="math/tex">\theta \in \mathbb{R}^d</script> to be <em><strong>much less</strong></em> than the <strong>number of possible images</strong></span>.<br />
 Such that the <strong>parameter space <script type="math/tex">\Theta</script></strong> is <span style="color: purple"><em><strong>indexing</strong></em> into the low-dimensional space inside the <strong>set of all probability distributions</strong></span>.<br />
 This is how we achieve <strong style="color: goldenrod">Generalization</strong> through <span style="color: goldenrod">function approximation</span>.</p>

    <p id="lst-p"><strong style="color: red">The Revised Goal for Learning Likelihood-based Models - Function Approximation:</strong></p>
    <ul>
      <li><strong>Original Goal</strong>: estimate <script type="math/tex">\mathbf{p}_{\text {data}}</script> from samples <script type="math/tex">\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(n)} \sim \mathbf{p}_{\text {data}}(\mathbf{x})</script>.</li>
      <li><strong>Revised Goal - Function Approximation</strong>: Find <script type="math/tex">\theta</script> (the parameter vector indexing into the distribution space) so that you approximately get the data distribution.<br />
  I.E. Learn <script type="math/tex">\theta</script> so that <script type="math/tex">p_{\theta}(x) \approx p_{\text {data}}(x)</script>.</li>
    </ul>

    <p id="lst-p"><strong style="color: red">New Challenges:</strong></p>
    <ul>
      <li>How do we design function approximators to effectively represent complex joint distributions over <script type="math/tex">x</script>, yet remain easy to train?</li>
      <li>There will be many choices for model design, each with different tradeoffs and different compatibility criteria.
        <ul>
          <li>Define “what does it mean for one probability distribution to be <em>approximately</em> equal to another”?<br />
  A <span style="color: purple">measure of distance between distributions: <strong>distance function</strong></span>.<br />
  It needs to be <strong>differentiable</strong> wrt <script type="math/tex">\theta</script>, works on <strong>finite-datasets</strong>, etc.</li>
          <li>How to “define <script type="math/tex">p_{\theta}</script>”?</li>
          <li>How to “learn/optimize <script type="math/tex">\theta</script>”?</li>
          <li>How to ensure software-hardware compatibility?</li>
        </ul>
      </li>
    </ul>

    <p><span style="color: purple">Designing the <strong>model</strong> and the <strong>training procedure (optimization)</strong> go hand-in-hand</span>.
 <br /></p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents24">Architecture and Learning in Likelihood-based Models:</strong><br />
 <strong style="color: red">Fitting Distributions:</strong>
    <ul>
      <li>Given data <script type="math/tex">\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(n)}</script> sampled from a “true” distribution <script type="math/tex">\mathbf{p}_ {\text {data}}</script></li>
      <li>Set up a <strong>model class</strong>: a set of parameterized distributions <script type="math/tex">\mathrm{p}_ {\theta}</script></li>
      <li>Pose a <strong>search problem over parameters</strong>:
        <p>$$\arg \min_ {\theta} \operatorname{loss}\left(\theta, \mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(n)}\right)$$</p>
      </li>
      <li><strong>Desiderata</strong> - Want the loss function + search procedure to:
        <ul>
          <li>Work with large datasets (<script type="math/tex">n</script> is large, say millions of training examples)</li>
          <li>Yield $\theta$ such that <script type="math/tex">p_ {\theta}</script> matches <script type="math/tex">p_{\text {data}}</script> — i.e. the training algorithm <em>“works”</em>.<br />
  Think of the <strong>loss</strong> as a <span style="color: purple"><strong><em>distance</em> between distributions</strong></span>.</li>
          <li>Note that the training procedure can only see the empirical data distribution, not the true data distribution: we want the model to <strong>generalize</strong>.</li>
        </ul>
      </li>
    </ul>

    <p id="lst-p"><strong style="color: red">Objective - Maximum Likelihood:</strong></p>
    <ul>
      <li><strong>Maximum Likelihood</strong>: given a dataset <script type="math/tex">\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(n)},</script> find <script type="math/tex">\theta</script> by solving the optimization problem
        <p>$$\arg \min _{\theta} \operatorname{loss}\left(\theta, \mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(n)}\right)=\frac{1}{n} \sum_{i=1}^{n}-\log p_{\theta}\left(\mathbf{x}^{(i)}\right)$$</p>
      </li>
      <li>Statistics tells us that if the <span style="color: purple">model family is expressive enough</span> and if <span style="color: purple">enough data is given</span>, then <span style="color: goldenrod">solving the maximum likelihood problem will yield parameters that generate the data</span>.<br />
  This is <strong>IMPORTANT</strong> since one of the main reasons for introducing and using these methods (e.g. <strong>MLE</strong>) is that <span style="color: purple"><strong>they <em>work</em> in practice</strong></span> i.e. leads to an algorithm we can run in practice that actually produces good models.</li>
      <li>Equivalent to minimizing KL divergence between the empirical data distribution and the model:
        <p>$$\hat{p}_{\text {data }}(\mathbf{x})=\frac{1}{n} \sum_{i=1}^{n} \mathbf{1}\left[\mathbf{x}=\mathbf{x}^{(i)}\right]$$</p>
        <p>$$\mathrm{KL}\left(\hat{p}_{\mathrm{data}} \| p_{\theta}\right)=\mathbb{E}_{\mathbf{x} \sim \hat{p}_{\mathrm{data}}}\left[-\log p_{\theta}(\mathbf{x})\right]-H\left(\hat{p}_ {\mathrm{data}}\right)$$</p>
        <p>I.E. the <strong>maximum likelihood objective</strong> exactly <span style="color: purple"><em><strong>measures</strong></em> how good of a <strong>compressor</strong> the model is</span>.</p>
      </li>
    </ul>

    <p id="lst-p"><strong style="color: red">Optimization - Stochastic Gradient Descent:</strong><br />
 <strong>Maximum likelihood</strong> is an <strong>optimization problem</strong>. We use <strong>SGD</strong> to solve it.</p>
    <ul>
      <li>SGD minimizes expectations: for <script type="math/tex">{f}</script> a differentiable function of <script type="math/tex">\theta,</script> it solves
        <p>$$\arg \min_ {\theta} \mathbb{E}[f(\theta)]$$</p>
      </li>
      <li>With maximum likelihood (which is an expectation in-of-itself), the optimization problem is
        <p>$$\arg \min _{\theta} \mathbb{E}_{\mathbf{x} \sim \hat{p}_{\text {data }}}\left[-\log p_{\theta}(\mathbf{x})\right]$$</p>
      </li>
      <li><strong>Why maximum likelihood + SGD?</strong><br />
  <strong>Same Theme</strong>: It <span style="color: purple"><em><strong>works</strong></em></span> with large datasets and is compatible with neural networks.</li>
    </ul>

    <p id="lst-p"><strong style="color: red">Designing the Model:</strong><br />
 Our goal is to design <strong>Neural Network models</strong> that <span style="color: purple"><em><strong>fit</strong></em> into the <strong>maximum likelihood + sgd framework</strong></span>.</p>
    <ul>
      <li><strong>Key requirement for maximum likelihood + SGD</strong>: <span style="color: purple">efficiently compute <script type="math/tex">\log p(x)</script> and its</span> <strong style="color: goldenrod">gradient</strong>.</li>
      <li><strong>The Model <script type="math/tex">p_{\theta}</script>:</strong> is chosen to be <strong style="color: goldenrod">Deep Neural Networks</strong><br />
  They work in the regime of high expressiveness and efficient computation (assuming specialized hardware).</li>
      <li><strong>Designing the Networks</strong>:
        <ul>
          <li>Any setting of <script type="math/tex">\theta</script> must define a <strong>valid probability distribution</strong> over <script type="math/tex">x</script>:
            <p>$$\forall \theta, \quad \sum_{\mathbf{x}} p_{\theta}(\mathbf{x})=1 \quad \text{ and } \quad p_{\theta}(\mathbf{x}) \geq 0 \quad \forall \mathbf{x}$$</p>
            <ul>
              <li><strong>Difficulty:</strong> The number of terms in the sum is the number of <strong>possible data points</strong>, thus, it is <strong>exponential in the dimension</strong>.<br />
  Thus, a naive implementation would have a forward pass w/ exponential time.</li>
              <li><strong>Energy-based Models</strong> do not have this constraint in the model definition, but then have to deal with that constraint in the training algorithm making it very hard to deal with/optimize.</li>
            </ul>
          </li>
          <li><script type="math/tex">\log p_{\theta}(x)</script> should be <strong>easy to evaluate and differentiate</strong> with respect to <script type="math/tex">\theta</script></li>
          <li>This can be <strong>tricky</strong> to set up!</li>
        </ul>
      </li>
    </ul>

    <p><strong style="color: red">Bayes Nets and Neural Nets:</strong><br />
 One way to <strong>satisfy the condition</strong> of <span style="color: purple">defining a valid probability distribution over <script type="math/tex">x</script></span> is to <strong>model the variables with a Bayes Net</strong>.</p>

    <p><strong>Main Idea:</strong><br />
 <span style="color: goldenrod">place a <strong>Bayes Net</strong> structure (a directed acyclic graph) over the variables in the data, and model the <strong>conditional distributions</strong> with Neural Networks</span>.</p>

    <p>This <span style="color: purple">Reduces the problem to <strong>designing conditional likelihood-based models for single variables</strong></span>.</p>

    <p>We know how to do this: the neural net takes variables being conditioned on as input, and outputs the distribution for the variable being predicted; NNs usually condition on a lot of stuff (features) and predict a single small variable (target <script type="math/tex">y</script>) this is done in practice all the time in supervised settings (e.g. classification).</p>

    <p>This (the BN representation) yields <strong>massive savings in the number of parameters to represent a joint distribution</strong>.<br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">A Bayes Net over five variables</button>
 <img src="https://cdn.mathpix.com/snip/images/bkKsJ-vXI0OZXqG7-e5wqAW28DsUGsNKhz9VllwBvaE.original.fullsize.png" alt="img" width="100%" hidden="" /></p>

    <p id="lst-p"><strong style="color: red">Does this work in practice?</strong></p>
    <ul>
      <li>Given a Bayes net structure, <span style="color: purple">setting the <strong>conditional distributions</strong> to <strong>neural networks</strong></span> will yield a <span style="color: purple"><em><strong>tractable</strong></em> <strong>log likelihood</strong> and <strong>gradient</strong></span>.<br />
  Great for <strong>maximum likelihood training</strong>:
        <p>$$\log p_{\theta}(\mathbf{x})=\sum_{i=1}^{d} \log p_{\theta}\left(x_{i} | \text { parents }\left(x_{i}\right)\right)$$</p>
      </li>
      <li><strong>Expressiveness:</strong> it is completely expressive.<br />
  Assuming a <strong>fully expressive Bayes Net structure</strong>: <span style="color: purple">any <strong>joint distribution</strong> can be written as a <strong>product of conditionals</strong></span>
        <p>$$\log p(\mathbf{x})=\sum_{i=1}^{d} \log p\left(x_{i} | \mathbf{x}_ {1: i-1}\right)$$</p>
      </li>
      <li>This is known as an <strong>Autoregressive Model</strong>.</li>
    </ul>

    <div class="borderexample">
      <p><span>
 <strong>Conclusion:</strong><br />
 <span style="color: goldenrod">An <em><strong>expressive</strong></em> <strong>Bayes Net structure</strong> with <strong>Neural Network conditional distributions</strong> yields an <em><strong>expressive</strong></em> model for <script type="math/tex">p(x)</script> with <em><strong>tractable</strong></em> <strong>maximum likelihood training</strong></span>. <br />
 </span></p>
    </div>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents25">Autoregressive Models:</strong></p>

    <p id="lst-p"><strong style="color: red">Notes:</strong></p>
    <ul>
      <li>Very useful if the problem we are modeling requires a fixed size output (e.g. auto-regressive models).</li>
      <li>Autoregressive models such as PixelRNN instead train a network that models the conditional distribution of every individual pixel given previous pixels (to the left and to the top). This is similar to plugging the pixels of the image into a char-rnn, but the RNNs run both horizontally and vertically over the image instead of just a 1D sequence of characters.<br />
 <br /></li>
    </ul>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents26">Asynchronous:</strong></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents27">Asynchronous:</strong></p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents28">Asynchronous:</strong></li>
</ol>

<hr />

<h2 id="content3">THIRD</h2>

<ol>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents31">Asynchronous:</strong></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents32">Asynchronous:</strong></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents33">Asynchronous:</strong></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents34">Asynchronous:</strong></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents35">Asynchronous:</strong></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents36">Asynchronous:</strong></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents37">Asynchronous:</strong></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents38">Asynchronous:</strong></p>
  </li>
</ol>



      <footer class="site-footer">
    <!--   <span class="site-footer-owner"><a href="http://localhost:8889">Ahmad Badary</a> is maintained by <a href="http://ahmedbadary.ml">Ahmad Badary</a>.</span> -->
    
<!--  -->
    <div class="wrapper">
        <div class="footer-col-wrapper">
            <div class="footer-col footer-col-1">
            <span class="site-footer-owner"><a href="http://localhost:8889">Site</a> maintained by <a href="http://ahmedbadary.ml">Ahmad Badary</a>.</span>
    <span class="site-footer-credits">
        <p>
            &copy; 2017. All rights reserved.
        </p> 
    </span>
            </div>
            <div class="footer-col footer-col-2">
            <div><p>         </p></div>
            </div>
            <div class="footer-col footer-col-3">
                <ul class="social-media-list">
                    
                      <li>
                        <a href="https://github.com/AhmedBadary">
                          <i class="fa fa-github"></i> GitHub
                        </a>
                      </li>
                    
                    
                      <li>
                        <a href="https://linkedin.com/in/ahmad-badary-656098121/">
                          <i class="fa fa-linkedin"></i> LinkedIn
                        </a>
                      </li>
                    
                    
                      <li>
                        <a href="https://www.facebook.com/ahmed.thabet.94">
                          <i class="fa fa-facebook"></i> Facebook
                        </a>
                      </li>
                    
                </ul>
            </div>
        </div>
    </div>
<!--  -->
</footer>


    </section>

  </body>

<!-- Table of Content Script -->
<script type="text/javascript">
var bodyContents = $(".bodyContents1");
$("<ol>").addClass("TOC1ul").appendTo(".TOC1");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC1ul");
     });
// 
var bodyContents = $(".bodyContents2");
$("<ol>").addClass("TOC2ul").appendTo(".TOC2");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC2ul");
     });
// 
var bodyContents = $(".bodyContents3");
$("<ol>").addClass("TOC3ul").appendTo(".TOC3");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC3ul");
     });
//
var bodyContents = $(".bodyContents4");
$("<ol>").addClass("TOC4ul").appendTo(".TOC4");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC4ul");
     });
//
var bodyContents = $(".bodyContents5");
$("<ol>").addClass("TOC5ul").appendTo(".TOC5");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC5ul");
     });
//
var bodyContents = $(".bodyContents6");
$("<ol>").addClass("TOC6ul").appendTo(".TOC6");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC6ul");
     });
//
var bodyContents = $(".bodyContents7");
$("<ol>").addClass("TOC7ul").appendTo(".TOC7");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC7ul");
     });
//
var bodyContents = $(".bodyContents8");
$("<ol>").addClass("TOC8ul").appendTo(".TOC8");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC8ul");
     });
//
var bodyContents = $(".bodyContents9");
$("<ol>").addClass("TOC9ul").appendTo(".TOC9");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC9ul");
     });

</script>

<!-- VIDEO BUTTONS SCRIPT -->
<script type="text/javascript">
  function iframePopInject(event) {
    var $button = $(event.target);
    // console.log($button.parent().next());
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $figure = $("<div>").addClass("video_container");
        $iframe = $("<iframe>").appendTo($figure);
        $iframe.attr("src", $button.attr("src"));
        // $iframe.attr("frameborder", "0");
        $iframe.attr("allowfullscreen", "true");
        $iframe.css("padding", "4px 6px");
        $button.next().css("display", "block");
        $figure.appendTo($button.next());
        $button.text("Hide Video")
    } else {
        $button.attr('value', 'show');
        $button.next().html("");
        $button.text("Show Video")
    }
}
</script>

<!-- BUTTON TRY -->
<script type="text/javascript">
  function iframePopA(event) {
    event.preventDefault();
    var $a = $(event.target).parent();
    console.log($a);
    if ($a.attr('value') == 'show') {
        $a.attr('value', 'hide');
        $figure = $("<div>");
        $iframe = $("<iframe>").addClass("popup_website_container").appendTo($figure);
        $iframe.attr("src", $a.attr("href"));
        $iframe.attr("frameborder", "1");
        $iframe.attr("allowfullscreen", "true");
        $iframe.css("padding", "4px 6px");
        $a.next().css("display", "block");
        $figure.appendTo($a.next().next());
        // $a.text("Hide Content")
        $('html, body').animate({
            scrollTop: $a.offset().top
        }, 1000);
    } else {
        $a.attr('value', 'show');
        $a.next().next().html("");
        // $a.text("Show Content")
    }

    $a.next().css("display", "inline");
}
</script>


<!-- TEXT BUTTON SCRIPT - INJECT -->
<script type="text/javascript">
  function showTextPopInject(event) {
    var $button = $(event.target);
    var txt = $button.attr("input");
    console.log(txt);
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $p = $("<p>");
        $p.html(txt);
        $button.next().css("display", "block");
        $p.appendTo($button.next());
        $button.text("Hide Content")
    } else {
        $button.attr('value', 'show');
        $button.next().html("");
        $button.text("Show Content")
    }

}
</script>

<!-- TEXT BUTTON SCRIPT - HIDDEN / HIDE / SHOW / HIDE/SHOW -->
<script type="text/javascript">
  function showTextPopHide(event) {
    var $button = $(event.target);
    // var txt = $button.attr("input");
    var txt = $button.text();
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $button.next().removeAttr("hidden");
        $button.text(txt + " - Hide Content");
    } else {
        $button.attr('value', 'show');
        $button.next().attr("hidden", "");
        $button.text(txt.replace(" - Hide Content",""));
    }
}
</script>

<!-- TEXT BUTTON SCRIPT - HIDDEN / HIDE / SHOW / HIDE/SHOW -->
<script type="text/javascript">
  function showText_withParent_PopHide(event) {
    var $button = $(event.target);
    var $parent = $button.parent();
    var txt = $button.text();
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $parent.next().removeAttr("hidden");
        $button.text(txt + " - Hide Content");
    } else {
        $button.attr('value', 'show');
        $parent.next().attr("hidden", "");
        $button.text(txt.replace(" - Hide Content",""));
    }
}
</script>

<!-- Print / Printing / printme -->
<!-- <script type="text/javascript">
i = 0

for (var i = 1; i < 6; i++) {
    var bodyContents = $(".bodyContents" + i);
    $("<p>").addClass("TOC1ul")  .appendTo(".TOC1");
    bodyContents.each(function(index, element) {
        var paragraph = $(element);
        $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC1ul");
         });
} 
</script>
 -->
 
</html>

