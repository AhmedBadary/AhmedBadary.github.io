<!DOCTYPE html>
<html lang="en-us">
  <head>
  <meta charset="UTF-8">
  <title>Ahmad Badary</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <link rel="stylesheet" href="/css/normalize.css">
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="/css/cayman.css">
  <link rel="stylesheet" href="/css/style.css">
  <link rel="shortcut icon" href="/main_files/favicon.ico" />
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="/css/customStyle.css">
  <title> » Ahmad Badary</title>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
</head>

  <body>
    <nav class="main-nav">
    <a href="https://ahmedbadary.github.io/" class="main-nav-logo">
        <img src="/main_files/logo.png">
    </a>
    <ul id="menu-main" class="main-nav-items">
        <li id="menu-item-1859" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-home menu-item-1859">
            <a href="/">Home</a>
        </li>
        <li id="menu-item-2869" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-2869">
            <a href="/work">Work</a>
        </li>
        <li id="menu-item-1892" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1892">
            <a href="/projects">Projects</a>
        </li>
        <li id="menu-item-1858" class="menu-item menu-item-type-post_type menu-item-object-page current_page_parent menu-item-1858">
            <a href="/blog">Blog</a>
        </li>
        <li id="menu-item-1862" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1862">
            <a href="/about">About</a>
        </li>
    </ul>
</nav>


<section class="page-header">
  <h1 class="project-name">CNNs <br /> Convolutional Neural Networks</h1>
  <h2 class="project-tagline"></h2>
  <a href="/#" class="btn">Home</a>
  <a href="/work" class="btn">Work-Space</a>
  <a href= /work_files/research/dl/cv.html class="btn">Previous</a>
</section>

<!-- <div>
  <ul class="posts">
    
      <li><span>02 Jan 2014</span> &raquo; <a href="/2014/01/02/introducing-Ahmad/">Introducing Ahmad</a></li>
    
  </ul>
</div> -->


    <section class="main-content">
      
      <div class="TOC">
  <h1 id="table-of-contents">Table of Contents</h1>

  <ul class="TOC1">
    <li><a href="#content1">Introduction</a></li>
  </ul>
  <ul class="TOC2">
    <li><a href="#content2">Architecture and Design</a></li>
  </ul>
  <ul class="TOC3">
    <li><a href="#content3">The Convolutional Layer</a></li>
  </ul>
  <ul class="TOC4">
    <li><a href="#content4">The Pooling Layer</a></li>
  </ul>
  <ul class="TOC5">
    <li><a href="#content5">Convolution and Pooling as an Infinitely Strong Prior</a></li>
  </ul>
  <ul class="TOC6">
    <li><a href="#content6">Variants of the Basic Convolution Function and Structured Outputs</a></li>
  </ul>

</div>

<hr />
<hr />

<ul>
  <li><a href="https://www.youtube.com/watch?v=KuXjwB4LzSA">What is a convolution? - 3b1b (youtube)</a></li>
</ul>

<p><a href="/work_files/research/dl/cnnx">CNNs in CV</a><br />
<a href="/work_files/research/dl/nlp/cnnsNnlp">CNNs in NLP</a><br />
<a href="/work_files/research/dl/arcts">CNNs Architectures</a><br />
<a href="https://medium.com/inveterate-learner/deep-learning-book-chapter-9-convolutional-networks-45e43bfc718d">Convnet Ch.9 Summary (blog)</a></p>

<ul>
  <li>
    <p><a href="https://arxiv.org/pdf/1601.04920.pdf">Understanding Deep Convolutional Networks (Paper!!)</a></p>
  </li>
  <li><a href="https://distill.pub/2019/computing-receptive-fields/">Computing Receptive Fields in CNNs (Blog)</a></li>
  <li><a href="https://poloclub.github.io/cnn-explainer/">CNN Explainer - CNNs Visualized in your browser! (Blog!!)</a></li>
</ul>

<h2 id="content1">Introduction</h2>

<ol>
  <li><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents11">CNNs:</strong> <br />
 In machine learning, a convolutional neural network (CNN, or ConvNet) is a class of deep, feed-forward artificial neural networks that has successfully been applied to analyzing visual imagery.<br />
 In general, it works on data that have <em>grid-like topology.</em>
    <blockquote>
      <p>E.g. Time-series data (1-d grid w/ samples at regular time intervals), image data (2-d grid of pixels).</p>
    </blockquote>

    <p>Convolutional networks are simply neural networks that use convolution in place of general matrix multiplication in at least one of their layers.</p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents12">The Big Idea:</strong> <br />
 CNNs use a variation of multilayer Perceptrons designed to require minimal preprocessing. In particular, they use the <a href="#bodyContents31">Convolution Operation</a>. <br />
 The Convolution leverage <em>three important ideas</em> that can help improve a machine learning system:
    <ol>
      <li><strong>Sparse Interactions/Connectivity/Weights:</strong><br />
 Unlike FNNs, where every input unit is connected to every output unit, CNNs have sparse interactions. This is accomplished by making the kernel smaller than the input.<br />
 <strong>Benefits:</strong>
        <ul>
          <li>This means that we need to <em>store fewer parameters</em>, which both,
            <ul>
              <li><em>Reduces the memory requirements</em> of the model and</li>
              <li><em>Improves</em> its <em>statistical efficiency</em></li>
            </ul>
          </li>
          <li>Also, Computing the output requires fewer operations</li>
          <li>In deep CNNs, the units in the deeper layers interact indirectly with large subsets of the input which allows modelling of complex interactions through sparse connections.</li>
        </ul>

        <blockquote>
          <p>These improvements in efficiency are usually quite large.<br />
 If there are \(m\) inputs and \(n\) outputs, then matrix multiplication requires \(m \times n\) parameters, and the algorithms used in practice have \(\mathcal{O}(m \times n)\) runtime (per example). If we limit the number of connections each output may have to \(k\), then the sparsely connected approach requires only \(k \times n\) parameters and \(\mathcal{O}(k \times n)\) runtime.</p>
        </blockquote>

        <p><button class="showText" value="show" onclick="showTextPopHide(event);">Figure: Sparse Connectivity from Below</button>
 <img src="/main_files/dl/cnn/14.png" alt="img" width="70%" hidden="" /><br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">Figure: Sparse Connectivity from Above</button>
 <img src="/main_files/dl/cnn/15.png" alt="img" width="70%" hidden="" /></p>
      </li>
      <li>
        <p><strong>Parameter Sharing:</strong> <br />
 refers to using the same parameter for more than one function in a model.</p>

        <p id="lst-p"><strong style="color: red">Benefits:</strong></p>
        <ul>
          <li>This means that rather than learning a separate set of parameters for every location, we <em>learn only one set of parameters</em>.
            <ul>
              <li>This does not affect the runtime of forward propagation—it is still \(\mathcal{O}(k \times n)\)</li>
              <li>But it does further reduce the storage requirements of the model to \(k\) parameters (\(k\) is usually several orders of magnitude smaller than \(m\))</li>
            </ul>
          </li>
        </ul>

        <p>Convolution is thus dramatically more efficient than dense matrix multiplication in terms of the memory requirements and statistical efficiency.<br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">Figure: Parameter Sharing</button>
 <img src="/main_files/dl/cnn/16.png" alt="img" width="70%" hidden="" /></p>
      </li>
      <li><strong>Equivariant Representations:</strong><br />
 For convolutions, the particular form of parameter sharing causes the layer to have a property called <strong>equivariance to translation</strong>.
        <blockquote>
          <p>A function is <strong>equivariant</strong> means that if the input changes, the output changes in the same way.<br />
     Specifically, a function \(f(x)\) is equivariant to a function \(g\) if \(f(g(x)) = g(f(x))\).</p>
        </blockquote>

        <p>Thus, if we move the object in the input, its representation will move the same amount in the output.</p>

        <p id="lst-p"><strong style="color: red">Benefits:</strong></p>
        <ul>
          <li>It is most useful when we know that some function of a small number of neighboring pixels is useful when applied to multiple input locations (e.g. edge detection)</li>
          <li>Shifting the position of an object in the input doesn’t confuse the NN</li>
          <li>Robustness against translated inputs/images</li>
        </ul>

        <p>Note: Convolution is <strong>not</strong> naturally equivariant to some other transformations, such as <em>changes in the scale</em> or <em>rotation</em> of an image.</p>
      </li>
    </ol>

    <p>Finally, the convolution provides a means for working with <strong>inputs of variable sizes</strong> (i.e. data that cannot be processed by neural networks defined by matrix multiplication with a fixed-shape matrix).</p>

    <p><button class="showText" value="show" onclick="showTextPopHide(event);">FC Mat-Mul as a small kernel</button>
 <img src="/main_files/gifs/nn_matmul.gif" alt="img" width="70%" hidden="" /><br />
 <br /></p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents13">Inspiration Model:</strong><br />
 Convolutional networks were inspired by biological processes in which the connectivity pattern between neurons is inspired by the organization of the animal visual cortex.<br />
 Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field.</li>
</ol>

<hr />

<h2 id="content2">Architecture and Design</h2>

<ol>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents20">Design:</strong><br />
 A CNN consists of an input and an output layer, as well as multiple hidden layers.<br />
 The hidden layers of a CNN typically consist of convolutional layers, pooling layers, fully connected layers and normalization layers.</p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents21">Volumes of Neurons:</strong> <br />
 Unlike neurons in traditional Feed-Forward networks, the layers of a ConvNet have neurons arranged in 3-dimensions: <strong>width, height, depth</strong>.
    <blockquote>
      <p>Note: <strong>Depth</strong> here refers to the third dimension of an activation volume, not to the depth of a full Neural Network, which can refer to the total number of layers in a network.</p>
    </blockquote>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents22">Connectivity:</strong> <br />
 The neurons in a layer will only be connected to a small region of the layer before it, instead of all of the neurons in a fully-connected manner.</p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents23">Functionality:</strong> <br />
 A ConvNet is made up of Layers.<br />
 Every Layer has a simple API: It transforms an input 3D volume to an output 3D volume with some differentiable function that may or may not have parameters.</p>

    <p><img src="/main_files/dl/cnn/1.png" alt="img" width="100%" /></p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents24">Layers:</strong> <br />
 We use three main types of layers to build ConvNet architectures:
    <ul>
      <li>Convolutional Layer:
        <ul>
          <li>Convolution (Linear Transformation)</li>
          <li>Activation (Non-Linear Transformation; e.g. ReLU)
            <blockquote>
              <p>Known as <strong>Detector Stage</strong></p>
            </blockquote>
          </li>
        </ul>
      </li>
      <li>Pooling Layer</li>
      <li>Fully-Connected Layer</li>
    </ul>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents241">Process:</strong></dt>
      <dd>ConvNets transform the original image layer by layer from the original pixel values to the final class scores.</dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents25">Example Architecture (CIFAR-10):</strong></dt>
      <dd>Model: [INPUT - CONV - RELU - POOL - FC]</dd>
      <dd>
        <ul>
          <li><strong>INPUT:</strong> [32x32x3] will hold the raw pixel values of the image, in this case an image of width 32, height 32, and with three color channels R,G,B.</li>
          <li><strong>CONV-Layer</strong> will compute the output of neurons that are connected to local regions in the input, each computing a dot product between their weights and a small region they are connected to in the input volume.  <br />
This may result in volume such as [\(32\times32\times12\)] if we decided to use 12 filters.</li>
          <li><strong>RELU-Layer:</strong>  will apply an element-wise activation function, thresholding at zero. This leaves the size of the volume unchanged ([\(32\times32\times12\)]).</li>
          <li><strong>POOL-Layer:</strong> will perform a down-sampling operation along the spatial dimensions (width, height), resulting in volume such as [\(16\times16\times12\)].</li>
          <li><strong>Fully-Connected:</strong> will compute the class scores, resulting in volume of size [\(1\times1\times10\)], where each of the 10 numbers correspond to a class score, such as among the 10 categories of CIFAR-10.<br />
As with ordinary Neural Networks and as the name implies, each neuron in this layer will be connected to all the numbers in the previous volume.</li>
        </ul>
      </dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents26">Fixed Functions VS Hyper-Parameters:</strong></dt>
      <dd>Some layers contain parameters and other don’t.</dd>
      <dd>
        <ul>
          <li><strong>CONV/FC layers</strong> perform transformations that are a function of not only the activations in the input volume, but also of the parameters (the weights and biases of the neurons).</li>
        </ul>
      </dd>
      <dd>
        <ul>
          <li><strong>RELU/POOL</strong> layers will implement a fixed function.</li>
        </ul>
      </dd>
      <dd>
        <blockquote>
          <p>The parameters in the CONV/FC layers will be trained with gradient descent so that the class scores that the ConvNet computes are consistent with the labels in the training set for each image.</p>
        </blockquote>
      </dd>
    </dl>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents27"><a href="http://cs231n.github.io/convolutional-networks/">Summary</a>:</strong>
    <ul>
      <li>A ConvNet architecture is in the simplest case a list of Layers that transform the image volume into an output volume (e.g. holding the class scores)</li>
      <li>There are a few distinct types of Layers (e.g. CONV/FC/RELU/POOL are by far the most popular)</li>
      <li>Each Layer accepts an input 3D volume and transforms it to an output 3D volume through a differentiable function</li>
      <li>Each Layer may or may not have parameters (e.g. CONV/FC do, RELU/POOL don’t)</li>
      <li>Each Layer may or may not have additional hyperparameters (e.g. CONV/FC/POOL do, RELU doesn’t)
        <blockquote>
          <p><a href="http://cs231n.github.io/convolutional-networks/">Click this for Credits</a></p>
        </blockquote>
      </li>
    </ul>

    <p><img src="/main_files/dl/cnn/2.png" alt="img" width="100%" /></p>
  </li>
</ol>

<hr />

<h2 id="content3">The Convolutional Layer</h2>

<p><img src="https://cdn.mathpix.com/snip/images/7aQe47vYUKI3QSjMCypArdV-ubxClScrkNIpzVxH2go.original.fullsize.png" alt="img" width="50%" /></p>

<ul>
  <li><a href="https://github.com/vdumoulin/conv_arithmetic">Convolution Arithmetic Visualization (Blog!)</a></li>
</ul>

<p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Convolution Operation Arithmetic Visualized</button></p>
<div hidden="">
  <p><em>N.B.: Blue maps are inputs, and cyan maps are outputs.</em></p>
  <table style="width:100%; table-layout:fixed;">
  <tr>
    <td>No padding, no strides</td>
    <td>Arbitrary padding, no strides</td>
    <td>Half padding, no strides</td>
    <td>Full padding, no strides</td>
  </tr>
  <tr>
    <td><img width="150px" src="/main_files/dl/cnn/gifs/no_padding_no_strides.gif" /></td>
    <td><img width="150px" src="/main_files/dl/cnn/gifs/arbitrary_padding_no_strides.gif" /></td>
    <td><img width="150px" src="/main_files/dl/cnn/gifs/same_padding_no_strides.gif" /></td>
    <td><img width="150px" src="/main_files/dl/cnn/gifs/full_padding_no_strides.gif" /></td>
  </tr>
  <tr>
    <td>No padding, strides</td>
    <td>Padding, strides</td>
    <td>Padding, strides (odd)</td>
    <td></td>
  </tr>
  <tr>
    <td><img width="150px" src="/main_files/dl/cnn/gifs/no_padding_strides.gif" /></td>
    <td><img width="150px" src="/main_files/dl/cnn/gifs/padding_strides.gif" /></td>
    <td><img width="150px" src="/main_files/dl/cnn/gifs/padding_strides_odd.gif" /></td>
    <td></td>
  </tr>
</table>
</div>

<p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);"><strong>Transposed</strong> Convolution Operation Arithmetic Visualized</button></p>
<div hidden="">
  <table style="width:100%; table-layout:fixed;">
  <tr>
    <td>No padding, no strides, transposed</td>
    <td>Arbitrary padding, no strides, transposed</td>
    <td>Half padding, no strides, transposed</td>
    <td>Full padding, no strides, transposed</td>
  </tr>
  <tr>
    <td><img width="150px" src="/main_files/dl/cnn/gifs/no_padding_no_strides_transposed.gif" /></td>
    <td><img width="150px" src="/main_files/dl/cnn/gifs/arbitrary_padding_no_strides_transposed.gif" /></td>
    <td><img width="150px" src="/main_files/dl/cnn/gifs/same_padding_no_strides_transposed.gif" /></td>
    <td><img width="150px" src="/main_files/dl/cnn/gifs/full_padding_no_strides_transposed.gif" /></td>
  </tr>
  <tr>
    <td>No padding, strides, transposed</td>
    <td>Padding, strides, transposed</td>
    <td>Padding, strides, transposed (odd)</td>
    <td></td>
  </tr>
  <tr>
    <td><img width="150px" src="/main_files/dl/cnn/gifs/no_padding_strides_transposed.gif" /></td>
    <td><img width="150px" src="/main_files/dl/cnn/gifs/padding_strides_transposed.gif" /></td>
    <td><img width="150px" src="/main_files/dl/cnn/gifs/padding_strides_odd_transposed.gif" /></td>
    <td></td>
  </tr>
</table>
</div>

<p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);"><strong>Dilated</strong> Convolution Operation Arithmetic Visualized</button></p>
<div hidden="">
  <table style="width:25%; table-layout:fixed;">
  <tr>
    <td>No padding, no stride, dilation</td>
  </tr>
  <tr>
    <td><img width="150px" src="/main_files/dl/cnn/gifs/dilation.gif" /></td>
  </tr>
</table>
</div>

<ol>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents31">Convolutions:</strong> <br />
 In its most general form, the convolution is a <strong>Linear Operation</strong> on two functions of real-valued arguments.</p>

    <p>Mathematically, a <strong>Convolution</strong> is a mathematical operation on two functions (\(f\) and \(g\)) to produce a third function, that is typically viewed as a modified version of one of the original functions, giving the integral of the point-wise multiplication of the two functions as a function of the amount that one of the original functions is translated.<br />
 The convolution could be thought of as a <strong>weighting function</strong> (e.g. for taking the weighted average of a series of numbers/function-outputs).</p>

    <p>The convolution of the <strong>continuous</strong> functions \(f\) and \(g\):</p>
    <p>$${\displaystyle {\begin{aligned}(f * g)(t)&amp;\,{\stackrel {\mathrm {def} }{=}}\ \int _{-\infty }^{\infty }f(\tau )g(t-\tau )\,d\tau \\&amp;=\int_{-\infty }^{\infty }f(t-\tau )g(\tau )\,d\tau .\end{aligned}}}$$</p>

    <p>The convolution of the <strong>discreet</strong> functions f and g:</p>
    <p>$${\displaystyle {\begin{aligned}(f * g)[n]&amp;=\sum_{m=-\infty }^{\infty }f[m]g[n-m]\\&amp;=\sum_{m=-\infty }^{\infty }f[n-m]g[m].\end{aligned}}} (commutativity)$$</p>
    <p id="lst-p">In this notation, we refer to:</p>
    <ul>
      <li>The function \(f\) as the <strong>Input</strong></li>
      <li>The function \(g\) as the <strong>Kernel/Filter</strong></li>
      <li>The output of the convolution as the <strong>Feature Map</strong></li>
    </ul>

    <p><strong>Commutativity:</strong><br />
 <button class="showText" value="show" onclick="showText_withParent_PopHide(event);">On Commutativity</button></p>
    <div hidden="">
      <p>Can be achieved by flipping the kernel with respect to the input; in the sense that as increases, the index into the \(m\) input increases, but the index into the kernel decreases.<br />
 While the commutative property is useful for writing proofs, it is not usually an important property of a neural network implementation.<br />
 Moreover, in a CNN, the convolution is used simultaneously with other functions, and the combination of these functions <strong>does not commute</strong> regardless of whether the convolution operation flips its kernel or not.<br />
 Because Convolutional networks usually use multichannel convolution, the linear operations they are based on are not guaranteed to be commutative, even if kernel flipping is used. These multichannel operations are only commutative if each operation has the same number of output channels as input channels.</p>
    </div>
    <p><br /></p>

    <ul>
      <li><a href="https://www.youtube.com/watch?v=KuXjwB4LzSA">What is a convolution? - 3b1b (youtube)</a></li>
    </ul>

    <p><strong style="color: red">Motivation:</strong><br />
 The convolution operation can be used to compute many results in different domains.  It originally arose in pure mathematics and probability theory as a way to <strong>combine probability distributions</strong>.</p>

    <p><strong style="color: red">Understanding the Convolution Operation:</strong></p>
    <ul>
      <li>
        <p>It can be seen as a sliding window of multiplying the values from one array with the other array.  It can also be seen as a matrix generated by the outer product of the two “vectors” and then summing the diagonals.</p>
      </li>
      <li>
        <p>The \(n\)th element of the convolution is the sum of the product of the elements in the two arrays such that the indices of the arrays sum up to \(n\):<br />
  <button class="showText" value="show" onclick="showTextPopHide(event);">Example n = 6</button>
  <img src="https://cdn.mathpix.com/snip/images/xdMDdcp9DPCs2K0y2Fr4Duy3Knogj0XaDpOMCGadPOs.original.fullsize.png" alt="img" width="100%" hidden="" /></p>

        <p>The formula for the <strong>convolution of \(a\) and \(b\)</strong>:</p>
        <p>$$(a * b)_n=\sum_{\substack{i, j \\ i+j=n}} a_i \cdot b_j$$</p>
      </li>
    </ul>

    <p><strong style="color: red">Where does it apply?</strong></p>
    <ol>
      <li></li>
      <li><strong>Image Processing:</strong>  different kernels give us different image processing effects like the examples below.
        <ol>
          <li><em><strong>Image Blurring</strong></em>: calculating a 2d <em>moving average</em> of the image results in a form of blurring.<br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">Example 1: Uniform Weight (1/9)</button>
 <img src="https://cdn.mathpix.com/snip/images/VvYtRW5CUbBDGL7WST9IDsTxo9hTRkceeswZWpt-PQ0.original.fullsize.png" alt="img" width="100%" hidden="" /><br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">Example 2: Gaussian Blur</button>
 <img src="https://cdn.mathpix.com/snip/images/9i7qqCaxki8deaDjRW39PoAi5U746d7pg-4baG4bXZI.original.fullsize.png" alt="img" width="100%" hidden="" /><br />
     <button class="showText" value="show" onclick="showTextPopHide(event);">Kernel sampled from a Gaussian Distribution</button>
     <img src="https://cdn.mathpix.com/snip/images/b8snfIbWgT-utzFtEWenIWhE2n4vr3ec0eHwe0W1lAI.original.fullsize.png" alt="img" width="100%" hidden="" /><br />
     <button class="showText" value="show" onclick="showTextPopHide(event);">Another View for Gaussian Blur</button>
     <img src="https://cdn.mathpix.com/snip/images/FJqUiP7DqNOE76SF3eddTE19EdH7SGYWXBq5g9VjInQ.original.fullsize.png" alt="img" width="100%" hidden="" /></li>
          <li><em><strong>Vertical Edge Detection</strong></em>: <br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">Example</button>
 <img src="https://cdn.mathpix.com/snip/images/tBcDgU1AkCy5_Gbg8Dfgw744V7YSowr5p9VtTaxakMc.original.fullsize.png" alt="img" width="100%" hidden="" /><br />
     <button class="showText" value="show" onclick="showTextPopHide(event);">Kernel = \(k\)</button>
     <img src="https://cdn.mathpix.com/snip/images/XzD_Q90DXxqWDaDc2LVmPsub7VgDC-mZT9L6afwZfqw.original.fullsize.png" alt="img" width="100%" hidden="" /></li>
          <li><em><strong>Horizontal Edge Detection</strong></em>:<br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">Example</button>
 <img src="https://cdn.mathpix.com/snip/images/K28Q_84akPwrDrJd0dio2rNrDdwOuyzMDFlYcmCwajc.original.fullsize.png" alt="img" width="100%" hidden="" /><br />
     <button class="showText" value="show" onclick="showTextPopHide(event);">Kernel = \(k^T\)</button>
     <img src="https://cdn.mathpix.com/snip/images/TwS9dysjBDXucHDQ_wFtEzW4AqstQV0ICwI2bxS6GMs.original.fullsize.png" alt="img" width="100%" hidden="" /></li>
          <li><em><strong>Image Sharpening</strong></em>:<br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">Example</button>
 <img src="https://cdn.mathpix.com/snip/images/nLzeqXNcyet0P0WnapHOK5Lat2dIkLNJ1whfJwC_4C4.original.fullsize.png" alt="img" width="100%" hidden="" /><br />
     <button class="showText" value="show" onclick="showTextPopHide(event);">Kernel</button>
     <img src="https://cdn.mathpix.com/snip/images/r3dP7AnqbL0SmOfq1H4Sj8sHAT3DHL-WBWBbK5uX8ww.original.fullsize.png" alt="img" width="100%" hidden="" /></li>
        </ol>
      </li>
      <li><strong>Probability:</strong>
        <ol>
          <li><em><strong>Sum of two Probability Distributions</strong></em>: the convolution of the probabilities of each event corresponds to <em><strong>adding two probability distributions</strong></em> together \(P_{X+Y}\):<br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">Example (unfair dice)</button>
 <img src="https://cdn.mathpix.com/snip/images/WZpxoN1zIFDNAgL_fsXVJkCokRmgE77JfUcQPilv7yc.original.fullsize.png" alt="img" width="100%" hidden="" /></li>
          <li>Calculating a <em><strong>moving average</strong></em> (equivalently, <em><strong>data smoothing</strong></em>):<br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">Example 1: (1/5th)</button>
 <img src="https://cdn.mathpix.com/snip/images/aETH3cqrFP_QotNkKKz3JDkkENkYsuzJN3VQKktSxzo.original.fullsize.png" alt="img" width="100%" hidden="" /><br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">Example 2: weighted moving average</button>
 <img src="https://cdn.mathpix.com/snip/images/ucCtWOYl-8Z7_Ek8gbuWtgboRP4vhJv2iqzmFQ_YfM0.original.fullsize.png" alt="img" width="100%" hidden="" /></li>
        </ol>
      </li>
      <li><strong>Differential Equations:</strong> solving DEs</li>
      <li><strong>Polynomials:</strong> in multiplying two polynomials, the coeffecients are the convolution of the coeffecients of the original polynomials (which are the sums of the diagonals of the convolution matrix):<br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">Polynomial Multiplication w/ Convolution</button>
 <img src="https://cdn.mathpix.com/snip/images/bDRyHAQC4apZEMlI0TGtaCEVGr5FQ_SYrny1JDsUL8s.original.fullsize.png" alt="img" width="100%" hidden="" /></li>
      <li><strong>Multiplication of two numbers</strong>:<br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">Example: Hand-Multiplication &lt;–&gt; Convolution</button>
 <img src="https://cdn.mathpix.com/snip/images/3JNrIx0LLJYx6CEN6v3MhIB3ke2gdB18APSZ_5GWw-Q.original.fullsize.png" alt="img" width="100%" hidden="" /></li>
    </ol>

    <p><strong style="color: red">Computing the Convolution Operation:</strong><br />
 We can compute the convolution of two arrays much faster by utilizing the <em><strong>FFT</strong></em> algorithm as implemented by the <code class="language-plaintext highlighter-rouge">scipy.signal.fftconvolve</code> function.
 <button class="showText" value="show" onclick="showTextPopHide(event);">Runtime Comparison</button>
 <img src="https://cdn.mathpix.com/snip/images/iOTBB61BPcocR6DcZHRQGOdGlpIgtLVONerT32uuKIg.original.fullsize.png" alt="img" width="100%" hidden="" /></p>

    <p><strong style="color: red">Deriving the faster algorithm for computing the convolution operation:</strong></p>

    <ul>
      <li>We utilize the <strong>connection between</strong> <em><strong>multiplication and convolutions</strong></em> to come up with a faster algorithm for computing the convolution</li>
      <li><strong>New Algorithm</strong> for computing the <strong>convolution of two arrays</strong> \(a, b\) (\(\mathcal{O}(N^2)\)):
        <ol>
          <li>Assume the Arrays are <em><strong>coeffecients</strong></em> of <strong>two polynomials</strong>:</li>
          <li><strong>Sample</strong> the polynomials at <strong>len(\(a\)), len(\(b\)) points</strong>  respectively:</li>
          <li><strong>Multiply the samples pointwise</strong></li>
          <li><strong>Solve</strong> the new system to <strong>recover the</strong> <em><strong>coeffecients</strong></em> which will be the <em><strong>convolution</strong></em> of \(a, b\)</li>
        </ol>
      </li>
    </ul>

    <p><strong style="color: red">Problems with this approach:</strong></p>
    <ul>
      <li><strong>Multiplying two polynomials</strong> by <em>expanding the products</em> is an <em><strong>\(\mathcal{O}(n^2)\)</strong></em> algorithm.<br />
  <button class="showText" value="show" onclick="showTextPopHide(event);">Visualization</button>
  <img src="https://cdn.mathpix.com/snip/images/TqD5ilf1CDsOBVZOlaEX4maH9uZVa-NiIYSmfLt2ko8.original.fullsize.png" alt="img" width="100%" hidden="" /></li>
    </ul>

    <p><strong style="color: red">Key Ideas for the solution:</strong></p>
    <ol>
      <li><strong>Recovering a polynomial</strong> of order \(n\) only requires <strong>\(n+1\) samples</strong></li>
      <li>Utilize the <strong>connection between</strong> <em><strong>multiplication of polynomials and convolutions</strong></em>
 I.E. we can translate one problem into the other.</li>
      <li>
        <p>Utilize the <em><strong>Discrete Fourier Transform (DFT)</strong></em> of the <strong>coeffecients</strong> to compute the samples of each constructed polynomial to reduce the runtime from \(\mathcal{O}(N^2)\) to \(\mathcal{O}(N \log(N))\)</p>

        <p><button class="showText" value="show" onclick="showTextPopHide(event);">Show</button>
 <img src="https://cdn.mathpix.com/snip/images/ax0PNZquP3WsvCrlRBIbg5NCe3bcVOgb3MeQMEgqx9E.original.fullsize.png" alt="img" width="100%" hidden="" /></p>
      </li>
    </ol>

    <p><strong style="color: red">Fast Convolution Algorithm using the Fast Fourier Transform (FFT) \(\mathcal{O}(N \log(N))\):</strong></p>
    <ol>
      <li>Compute the FFT of each array (as coeffecients)
 I.E. Treat them as polynomials and evaluate them at the <em><strong>roots of unity</strong></em><br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">FFT</button>
 <img src="https://cdn.mathpix.com/snip/images/QMHRIEb5mbzNqwTRWb5pvEYZfehSSZe46XW2mhSILag.original.fullsize.png" alt="img" width="100%" hidden="" /></li>
      <li>Multiply the FFT of each array, pointwise
 <button class="showText" value="show" onclick="showTextPopHide(event);">Pointwise Multiplication</button>
 <img src="https://cdn.mathpix.com/snip/images/L1aKDafPWVDtAdNPL-1TlybLJQazv3v-tttguq6EOsA.original.fullsize.png" alt="img" width="100%" hidden="" /></li>
      <li>Compute the <em><strong>inverse</strong></em> FFT of the new result of multiplication<br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">Inverse FFT</button>
 <img src="https://cdn.mathpix.com/snip/images/L1aKDafPWVDtAdNPL-1TlybLJQazv3v-tttguq6EOsA.original.fullsize.png" alt="img" width="100%" hidden="" /></li>
    </ol>

    <p><strong style="color: red">Key Results:</strong><br />
 The connection between all these applications of Convolutions, and its’ connection to the <em><strong>FFT</strong></em> implies that we can compute all the results above in \(\mathcal{O}(N \log(N))\) time (e.g. sum of probabilities, image processing, etc.).</p>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents32">Cross-Correlation:</strong></dt>
      <dd>Cross-Correlation is a measure of similarity of two series as a function of the displacement of one relative to the other.</dd>
      <dd>The <strong>continuous</strong> cross-correlation on continuous functions f and g:</dd>
      <dd>
\[(f\star g)(\tau )\ {\stackrel {\mathrm {def} }{=}}\int_{-\infty }^{\infty }f^{*}(t)\ g(t+\tau )\,dt,\]
      </dd>
      <dd>The <strong>discrete</strong> cross-correlation on discreet functions f and g:</dd>
      <dd>
        <p>$$(f\star g)[n]\ {\stackrel {\mathrm {def} }{=}}\sum _{m=-\infty }^{\infty }f^{*}[m]\ g[m+n].$$</p>
        <p><br /></p>
      </dd>
    </dl>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents33">Convolutions and Cross-Correlation:</strong>
    <ul>
      <li>Convolution is similar to cross-correlation.</li>
      <li><em>For discrete real valued signals</em>, they differ only in a time reversal in one of the signals.</li>
      <li><em>For continuous signals</em>, the cross-correlation operator is the <strong>adjoint operator</strong> of the convolution operator.<br />
 <br /></li>
    </ul>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents34">CNNs, Convolutions, and Cross-Correlation:</strong> <br />
 The term Convolution in the name “Convolution Neural Network” is unfortunately a <strong>misnomer</strong>.<br />
 CNNs actually <strong>use Cross-Correlation</strong> instead as their similarity operator.<br />
 The term ‘convolution’ has stuck in the name by convention.<br />
 <br /></p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents315">Convolution in DL:</strong> <br />
The Convolution operation:
    <p>$$s(t)=(x * w)(t)=\sum_{a=-\infty}^{\infty} x(a) w(t-a)$$</p>
    <p>we usually assume that these functions are zero everywhere but in the finite set of points for which we store the values.<br />
<br /></p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents316">Convolution Over Two Axis:</strong> <br />
If we use a 2D image \(I\) as our input, we probably also want to use a two-dimensional kernel \(K\):
    <p>$$S(i, j)=(I * K)(i, j)=\sum_{m} \sum_{n} I(m, n) K(i-m, j-n)$$</p>

    <p>In practice we use the following formula instead (commutativity):</p>
    <p>$$S(i, j)=(K * I)(i, j)=\sum_{m} \sum_{n} I(i-m, j-n) K(m, n)$$</p>
    <p>Usually the latter formula is more straightforward to implement in a machine learning library, because there is less variation in the range of valid values of \(m\) and \(n\).</p>

    <p>The Cross-Correlation is usually implemented by ML-libs:</p>
    <p>$$S(i, j)=(K * I)(i, j)=\sum_{m} \sum_{n} I(i+m, j+n) K(m, n)$$</p>

    <p><button class="showText" value="show" onclick="showTextPopHide(event);"><a href="https://www.allaboutcircuits.com/technical-articles/two-dimensional-convolution-in-image-processing/">Explanation for the Convolution Function (Math representation) in 2D</a></button>
<img src="https://cdn.mathpix.com/snip/images/QBBwhLsz9WuCAqBDkGqso-_nzRZF-SSGbr5rXJXGbY0.original.fullsize.png" alt="img" width="100%" hidden="" /></p>

    <p><button class="showText" value="show" onclick="showTextPopHide(event);">2D Convolution Animation (wikipedia)</button>
<img src="https://upload.wikimedia.org/wikipedia/commons/1/19/2D_Convolution_Animation.gif" alt="img" width="100%" hidden="" /><br />
<br /></p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents317">The Mathematics of the Convolution Operation:</strong>
    <ul>
      <li>The operation can be broken into matrix multiplications using the Toeplitz matrix representation for 1D and block-circulant matrix for 2D convolution:
        <ul>
          <li><strong>Discrete convolution</strong> can be viewed as <strong>multiplication by a matrix</strong>, but the matrix has several entries constrained to be equal to other entries.
            <blockquote>
              <p>For example, for <strong>univariate discrete convolution</strong>, each row of the matrix is constrained to be equal to the row above shifted by one element. This is known as a <em><strong>Toeplitz matrix</strong></em>.<br />
  A <strong>Toeplitz matrix</strong> has the property that values along all diagonals are constant.</p>
            </blockquote>

            <p><!-- * <button>Figure: Toeplitz Matrix</button>{: .showText value="show" onclick="showTextPopHide(event);"}  
   -->
  <img src="/main_files/dl/cnn/20.png" alt="img" width="50%" /></p>
          </li>
          <li>In <strong>two dimensions</strong>, a <strong>doubly block circulant matrix</strong> corresponds to convolution.
            <blockquote>
              <p>A matrix which is circulant with respect to its sub-matrices is called a <strong>block circulant matrix</strong>. If each of the submatrices is itself circulant, the matrix is called <strong>doubly block-circulant matrix</strong>.<br />
      <!-- * <button>Figure: Block-Circulant Matrix</button>{: .showText value="show" onclick="showTextPopHide(event);"}   -->
      <img src="/main_files/dl/cnn/21.png" alt="img" width="40%" /></p>
            </blockquote>
          </li>
        </ul>
      </li>
      <li>Convolution usually corresponds to a <strong>very sparse matrix</strong> (a matrix whose entries are mostly equal to zero).<br />
  This is because the kernel is usually much smaller than the input image.</li>
      <li>Any neural network algorithm that works with matrix multiplication and does not depend on specific properties of the matrix structure should work with convolution, without requiring any further changes to the neural network.</li>
      <li>Typical convolutional neural networks do make use of further specializations in order to deal with large inputs efficiently, but these are not strictly necessary from a theoretical perspective.<br />
<br /></li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents35">The Convolution operation in a CONV Layer:</strong>
    <ul>
      <li>The CONV layer’s <strong>parameters</strong> consist of <strong>a set of learnable filters</strong>.
        <ul>
          <li>Every filter is small spatially (along width and height), but extends through the full depth of the input volume.
            <blockquote>
              <p>For example, a typical filter on a first layer of a ConvNet might have size 5x5x3 (i.e. 5 pixels width and height, and 3 because images have depth 3, the color channels).</p>
            </blockquote>
          </li>
        </ul>
      </li>
      <li>In the <strong>forward pass</strong>, we slide (convolve) each filter across the width and height of the input volume and compute dot products between the entries of the filter and the input at any position.
        <ul>
          <li>As we slide the filter over the width and height of the input volume we will produce a 2-dimensional activation map that gives the responses of that filter at every spatial position.
            <blockquote>
              <p>Intuitively, the network will learn filters that activate when they see some type of visual feature such as an edge of some orientation or a blotch of some color on the first layer, or eventually entire honeycomb or wheel-like patterns on higher layers of the network.</p>
            </blockquote>
          </li>
          <li>Now, we will have an entire set of filters in each CONV layer (e.g. 12 filters), and each of them will produce a separate 2-dimensional activation map.</li>
          <li>We will <strong>stack</strong> these activation maps along the depth dimension and produce the output volume.</li>
        </ul>
      </li>
    </ul>

    <p style="color: red">As a result <i>(of what?)</i>, the network learns filters that activate when it detects some specific type of feature at some spatial position in the input. </p>
    <p><br /></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents26">The Brain Perspective:</strong><br />
 Every entry in the 3D output volume can also be interpreted as an output of a neuron that looks at only a small region in the input and shares parameters with all neurons to the left and right spatially.<br />
 <br /></p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents27">Local Connectivity:</strong>
    <ul>
      <li>Convolutional networks exploit spatially local correlation by enforcing a local connectivity pattern between neurons of adjacent layers:
        <ul>
          <li>Each neuron is connected to only a small region of the input volume.</li>
        </ul>
      </li>
      <li>The <strong>Receptive Field</strong> of the neuron defines the extent of this connectivity as a hyperparameter.
        <blockquote>
          <p>For example, suppose the input volume has size \([32\times32\times3]\) and the receptive field (or the filter size) is \(5\times5\), then each neuron in the Conv Layer will have weights to a \([5\times5\times3]\) region in the input volume, for a total of \(5*5*3 = 75\) weights (and \(+1\) bias parameter).</p>
        </blockquote>
      </li>
    </ul>

    <p style="color: red">Such an architecture ensures that the learnt filters produce the strongest response to a spatially local input pattern.</p>
    <p><br /></p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents28">Spatial Arrangement:</strong><br />
 There are <strong>three</strong> hyperparameters control the size of the output volume:
    <ol>
      <li><strong>The Depth</strong> of the output volume is a hyperparameter that corresponds to the number of filters we would like to use (each learning to look for something different in the input).</li>
      <li><strong>The Stride</strong> controls how depth columns around the spatial dimensions (width and height) are allocated.
        <blockquote>
          <p>e.g. When the stride is 1 then we move the filters one pixel at a time.</p>
        </blockquote>

        <ul>
          <li>The <strong>Smaller</strong> the stride, the <strong>more overlapping regions</strong> exist and the <strong>bigger the volume</strong>.</li>
          <li>The <strong>bigger</strong> the stride, the <strong>less overlapping regions</strong> exist and the         <strong>smaller the volume</strong>.</li>
        </ul>
      </li>
      <li>The <strong>Padding</strong> is a hyperparameter whereby we pad the input volume with zeros around the border. <br />
 This allows to <em>control the spatial size</em> of <em>the output</em> volumes.<br />
 <br /></li>
    </ol>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents29">The Spatial Size of the Output Volume:</strong><br />
 We compute the spatial size of the output volume as a function of:
    <ul>
      <li><strong>\(W\)</strong>: The input volume size.</li>
      <li><strong>\(F\)</strong>: \(\:\)The receptive field size of the Conv Layer neurons.</li>
      <li><strong>\(S\)</strong>: \(\:\)The stride with which they are applied.</li>
      <li><strong>\(P\)</strong>: \(\:\)The amount of zero padding used on the border.<br />
 Thus, the <strong>Total Size of the Output</strong>:</li>
    </ul>
    <p>$$\dfrac{W−F+2P}{S} + 1$$</p>

    <p><strong>Potential Issue</strong>: If this number is not an integer, then the strides are set incorrectly and the neurons cannot be tiled to fit across the input volume in a symmetric way.</p>
    <ul>
      <li><strong>Fix</strong>: In general, setting zero padding to be \({\displaystyle P = \dfrac{K-1}{2}}\) when the stride is \({\displaystyle S = 1}\) ensures that the input volume and output volume will have the same size spatially.<br />
 <br /></li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents210">Calculating the Number of Parameters:</strong> <br />
Given:
    <ul>
      <li><strong>Input Volume</strong>:  \(32\times32\times3\)</li>
      <li><strong>Filters</strong>:  \(10\:\:\: (5\times5)\)</li>
      <li><strong>Stride</strong>:  \(1\)</li>
      <li><strong>Pad</strong>:  \(2\)</li>
    </ul>

    <p>The number of parameters equals the number of parameters in each filter \(= 5*5*3 + 1 = 76\) (+1 for <strong>bias</strong>) times the number of filters \(76 * 10 = 760\).<br />
<br /></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents311">The Convolution Layer:</strong> <br />
<img src="/main_files/dl/cnn/3.png" alt="img" width="70%" /><br />
<strong>__</strong><br />
<img src="/main_files/dl/cnn/8.png" alt="img" width="70%" /><br />
<strong>__</strong></p>

    <p><strong>The Conv Layer and the Brain:</strong><br />
<img src="/main_files/dl/cnn/10.png" alt="img" width="70%" /><br />
<strong>__</strong><br />
<img src="/main_files/dl/cnn/11.png" alt="img" width="70%" /><br />
<strong>__</strong><br />
<img src="/main_files/dl/cnn/12.png" alt="img" width="70%" /><br />
<strong>__</strong><br />
<br /></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents312">From FC-layers to Conv-layers:</strong> <br />
<img src="/main_files/dl/cnn/4.png" alt="img" width="70%" /><br />
<em>**
<img src="/main_files/dl/cnn/5.png" alt="img" width="70%" /><br />
**</em>
<img src="/main_files/dl/cnn/6.png" alt="img" width="70%" /><br />
***
<br /></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents313">\(1\times1\) Convolutions:</strong> <br />
<img src="/main_files/dl/cnn/9.png" alt="img" width="70%" /><br />
<br /></p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents30">Notes:</strong>
    <ul>
      <li><strong>Summary</strong>:
        <ul>
          <li>ConvNets stack CONV,POOL,FC layers</li>
          <li>Trend towards smaller filters and deeper architectures</li>
          <li>Trend towards getting rid of POOL/FC layers (just CONV)</li>
          <li>Typical architectures look like [(CONV-RELU) * N-POOL?] * M-(FC-RELU) * K, SOFTMAX<br />
  where \(N\) is usually up to ~5, \(M\) is large, \(0 &lt;= K &lt;= 2\).<br />
  But recent advances such as ResNet/GoogLeNet challenge this paradigm</li>
        </ul>
      </li>
      <li><strong>Effect of Different Biases</strong>:<br />
  Separating the biases may slightly reduce the statistical efficiency of the model, but it allows the model to correct for differences in the image statistics at different locations. For example, when using implicit zero padding, detector units at the edge of the image receive less total input and may need larger biases.</li>
      <li>In the kinds of architectures typically used for classification of a single object in an image, the greatest reduction in the spatial dimensions of the network comes from using pooling layers with large stride.</li>
    </ul>
  </li>
</ol>

<hr />

<h2 id="content4">The Pooling Layer</h2>

<ol>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents4" id="bodyContents41">The Pooling Operation/Function:</strong> <br />
 The pooling function calculates a <strong>summary statistic</strong> of the nearby pixels at the point of operation.<br />
 Some common statistics are <em>max, mean, weighted average</em> and <em>\(L^2\) norm</em> of a surrounding rectangular window.<br />
 <br /></p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents4" id="bodyContents42">The Key Ideas/Properties:</strong> <br />
 In all cases, pooling helps to make the representation approximately <strong>invariant to small translations</strong> of the input.
    <blockquote>
      <p>Invariance to translation means that if we translate the input by a small amount, the values of most of the pooled outputs do not change.<br />
 Invariance to local translation can be a useful property if we care more about whether some feature is present than exactly where it is.</p>
    </blockquote>

    <p><button class="showText" value="show" onclick="showTextPopHide(event);">Figure: MaxPooling</button>
 <img src="/main_files/dl/cnn/17.png" alt="img" width="70%" hidden="" /></p>

    <p><strong>(Learned) Invariance to other transformations:</strong><br />
 Pooling over spatial regions produces invariance to translation, but if we <em>pool over the outputs of separately parametrized convolutions</em>, the features can learn which transformations to become invariant to.<br />
 This property has been used in <strong>Maxout networks</strong>.<br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">Figure: Examples of Learned Invariance</button>
 <img src="/main_files/dl/cnn/18.png" alt="img" width="70%" hidden="" /></p>

    <p>For many tasks, pooling is essential for handling inputs of varying size.</p>
    <blockquote>
      <p>This is usually accomplished by varying the size of an offset between pooling regions so that the classification layer always receives the same number of summary statistics regardless of the input size. For example, the final pooling layer of the network may be defined to output four sets of summary statistics, one for each quadrant of an image, regardless of the image size.</p>
    </blockquote>

    <p>One can use fewer pooling units than detector units, since they provide a summary; thus, by reporting summary statistics for pooling regions spaced \(k\) pixels apart rather than \(1\) pixel apart, we can improve the computational efficiency of the network because the next layer has roughly \(k\) times fewer inputs to process.<br />
 This reduction in the input size can also result in improved statistical efficiency and reduced memory requirements for storing the parameters.<br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">Figure: Downsampling</button>
 <img src="/main_files/dl/cnn/19.png" alt="img" width="70%" hidden="" /><br />
 <br /></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents4" id="bodyContents43">Theoretical Guidelines for choosing the pooling function:</strong> <br />
 <a href="http://www.di.ens.fr/willow/pdfs/icml2010b.pdf">Link</a><br />
 <br /></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents4" id="bodyContents44">Variations:</strong> <br />
 <strong>Dynamical Pooling:</strong><br />
 It is also possible to dynamically pool features together, for example, by running a clustering algorithm on the locations of interesting features (Boureau et al., 2011) <a href="http://yann.lecun.com/exdb/publis/pdf/boureau-iccv-11.pdf">link</a>. This approach yields a different set of pooling regions for each image.</p>

    <p><strong>Learned Pooling:</strong><br />
 Another approach is to learn a single pooling structure that is then applied to all images (Jia et al., 2012).<br />
 <br /></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents4" id="bodyContents45">Pooling and Top-Down Architectures:</strong> <br />
 Pooling can complicate some kinds of neural network architectures that use top-down information, such as Boltzmann machines and autoencoders.<br />
 <br /></p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents4" id="bodyContents46">The Pooling Layer (summary):</strong> <br />
 <img src="/main_files/dl/cnn/13.png" alt="img" width="100%" /><br />
 <br /></li>
</ol>

<!-- 7. **The Pooling Layer (summary):**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents47}   
    ![img](/main_files/dl/cnn/13.png){: width="100%"}   -->

<!-- 7. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents47}  -->

<p><strong style="color: SteelBlue" class="bodyContents4" id="bodyContents48">Notes:</strong></p>
<ul>
  <li><strong>Pooling Layer</strong>:
    <ul>
      <li>Makes the representations smaller and more manageable</li>
      <li>Operates over each activation map independently (i.e. preserves depth)</li>
    </ul>
  </li>
  <li>You can use the stride instead of the pooling to downsample</li>
</ul>

<hr />

<h2 id="content5">Convolution and Pooling as an Infinitely Strong Prior</h2>

<ol>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents5" id="bodyContents51">A Prior Probability Distribution:</strong><br />
 This is a probability distribution over the parameters of a model that encodes our beliefs about what models are reasonable, before we have seen any data.<br />
 <br /></p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents5" id="bodyContents52">What is a weight prior?:</strong><br />
 Assumptions about the weights (before learning) in terms of acceptable values and range are encoded into the prior distribution of the weights.
    <ul>
      <li>A <strong>Weak Prior</strong>:  has a high <em>entropy</em>, and thus, variance and shows that there is low confidence in the initial value of the weight.</li>
      <li>A <strong>Strong Prior</strong>: in turn has low entropy/variance, and shows a narrow range of values about which we are confident before learning begins.</li>
      <li>A <strong>Infinitely Strong Prior</strong>: demarkets certain values as forbidden completely, assigning them zero probability.<br />
 <br /></li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents5" id="bodyContents53">Convolutional Layer as a FC Layer:</strong><br />
 If we view the conv-layer as a FC-layer, the:
    <ul>
      <li><strong>Convolution</strong>: operation imposes an <em><strong>infinitely strong prior</strong></em> by making the following restrictions on the weights:
        <ul>
          <li>Adjacent units must have the same weight but shifted in space.</li>
          <li>Except for a small spatially connected region, all other weights must be zero.</li>
        </ul>
      </li>
      <li><strong>Pooling</strong>: operation imposes an <em><strong>infinitely strong prior</strong></em> by:
        <ul>
          <li>Requiring features to be <strong>Translation Invariant</strong>. <br />
 <br /></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents5" id="bodyContents54">Key Insights/Takeaways:</strong>
    <ul>
      <li>Convolution and pooling can cause underfitting if the priors imposed are not suitable for the task. When a task involves incorporating information from very distant locations in the input, then the prior imposed by convolution may be inappropriate.
        <blockquote>
          <p>As an example, consider this scenario. We may want to learn different features for different parts of an input. But the compulsion to used tied weights (enforced by standard convolution) on all parts of an image, forces us to either compromise or use more kernels (extract more features).</p>
        </blockquote>
      </li>
      <li>Convolutional models should only be compared with other convolutional models. This is because other models which are permutation invariant can learn even when input features are permuted (thus loosing spatial relationships). Such models need to learn these spatial relationships (which are hard-coded in CNNs).</li>
    </ul>
  </li>
</ol>

<hr />

<h2 id="content6">Variants of the Basic Convolution Function and Structured Outputs</h2>

<ol>
  <li><strong style="color: SteelBlue" class="bodyContents6" id="bodyContents61">Practical Considerations for Implementing the Convolution Function:</strong>
    <ul>
      <li>In general a convolution layer consists of application of <em><strong>several different kernels to the input.</strong></em> This allows the extraction of several different features at all locations in the input. This means that in each layer, a single kernel (filter) isn’t applied. Multiple kernels (filters), usually a power of 2, are used as different feature detectors.</li>
      <li>The <em>input</em> is generally not real-valued but instead <em><strong>vector valued</strong></em> (e.g. RGB values at each pixel or the feature values computed by the previous layer at each pixel position). Multi-channel convolutions are commutative only if number of output and input channels is the same.</li>
      <li><strong>Strided Convolutions</strong> are a means to do <em><strong>DownSampling</strong></em>; they are used to reduce computational cost, by calculating features at a <em><strong>coarser level</strong></em>. The effect of strided convolution is the same as that of a convolution followed by a downsampling stage. This can be used to reduce the representation size.
        <p>$$Z_{i, j, k}=c(\mathrm{K}, \mathrm{V}, s)_{i, j, k}=\sum_{l, m, n}\left[V_{l,(j-1) \times s+m,(k-1) \times s+n} K_{i, l, m, n}\right] \tag{9.8}$$</p>
      </li>
      <li><strong>Zero Padding</strong> is used to make output dimensions and kernel size independent (i.e. to control the output dimension regardless of the size of the kernel). There are three types:
        <ol>
          <li><strong>Valid</strong>: The output is computed only at places where the entire kernel lies inside the input. Essentially, <em>no zero padding</em> is performed. For a kernel of size \(k\) in any dimension, the input shape of \(m\) in the direction will become \(m-k+1\) in the output. This shrinkage restricts architecture depth.</li>
          <li><strong>Same</strong>: The input is zero padded such that the <em>spatial size of the input and output is <strong>same</strong></em>. Essentially, for a dimension where kernel size is \(k\), the input is padded by \(k-1\) zeros in that dimension. Since the number of output units connected to border pixels is less than that for center pixels, it may under-represent border pixels.</li>
          <li><strong>Full</strong>: The input is padded by enough zeros such that <em>each input pixel is connected to the same number of output units</em>.
            <blockquote>
              <p>The optimal amount of Zero-Padding usually lies between “valid” and “same” convolution.</p>
            </blockquote>
          </li>
        </ol>
      </li>
      <li><strong>Locally Connected Layers</strong>/<strong>Unshared Convolution</strong>: has the same connectivity graph as a convolution operation, but <em><strong>without parameter sharing</strong></em> (i.e. each output unit performs a linear operation on its neighbourhood but the parameters are not shared across output units.).<br />
  This allows models to capture local connectivity while allowing different features to be computed at different spatial locations; at the <em>expense</em> of having <em>a lot more parameters</em>.
        <p>$$Z_{i, j, k}=\sum_{l, m, n}\left[V_{l, j+m-1, k+n-1} w_{i, j, k, l, m, n}\right] \tag{9.9}$$</p>
        <blockquote>
          <p>They’re useful when we know that each feature should be a function of a small part of space, but there is no reason to think that the same feature should occur across all of space.<br />
  For example, if we want to tell if an image is a picture of a face, we only need to look for the mouth in the bottom half of the image.</p>
        </blockquote>
      </li>
      <li><strong>Tiled Convolution</strong>: offers a middle ground between Convolution and locally-connected layers. Rather than learning a separate set of weights at <em>every</em> spatial location, it learns/uses a set of kernels that are cycled through as we move through space.<br />
  This means that immediately neighboring locations will have different filters, as in a locally connected layer, but the memory requirements for storing the parameters will increase only by a factor of the size of this set of kernels, rather than by the size of the entire output feature map.
        <p>$$Z_{i, j, k}=\sum_{l, m, n} V_{l, j+m-1, k+n-1} K_{i, l, m, n, j \% t+1, k \% t+1} \tag{9.10}$$</p>
      </li>
      <li><strong>Max-Pooling, and Locally Connected Layers and Tiled Layers</strong>: When max pooling operation is applied to locally connected layer or tiled convolution, the model has the ability to become transformation invariant because adjacent filters have the freedom to learn a transformed version of the same feature.
        <blockquote>
          <p>This essentially similar to the property leveraged by pooling over channels rather than spatially.</p>
        </blockquote>
      </li>
      <li><strong>Different Connections</strong>: Besides locally-connected layers and tiled convolution, another extension can be to restrict the kernels to operate on certain input channels. One way to implement this is to connect the first m input channels to the first n output channels, the next m input channels to the next n output channels and so on. This method decreases the number of parameters in the model without decreasing the number of output units.</li>
      <li><strong>Other Operations</strong>: The following three operations—convolution, backprop from output to weights, and backprop from output to inputs—are sufficient to compute all the gradients needed to train any depth of feedforward convolutional network, as well as to train convolutional networks with reconstruction functions based on the transpose of convolution.
        <blockquote>
          <p>See Goodfellow (2010) for a full derivation of the equations in the fully general multidimensional, multiexample case.</p>
        </blockquote>
      </li>
      <li><strong>Bias</strong>:  Bias terms can be used in different ways in the convolution stage.
        <ul>
          <li>For locally connected layer and tiled convolution, we can use a bias per output unit and kernel respectively.</li>
          <li>In case of traditional convolution, a single bias term per output channel is used.</li>
          <li>If the <em>input size is fixed</em>, a bias per output unit may be used to <em>counter the effect of regional image statistics and smaller activations at the boundary due to zero padding</em>.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents6" id="bodyContents62">Structured Outputs:</strong><br />
 Convolutional networks can be trained to output high-dimensional structured output rather than just a classification score.<br />
 A good example is the task of <strong>image segmentation</strong> where each pixel needs to be associated with an object class.<br />
 Here the output is the same size (spatially) as the input. The model outputs a tensor \(S\)  where \(S_{i, j, k}\) is the probability that pixel \((j,k)\) belongs to class \(i\).
    <ul>
      <li><strong>Problem</strong>: One issue that often comes up is that the output plane can be smaller than the input plane.</li>
      <li><strong>Solutions</strong>:
        <ul>
          <li>To produce an output map as the same size as the input map, only same-padded convolutions can be stacked.</li>
          <li>Avoid Pooling Completely <em>(Jain et al. 2007)</em></li>
          <li>Emit a lower-Resolution grid of labels <em>(Pinheiro and Collobert, 2014, 2015)</em></li>
          <li><strong>Recurrent-Convolutional Models</strong>: The output of the first labelling stage can be refined successively by another convolutional model. If the models use tied parameters, this gives rise to a type of recursive model.</li>
          <li>Another model that has gained popularity for segmentation tasks (especially in the medical imaging community) is the <a href="https://arxiv.org/abs/1505.04597">U-Net</a>. The up-convolution mentioned is just a direct upsampling by repetition followed by a convolution with same padding.</li>
        </ul>
      </li>
    </ul>

    <p>The output can be further processed under the assumption that contiguous regions of pixels will tend to belong to the same label. Graphical models can describe this relationship. Alternately, <a href="https://www.robots.ox.ac.uk/~vgg/rg/papers/tompson2014.pdf">CNNs can learn to optimize the graphical models training objective</a>.</p>
  </li>
</ol>

<!-- ## Seven
{: #content7}

1. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents7 #bodyContents71}

2. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents7 #bodyContents72}

3. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents7 #bodyContents73}

4. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents7 #bodyContents74}

5. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents7 #bodyContents75}

6. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents7 #bodyContents76}

7. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents7 #bodyContents77}

8. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents7 #bodyContents78}

## Eight
{: #content8}

1. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents8 #bodyContents81}

2. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents8 #bodyContents82}

3. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents8 #bodyContents83}

4. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents8 #bodyContents84}

5. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents8 #bodyContents85}

6. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents8 #bodyContents86}

7. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents8 #bodyContents87}

8. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents8 #bodyContents88}
 -->

<hr />

<h2 id="contentx">Extra</h2>

<!-- 1. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents9 #bodyContents91}  -->

<ol>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents9" id="bodyContents92">Image Features:</strong></dt>
      <dd>are certain quantities that are calculated from the image to <em>better describe the information in the image</em>, and to <em>reduce the size of the input vectors</em>.</dd>
      <dd>
        <ul>
          <li>Examples:
            <ul>
              <li><strong>Color Histogram</strong>: Compute a (bucket-based) vector of colors with their respective amounts in the image.</li>
              <li><strong>Histogram of Oriented Gradients (HOG)</strong>: we count the occurrences of gradient orientation in localized portions of the image.</li>
              <li><strong>Bag of Words</strong>: a <em>bag of visual words</em> is a vector of occurrence counts of a vocabulary of local image features.
                <blockquote>
                  <p>The <strong>visual words</strong> can be extracted using a clustering algorithm; K-Means.</p>
                </blockquote>
              </li>
            </ul>
          </li>
        </ul>
      </dd>
    </dl>
  </li>
</ol>


      <footer class="site-footer">
    <!--   <span class="site-footer-owner"><a href="http://localhost:8889">Ahmad Badary</a> is maintained by <a href="http://ahmedbadary.ml">Ahmad Badary</a>.</span> -->
    
<!--  -->
    <div class="wrapper">
        <div class="footer-col-wrapper">
            <div class="footer-col footer-col-1">
            <span class="site-footer-owner"><a href="http://localhost:8889">Site</a> maintained by <a href="http://ahmedbadary.ml">Ahmad Badary</a>.</span>
    <span class="site-footer-credits">
        <p>
            &copy; 2017. All rights reserved.
        </p> 
    </span>
            </div>
            <div class="footer-col footer-col-2">
            <div><p>         </p></div>
            </div>
            <div class="footer-col footer-col-3">
                <ul class="social-media-list">
                    
                      <li>
                        <a href="https://github.com/AhmedBadary">
                          <i class="fa fa-github"></i> GitHub
                        </a>
                      </li>
                    
                    
                      <li>
                        <a href="https://linkedin.com/in/ahmad-badary-656098121/">
                          <i class="fa fa-linkedin"></i> LinkedIn
                        </a>
                      </li>
                    
                    
                      <li>
                        <a href="https://www.facebook.com/ahmed.thabet.94">
                          <i class="fa fa-facebook"></i> Facebook
                        </a>
                      </li>
                    
                </ul>
            </div>
        </div>
    </div>
<!--  -->
</footer>


    </section>

  </body>

<!-- Table of Content Script -->
<script type="text/javascript">
var bodyContents = $(".bodyContents1");
$("<ol>").addClass("TOC1ul").appendTo(".TOC1");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC1ul");
     });
// 
var bodyContents = $(".bodyContents2");
$("<ol>").addClass("TOC2ul").appendTo(".TOC2");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC2ul");
     });
// 
var bodyContents = $(".bodyContents3");
$("<ol>").addClass("TOC3ul").appendTo(".TOC3");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC3ul");
     });
//
var bodyContents = $(".bodyContents4");
$("<ol>").addClass("TOC4ul").appendTo(".TOC4");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC4ul");
     });
//
var bodyContents = $(".bodyContents5");
$("<ol>").addClass("TOC5ul").appendTo(".TOC5");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC5ul");
     });
//
var bodyContents = $(".bodyContents6");
$("<ol>").addClass("TOC6ul").appendTo(".TOC6");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC6ul");
     });
//
var bodyContents = $(".bodyContents7");
$("<ol>").addClass("TOC7ul").appendTo(".TOC7");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC7ul");
     });
//
var bodyContents = $(".bodyContents8");
$("<ol>").addClass("TOC8ul").appendTo(".TOC8");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC8ul");
     });
//
var bodyContents = $(".bodyContents9");
$("<ol>").addClass("TOC9ul").appendTo(".TOC9");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC9ul");
     });

</script>

<!-- VIDEO BUTTONS SCRIPT -->
<script type="text/javascript">
  function iframePopInject(event) {
    var $button = $(event.target);
    // console.log($button.parent().next());
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $figure = $("<div>").addClass("video_container");
        $iframe = $("<iframe>").appendTo($figure);
        $iframe.attr("src", $button.attr("src"));
        // $iframe.attr("frameborder", "0");
        $iframe.attr("allowfullscreen", "true");
        $iframe.css("padding", "4px 6px");
        $button.next().css("display", "block");
        $figure.appendTo($button.next());
        $button.text("Hide Video")
    } else {
        $button.attr('value', 'show');
        $button.next().html("");
        $button.text("Show Video")
    }
}
</script>

<!-- BUTTON TRY -->
<script type="text/javascript">
  function iframePopA(event) {
    event.preventDefault();
    var $a = $(event.target).parent();
    console.log($a);
    if ($a.attr('value') == 'show') {
        $a.attr('value', 'hide');
        $figure = $("<div>");
        $iframe = $("<iframe>").addClass("popup_website_container").appendTo($figure);
        $iframe.attr("src", $a.attr("href"));
        $iframe.attr("frameborder", "1");
        $iframe.attr("allowfullscreen", "true");
        $iframe.css("padding", "4px 6px");
        $a.next().css("display", "block");
        $figure.appendTo($a.next().next());
        // $a.text("Hide Content")
        $('html, body').animate({
            scrollTop: $a.offset().top
        }, 1000);
    } else {
        $a.attr('value', 'show');
        $a.next().next().html("");
        // $a.text("Show Content")
    }

    $a.next().css("display", "inline");
}
</script>


<!-- TEXT BUTTON SCRIPT - INJECT -->
<script type="text/javascript">
  function showTextPopInject(event) {
    var $button = $(event.target);
    var txt = $button.attr("input");
    console.log(txt);
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $p = $("<p>");
        $p.html(txt);
        $button.next().css("display", "block");
        $p.appendTo($button.next());
        $button.text("Hide Content")
    } else {
        $button.attr('value', 'show');
        $button.next().html("");
        $button.text("Show Content")
    }

}
</script>

<!-- TEXT BUTTON SCRIPT - HIDDEN / HIDE / SHOW / HIDE/SHOW -->
<script type="text/javascript">
  function showTextPopHide(event) {
    var $button = $(event.target);
    // var txt = $button.attr("input");
    var txt = $button.text();
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $button.next().removeAttr("hidden");
        $button.text(txt + " - Hide Content");
    } else {
        $button.attr('value', 'show');
        $button.next().attr("hidden", "");
        $button.text(txt.replace(" - Hide Content",""));
    }
}
</script>

<!-- TEXT BUTTON SCRIPT - HIDDEN / HIDE / SHOW / HIDE/SHOW -->
<script type="text/javascript">
  function showText_withParent_PopHide(event) {
    var $button = $(event.target);
    var $parent = $button.parent();
    var txt = $button.text();
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $parent.next().removeAttr("hidden");
        $button.text(txt + " - Hide Content");
    } else {
        $button.attr('value', 'show');
        $parent.next().attr("hidden", "");
        $button.text(txt.replace(" - Hide Content",""));
    }
}
</script>

<!-- Print / Printing / printme -->
<!-- <script type="text/javascript">
i = 0

for (var i = 1; i < 6; i++) {
    var bodyContents = $(".bodyContents" + i);
    $("<p>").addClass("TOC1ul")  .appendTo(".TOC1");
    bodyContents.each(function(index, element) {
        var paragraph = $(element);
        $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC1ul");
         });
} 
</script>
 -->
 
</html>

