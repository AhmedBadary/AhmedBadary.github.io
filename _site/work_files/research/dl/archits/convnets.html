<!DOCTYPE html>
<html lang="en-us">
  <head>
  <meta charset="UTF-8">
  <title>Ahmad Badary</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <link rel="stylesheet" href="/css/normalize.css">
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="/css/cayman.css">
  <link rel="stylesheet" href="/css/style.css">
  <link rel="shortcut icon" href="/main_files/favicon.ico" />
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="/css/customStyle.css">
  <title> » Ahmad Badary</title>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
</head>

  <body>
    <nav class="main-nav">
    <a href="https://ahmedbadary.github.io/" class="main-nav-logo">
        <img src="/main_files/logo.png">
    </a>
    <ul id="menu-main" class="main-nav-items">
        <li id="menu-item-1859" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-home menu-item-1859">
            <a href="/">Home</a>
        </li>
        <li id="menu-item-2869" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-2869">
            <a href="/work">Work</a>
        </li>
        <li id="menu-item-1892" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1892">
            <a href="/projects">Projects</a>
        </li>
        <li id="menu-item-1858" class="menu-item menu-item-type-post_type menu-item-object-page current_page_parent menu-item-1858">
            <a href="/blog">Blog</a>
        </li>
        <li id="menu-item-1862" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1862">
            <a href="/about">About</a>
        </li>
    </ul>
</nav>


<section class="page-header">
  <h1 class="project-name">CNNs <br /> Convolutional Neural Networks</h1>
  <h2 class="project-tagline"></h2>
  <a href="/#" class="btn">Home</a>
  <a href="/work" class="btn">Work-Space</a>
  <a href= /work_files/research/dl/cv.html class="btn">Previous</a>
</section>

<!-- <div>
  <ul class="posts">
    
      <li><span>02 Jan 2014</span> &raquo; <a href="/2014/01/02/introducing-Ahmad/">Introducing Ahmad</a></li>
    
  </ul>
</div> -->


    <section class="main-content">
      
      <div class="TOC">
  <h1 id="table-of-contents">Table of Contents</h1>

  <ul class="TOC1">
    <li><a href="#content1">Introduction</a></li>
  </ul>
  <ul class="TOC2">
    <li><a href="#content2">Architecture and Design</a></li>
  </ul>
  <ul class="TOC3">
    <li><a href="#content3">The Convolutional Layer</a></li>
  </ul>
  <ul class="TOC4">
    <li><a href="#content4">The Pooling Layer</a></li>
  </ul>

</div>

<hr />
<hr />

<p><a href="/work_files/research/dl/cnnx">CNNs in CV</a><br />
<a href="/work_files/research/dl/nlp/cnnsNnlp">CNNs in NLP</a><br />
<a href="/work_files/research/dl/arcts">CNNs Architectures</a></p>

<p><a href="https://medium.com/inveterate-learner/deep-learning-book-chapter-9-convolutional-networks-45e43bfc718d">Convnet Ch.9 Summary</a></p>

<h2 id="content1">Introduction</h2>

<ol>
  <li><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents11">CNNs:</strong> <br />
 In machine learning, a convolutional neural network (CNN, or ConvNet) is a class of deep, feed-forward artificial neural networks that has successfully been applied to analyzing visual imagery.<br />
 In general, it works on data that have <em>grid-like topology.</em>
    <blockquote>
      <p>E.g. Time-series data (1-d grid w/ samples at regular time intervals), image data (2-d grid of pixels).</p>
    </blockquote>

    <p>Convolutional networks are simply neural networks that use convolution in place of general matrix multiplication in at least one of their layers.</p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents12">The Big Idea:</strong> <br />
 CNNs use a variation of multilayer Perceptrons designed to require minimal preprocessing. In particular, they use the <a href="#bodyContents31">Convolution Operation</a>. <br />
 The Convolution leverage <em>three important ideas</em> that can help improve a machine learning system:
    <ol>
      <li><strong>Sparse Interactions/Connectivity/Weights:</strong><br />
 Unlike FNNs, where every input unit is connected to every output unit, CNNs have sparse interactions. This is accomplished by making the kernel smaller than the input.<br />
 <strong>Benefits:</strong>
        <ul>
          <li>This means that we need to <em>store fewer parameters</em>, which both,
            <ul>
              <li><em>Reduces the memory requirements</em> of the model and</li>
              <li><em>Improves</em> its <em>statistical efficiency</em></li>
            </ul>
          </li>
          <li>Also, Computing the output requires fewer operations</li>
          <li>In deep CNNs, the units in the deeper layers interact indirectly with large subsets of the input which allows modelling of complex interactions through sparse connections.</li>
        </ul>

        <blockquote>
          <p>These improvements in efficiency are usually quite large.<br />
 If there are <script type="math/tex">m</script> inputs and <script type="math/tex">n</script> outputs, then matrix multiplication requires <script type="math/tex">m \times n</script> parameters, and the algorithms used in practice have <script type="math/tex">\mathcal{O}(m \times n)</script> runtime (per example). If we limit the number of connections each output may have to <script type="math/tex">k</script>, then the sparsely connected approach requires only <script type="math/tex">k \times n</script> parameters and <script type="math/tex">\mathcal{O}(k \times n)</script> runtime.</p>
        </blockquote>

        <p><button class="showText" value="show" onclick="showTextPopHide(event);">Figure: Sparse Connectivity from Below</button>
 <img src="/main_files/dl/cnn/14.png" alt="img" width="70%" hidden="" /><br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">Figure: Sparse Connectivity from Above</button>
 <img src="/main_files/dl/cnn/15.png" alt="img" width="70%" hidden="" /></p>
      </li>
      <li>
        <p><strong>Parameter Sharing:</strong> <br />
 refers to using the same parameter for more than one function in a model.</p>

        <p id="lst-p"><strong style="color: red">Benefits:</strong></p>
        <ul>
          <li>This means that rather than learning a separate set of parameters for every location, we <em>learn only one set of parameters</em>.
            <ul>
              <li>This does not affect the runtime of forward propagation—it is still <script type="math/tex">\mathcal{O}(k \times n)</script></li>
              <li>But it does further reduce the storage requirements of the model to <script type="math/tex">k</script> parameters (<script type="math/tex">k</script> is usually several orders of magnitude smaller than <script type="math/tex">m</script>)</li>
            </ul>
          </li>
        </ul>

        <p>Convolution is thus dramatically more efficient than dense matrix multiplication in terms of the memory requirements and statistical efficiency.<br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">Figure: Parameter Sharing</button>
 <img src="/main_files/dl/cnn/16.png" alt="img" width="70%" hidden="" /></p>
      </li>
      <li><strong>Equivariant Representations:</strong><br />
 For convolutions, the particular form of parameter sharing causes the layer to have a property called <strong>equivariance to translation</strong>.
        <blockquote>
          <p>A function is <strong>equivariant</strong> means that if the input changes, the output changes in the same way.<br />
     Specifically, a function <script type="math/tex">f(x)</script> is equivariant to a function <script type="math/tex">g</script> if <script type="math/tex">f(g(x)) = g(f(x))</script>.</p>
        </blockquote>

        <p>Thus, if we move the object in the input, its representation will move the same amount in the output.</p>

        <p id="lst-p"><strong style="color: red">Benefits:</strong></p>
        <ul>
          <li>It is most useful when we know that some function of a small number of neighboring pixels is useful when applied to multiple input locations (e.g. edge detection)</li>
          <li>Shifting the position of an object in the input doesn’t confuse the NN</li>
          <li>Robustness against translated inputs/images</li>
        </ul>

        <p>Note: Convolution is <strong>not</strong> naturally equivariant to some other transformations, such as <em>changes in the scale</em> or <em>rotation</em> of an image.</p>
      </li>
    </ol>

    <p>Finally, the convolution provides a means for working with <strong>inputs of variable sizes</strong> (i.e. data that cannot be processed by neural networks defined by matrix multiplication with a fixed-shape matrix).</p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents13">Inspiration Model:</strong><br />
 Convolutional networks were inspired by biological processes in which the connectivity pattern between neurons is inspired by the organization of the animal visual cortex.<br />
 Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field.</li>
</ol>

<hr />

<h2 id="content2">Architecture and Design</h2>

<ol>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents20">Design:</strong><br />
 A CNN consists of an input and an output layer, as well as multiple hidden layers.<br />
 The hidden layers of a CNN typically consist of convolutional layers, pooling layers, fully connected layers and normalization layers.</p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents21">Volumes of Neurons:</strong> <br />
 Unlike neurons in traditional Feed-Forward networks, the layers of a ConvNet have neurons arranged in 3-dimensions: <strong>width, height, depth</strong>.
    <blockquote>
      <p>Note: <strong>Depth</strong> here refers to the third dimension of an activation volume, not to the depth of a full Neural Network, which can refer to the total number of layers in a network.</p>
    </blockquote>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents22">Connectivity:</strong> <br />
 The neurons in a layer will only be connected to a small region of the layer before it, instead of all of the neurons in a fully-connected manner.</p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents23">Functionality:</strong> <br />
 A ConvNet is made up of Layers.<br />
 Every Layer has a simple API: It transforms an input 3D volume to an output 3D volume with some differentiable function that may or may not have parameters.</p>

    <p><img src="/main_files/dl/cnn/1.png" alt="img" width="100%" /></p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents24">Layers:</strong> <br />
 We use three main types of layers to build ConvNet architectures:
    <ul>
      <li>Convolutional Layer:
        <ul>
          <li>Convolution (Linear Transformation)</li>
          <li>Activation (Non-Linear Transformation; e.g. ReLU)
            <blockquote>
              <p>Known as <strong>Detector Stage</strong></p>
            </blockquote>
          </li>
        </ul>
      </li>
      <li>Pooling Layer</li>
      <li>Fully-Connected Layer</li>
    </ul>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents241">Process:</strong></dt>
      <dd>ConvNets transform the original image layer by layer from the original pixel values to the final class scores.</dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents25">Example Architecture (CIFAR-10):</strong></dt>
      <dd>Model: [INPUT - CONV - RELU - POOL - FC]</dd>
      <dd>
        <ul>
          <li><strong>INPUT:</strong> [32x32x3] will hold the raw pixel values of the image, in this case an image of width 32, height 32, and with three color channels R,G,B.</li>
          <li><strong>CONV-Layer</strong> will compute the output of neurons that are connected to local regions in the input, each computing a dot product between their weights and a small region they are connected to in the input volume.  <br />
This may result in volume such as [<script type="math/tex">32\times32\times12</script>] if we decided to use 12 filters.</li>
          <li><strong>RELU-Layer:</strong>  will apply an element-wise activation function, thresholding at zero. This leaves the size of the volume unchanged ([<script type="math/tex">32\times32\times12</script>]).</li>
          <li><strong>POOL-Layer:</strong> will perform a down-sampling operation along the spatial dimensions (width, height), resulting in volume such as [<script type="math/tex">16\times16\times12</script>].</li>
          <li><strong>Fully-Connected:</strong> will compute the class scores, resulting in volume of size [<script type="math/tex">1\times1\times10</script>], where each of the 10 numbers correspond to a class score, such as among the 10 categories of CIFAR-10.<br />
As with ordinary Neural Networks and as the name implies, each neuron in this layer will be connected to all the numbers in the previous volume.</li>
        </ul>
      </dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents26">Fixed Functions VS Hyper-Parameters:</strong></dt>
      <dd>Some layers contain parameters and other don’t.</dd>
      <dd>
        <ul>
          <li><strong>CONV/FC layers</strong> perform transformations that are a function of not only the activations in the input volume, but also of the parameters (the weights and biases of the neurons).</li>
        </ul>
      </dd>
      <dd>
        <ul>
          <li><strong>RELU/POOL</strong> layers will implement a fixed function.</li>
        </ul>
      </dd>
      <dd>
        <blockquote>
          <p>The parameters in the CONV/FC layers will be trained with gradient descent so that the class scores that the ConvNet computes are consistent with the labels in the training set for each image.</p>
        </blockquote>
      </dd>
    </dl>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents27"><a href="http://cs231n.github.io/convolutional-networks/">Summary</a>:</strong>
    <ul>
      <li>A ConvNet architecture is in the simplest case a list of Layers that transform the image volume into an output volume (e.g. holding the class scores)</li>
      <li>There are a few distinct types of Layers (e.g. CONV/FC/RELU/POOL are by far the most popular)</li>
      <li>Each Layer accepts an input 3D volume and transforms it to an output 3D volume through a differentiable function</li>
      <li>Each Layer may or may not have parameters (e.g. CONV/FC do, RELU/POOL don’t)</li>
      <li>Each Layer may or may not have additional hyperparameters (e.g. CONV/FC/POOL do, RELU doesn’t)
        <blockquote>
          <p><a href="http://cs231n.github.io/convolutional-networks/">Click this for Credits</a></p>
        </blockquote>
      </li>
    </ul>

    <p><img src="/main_files/dl/cnn/2.png" alt="img" width="100%" /></p>
  </li>
</ol>

<hr />

<h2 id="content3">The Convolutional Layer</h2>

<ol>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents31">Convolutions:</strong> <br />
 In its most general form, the convolution is a <strong>Linear Operation</strong> on two functions of real-valued arguments.</p>

    <p>Mathematically, a <strong>Convolution</strong> is a mathematical operation on two functions (<script type="math/tex">f</script> and <script type="math/tex">g</script>) to produce a third function, that is typically viewed as a modified version of one of the original functions, giving the integral of the point-wise multiplication of the two functions as a function of the amount that one of the original functions is translated.<br />
 The convolution could be thought of as a <strong>weighting function</strong> (e.g. for taking the weighted average of a series of numbers/function-outputs).</p>

    <p>The convolution of the <strong>continuous</strong> functions <script type="math/tex">f</script> and <script type="math/tex">g</script>:</p>
    <p>$${\displaystyle {\begin{aligned}(f * g)(t)&amp;\,{\stackrel {\mathrm {def} }{=}}\ \int _{-\infty }^{\infty }f(\tau )g(t-\tau )\,d\tau \\&amp;=\int_{-\infty }^{\infty }f(t-\tau )g(\tau )\,d\tau .\end{aligned}}}$$</p>

    <p>The convolution of the <strong>discreet</strong> functions f and g:</p>
    <p>$${\displaystyle {\begin{aligned}(f * g)[n]&amp;=\sum_{m=-\infty }^{\infty }f[m]g[n-m]\\&amp;=\sum_{m=-\infty }^{\infty }f[n-m]g[m].\end{aligned}}} (commutativity)$$</p>
    <p>In this notation, we refer to:</p>
    <ul>
      <li>The function <script type="math/tex">f</script> as the <strong>Input</strong></li>
      <li>The function <script type="math/tex">g</script> as the <strong>Kernel/Filter</strong></li>
      <li>The output of the convolution as the <strong>Feature Map</strong></li>
    </ul>

    <p><strong>Commutativity:</strong><br />
 Can be achieved by flipping the kernel with respect to the input; in the sense that as increases, the index into the <script type="math/tex">m</script> input increases, but the index into the kernel decreases.<br />
 While the commutative property is useful for writing proofs, it is not usually an important property of a neural network implementation.<br />
 Moreover, in a CNN, the convolution is used simultaneously with other functions, and the combination of these functions <strong>does not commute</strong> regardless of whether the convolution operation flips its kernel or not.<br />
 Because convolutional networks usually use multichannel convolution, the linear operations they are based on are not guaranteed to be commutative, even if kernel flipping is used. These multichannel operations are only commutative if each operation has the same number of output channels as input channels.</p>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents32">Cross-Correlation:</strong></dt>
      <dd>Cross-Correlation is a measure of similarity of two series as a function of the displacement of one relative to the other.</dd>
      <dd>The <strong>continuous</strong> cross-correlation on continuous functions f and g:</dd>
      <dd>
        <script type="math/tex; mode=display">(f\star g)(\tau )\ {\stackrel {\mathrm {def} }{=}}\int _{-\infty }^{\infty }f^{*}(t)\ g(t+\tau )\,dt,</script>
      </dd>
      <dd>The <strong>discrete</strong> cross-correlation on discreet functions f and g:</dd>
      <dd>
        <script type="math/tex; mode=display">(f\star g)[n]\ {\stackrel {\mathrm {def} }{=}}\sum _{m=-\infty }^{\infty }f^{*}[m]\ g[m+n].</script>
      </dd>
    </dl>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents33">Convolutions and Cross-Correlation:</strong>
    <ul>
      <li>Convolution is similar to cross-correlation.</li>
      <li><em>For discrete real valued signals</em>, they differ only in a time reversal in one of the signals.</li>
      <li><em>For continuous signals</em>, the cross-correlation operator is the <strong>adjoint operator</strong> of the convolution operator.</li>
    </ul>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents34">CNNs, Convolutions, and Cross-Correlation:</strong> <br />
 The term Convolution in the name “Convolution Neural Network” is unfortunately a <strong>misnomer</strong>.<br />
 CNNs actually <strong>use Cross-Correlation</strong> instead as their similarity operator.<br />
 The term ‘convolution’ has stuck in the name by convention.</p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents315">Convolution in DL:</strong> <br />
The Convolution operation:
    <p>$$s(t)=(x * w)(t)=\sum_{a=-\infty}^{\infty} x(a) w(t-a)$$</p>
    <p>we usually assume that these functions are zero everywhere but in the finite set of points for which we store the values.</p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents316">Convolution Over Two Axis:</strong> <br />
If we use a two-dimensional image <script type="math/tex">I</script> as our input, we probably also want to use a two-dimensional kernel <script type="math/tex">K</script>:
    <p>$$S(i, j)=(I * K)(i, j)=\sum_{m} \sum_{n} I(m, n) K(i-m, j-n)$$</p>

    <p>In practice we use the following formula instead (commutativity):</p>
    <p>$$S(i, j)=(K * I)(i, j)=\sum_{m} \sum_{n} I(i-m, j-n) K(m, n)$$</p>
    <p>Usually the latter formula is more straightforward to implement in a machine learning library, because there is less variation in the range of valid values of <script type="math/tex">m</script> and <script type="math/tex">n</script>.</p>

    <p>The Cross-Correlation is usually implemented by ML-libs:</p>
    <p>$$S(i, j)=(K * I)(i, j)=\sum_{m} \sum_{n} I(i+m, j+n) K(m, n)$$</p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents317">The Mathematics of the Convolution Operation:</strong>
    <ul>
      <li>The operation can be broken into matrix multiplications using the Toeplitz matrix representation for 1D and block-circulant matrix for 2D convolution:
        <ul>
          <li><strong>Discrete convolution</strong> can be viewed as <strong>multiplication by a matrix</strong>, but the matrix has several entries constrained to be equal to other entries.
            <blockquote>
              <p>For example, for <strong>univariate discrete convolution</strong>, each row of the matrix is constrained to be equal to the row above shifted by one element. This is known as a <em><strong>Toeplitz matrix</strong></em>.<br />
  A <strong>Toeplitz matrix</strong> has the property that values along all diagonals are constant.</p>
            </blockquote>

            <p><!-- * <button>Figure: Toeplitz Matrix</button>{: .showText value="show" onclick="showTextPopHide(event);"}  
   -->
  <img src="/main_files/dl/cnn/20.png" alt="img" width="80%" /></p>
          </li>
          <li>In <strong>two dimensions</strong>, a <strong>doubly block circulant matrix</strong> corresponds to convolution.
            <blockquote>
              <p>A matrix which is circulant with respect to its sub-matrices is called a <strong>block circulant matrix</strong>. If each of the submatrices is itself circulant, the matrix is called <strong>doubly block-circulant matrix</strong>.<br />
      <!-- * <button>Figure: Block-Circulant Matrix</button>{: .showText value="show" onclick="showTextPopHide(event);"}   -->
      <img src="/main_files/dl/cnn/21.png" alt="img" width="60%" /></p>
            </blockquote>
          </li>
        </ul>
      </li>
      <li>Convolution usually corresponds to a <strong>very sparse matrix</strong> (a matrix whose entries are mostly equal to zero).<br />
  This is because the kernel is usually much smaller than the input image.</li>
      <li>Any neural network algorithm that works with matrix multiplication and does not depend on specific properties of the matrix structure should work with convolution, without requiring any further changes to the neural network.</li>
      <li>Typical convolutional neural networks do make use of further specializations in order to deal with large inputs efficiently, but these are not strictly necessary from a theoretical perspective.</li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents35">The Convolution operation in a CONV Layer:</strong>
    <ul>
      <li>The CONV layer’s <strong>parameters</strong> consist of <strong>a set of learnable filters</strong>.
        <ul>
          <li>Every filter is small spatially (along width and height), but extends through the full depth of the input volume.
            <blockquote>
              <p>For example, a typical filter on a first layer of a ConvNet might have size 5x5x3 (i.e. 5 pixels width and height, and 3 because images have depth 3, the color channels).</p>
            </blockquote>
          </li>
        </ul>
      </li>
      <li>In the <strong>forward pass</strong>, we slide (convolve) each filter across the width and height of the input volume and compute dot products between the entries of the filter and the input at any position.
        <ul>
          <li>As we slide the filter over the width and height of the input volume we will produce a 2-dimensional activation map that gives the responses of that filter at every spatial position.
            <blockquote>
              <p>Intuitively, the network will learn filters that activate when they see some type of visual feature such as an edge of some orientation or a blotch of some color on the first layer, or eventually entire honeycomb or wheel-like patterns on higher layers of the network.</p>
            </blockquote>
          </li>
          <li>Now, we will have an entire set of filters in each CONV layer (e.g. 12 filters), and each of them will produce a separate 2-dimensional activation map.</li>
        </ul>
      </li>
      <li>We will <strong>stack</strong> these activation maps along the depth dimension and produce the output volume.</li>
    </ul>

    <p style="color: red">As a result, the network learns filters that activate when it detects some specific type of feature at some spatial position in the input. </p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents26">The Brain Perspective:</strong><br />
 Every entry in the 3D output volume can also be interpreted as an output of a neuron that looks at only a small region in the input and shares parameters with all neurons to the left and right spatially.</p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents27">Local Connectivity:</strong>
    <ul>
      <li>Convolutional networks exploit spatially local correlation by enforcing a local connectivity pattern between neurons of adjacent layers:
        <ul>
          <li>Each neuron is connected to only a small region of the input volume.</li>
        </ul>
      </li>
      <li>The <strong>Receptive Field</strong> of the neuron defines the extent of this connectivity as a hyperparameter.
        <blockquote>
          <p>For example, suppose the input volume has size <script type="math/tex">[32\times32\times3]</script> and the receptive field (or the filter size) is <script type="math/tex">5\times5</script>, then each neuron in the Conv Layer will have weights to a <script type="math/tex">[5\times5\times3]</script> region in the input volume, for a total of <script type="math/tex">5*5*3 = 75</script> weights (and <script type="math/tex">+1</script> bias parameter).</p>
        </blockquote>
      </li>
    </ul>

    <p style="color: red">Such an architecture ensures that the learnt filters produce the strongest response to a spatially local input pattern.</p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents28">Spatial Arrangement:</strong><br />
 There are <strong>three</strong> hyperparameters control the size of the output volume:
    <ol>
      <li><strong>The Depth</strong> of the output volume is a hyperparameter that corresponds to the number of filters we would like to use (each learning to look for something different in the input).</li>
      <li><strong>The Stride</strong> controls how depth columns around the spatial dimensions (width and height) are allocated.
        <blockquote>
          <p>e.g. When the stride is 1 then we move the filters one pixel at a time.</p>
        </blockquote>

        <ul>
          <li>The <strong>Smaller</strong> the stride, the <strong>more overlapping regions</strong> exist and the <strong>bigger the volume</strong>.</li>
          <li>The <strong>bigger</strong> the stride, the <strong>less overlapping regions</strong> exist and the         <strong>smaller the volume</strong>.</li>
        </ul>
      </li>
      <li>The <strong>Padding</strong> is a hyperparameter whereby we pad the input the input volume with zeros around the border. <br />
 This allows to <em>control the spatial size</em> of <em>the output</em> volumes.</li>
    </ol>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents29">The Spatial Size of the Output Volume:</strong><br />
 We compute the spatial size of the output volume as a function of:
    <ul>
      <li><strong><script type="math/tex">W</script></strong>: The input volume size.</li>
      <li><strong><script type="math/tex">F</script></strong>: <script type="math/tex">\:\:</script>The receptive field size of the Conv Layer neurons.</li>
      <li><strong><script type="math/tex">S</script></strong>: The stride with which they are applied.</li>
      <li><strong><script type="math/tex">P</script></strong>: The amount of zero padding used on the border.<br />
 Thus, the <strong>Total Size of the Output</strong>:</li>
    </ul>
    <p>$$\dfrac{W−F+2P}{S} + 1$$</p>

    <p><strong>Potential Issue</strong>: If this number is not an integer, then the strides are set incorrectly and the neurons cannot be tiled to fit across the input volume in a symmetric way.</p>
    <ul>
      <li><strong>Fix</strong>: In general, setting zero padding to be <script type="math/tex">{\displaystyle P = \dfrac{K-1}{2}}</script> when the stride is <script type="math/tex">{\displaystyle S = 1}</script> ensures that the input volume and output volume will have the same size spatially.</li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents210">Calculating the Number of Parameters:</strong> <br />
Given:
    <ul>
      <li><strong>Input Volume</strong>:  <script type="math/tex">32\times32\times3</script></li>
      <li><strong>Filters</strong>:  <script type="math/tex">10 5\times5</script></li>
      <li><strong>Stride</strong>:  <script type="math/tex">1</script></li>
      <li><strong>Pad</strong>:  <script type="math/tex">2</script></li>
    </ul>

    <p>The number of parameters equals the number of parameters in each filter <script type="math/tex">= 5*5*3 + 1 = 76</script> (+1 for <strong>bias</strong>) times the number of filters <script type="math/tex">76 * 10 = 760</script>.</p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents311">The Convolution Layer:</strong> <br />
<img src="/main_files/dl/cnn/3.png" alt="img" width="70%" /><br />
<img src="/main_files/dl/cnn/8.png" alt="img" width="70%" /></p>

    <p><strong>The Conv Layer and the Brain:</strong><br />
<img src="/main_files/dl/cnn/10.png" alt="img" width="70%" /><br />
<img src="/main_files/dl/cnn/11.png" alt="img" width="70%" /><br />
<img src="/main_files/dl/cnn/12.png" alt="img" width="70%" /></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents312">From FC-layers to Conv-layers:</strong> <br />
<img src="/main_files/dl/cnn/4.png" alt="img" width="70%" /><br />
<img src="/main_files/dl/cnn/5.png" alt="img" width="70%" /><br />
<img src="/main_files/dl/cnn/6.png" alt="img" width="70%" /></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents313"><script type="math/tex">1\times1</script> Convolutions:</strong> <br />
<img src="/main_files/dl/cnn/9.png" alt="img" width="70%" /></p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents30">Notes:</strong>
    <ul>
      <li><strong>Summary</strong>:
        <ul>
          <li>ConvNets stack CONV,POOL,FC layers</li>
          <li>Trend towards smaller filters and deeper architectures</li>
          <li>Trend towards getting rid of POOL/FC layers (just CONV)</li>
          <li>Typical architectures look like [(CONV-RELU) * N-POOL?] * M-(FC-RELU) * K, SOFTMAX<br />
  where <script type="math/tex">N</script> is usually up to ~5, <script type="math/tex">M</script> is large, <script type="math/tex">% <![CDATA[
0 <= K <= 2 %]]></script>.<br />
  But recent advances such as ResNet/GoogLeNet challenge this paradigm</li>
        </ul>
      </li>
      <li><strong>Effect of Different Biases</strong>:<br />
  Separating the biases may slightly reduce the statistical efficiency of the model, but it allows the model to correct for differences in the image statistics at different locations. For example, when using implicit zero padding, detector units at the edge of the image receive less total input and may need larger biases.</li>
      <li>In the kinds of architectures typically used for classification of a single object in an image, the greatest reduction in the spatial dimensions of the network comes from using pooling layers with large stride.</li>
    </ul>
  </li>
</ol>

<hr />

<h2 id="content4">The Pooling Layer</h2>

<ol>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents4" id="bodyContents41">The Pooling Operation/Function:</strong> <br />
 The pooling function calculates a <strong>summary statistic</strong> of the nearby pixels at the point of operation.<br />
 Some common statistics are <em>max, mean, weighted average</em> and <em><script type="math/tex">L^2</script> norm</em> of a surrounding rectangular window.</p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents4" id="bodyContents42">The Key Ideas/Properties:</strong> <br />
 In all cases, pooling helps to make the representation approximately <strong>invariant to small translations</strong> of the input.
    <blockquote>
      <p>Invariance to translation means that if we translate the input by a small amount, the values of most of the pooled outputs do not change.<br />
 Invariance to local translation can be a useful property if we care more about whether some feature is present than exactly where it is.</p>
    </blockquote>

    <p><button class="showText" value="show" onclick="showTextPopHide(event);">Figure: MaxPooling</button>
 <img src="/main_files/dl/cnn/17.png" alt="img" width="70%" hidden="" /></p>

    <p><strong>(Learned) Invariance to other transformations:</strong><br />
 Pooling over spatial regions produces invariance to translation, but if we <em>pool over the outputs of separately parametrized convolutions</em>, the features can learn which transformations to become invariant to.<br />
 This property has been used in <strong>Maxout networks</strong>.<br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">Figure: Examples of Learned Invariance</button>
 <img src="/main_files/dl/cnn/18.png" alt="img" width="70%" hidden="" /></p>

    <p>For many tasks, pooling is essential for handling inputs of varying size.</p>
    <blockquote>
      <p>This is usually accomplished by varying the size of an offset between pooling regions so that the classification layer always receives the same number of summary statistics regardless of the input size. For example, the final pooling layer of the network may be defined to output four sets of summary statistics, one for each quadrant of an image, regardless of the image size.</p>
    </blockquote>

    <p>One can use fewer pooling units than detector units, since they provide a summary; thus, by reporting summary statistics for pooling regions spaced <script type="math/tex">k</script> pixels apart rather than <script type="math/tex">1</script> pixel apart, we can improve the computational efficiency of the network because the next layer has roughly <script type="math/tex">k</script> times fewer inputs to process.<br />
 This reduction in the input size can also result in improved statistical efficiency and reduced memory requirements for storing the parameters.<br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">Figure: Downsampling</button>
 <img src="/main_files/dl/cnn/19.png" alt="img" width="70%" hidden="" /></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents4" id="bodyContents43">Theoretical Guidelines for choosing the pooling function:</strong> <br />
 <a href="http://www.di.ens.fr/willow/pdfs/icml2010b.pdf">Link</a></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents4" id="bodyContents44">Variations:</strong> <br />
 <strong>Dynamical Pooling:</strong><br />
 It is also possible to dynamically pool features together, for example, by running a clustering algorithm on the locations of interesting features (Boureau et al., 2011) <a href="http://yann.lecun.com/exdb/publis/pdf/boureau-iccv-11.pdf">link</a>. This approach yields a different set of pooling regions for each image.</p>

    <p><strong>Learned Pooling:</strong><br />
 Another approach is to learn a single pooling structure that is then applied to all images (Jia et al., 2012).</p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents4" id="bodyContents45">Pooling and Top-Down Architectures:</strong> <br />
 Pooling can complicate some kinds of neural network architectures that use top-down information, such as Boltzmann machines and autoencoders.</p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents4" id="bodyContents46">The Pooling Layer (summary):</strong> <br />
 <img src="/main_files/dl/cnn/13.png" alt="img" width="100%" /></li>
</ol>

<!-- 7. **The Pooling Layer (summary):**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents47}   
    ![img](/main_files/dl/cnn/13.png){: width="100%"}   -->

<!-- 7. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents47}  -->

<p><strong style="color: SteelBlue" class="bodyContents4" id="bodyContents48">Notes:</strong></p>
<ul>
  <li><strong>Pooling Layer</strong>:
    <ul>
      <li>Makes the representations smaller and more manageable</li>
      <li>Operates over each activation map independently (i.e. preserves depth)</li>
    </ul>
  </li>
  <li>You can use the stride instead of the pooling to downsample</li>
</ul>

<hr />

<h2 id="content5">Convolution and Pooling as an Infinitely Strong Prior</h2>

<ol>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents5" id="bodyContents51">A Prior Probability Distribution:</strong><br />
 This is a probability distribution over the parameters of a model that encodes our beliefs about what models are reasonable, before we have seen any data.</p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents5" id="bodyContents52">What is a weight prior?:</strong><br />
 Assumptions about the weights (before learning) in terms of acceptable values and range are encoded into the prior distribution of the weights.
    <ul>
      <li>A <strong>Weak Prior</strong>:  has a high <em>entropy</em>, and thus, variance and shows that there is low confidence in the initial value of the weight.</li>
      <li>A <strong>Strong Prior</strong>: in turn has low entropy/variance, and shows a narrow range of values about which we are confident before learning begins.</li>
      <li>A <strong>Infinitely Strong Prior</strong>: demarkets certain values as forbidden completely, assigning them zero probability.</li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents5" id="bodyContents53">Convolutional Layer as a FC Layer:</strong><br />
 If we view the conv-layer as a FC-layer, the:
    <ul>
      <li><strong>Convolution</strong>: operation imposes an <em><strong>infinitely strong prior</strong></em> by making the following restrictions on the weights:
        <ul>
          <li>Adjacent units must have the same weight but shifted in space.</li>
          <li>Except for a small spatially connected region, all other weights must be zero.</li>
        </ul>
      </li>
      <li><strong>Pooling</strong>: operation imposes an <em><strong>infinitely strong prior</strong></em> by:
        <ul>
          <li>Requiring features to be <strong>Translation Invariant</strong>.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents5" id="bodyContents54">Key Insights/Takeaways:</strong>
    <ul>
      <li>Convolution and pooling can cause underfitting if the priors imposed are not suitable for the task. When a task involves incorporating information from very distant locations in the input, then the prior imposed by convolution may be inappropriate.
        <blockquote>
          <p>As an example, consider this scenario. We may want to learn different features for different parts of an input. But the compulsion to used tied weights (enforced by standard convolution) on all parts of an image, forces us to either compromise or use more kernels (extract more features).</p>
        </blockquote>
      </li>
      <li>Convolutional models should only be compared with other convolutional models. This is because other models which are permutation invariant can learn even when input features are permuted (thus loosing spatial relationships). Such models need to learn these spatial relationships (which are hard-coded in CNNs).</li>
    </ul>
  </li>
</ol>

<hr />

<h2 id="content6">Variants of the Basic Convolution Function and Structured Outputs</h2>

<ol>
  <li><strong style="color: SteelBlue" class="bodyContents6" id="bodyContents61">Practical Considerations for Implementing the Convolution Function:</strong>
    <ul>
      <li>In general a convolution layer consists of application of <em><strong>several different kernels to the input.</strong></em> This allows the extraction of several different features at all locations in the input. This means that in each layer, a single kernel (filter) isn’t applied. Multiple kernels (filters), usually a power of 2, are used as different feature detectors.</li>
      <li>The <em>input</em> is generally not real-valued but instead <em><strong>vector valued</strong></em> (e.g. RGB values at each pixel or the feature values computed by the previous layer at each pixel position). Multi-channel convolutions are commutative only if number of output and input channels is the same.</li>
      <li><strong>Strided Convolutions</strong> are a means to do <em><strong>DownSampling</strong></em>; they are used to reduce computational cost, by calculating features at a <em><strong>coarser level</strong></em>. The effect of strided convolution is the same as that of a convolution followed by a downsampling stage. This can be used to reduce the representation size.
        <p>$$Z_{i, j, k}=c(\mathrm{K}, \mathrm{V}, s)_{i, j, k}=\sum_{l, m, n}\left[V_{l,(j-1) \times s+m,(k-1) \times s+n} K_{i, l, m, n}\right] \tag{9.8}$$</p>
      </li>
      <li><strong>Zero Padding</strong> is used to make output dimensions and kernel size independent (i.e. to control the output dimension regardless of the size of the kernel). There are three types:
        <ol>
          <li><strong>Valid</strong>: The output is computed only at places where the entire kernel lies inside the input. Essentially, <em>no zero padding</em> is performed. For a kernel of size <script type="math/tex">k</script> in any dimension, the input shape of <script type="math/tex">m</script> in the direction will become <script type="math/tex">m-k+1</script> in the output. This shrinkage restricts architecture depth.</li>
          <li><strong>Same</strong>: The input is zero padded such that the <em>spatial size of the input and output is <strong>same</strong></em>. Essentially, for a dimension where kernel size is <script type="math/tex">k</script>, the input is padded by <script type="math/tex">k-1</script> zeros in that dimension. Since the number of output units connected to border pixels is less than that for center pixels, it may under-represent border pixels.</li>
          <li><strong>Full</strong>: The input is padded by enough zeros such that <em>each input pixel is connected to the same number of output units</em>.
            <blockquote>
              <p>The optimal amount of Zero-Padding usually lies between “valid” and “same” convolution.</p>
            </blockquote>
          </li>
        </ol>
      </li>
      <li><strong>Locally Connected Layers</strong>/<strong>Unshared Convolution</strong>: has the same connectivity graph as a convolution operation, but <em><strong>without parameter sharing</strong></em> (i.e. each output unit performs a linear operation on its neighbourhood but the parameters are not shared across output units.).<br />
  This allows models to capture local connectivity while allowing different features to be computed at different spatial locations; at the <em>expense</em> of having <em>a lot more parameters</em>.
        <p>$$Z_{i, j, k}=\sum_{l, m, n}\left[V_{l, j+m-1, k+n-1} w_{i, j, k, l, m, n}\right] \tag{9.9}$$</p>
        <blockquote>
          <p>They’re useful when we know that each feature should be a function of a small part of space, but there is no reason to think that the same feature should occur across all of space.<br />
  For example, if we want to tell if an image is a picture of a face, we only need to look for the mouth in the bottom half of the image.</p>
        </blockquote>
      </li>
      <li><strong>Tiled Convolution</strong>: offers a middle ground between Convolution and locally-connected layers. Rather than learning a separate set of weights at <em>every</em> spatial location, it learns/uses a set of kernels that are cycled through as we move through space.<br />
  This means that immediately neighboring locations will have different filters, as in a locally connected layer, but the memory requirements for storing the parameters will increase only by a factor of the size of this set of kernels, rather than by the size of the entire output feature map.
        <p>$$Z_{i, j, k}=\sum_{l, m, n} V_{l, j+m-1, k+n-1} K_{i, l, m, n, j \% t+1, k \% t+1} \tag{9.10}$$</p>
      </li>
      <li><strong>Max-Pooling, and Locally Connected Layers and Tiled Layers</strong>: When max pooling operation is applied to locally connected layer or tiled convolution, the model has the ability to become transformation invariant because adjacent filters have the freedom to learn a transformed version of the same feature.
        <blockquote>
          <p>This essentially similar to the property leveraged by pooling over channels rather than spatially.</p>
        </blockquote>
      </li>
      <li><strong>Different Connections</strong>: Besides locally-connected layers and tiled convolution, another extension can be to restrict the kernels to operate on certain input channels. One way to implement this is to connect the first m input channels to the first n output channels, the next m input channels to the next n output channels and so on. This method decreases the number of parameters in the model without decreasing the number of output units.</li>
      <li><strong>Other Operations</strong>: The following three operations—convolution, backprop from output to weights, and backprop from output to inputs—are sufficient to compute all the gradients needed to train any depth of feedforward convolutional network, as well as to train convolutional networks with reconstruction functions based on the transpose of convolution.
        <blockquote>
          <p>See Goodfellow (2010) for a full derivation of the equations in the fully general multidimensional, multiexample case.</p>
        </blockquote>
      </li>
      <li><strong>Bias</strong>:  Bias terms can be used in different ways in the convolution stage.
        <ul>
          <li>For locally connected layer and tiled convolution, we can use a bias per output unit and kernel respectively.</li>
          <li>In case of traditional convolution, a single bias term per output channel is used.</li>
          <li>If the <em>input size is fixed</em>, a bias per output unit may be used to <em>counter the effect of regional image statistics and smaller activations at the boundary due to zero padding</em>.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents6" id="bodyContents62">Structured Outputs:</strong><br />
 Convolutional networks can be trained to output high-dimensional structured output rather than just a classification score.<br />
 A good example is the task of <strong>image segmentation</strong> where each pixel needs to be associated with an object class.<br />
 Here the output is the same size (spatially) as the input. The model outputs a tensor <script type="math/tex">S</script>  where <script type="math/tex">S_{i, j, k}</script> is the probability that pixel <script type="math/tex">(j,k)</script> belongs to class <script type="math/tex">i</script>.
    <ul>
      <li><strong>Problem</strong>: One issue that often comes up is that the output plane can be smaller than the input plane.</li>
      <li><strong>Solutions</strong>:
        <ul>
          <li>To produce an output map as the same size as the input map, only same-padded convolutions can be stacked.</li>
          <li>Avoid Pooling Completely <em>(Jain et al. 2007)</em></li>
          <li>Emit a lower-Resolution grid of labels <em>(Pinheiro and Collobert, 2014, 2015)</em></li>
          <li><strong>Recurrent-Convolutional Models</strong>: The output of the first labelling stage can be refined successively by another convolutional model. If the models use tied parameters, this gives rise to a type of recursive model.</li>
          <li>Another model that has gained popularity for segmentation tasks (especially in the medical imaging community) is the <a href="https://arxiv.org/abs/1505.04597">U-Net</a>. The up-convolution mentioned is just a direct upsampling by repetition followed by a convolution with same padding.</li>
        </ul>
      </li>
    </ul>

    <p>The output can be further processed under the assumption that contiguous regions of pixels will tend to belong to the same label. Graphical models can describe this relationship. Alternately, <a href="https://www.robots.ox.ac.uk/~vgg/rg/papers/tompson2014.pdf">CNNs can learn to optimize the graphical models training objective</a>.</p>
  </li>
</ol>

<!-- ## Seven
{: #content7}

1. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents7 #bodyContents71}

2. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents7 #bodyContents72}

3. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents7 #bodyContents73}

4. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents7 #bodyContents74}

5. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents7 #bodyContents75}

6. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents7 #bodyContents76}

7. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents7 #bodyContents77}

8. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents7 #bodyContents78}

## Eight
{: #content8}

1. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents8 #bodyContents81}

2. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents8 #bodyContents82}

3. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents8 #bodyContents83}

4. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents8 #bodyContents84}

5. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents8 #bodyContents85}

6. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents8 #bodyContents86}

7. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents8 #bodyContents87}

8. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents8 #bodyContents88}
 -->

<hr />

<h2 id="contentx">Distinguishing features</h2>

<ol>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents9" id="bodyContents91">Asynchronous:</strong></p>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents9" id="bodyContents92">Image Features:</strong></dt>
      <dd>are certain quantities that are calculated from the image to <em>better describe the information in the image</em>, and to <em>reduce the size of the input vectors</em>.</dd>
      <dd>
        <ul>
          <li>Examples:
            <ul>
              <li><strong>Color Histogram</strong>: Compute a (bucket-based) vector of colors with their respective amounts in the image.</li>
              <li><strong>Histogram of Oriented Gradients (HOG)</strong>: we count the occurrences of gradient orientation in localized portions of the image.</li>
              <li><strong>Bag of Words</strong>: a <em>bag of visual words</em> is a vector of occurrence counts of a vocabulary of local image features.
                <blockquote>
                  <p>The <strong>visual words</strong> can be extracted using a clustering algorithm; K-Means.</p>
                </blockquote>
              </li>
            </ul>
          </li>
        </ul>
      </dd>
    </dl>
  </li>
</ol>


      <footer class="site-footer">
    <!--   <span class="site-footer-owner"><a href="http://localhost:8889">Ahmad Badary</a> is maintained by <a href="http://ahmedbadary.ml">Ahmad Badary</a>.</span> -->
    
<!--  -->
    <div class="wrapper">
        <div class="footer-col-wrapper">
            <div class="footer-col footer-col-1">
            <span class="site-footer-owner"><a href="http://localhost:8889">Site</a> maintained by <a href="http://ahmedbadary.ml">Ahmad Badary</a>.</span>
    <span class="site-footer-credits">
        <p>
            &copy; 2017. All rights reserved.
        </p> 
    </span>
            </div>
            <div class="footer-col footer-col-2">
            <div><p>         </p></div>
            </div>
            <div class="footer-col footer-col-3">
                <ul class="social-media-list">
                    
                      <li>
                        <a href="https://github.com/AhmedBadary">
                          <i class="fa fa-github"></i> GitHub
                        </a>
                      </li>
                    
                    
                      <li>
                        <a href="https://linkedin.com/in/ahmad-badary-656098121/">
                          <i class="fa fa-linkedin"></i> LinkedIn
                        </a>
                      </li>
                    
                    
                      <li>
                        <a href="https://www.facebook.com/ahmed.thabet.94">
                          <i class="fa fa-facebook"></i> Facebook
                        </a>
                      </li>
                    
                </ul>
            </div>
        </div>
    </div>
<!--  -->
</footer>


    </section>

  </body>

<!-- Table of Content Script -->
<script type="text/javascript">
var bodyContents = $(".bodyContents1");
$("<ol>").addClass("TOC1ul").appendTo(".TOC1");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC1ul");
     });
// 
var bodyContents = $(".bodyContents2");
$("<ol>").addClass("TOC2ul").appendTo(".TOC2");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC2ul");
     });
// 
var bodyContents = $(".bodyContents3");
$("<ol>").addClass("TOC3ul").appendTo(".TOC3");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC3ul");
     });
//
var bodyContents = $(".bodyContents4");
$("<ol>").addClass("TOC4ul").appendTo(".TOC4");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC4ul");
     });
//
var bodyContents = $(".bodyContents5");
$("<ol>").addClass("TOC5ul").appendTo(".TOC5");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC5ul");
     });
//
var bodyContents = $(".bodyContents6");
$("<ol>").addClass("TOC6ul").appendTo(".TOC6");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC6ul");
     });
//
var bodyContents = $(".bodyContents7");
$("<ol>").addClass("TOC7ul").appendTo(".TOC7");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC7ul");
     });
//
var bodyContents = $(".bodyContents8");
$("<ol>").addClass("TOC8ul").appendTo(".TOC8");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC8ul");
     });
//
var bodyContents = $(".bodyContents9");
$("<ol>").addClass("TOC9ul").appendTo(".TOC9");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC9ul");
     });

</script>

<!-- VIDEO BUTTONS SCRIPT -->
<script type="text/javascript">
  function iframePopInject(event) {
    var $button = $(event.target);
    // console.log($button.parent().next());
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $figure = $("<div>").addClass("video_container");
        $iframe = $("<iframe>").appendTo($figure);
        $iframe.attr("src", $button.attr("src"));
        // $iframe.attr("frameborder", "0");
        $iframe.attr("allowfullscreen", "true");
        $iframe.css("padding", "4px 6px");
        $button.next().css("display", "block");
        $figure.appendTo($button.next());
        $button.text("Hide Video")
    } else {
        $button.attr('value', 'show');
        $button.next().html("");
        $button.text("Show Video")
    }
}
</script>

<!-- BUTTON TRY -->
<script type="text/javascript">
  function iframePopA(event) {
    event.preventDefault();
    var $a = $(event.target).parent();
    console.log($a);
    if ($a.attr('value') == 'show') {
        $a.attr('value', 'hide');
        $figure = $("<div>");
        $iframe = $("<iframe>").addClass("popup_website_container").appendTo($figure);
        $iframe.attr("src", $a.attr("href"));
        $iframe.attr("frameborder", "1");
        $iframe.attr("allowfullscreen", "true");
        $iframe.css("padding", "4px 6px");
        $a.next().css("display", "block");
        $figure.appendTo($a.next().next());
        // $a.text("Hide Content")
        $('html, body').animate({
            scrollTop: $a.offset().top
        }, 1000);
    } else {
        $a.attr('value', 'show');
        $a.next().next().html("");
        // $a.text("Show Content")
    }

    $a.next().css("display", "inline");
}
</script>


<!-- TEXT BUTTON SCRIPT - INJECT -->
<script type="text/javascript">
  function showTextPopInject(event) {
    var $button = $(event.target);
    var txt = $button.attr("input");
    console.log(txt);
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $p = $("<p>");
        $p.html(txt);
        $button.next().css("display", "block");
        $p.appendTo($button.next());
        $button.text("Hide Content")
    } else {
        $button.attr('value', 'show');
        $button.next().html("");
        $button.text("Show Content")
    }

}
</script>

<!-- TEXT BUTTON SCRIPT - HIDDEN / HIDE / SHOW / HIDE/SHOW -->
<script type="text/javascript">
  function showTextPopHide(event) {
    var $button = $(event.target);
    // var txt = $button.attr("input");
    var txt = $button.text();
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $button.next().removeAttr("hidden");
        $button.text(txt + " - Hide Content");
    } else {
        $button.attr('value', 'show');
        $button.next().attr("hidden", "");
        $button.text(txt.replace(" - Hide Content",""));
    }
}
</script>

<!-- TEXT BUTTON SCRIPT - HIDDEN / HIDE / SHOW / HIDE/SHOW -->
<script type="text/javascript">
  function showText_withParent_PopHide(event) {
    var $button = $(event.target);
    var $parent = $button.parent();
    var txt = $button.text();
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $parent.next().removeAttr("hidden");
        $button.text(txt + " - Hide Content");
    } else {
        $button.attr('value', 'show');
        $parent.next().attr("hidden", "");
        $button.text(txt.replace(" - Hide Content",""));
    }
}
</script>

<!-- Print / Printing / printme -->
<!-- <script type="text/javascript">
i = 0

for (var i = 1; i < 6; i++) {
    var bodyContents = $(".bodyContents" + i);
    $("<p>").addClass("TOC1ul")  .appendTo(".TOC1");
    bodyContents.each(function(index, element) {
        var paragraph = $(element);
        $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC1ul");
         });
} 
</script>
 -->
 
</html>

