<!DOCTYPE html>
<html lang="en-us">
  <head>
  <meta charset="UTF-8">
  <title>Ahmad Badary</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <link rel="stylesheet" href="/css/normalize.css">
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="/css/cayman.css">
  <link rel="stylesheet" href="/css/style.css">
  <link rel="shortcut icon" href="/main_files/favicon.ico" />
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="/css/customStyle.css">
  <title> » Ahmad Badary</title>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
</head>

  <body>
    <nav class="main-nav">
    <a href="https://ahmedbadary.github.io/" class="main-nav-logo">
        <img src="/main_files/logo.png">
    </a>
    <ul id="menu-main" class="main-nav-items">
        <li id="menu-item-1859" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-home menu-item-1859">
            <a href="/">Home</a>
        </li>
        <li id="menu-item-2869" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-2869">
            <a href="/work">Work</a>
        </li>
        <li id="menu-item-1892" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1892">
            <a href="/projects">Projects</a>
        </li>
        <li id="menu-item-1858" class="menu-item menu-item-type-post_type menu-item-object-page current_page_parent menu-item-1858">
            <a href="/blog">Blog</a>
        </li>
        <li id="menu-item-1862" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1862">
            <a href="/about">About</a>
        </li>
    </ul>
</nav>


<section class="page-header">
  <h1 class="project-name">Activation Functions</h1>
  <h2 class="project-tagline"></h2>
  <a href="/#" class="btn">Home</a>
  <a href="/work" class="btn">Work-Space</a>
  <a href= /work_files/research/dl/concepts.html class="btn">Previous</a>
</section>

<!-- <div>
  <ul class="posts">
    
      <li><span>02 Jan 2014</span> &raquo; <a href="/2014/01/02/introducing-Ahmad/">Introducing Ahmad</a></li>
    
  </ul>
</div> -->


    <section class="main-content">
      
      <div class="TOC">
  <h1 id="table-of-contents">Table of Contents</h1>

  <ul class="TOC1">
    <li><a href="#content1">Introduction</a></li>
  </ul>
  <ul class="TOC2">
    <li><a href="#content2">Activation Functions</a></li>
  </ul>
</div>

<hr />
<hr />

<ul>
  <li><a href="https://stats.stackexchange.com/questions/115258/comprehensive-list-of-activation-functions-in-neural-networks-with-pros-cons">Comprehensive list of activation functions in neural networks with pros/cons</a></li>
  <li><a href="https://www.reddit.com/r/MachineLearning/comments/dekblo/d_state_of_the_art_activation_function_gelu_selu/">State Of The Art Activation Function: GELU, SELU, ELU, ReLU and more. With visualization of the activation functions and their derivatives (reddit!)</a></li>
</ul>

<h2 id="content1">Introduction</h2>

<ol>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents11">Activation Functions:</strong><br />
 In NNs, the <strong>activation function</strong> of a node defines the output of that node given an input or set of inputs.<br />
 The activation function is an abstraction representing the rate of action potential firing in the cell.<br />
 <br /></p>

    <p><!-- 2. **Motivation:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12}   --></p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents13">Desirable Properties:</strong>
    <ul>
      <li><strong>Non-Linearity</strong>:<br />
 When the activation function is non-linear, then a two-layer neural network can be proven to be a universal function approximator. The identity activation function does not satisfy this property. When multiple layers use the identity activation function, the entire network is equivalent to a single-layer model.</li>
      <li><strong>Range</strong>:<br />
 When the range of the activation function is finite, gradient-based training methods tend to be more stable, because pattern presentations significantly affect only limited weights. When the range is infinite, training is generally more efficient because pattern presentations significantly affect most of the weights. In the latter case, smaller learning rates are typically necessary.</li>
      <li><strong>Continuously Differentiable</strong>:<br />
 This property is desirable for enabling gradient-based optimization methods. The binary step activation function is not differentiable at 0, and it differentiates to 0 for all other values, so gradient-based methods can make no progress with it.</li>
      <li><strong>Monotonicity</strong>:
        <ul>
          <li>When the activation function is monotonic, the error surface associated with a single-layer model is guaranteed to be convex.</li>
          <li>During the training phase, backpropagation informs each neuron how much it should influence each neuron in the next layer. If the activation function isn’t monotonic then increasing the neuron’s weight might cause it to have less influence, the opposite of what was intended.
            <blockquote>
              <p>However, Monotonicity isn’t required. Several papers use non monotonic trained activation functions.<br />
Gradient descent finds a local minimum even with non-monotonic activation functions. It might only take longer.</p>
            </blockquote>
          </li>
          <li>From a biological perspective, an “activation” depends on the sum of inputs, and once the sum surpasses a threshold, “firing” occurs. This firing should happen even if the sum surpasses the threshold by a small or a large amount; making monotonicity a desirable property to not limit the range of the “sum”.</li>
        </ul>
      </li>
      <li><strong>Smoothness with Monotonic Derivatives</strong>:<br />
 These have been shown to generalize better in some cases.</li>
      <li><strong>Approximating Identity near Origin</strong>:<br />
 Equivalent to <script type="math/tex">{\displaystyle f(0)=0}</script> and <script type="math/tex">{\displaystyle f'(0)=1}</script>, and <script type="math/tex">{\displaystyle f'}</script> is continuous at <script type="math/tex">0</script>.<br />
 When activation functions have this property, the neural network will learn efficiently when its weights are initialized with small random values. When the activation function does not approximate identity near the origin, special care must be used when initializing the weights.</li>
      <li><strong>Zero-Centered Range</strong>:<br />
 Has effects of centering the data (zero mean) by centering the activations. Makes learning easier.
        <blockquote>
          <p><a href="https://www.youtube.com/watch?v=FDCfw-YqWTE&amp;list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&amp;index=10&amp;t=0s">WHY NORMALIZING THE DATA/SIGNAL IS IMPORTANT</a></p>
        </blockquote>
      </li>
    </ul>

    <p><br /></p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents14">Undesirable Properties:</strong>
    <ul>
      <li><strong>Saturation</strong>:<br />
 An activation functions output, with finite range, may saturate near its tail or head (e.g. <script type="math/tex">\{0, 1\}</script> for sigmoid). This leads to a problem called <strong>vanishing gradient</strong>.</li>
      <li><strong>Vanishing Gradients</strong>:<br />
 Happens when the gradient of an activation function is very small/zero. This usually happens when the activation function <strong>saturates</strong> at either of its tails.<br />
 The chain-rule will <em><strong>multiply</strong></em> the local gradient (of activation function) with the whole objective. Thus, when gradient is small/zero, it will “kill” the gradient <script type="math/tex">\rightarrow</script> no signal will flow through the neuron to its weights or to its data.<br />
 <strong>Slows/Stops learning completely</strong>.</li>
      <li><strong>Range Not Zero-Centered</strong>:<br />
 This is undesirable since neurons in later layers of processing in a Neural Network would be receiving data that is not zero-centered. This has implications on the dynamics during gradient descent, because if the data coming into a neuron is always positive (e.g. <script type="math/tex">x>0</script> elementwise in <script type="math/tex">f=w^Tx+b</script>), then the gradient on the weights <script type="math/tex">w</script> will during backpropagation become either all be positive, or all negative (depending on the gradient of the whole expression <script type="math/tex">f</script>). This could introduce undesirable zig-zagging dynamics in the gradient updates for the weights. However, notice that once these gradients are added up across a batch of data the final update for the weights can have variable signs, somewhat mitigating this issue. Therefore, this is an inconvenience but it has less severe consequences compared to the saturated activation problem above.<br />
 <strong>Makes optimization harder.</strong> <br />
 <br /></li>
    </ul>
  </li>
</ol>

<!-- 5. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents15}  

6. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents16}  
 -->

<hr />

<h2 id="content2">Activation Functions</h2>

<p><img src="/main_files/concepts/16.png" alt="img" max-width="180%" width="180%" /></p>

<ol>
  <li><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents21">Sigmoid:</strong>
    <p>$$S(z)=\frac{1}{1+e^{-z}} \\ S^{\prime}(z)=S(z) \cdot(1-S(z))$$</p>
    <p><img src="/main_files/concepts/3.png" alt="img" width="68%" class="center-image" /><br />
 <strong style="color: red">Properties:</strong><br />
 Never use as activation, use as an output unit for binary classification.</p>
    <ul>
      <li><strong>Pros</strong>:
        <ul>
          <li>Has a nice interpretation as the firing rate of a neuron</li>
        </ul>
      </li>
      <li><strong>Cons</strong>:
        <ul>
          <li>They Saturate and kill gradients <script type="math/tex">\rightarrow</script> Gives rise to <strong>vanishing gradients</strong>[^1] <script type="math/tex">\rightarrow</script> Stop Learning</li>
          <li>Happens when initialization weights are too large</li>
          <li>or sloppy with data preprocessing</li>
          <li>Neurons Activation saturates at either tail of <script type="math/tex">0</script> or <script type="math/tex">1</script></li>
          <li>Output NOT <strong>Zero-Centered</strong> <script type="math/tex">\rightarrow</script> Gradient updates go too far in different directions <script type="math/tex">\rightarrow</script> makes optimization harder</li>
          <li>The local gradient <script type="math/tex">(z * (1-z))</script> achieves maximum at <script type="math/tex">0.25</script>, when <script type="math/tex">z = 0.5</script>. <script type="math/tex">\rightarrow</script> very time the gradient signal flows through a sigmoid gate, its magnitude always diminishes by one quarter (or more) <script type="math/tex">\rightarrow</script> with basic SGD, the lower layers of a network train much slower than the higher one</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents22">Tanh:</strong>
    <p>$$\tanh (z)=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}} \\ \tanh ^{\prime}(z)=1-\tanh (z)^{2}$$</p>
    <p><img src="/main_files/concepts/4.png" alt="img" width="68%" class="center-image" /></p>

    <p><strong style="color: red">Properties:</strong><br />
 Strictly superior to Sigmoid (scaled version of sigmoid | stronger gradient). Good for activation.</p>
    <ul>
      <li><strong>Pros</strong>:
        <ul>
          <li>Zero Mean/Centered</li>
        </ul>
      </li>
      <li><strong>Cons</strong>:
        <ul>
          <li>They Saturate and kill gradients <script type="math/tex">\rightarrow</script> Gives rise to <strong>vanishing gradients</strong>[^1] <script type="math/tex">\rightarrow</script> Stop Learning<br />
 <br /></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents23">ReLU:</strong>
    <p>$$R(z)=\left\{\begin{array}{cc}{z} &amp; {z&gt;0} \\ {0} &amp; {z&lt;=0}\end{array}\right\} \\  R^{\prime}(z)=\left\{\begin{array}{ll}{1} &amp; {z&gt;0} \\ {0} &amp; {z&lt;0}\end{array}\right\}$$</p>
    <p><img src="/main_files/concepts/5.png" alt="img" width="68%" class="center-image" /></p>

    <p><strong style="color: red">Properties:</strong><br />
 The best for activation (Better gradients).</p>
    <ul>
      <li><strong>Pros</strong>:
        <ul>
          <li>Non-saturation of gradients which <em>accelerates convergence</em> of SGD</li>
          <li>Sparsity effects and induced regularization. <a href="https://stats.stackexchange.com/questions/176794/how-does-rectilinear-activation-function-solve-the-vanishing-gradient-problem-in/176905#176905">discussion</a><br />
  ReLU (as usually used in neural networks) introduces sparsity in <span style="color: purple"><strong>activations</strong></span> not in <em>weights</em> or <em>biases</em>.</li>
          <li>Not computationally expensive</li>
        </ul>
      </li>
      <li><strong>Cons</strong>:
        <ul>
          <li><strong>ReLU not zero-centered problem</strong>:<br />
  The problem that ReLU is not zero-centered can be solved/mitigated by using <strong>batch normalization</strong>, which normalizes the signal before activation:
            <blockquote>
              <p>From paper: We add the BN transform immediately before the nonlinearity, by normalizing <script type="math/tex">x =  Wu + b</script>; normalizing it is likely to produce activations with a stable distribution.</p>
              <ul>
                <li><a href="https://www.youtube.com/watch?v=FDCfw-YqWTE&amp;list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&amp;index=10&amp;t=0s">WHY NORMALIZING THE SIGNAL IS IMPORTANT</a></li>
              </ul>
            </blockquote>
          </li>
          <li><strong>Dying ReLUs (Dead Neurons):</strong><br />
  If a neuron gets clamped to zero in the forward pass (it doesn’t “fire” / <script type="math/tex">% <![CDATA[
x<0 %]]></script>), then its weights will get zero gradient. Thus, if a ReLU neuron is unfortunately initialized such that it never fires, or if a neuron’s weights ever get knocked off with a large update during training into this regime (usually as a symptom of aggressive learning rates), then this neuron will remain permanently dead.</li>
          <li><a href="https://www.youtube.com/embed/gYpoJMlgyXA?start=1249" value="show" onclick="iframePopA(event)"><strong>cs231n Explanation</strong></a>
  <a href="https://www.youtube.com/embed/gYpoJMlgyXA?start=1249"></a>
            <div></div>
          </li>
          <li><strong>Infinite Range</strong>:<br />
  Can blow up the activation.<br />
 <br /></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents24">Leaky-ReLU:</strong>
    <p>$$R(z)=\left\{\begin{array}{cc}{z} &amp; {z&gt;0} \\ {\alpha z} &amp; {z&lt;=0}\end{array}\right\} \\ 
     R^{\prime}(z)=\left\{\begin{array}{ll}{1} &amp; {z&gt;0} \\ {\alpha} &amp; {z&lt;0}\end{array}\right\}$$</p>
    <p><img src="/main_files/concepts/6.png" alt="img" width="68%" class="center-image" /></p>

    <p><strong style="color: red">Properties:</strong><br />
 Sometimes useful. Worth trying.</p>
    <ul>
      <li><strong>Pros</strong>:
        <ul>
          <li>Leaky ReLUs are one attempt to fix the “dying ReLU” problem by having a small negative slope (of 0.01, or so).</li>
        </ul>
      </li>
      <li><strong>Cons</strong>:<br />
  The consistency of the benefit across tasks is presently unclear.<br />
 <br /></li>
    </ul>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents25">ELU:</strong></p>

    <p>&lt;!–</p>
    <ol>
      <li>
        <p><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents26">Asynchronous:</strong></p>
      </li>
      <li>
        <p><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents27">Asynchronous:</strong><br />
  –&gt;</p>
      </li>
    </ol>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents28">Notes:</strong>
    <ul>
      <li>It is very rare to mix and match different types of neurons in the same network, even though there is no fundamental problem with doing so.</li>
      <li><strong>Identity Mappings</strong>:<br />
 When an activation function cannot achieve an identity mapping (e.g. ReLU map all negative inputs to zero); then adding extra depth actually decreases the best performance, in the case a shallower one would suffice (Deep Residual Net paper).<br />
 <br /></li>
    </ul>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents21">Softmax:</strong></p>

    <p id="lst-p"><strong style="color: red">Motivation:</strong></p>
    <ul>
      <li><strong>Information Theory</strong> - from the perspective of information theory the softmax function can be seen as trying to minimize the cross-entropy between the predictions and the truth.</li>
      <li><strong>Probability Theory</strong> - from this perspective since <script type="math/tex">\hat{y}_ i</script> represent log-probabilities we are in fact looking at the log-probabilities, thus when we perform exponentiation we end up with the raw probabilities. In this case the softmax equation find the MLE (Maximum Likelihood Estimate).<br />
  If a neuron’s output is a log probability, then the summation of many neurons’ outputs is a multiplication of their probabilities. That’s more commonly useful than a sum of probabilities.</li>
      <li>
        <p>It is a softened version of the <strong>argmax</strong> function (limit as <script type="math/tex">T \rightarrow 0</script>)</p>
      </li>
      <li><strong style="color: red">Properties</strong>
        <ul>
          <li>There is one nice attribute of Softmax as compared with standard normalisation:<br />
  It react to low stimulation (think blurry image) of your neural net with rather uniform distribution and to high stimulation (ie. large numbers, think crisp image) with probabilities close to 0 and 1. <br />
  While standard normalisation does not care as long as the proportion are the same.<br />
  Have a look what happens when soft max has 10 times larger input, ie your neural net got a crisp image and a lot of neurones got activated.<br />
  <button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Example SM</button>
            <blockquote hidden="">
              <blockquote>
                <blockquote>
                  <blockquote>
                    <p>softmax([1,2])              # blurry image of a ferret<br />
      [0.26894142,      0.73105858])  #     it is a cat perhaps !?<br />
      »&gt; softmax([10,20])            # crisp image of a cat<br />
      [0.0000453978687, 0.999954602]) #     it is definitely a CAT !</p>
                  </blockquote>
                </blockquote>
              </blockquote>
            </blockquote>
            <p>And then compare it with standard normalisation:<br />
  <button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Example Normalization</button></p>
            <blockquote hidden="">
              <blockquote>
                <blockquote>
                  <blockquote>
                    <p>std_norm([1,2])                      # blurry image of a ferret<br />
      [0.3333333333333333, 0.6666666666666666] #     it is a cat perhaps !?<br />
      »&gt; std_norm([10,20])                    # crisp image of a cat<br />
      [0.3333333333333333, 0.6666666666666666] #     it is a cat perhaps !?</p>
                  </blockquote>
                </blockquote>
              </blockquote>
            </blockquote>
          </li>
        </ul>
      </li>
    </ul>

    <p id="lst-p"><strong style="color: red">Notes:</strong></p>
    <ul>
      <li>Alternatives to Softmax:
        <ul>
          <li><a href="https://arxiv.org/pdf/1511.05042.pdf">AN EXPLORATION OF SOFTMAX ALTERNATIVES BELONGING TO THE SPHERICAL LOSS FAMILY (paper)</a></li>
          <li><a href="http://proceedings.mlr.press/v48/martins16.pdf">From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification (paper)</a><br />
 <br /></li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<!-- 
    __Desirable Properties:__{: style="color: red"}  
    {: #lst-p}
    * __Non-Linearity__:  
    When the activation function is non-linear, then a two-layer neural network can be proven to be a universal function approximator. The identity activation function does not satisfy this property. When multiple layers use the identity activation function, the entire network is equivalent to a single-layer model. 
    * __Range__:  
    When the range of the activation function is finite, gradient-based training methods tend to be more stable, because pattern presentations significantly affect only limited weights. When the range is infinite, training is generally more efficient because pattern presentations significantly affect most of the weights. In the latter case, smaller learning rates are typically necessary.  
    * __Continuously Differentiable__:  
    This property is desirable for enabling gradient-based optimization methods. The binary step activation function is not differentiable at 0, and it differentiates to 0 for all other values, so gradient-based methods can make no progress with it.
    * __Monotonicity__:  
    When the activation function is monotonic, the error surface associated with a single-layer model is guaranteed to be convex.  
    * __Smoothness with Monotonic Derivatives__:  
    These have been shown to generalize better in some cases.  
    * __Approximating Identity near Origin__:  
    Equivalent to $${\displaystyle f(0)=0}$$ and $${\displaystyle f'(0)=1}$$, and $${\displaystyle f'}$$ is continuous at $$0$$.  
    When activation functions have this property, the neural network will learn efficiently when its weights are initialized with small random values. When the activation function does not approximate identity near the origin, special care must be used when initializing the weights.  
    * __Zero-Centered Range__:  
    Has effects of centering the data (zero mean) by centering the activations. Makes learning easier.   
    > [WHY NORMALIZING THE DATA/SIGNAL IS IMPORTANT](https://www.youtube.com/watch?v=FDCfw-YqWTE&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&index=10&t=0s)
            
    __Undesirable Properties:__{: style="color: red"}  
    {: #lst-p}
    {: #lst-p}
    * __Saturation__:  
        An activation functions output, with finite range, may saturate near its tail or head (e.g. $$\{0, 1\}$$ for sigmoid). This leads to a problem called __vanishing gradient__.  
    * __Vanishing Gradients__:  
        Happens when the gradient of an activation function is very small/zero. This usually happens when the activation function __saturates__ at either of its tails.  
        The chain-rule will *__multiply__* the local gradient (of activation function) with the whole objective. Thus, when gradient is small/zero, it will "kill" the gradient $$\rightarrow$$ no signal will flow through the neuron to its weights or to its data.  
        __Slows/Stops learning completely__.  
    * __Range Not Zero-Centered__:  
        This is undesirable since neurons in later layers of processing in a Neural Network would be receiving data that is not zero-centered. This has implications on the dynamics during gradient descent, because if the data coming into a neuron is always positive (e.g. $$x>0$$ elementwise in $$f=w^Tx+b$$), then the gradient on the weights $$w$$ will during backpropagation become either all be positive, or all negative (depending on the gradient of the whole expression $$f$$). This could introduce undesirable zig-zagging dynamics in the gradient updates for the weights. However, notice that once these gradients are added up across a batch of data the final update for the weights can have variable signs, somewhat mitigating this issue. Therefore, this is an inconvenience but it has less severe consequences compared to the saturated activation problem above.  
        __Makes optimization harder.__   

    __Activation Functions:__{: style="color: red"}  
    {: #lst-p}
    ![img](/main_files/concepts/16.png){: max-width="180%" width="180%"}  
    * __Properties__:                  
        * __Sigmoid__:  
            Never use as activation, use as an output unit for binary classification.  
            * __Pros__:  
                * Has a nice interpretation as the firing rate of a neuron  
            * __Cons__:  
                * They Saturate and kill gradients $$\rightarrow$$ Gives rise to __vanishing gradients__[^1] $$\rightarrow$$ Stop Learning  
                    * Happens when initialization weights are too large  
                    * or sloppy with data preprocessing  
                    * Neurons Activation saturates at either tail of $$0$$ or $$1$$  
                * Output NOT __Zero-Centered__ $$\rightarrow$$ Gradient updates go too far in different directions $$\rightarrow$$ makes optimization harder   
                * The local gradient $$(z * (1-z))$$ achieves maximum at $$0.25$$, when $$z = 0.5$$. $$\rightarrow$$ very time the gradient signal flows through a sigmoid gate, its magnitude always diminishes by one quarter (or more) $$\rightarrow$$ with basic SGD, the lower layers of a network train much slower than the higher one  
        * __Tanh__:  
            Strictly superior to Sigmoid (scaled version of sigmoid \| stronger gradient). Good for activation.  
            * __Pros__:  
                * Zero Mean/Centered  
            * __Cons__:  
                * They Saturate and kill gradients $$\rightarrow$$ Gives rise to __vanishing gradients__[^1] $$\rightarrow$$ Stop Learning  
        * __ReLU__:  
            The best for activation (Better gradients).  
            * __Pros__:  
                * Non-saturation of gradients which _accelerates convergence_ of SGD  
                * Sparsity effects and induced regularization. [discussion](https://stats.stackexchange.com/questions/176794/how-does-rectilinear-activation-function-solve-the-vanishing-gradient-problem-in/176905#176905)  
                * Not computationally expensive  
            * __Cons__:  
                * __ReLU not zero-centered problem__:  
                    The problem that ReLU is not zero-centered can be solved/mitigated by using __batch normalization__, which normalizes the signal before activation:  
                    > From paper: We add the BN transform immediately before the nonlinearity, by normalizing $$x =  Wu + b$$; normalizing it is likely to produce activations with a stable distribution.  
                    > * [WHY NORMALIZING THE SIGNAL IS IMPORTANT](https://www.youtube.com/watch?v=FDCfw-YqWTE&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&index=10&t=0s)
                * __Dying ReLUs (Dead Neurons):__  
                    If a neuron gets clamped to zero in the forward pass (it doesn’t "fire" / $$x<0$$), then its weights will get zero gradient. Thus, if a ReLU neuron is unfortunately initialized such that it never fires, or if a neuron’s weights ever get knocked off with a large update during training into this regime (usually as a symptom of aggressive learning rates), then this neuron will remain permanently dead.  
                    * [**cs231n Explanation**](https://www.youtube.com/embed/gYpoJMlgyXA?start=1249){: value="show" onclick="iframePopA(event)"}
                    <a href="https://www.youtube.com/embed/gYpoJMlgyXA?start=1249"></a>
                        <div markdown="1"> </div>    
                * __Infinite Range__:  
                    Can blow up the activation.  
        * __Leaky Relu__:  
            Sometimes useful. Worth trying.  
            * __Pros__:  
                * Leaky ReLUs are one attempt to fix the “dying ReLU” problem by having a small negative slope (of 0.01, or so).  
            * __Cons__:  
                The consistency of the benefit across tasks is presently unclear.  
        * __ELU__:  
            
    * __Derivatives of Activation Functions__:  
        * __Sigmoid__:  
            <p>$$S(z)=\frac{1}{1+e^{-z}} \\ S^{\prime}(z)=S(z) \cdot(1-S(z))$$</p>  
            ![img](/main_files/concepts/3.png){: width="68%" .center-image}  
        * __Tanh__:  
            <p>$$\tanh (z)=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}} \\ \tanh ^{\prime}(z)=1-\tanh (z)^{2}$$</p>  
            ![img](/main_files/concepts/4.png){: width="68%" .center-image}  
        * __Relu__:  
            <p>$$R(z)=\left\{\begin{array}{cc}{z} & {z>0} \\ {0} & {z<=0}\end{array}\right\} \\  R^{\prime}(z)=\left\{\begin{array}{ll}{1} & {z>0} \\ {0} & {z<0}\end{array}\right\}$$</p>  
            ![img](/main_files/concepts/5.png){: width="68%" .center-image}  
        * __Leaky Relu__:  
            <p>$$R(z)=\left\{\begin{array}{cc}{z} & {z>0} \\ {\alpha z} & {z<=0}\end{array}\right\} \\ 
            R^{\prime}(z)=\left\{\begin{array}{ll}{1} & {z>0} \\ {\alpha} & {z<0}\end{array}\right\}$$</p>  
            ![img](/main_files/concepts/6.png){: width="68%" .center-image}  
        * [Further Reading](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html)  

    __Notes:__{: style="color: red"}  
    {: #lst-p}
    * It is very rare to mix and match different types of neurons in the same network, even though there is no fundamental problem with doing so.  
    * __Identity Mappings__:  
        When an activation function cannot achieve an identity mapping (e.g. ReLU map all negative inputs to zero); then adding extra depth actually decreases the best performance, in the case a shallower one would suffice (Deep Residual Net paper).   -->


      <footer class="site-footer">
    <!--   <span class="site-footer-owner"><a href="http://localhost:8889">Ahmad Badary</a> is maintained by <a href="http://ahmedbadary.ml">Ahmad Badary</a>.</span> -->
    
<!--  -->
    <div class="wrapper">
        <div class="footer-col-wrapper">
            <div class="footer-col footer-col-1">
            <span class="site-footer-owner"><a href="http://localhost:8889">Site</a> maintained by <a href="http://ahmedbadary.ml">Ahmad Badary</a>.</span>
    <span class="site-footer-credits">
        <p>
            &copy; 2017. All rights reserved.
        </p> 
    </span>
            </div>
            <div class="footer-col footer-col-2">
            <div><p>         </p></div>
            </div>
            <div class="footer-col footer-col-3">
                <ul class="social-media-list">
                    
                      <li>
                        <a href="https://github.com/AhmedBadary">
                          <i class="fa fa-github"></i> GitHub
                        </a>
                      </li>
                    
                    
                      <li>
                        <a href="https://linkedin.com/in/ahmad-badary-656098121/">
                          <i class="fa fa-linkedin"></i> LinkedIn
                        </a>
                      </li>
                    
                    
                      <li>
                        <a href="https://www.facebook.com/ahmed.thabet.94">
                          <i class="fa fa-facebook"></i> Facebook
                        </a>
                      </li>
                    
                </ul>
            </div>
        </div>
    </div>
<!--  -->
</footer>


    </section>

  </body>

<!-- Table of Content Script -->
<script type="text/javascript">
var bodyContents = $(".bodyContents1");
$("<ol>").addClass("TOC1ul").appendTo(".TOC1");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC1ul");
     });
// 
var bodyContents = $(".bodyContents2");
$("<ol>").addClass("TOC2ul").appendTo(".TOC2");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC2ul");
     });
// 
var bodyContents = $(".bodyContents3");
$("<ol>").addClass("TOC3ul").appendTo(".TOC3");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC3ul");
     });
//
var bodyContents = $(".bodyContents4");
$("<ol>").addClass("TOC4ul").appendTo(".TOC4");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC4ul");
     });
//
var bodyContents = $(".bodyContents5");
$("<ol>").addClass("TOC5ul").appendTo(".TOC5");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC5ul");
     });
//
var bodyContents = $(".bodyContents6");
$("<ol>").addClass("TOC6ul").appendTo(".TOC6");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC6ul");
     });
//
var bodyContents = $(".bodyContents7");
$("<ol>").addClass("TOC7ul").appendTo(".TOC7");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC7ul");
     });
//
var bodyContents = $(".bodyContents8");
$("<ol>").addClass("TOC8ul").appendTo(".TOC8");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC8ul");
     });
//
var bodyContents = $(".bodyContents9");
$("<ol>").addClass("TOC9ul").appendTo(".TOC9");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC9ul");
     });

</script>

<!-- VIDEO BUTTONS SCRIPT -->
<script type="text/javascript">
  function iframePopInject(event) {
    var $button = $(event.target);
    // console.log($button.parent().next());
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $figure = $("<div>").addClass("video_container");
        $iframe = $("<iframe>").appendTo($figure);
        $iframe.attr("src", $button.attr("src"));
        // $iframe.attr("frameborder", "0");
        $iframe.attr("allowfullscreen", "true");
        $iframe.css("padding", "4px 6px");
        $button.next().css("display", "block");
        $figure.appendTo($button.next());
        $button.text("Hide Video")
    } else {
        $button.attr('value', 'show');
        $button.next().html("");
        $button.text("Show Video")
    }
}
</script>

<!-- BUTTON TRY -->
<script type="text/javascript">
  function iframePopA(event) {
    event.preventDefault();
    var $a = $(event.target).parent();
    console.log($a);
    if ($a.attr('value') == 'show') {
        $a.attr('value', 'hide');
        $figure = $("<div>");
        $iframe = $("<iframe>").addClass("popup_website_container").appendTo($figure);
        $iframe.attr("src", $a.attr("href"));
        $iframe.attr("frameborder", "1");
        $iframe.attr("allowfullscreen", "true");
        $iframe.css("padding", "4px 6px");
        $a.next().css("display", "block");
        $figure.appendTo($a.next().next());
        // $a.text("Hide Content")
        $('html, body').animate({
            scrollTop: $a.offset().top
        }, 1000);
    } else {
        $a.attr('value', 'show');
        $a.next().next().html("");
        // $a.text("Show Content")
    }

    $a.next().css("display", "inline");
}
</script>


<!-- TEXT BUTTON SCRIPT - INJECT -->
<script type="text/javascript">
  function showTextPopInject(event) {
    var $button = $(event.target);
    var txt = $button.attr("input");
    console.log(txt);
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $p = $("<p>");
        $p.html(txt);
        $button.next().css("display", "block");
        $p.appendTo($button.next());
        $button.text("Hide Content")
    } else {
        $button.attr('value', 'show');
        $button.next().html("");
        $button.text("Show Content")
    }

}
</script>

<!-- TEXT BUTTON SCRIPT - HIDDEN / HIDE / SHOW / HIDE/SHOW -->
<script type="text/javascript">
  function showTextPopHide(event) {
    var $button = $(event.target);
    // var txt = $button.attr("input");
    var txt = $button.text();
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $button.next().removeAttr("hidden");
        $button.text(txt + " - Hide Content");
    } else {
        $button.attr('value', 'show');
        $button.next().attr("hidden", "");
        $button.text(txt.replace(" - Hide Content",""));
    }
}
</script>

<!-- TEXT BUTTON SCRIPT - HIDDEN / HIDE / SHOW / HIDE/SHOW -->
<script type="text/javascript">
  function showText_withParent_PopHide(event) {
    var $button = $(event.target);
    var $parent = $button.parent();
    var txt = $button.text();
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $parent.next().removeAttr("hidden");
        $button.text(txt + " - Hide Content");
    } else {
        $button.attr('value', 'show');
        $parent.next().attr("hidden", "");
        $button.text(txt.replace(" - Hide Content",""));
    }
}
</script>

<!-- Print / Printing / printme -->
<!-- <script type="text/javascript">
i = 0

for (var i = 1; i < 6; i++) {
    var bodyContents = $(".bodyContents" + i);
    $("<p>").addClass("TOC1ul")  .appendTo(".TOC1");
    bodyContents.each(function(index, element) {
        var paragraph = $(element);
        $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC1ul");
         });
} 
</script>
 -->
 
</html>

