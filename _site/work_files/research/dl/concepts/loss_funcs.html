<!DOCTYPE html>
<html lang="en-us">
  <head>
  <meta charset="UTF-8">
  <title>Ahmad Badary</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <link rel="stylesheet" href="/css/normalize.css">
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="/css/cayman.css">
  <link rel="stylesheet" href="/css/style.css">
  <link rel="shortcut icon" href="/main_files/favicon.ico" />
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="/css/customStyle.css">
  <title> » Ahmad Badary</title>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
</head>

  <body>
    <nav class="main-nav">
    <a href="https://ahmedbadary.github.io/" class="main-nav-logo">
        <img src="/main_files/logo.png">
    </a>
    <ul id="menu-main" class="main-nav-items">
        <li id="menu-item-1859" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-home menu-item-1859">
            <a href="/">Home</a>
        </li>
        <li id="menu-item-2869" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-2869">
            <a href="/work">Work</a>
        </li>
        <li id="menu-item-1892" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1892">
            <a href="/projects">Projects</a>
        </li>
        <li id="menu-item-1858" class="menu-item menu-item-type-post_type menu-item-object-page current_page_parent menu-item-1858">
            <a href="/blog">Blog</a>
        </li>
        <li id="menu-item-1862" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1862">
            <a href="/about">About</a>
        </li>
    </ul>
</nav>


<section class="page-header">
  <h1 class="project-name">Loss Functions</h1>
  <h2 class="project-tagline"></h2>
  <a href="/#" class="btn">Home</a>
  <a href="/work" class="btn">Work-Space</a>
  <a href= /work_files/research/dl/concepts.html class="btn">Previous</a>
</section>

<!-- <div>
  <ul class="posts">
    
      <li><span>02 Jan 2014</span> &raquo; <a href="/2014/01/02/introducing-Ahmad/">Introducing Ahmad</a></li>
    
  </ul>
</div> -->


    <section class="main-content">
      
      <div class="TOC">
  <h1 id="table-of-contents">Table of Contents</h1>

  <ul class="TOC1">
    <li><a href="#content1">Loss Functions</a></li>
  </ul>
  <ul class="TOC2">
    <li><a href="#content2">Loss Functions for Regression</a></li>
  </ul>
  <ul class="TOC3">
    <li><a href="#content3">Loss Functions for Classification</a></li>
  </ul>
  <!--       * [FOURTH](#content4)
  {: .TOC4}
  * [FIFTH](#content5)
  {: .TOC5}
  * [SIXTH](#content6)
  {: .TOC6} -->
</div>

<hr />
<hr />

<p><a href="https://isaacchanghau.github.io/post/loss_functions/">Loss Functions (blog)</a><br />
<a href="https://jhui.github.io/2017/01/05/Deep-learning-Information-theory/">Information Theory (Cross-Entropy and MLE, MSE, Nash, etc.)</a></p>

<h1 id="content1">Loss Functions</h1>

<h3 id="loss-functions"><strong style="color: SteelBlue; font-size: 1.27em" class="bodyContents1" id="bodyContents11">Loss Functions</strong></h3>
<p>Abstractly, a <strong>loss function</strong> or <strong>cost function</strong> is a function that maps an event or values of one or more variables onto a real number, intuitively, representing some “cost” associated with the event.</p>

<p>Formally, a <strong>loss function</strong> is a function <script type="math/tex">L :(\hat{y}, y) \in \mathbb{R} \times Y \longmapsto L(\hat{y}, y) \in \mathbb{R}</script>  that takes as inputs the predicted value <script type="math/tex">\hat{y}</script> corresponding to the real data value <script type="math/tex">y</script> and outputs how different they are.</p>

<hr />

<h1 id="content2">Loss Functions for Regression</h1>

<p><img src="/main_files/dl/concepts/loss_funcs/6.png" alt="img" width="61%" /><br />
<br /></p>

<h3 id="introduction"><strong style="color: SteelBlue; font-size: 1.27em" class="bodyContents2" id="bodyContents211">Introduction</strong></h3>
<p>Regression Losses usually only depend on the <strong>residual</strong> <script type="math/tex">r = y - \hat{y}</script> (i.e. what you have to add to your prediction to match the target)</p>

<p><strong style="color: red">Distance-Based Loss Functions:</strong><br />
A Loss function <script type="math/tex">L(\hat{y}, y)</script> is called <strong>distance-based</strong> if it:</p>
<ul>
  <li>Only depends on the <strong>residual</strong>:
    <p>$$L(\hat{y}, y) = \psi(y-\hat{y})  \:\: \text{for some } \psi : \mathbb{R} \longmapsto \mathbb{R}$$</p>
  </li>
  <li>Loss is <script type="math/tex">0</script> when residual is <script type="math/tex">0</script>:
    <p>$$\psi(0) = 0$$</p>
  </li>
</ul>

<p><strong style="color: red">Translation Invariance:</strong><br />
Distance-based losses are translation-invariant:</p>
<p>$$L(\hat{y}+a, y+a) = L(\hat{y}, y)$$</p>

<blockquote>
  <p>Sometimes <strong>Relative-Error</strong> <script type="math/tex">\dfrac{\hat{y}-y}{y}</script> is a more <em>natural</em> loss but it is NOT translation-invariant</p>
</blockquote>

<p><br /></p>

<h3 id="mse"><strong style="color: SteelBlue; font-size: 1.27em" class="bodyContents2" id="bodyContents21">MSE</strong></h3>
<p>The <strong>MSE</strong> minimizes the sum of <em><strong>squared differences</strong></em> between the predicted values and the target values.</p>
<p>$$L(\hat{y}, y) = \dfrac{1}{n} \sum_{i=1}^{n}\left(y_{i}-\hat{y}_ {i}\right)^{2}$$</p>
<p><img src="/main_files/dl/concepts/loss_funcs/1.png" alt="img" width="30%" class="center-image" /></p>

<p><button class="showText" value="show" onclick="showTextPopHide(event);">Derivation</button>
<img src="/main_files/dl/concepts/loss_funcs/5.png" alt="img" width="100%" hidden="" /><br />
<br /></p>

<h3 id="mae"><strong style="color: SteelBlue; font-size: 1.27em" class="bodyContents2" id="bodyContents22">MAE</strong></h3>
<p>The <strong>MAE</strong> minimizes the sum of <em><strong>absolute differences</strong></em> between the predicted values and the target values.</p>
<p>$$L(\hat{y}, y) = \dfrac{1}{n} \sum_{i=1}^{n}\vert y_{i}-\hat{y}_ {i}\vert$$</p>

<p><strong style="color: red">Properties:</strong></p>
<ul>
  <li>Solution may be <strong>Non-unique</strong></li>
  <li><strong>Robustness</strong> to outliers</li>
  <li><strong id="bodyContents22stability">Unstable Solutions:</strong>  <br />
  <button class="showText" value="show" onclick="showTextPopHide(event);">Explanation</button>
  <em hidden="">The instability property of the method of least absolute deviations means that, for a small horizontal adjustment of a datum, the regression line may jump a large amount. The method has continuous solutions for some data configurations; however, by moving a datum a small amount, one could “jump past” a configuration which has multiple solutions that span a region. After passing this region of solutions, the least absolute deviations line has a slope that may differ greatly from that of the previous line. In contrast, the least squares solutions is stable in that, for any small adjustment of a data point, the regression line will always move only slightly; that is, the regression parameters are continuous functions of the data.</em></li>
  <li><strong>Data-points “Latching”<sup id="fnref:3"><a href="#fn:3" class="footnote">1</a></sup>:</strong>
    <ul>
      <li><strong>Unique Solution</strong>:<br />
  If there are <script type="math/tex">k</script> <em><strong>features</strong></em> (including the constant), then at least one optimal regression surface will pass through <script type="math/tex">k</script> of the <em><strong>data points</strong></em>; unless there are multiple solutions.</li>
      <li><strong>Multiple Solutions</strong>:<br />
  The region of valid least absolute deviations solutions will be <strong>bounded by at least <script type="math/tex">k</script> lines</strong>, each of which <strong>passes through at least <script type="math/tex">k</script> data points</strong>.
        <blockquote>
          <p><a href="https://en.wikipedia.org/wiki/Least_absolute_deviations#Other_properties">Wikipedia</a></p>
        </blockquote>
      </li>
    </ul>
  </li>
  <li><br /></li>
</ul>

<h3 id="huber-loss"><strong style="color: SteelBlue; font-size: 1.27em" class="bodyContents2" id="bodyContents23">Huber Loss</strong></h3>

<p>AKA: <strong>Smooth Mean Absolute Error</strong></p>
<p>$$L(\hat{y}, y) = \left\{\begin{array}{cc}{\frac{1}{2}(y-\hat{y})^{2}} &amp; {\text { if }|(y-\hat{y})|&lt;\delta} \\ {\delta(y-\hat{y})-\frac{1}{2} \delta} &amp; {\text { otherwise }}\end{array}\right.$$</p>

<p><strong style="color: red">Properties:</strong></p>
<ul>
  <li>It’s <strong style="color: green">less sensitive</strong> to outliers than the <em>MSE</em> as it treats error as square only inside an interval.</li>
</ul>

<p><strong style="color: red">Code:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">Huber</span><span class="p">(</span><span class="n">yHat</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">delta</span><span class="o">=</span><span class="mf">1.</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">yHat</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">delta</span><span class="p">,</span><span class="o">.</span><span class="mi">5</span><span class="o">*</span><span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">yHat</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="p">,</span> <span class="n">delta</span><span class="o">*</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">yHat</span><span class="p">)</span><span class="o">-</span><span class="mf">0.5</span><span class="o">*</span><span class="n">delta</span><span class="p">))</span>
</code></pre></div></div>
<p><br /></p>

<h3 id="kl-divergence"><strong style="color: SteelBlue; font-size: 1.27em" class="bodyContents2" id="bodyContents25">KL-Divergence</strong></h3>

<p>$$L(\hat{y}, y) = $$</p>

<p><br /></p>

<h3 id="analysis-and-discussion"><strong style="color: SteelBlue; font-size: 1.27em" class="bodyContents2" id="bodyContents26">Analysis and Discussion</strong></h3>
<p><strong style="color: red">MSE vs MAE:</strong></p>

<table>
  <tbody>
    <tr>
      <td><strong>MSE</strong></td>
      <td><strong>MAE</strong></td>
    </tr>
    <tr>
      <td>Sensitive to <em>outliers</em></td>
      <td>Robust to <em>outliers</em></td>
    </tr>
    <tr>
      <td>Differentiable Everywhere</td>
      <td>Non-Differentiable at <script type="math/tex">0</script></td>
    </tr>
    <tr>
      <td>Stable<sup id="fnref:1"><a href="#fn:1" class="footnote">2</a></sup> Solutions</td>
      <td>Unstable Solutions</td>
    </tr>
    <tr>
      <td>Unique Solution</td>
      <td>Possibly multiple<sup id="fnref:2"><a href="#fn:2" class="footnote">3</a></sup> solutions</td>
    </tr>
  </tbody>
</table>

<ul>
  <li><strong>Statistical Efficiency</strong>:
    <ul>
      <li>“For normal observations MSE is about <script type="math/tex">12\%</script> more efficient than MAE” - Fisher</li>
      <li><script type="math/tex">1\%</script> Error is enough to make MAE more efficient</li>
      <li>2/1000 bad observations, make the median more efficient than the mean</li>
    </ul>
  </li>
  <li>Subgradient methods are slower than gradient descent
    <ul>
      <li>you get a lot better convergence rate guarantees for MSE</li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h3 id="notes"><strong style="color: SteelBlue; font-size: 1.27em" class="bodyContents2" id="bodyContents27">Notes</strong></h3>

<hr />

<h1 id="content3">Loss Functions for Classification</h1>

<p><img src="/main_files/dl/concepts/loss_funcs/0.png" alt="img" width="65%" id="losses" /><br />
<br /></p>

<h3 id="0-1-loss"><strong style="color: SteelBlue; font-size: 1.27em" class="bodyContents3" id="bodyContents311"><script type="math/tex">0-1</script> Loss</strong></h3>

<p>$$L(\hat{y}, y) = I(\hat{y} \neq y) = \left\{\begin{array}{ll}{0} &amp; {\hat{y}=y} \\ {1} &amp; {\hat{y} \neq y}\end{array}\right.$$</p>
<p><br /></p>

<h3 id="mse-1"><strong style="color: SteelBlue; font-size: 1.27em" class="bodyContents3" id="bodyContents31">MSE</strong></h3>

<p>We can write the loss in terms of the margin <script type="math/tex">m = y\hat{y}</script>:<br />
<script type="math/tex">L(\hat{y}, y)=(y - \hat{y})^{2}=(1-y\hat{y})^{2}=(1-m)^{2}</script></p>
<blockquote>
  <p>Since <script type="math/tex">y \in {-1,1} \implies y^2 = 1</script></p>
</blockquote>

<p>$$L(\hat{y}, y) = (1-y \hat{y})^{2}$$</p>

<p><img src="/main_files/dl/concepts/loss_funcs/1.png" alt="img" width="30%" class="center-image" /></p>

<p><button class="showText" value="show" onclick="showTextPopHide(event);">Derivation</button>
<img src="/main_files/dl/concepts/loss_funcs/5.png" alt="img" width="100%" hidden="" /></p>

<p><br /></p>

<h3 id="hinge-loss"><strong style="color: SteelBlue; font-size: 1.27em" class="bodyContents3" id="bodyContents32">Hinge Loss</strong></h3>

<p>$$L(\hat{y}, y) = \max (0,1-y \hat{y})=|1-y \hat{y}|_ {+}$$</p>

<p><strong style="color: red">Properties:</strong></p>
<ul>
  <li>Continuous, Convex, Non-Differentiable</li>
</ul>

<p><img src="/main_files/dl/concepts/loss_funcs/3.png" alt="img" width="30%" class="center-image" /><br />
<br /></p>

<h3 id="logistic-loss"><strong style="color: SteelBlue; font-size: 1.27em" class="bodyContents3" id="bodyContents33">Logistic Loss</strong></h3>

<p>AKA: <strong>Log-Loss</strong>, <strong>Logarithmic Loss</strong></p>

<p>$$L(\hat{y}, y) = \log{\left(1+e^{-y \hat{y}}\right)}$$</p>

<p><img src="/main_files/dl/concepts/loss_funcs/2.png" alt="img" width="30%" class="center-image" /></p>

<p id="lst-p"><strong style="color: red">Properties:</strong></p>
<ul>
  <li>The logistic loss function does not assign zero penalty to any points. Instead, functions that correctly classify points with high confidence (i.e., with high values of <script type="math/tex">{\displaystyle \vert f({\vec {x}})\vert}</script>) are penalized less. This structure leads the logistic loss function to be sensitive to outliers in the data.<br />
<br /></li>
</ul>

<h3 id="cross-entropy-log-loss"><strong style="color: SteelBlue; font-size: 1.27em" class="bodyContents3" id="bodyContents34">Cross-Entropy (Log Loss)</strong></h3>

<p>$$L(\hat{y}, y) = -\sum_{i} y_i \log \left(\hat{y}_ {i}\right)$$</p>

<p><strong style="color: red">Binary Cross-Entropy:</strong></p>
<p>$$L(\hat{y}, y) = -\left[y \log \hat{y}+\left(1-y\right) \log \left(1-\hat{y}_ {n}\right)\right]$$</p>

<p><img src="/main_files/dl/concepts/loss_funcs/4.png" alt="img" width="30%" class="center-image" /></p>

<p><strong style="color: red">Cross-Entropy and Negative-Log-Probability:</strong><br />
The <strong>Cross-Entropy</strong> is equal to the <strong>Negative-Log-Probability</strong> (of predicting the true class) in the case that the true distribution that we are trying to match is <em><strong>peaked at a single point</strong></em> and is <em><strong>identically zero everywhere else</strong></em>; this is usually the case in ML when we are using a <em>one-hot encoded vector</em> with one class <script type="math/tex">y = [0 \: 0 \: \ldots \: 0 \: 1 \: 0 \: \ldots \: 0]</script> peaked at the <script type="math/tex">j</script>-th position <br />
<script type="math/tex">\implies</script></p>
<p>$$L(\hat{y}, y) = -\sum_{i} y_i \log \left(\hat{y}_ {i}\right) = - \log (\hat{y}_ {j})$$</p>

<p><strong style="color: red">Cross-Entropy and Log-Loss:</strong>  <br />
The <strong>Cross-Entropy</strong> is equal to the <strong>Log-Loss</strong> in the case of <script type="math/tex">0, 1</script> classification.</p>

<p><strong>Equivalence of <em>Binary Cross-Entropy</em> and <em>Logistic-Loss</em>:</strong><br />
Given <script type="math/tex">p \in\{y, 1-y\}</script> and <script type="math/tex">q \in\{\hat{y}, 1-\hat{y}\}</script>:</p>
<p>$$H(p,q)=-\sum_{x }p(x)\,\log q(x) = -y \log \hat{y}-(1-y) \log (1-\hat{y}) = L(\hat{y}, y)$$</p>

<ul>
  <li>Notice the following property of the <strong>logistic function</strong> <script type="math/tex">\sigma</script> (used in derivation below): <br />
  <script type="math/tex">\sigma(-x) = 1-\sigma(x)</script></li>
</ul>

<p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Derivation</button>
<em hidden="">Given:</em></p>
<ul hidden="">
  <li><script type="math/tex">\hat{y} = \sigma(yf(x))</script>,<sup id="fnref:5"><a href="#fn:5" class="footnote">4</a></sup></li>
  <li><script type="math/tex">y \in \{-1, 1\}</script>,</li>
  <li><script type="math/tex">\hat{y}' = \sigma(f(x))</script>,</li>
  <li><script type="math/tex">% <![CDATA[
y' = (1+y)/2 = \left\{\begin{array}{ll}{1} & {\text { for }} y' = 1 \\ {0} & {\text { for }} y = -1\end{array}\right. \in \{0, 1\} %]]></script><sup id="fnref:4"><a href="#fn:4" class="footnote">5</a></sup></li>
  <li>We start with the modified binary cross-entropy<br />
  <script type="math/tex">% <![CDATA[
\begin{aligned} -y' \log \hat{y}'-(1-y') \log (1-\hat{y}') &= \left\{\begin{array}{ll}{-\log\hat{y}'} & {\text { for }} y' = 1 \\ {-\log(1-\hat{y}')} & {\text { for }} y' = 0\end{array}\right. \\ \\
  &= \left\{\begin{array}{ll}{-\log\sigma(f(x))} & {\text { for }} y' = 1 \\ {-\log(1-\sigma(f(x)))} & {\text { for }} y' = 0\end{array}\right. \\ \\
  &= \left\{\begin{array}{ll}{-\log\sigma(1\times f(x))} & {\text { for }} y' = 1 \\ {-\log(\sigma((-1)\times f(x)))} & {\text { for }} y' = 0\end{array}\right. \\ \\
  &= \left\{\begin{array}{ll}{-\log\sigma(yf(x))} & {\text { for }} y' = 1 \\ {-\log(\sigma(yf(x)))} & {\text { for }} y' = 0\end{array}\right. \\ \\
  &= \left\{\begin{array}{ll}{-\log\hat{y}} & {\text { for }} y' = 1 \\ {-\log\hat{y}} & {\text { for }} y' = 0\end{array}\right. \\ \\
  &= -\log\hat{y} \\ \\
  &= \log\left[\dfrac{1}{\hat{y}}\right] \\ \\
  &= \log\left[\hat{y}^{-1}\right] \\ \\
  &= \log\left[\sigma(yf(x))^{-1}\right] \\ \\
  &= \log\left[ \left(\dfrac{1}{1+e^{-yf(x)}}\right)^{-1}\right] \\ \\
  &= \log \left(1+e^{-yf(x)}\right)\end{aligned} %]]></script></li>
</ul>

<blockquote>
  <p><a href="https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a">Reference (Understanding binary-cross-entropy-log-loss)</a></p>
</blockquote>

<p><strong style="color: red">Cross-Entropy as Negative-Log-Likelihood (w/ equal probability outcomes):</strong></p>

<p><strong style="color: red">Cross-Entropy and KL-Div:</strong><br />
When comparing a distribution <script type="math/tex">{\displaystyle q}</script> against a fixed reference distribution <script type="math/tex">{\displaystyle p}</script>, cross entropy and KL divergence are identical up to an additive constant (since <script type="math/tex">{\displaystyle p}</script> is fixed): both take on their minimal values when <script type="math/tex">{\displaystyle p=q}</script>, which is <script type="math/tex">{\displaystyle 0}</script> for KL divergence, and <script type="math/tex">{\displaystyle \mathrm {H} (p)}</script> for cross entropy.</p>
<blockquote>
  <p>Basically, minimizing either will result in the same solution.</p>
</blockquote>

<p><strong style="color: red">Cross-Entropy VS MSE (&amp; Classification Loss):</strong><br />
Basically, CE &gt; MSE because the gradient of MSE <script type="math/tex">z(1-z)</script> leads to saturation when then output <script type="math/tex">z</script> of a neuron is near <script type="math/tex">0</script> or <script type="math/tex">1</script> making the gradient very small and, thus, slowing down training.<br />
CE &gt; Class-Loss because Class-Loss is binary and doesn’t take into account <em>“how well”</em> are we actually approximating the probabilities as opposed to just having the target class be slightly higher than the rest (e.g. <script type="math/tex">[c_1=0.3, c_2=0.3, c_3=0.4]</script>).</p>
<ul>
  <li><a href="https://jamesmccaffrey.wordpress.com/2013/11/05/why-you-should-use-cross-entropy-error-instead-of-classification-error-or-mean-squared-error-for-neural-network-classifier-training/">Why You Should Use Cross-Entropy Error Instead Of Classification Error Or Mean Squared Error For Neural Network Classifier Training</a></li>
</ul>

<p><br /></p>

<h3 id="exponential-loss"><strong style="color: SteelBlue; font-size: 1.27em" class="bodyContents3" id="bodyContents35">Exponential Loss</strong></h3>

<p>$$L(\hat{y}, y) = e^{-\beta y \hat{y}}$$</p>
<p><br /></p>

<h3 id="perceptron-loss"><strong style="color: SteelBlue; font-size: 1.27em" class="bodyContents3" id="bodyContents36">Perceptron Loss</strong></h3>

<p>$${\displaystyle L(z, y_i) = {\begin{cases}0&amp;{\text{if }}\ y_i\cdot z_i \geq 0\\-y_i z&amp;{\text{otherwise}}\end{cases}}}$$</p>
<p><br /></p>

<h3 id="notes-1"><strong style="color: SteelBlue; font-size: 1.27em" class="bodyContents3" id="bodyContents37">Notes</strong></h3>
<ul>
  <li><strong>Logistic loss</strong> diverges faster than <strong>hinge loss</strong> <a href="#losses">(image)</a>. So, in general, it will be more sensitive to outliers. <a href="https://towardsdatascience.com/support-vector-machine-vs-logistic-regression-94cc2975433f">Reference. Bad info?</a></li>
</ul>

<p><br /><br /></p>

<div class="footnotes">
  <ol>
    <li id="fn:3">
      <p><a href="http://articles.adsabs.harvard.edu//full/1982AJ.....87..928B/0000936.000.html">Reference</a> <a href="#fnref:3" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:1">
      <p><a href="#bodyContents22stability">Stability</a> <a href="#fnref:1" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>Reason is that the errors are equally weighted; so, tilting the regression line (within a region) will decrease the distance to a particular point and will increase the distance to other points by the same amount. <a href="#fnref:2" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:5">
      <p><script type="math/tex">f(x) = w^Tx</script> in logistic regression <a href="#fnref:5" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:4">
      <p>We have to redefine the indicator/target variable to establish the equality. <a href="#fnref:4" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>


      <footer class="site-footer">
    <!--   <span class="site-footer-owner"><a href="http://localhost:8889">Ahmad Badary</a> is maintained by <a href="http://ahmedbadary.ml">Ahmad Badary</a>.</span> -->
    
<!--  -->
    <div class="wrapper">
        <div class="footer-col-wrapper">
            <div class="footer-col footer-col-1">
            <span class="site-footer-owner"><a href="http://localhost:8889">Site</a> maintained by <a href="http://ahmedbadary.ml">Ahmad Badary</a>.</span>
    <span class="site-footer-credits">
        <p>
            &copy; 2017. All rights reserved.
        </p> 
    </span>
            </div>
            <div class="footer-col footer-col-2">
            <div><p>         </p></div>
            </div>
            <div class="footer-col footer-col-3">
                <ul class="social-media-list">
                    
                      <li>
                        <a href="https://github.com/AhmedBadary">
                          <i class="fa fa-github"></i> GitHub
                        </a>
                      </li>
                    
                    
                      <li>
                        <a href="https://linkedin.com/in/ahmad-badary-656098121/">
                          <i class="fa fa-linkedin"></i> LinkedIn
                        </a>
                      </li>
                    
                    
                      <li>
                        <a href="https://www.facebook.com/ahmed.thabet.94">
                          <i class="fa fa-facebook"></i> Facebook
                        </a>
                      </li>
                    
                </ul>
            </div>
        </div>
    </div>
<!--  -->
</footer>


    </section>

  </body>

<!-- Table of Content Script -->
<script type="text/javascript">
var bodyContents = $(".bodyContents1");
$("<ol>").addClass("TOC1ul").appendTo(".TOC1");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC1ul");
     });
// 
var bodyContents = $(".bodyContents2");
$("<ol>").addClass("TOC2ul").appendTo(".TOC2");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC2ul");
     });
// 
var bodyContents = $(".bodyContents3");
$("<ol>").addClass("TOC3ul").appendTo(".TOC3");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC3ul");
     });
//
var bodyContents = $(".bodyContents4");
$("<ol>").addClass("TOC4ul").appendTo(".TOC4");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC4ul");
     });
//
var bodyContents = $(".bodyContents5");
$("<ol>").addClass("TOC5ul").appendTo(".TOC5");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC5ul");
     });
//
var bodyContents = $(".bodyContents6");
$("<ol>").addClass("TOC6ul").appendTo(".TOC6");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC6ul");
     });
//
var bodyContents = $(".bodyContents7");
$("<ol>").addClass("TOC7ul").appendTo(".TOC7");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC7ul");
     });
//
var bodyContents = $(".bodyContents8");
$("<ol>").addClass("TOC8ul").appendTo(".TOC8");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC8ul");
     });
//
var bodyContents = $(".bodyContents9");
$("<ol>").addClass("TOC9ul").appendTo(".TOC9");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC9ul");
     });

</script>

<!-- VIDEO BUTTONS SCRIPT -->
<script type="text/javascript">
  function iframePopInject(event) {
    var $button = $(event.target);
    // console.log($button.parent().next());
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $figure = $("<div>").addClass("video_container");
        $iframe = $("<iframe>").appendTo($figure);
        $iframe.attr("src", $button.attr("src"));
        // $iframe.attr("frameborder", "0");
        $iframe.attr("allowfullscreen", "true");
        $iframe.css("padding", "4px 6px");
        $button.next().css("display", "block");
        $figure.appendTo($button.next());
        $button.text("Hide Video")
    } else {
        $button.attr('value', 'show');
        $button.next().html("");
        $button.text("Show Video")
    }
}
</script>

<!-- BUTTON TRY -->
<script type="text/javascript">
  function iframePopA(event) {
    event.preventDefault();
    var $a = $(event.target).parent();
    console.log($a);
    if ($a.attr('value') == 'show') {
        $a.attr('value', 'hide');
        $figure = $("<div>");
        $iframe = $("<iframe>").addClass("popup_website_container").appendTo($figure);
        $iframe.attr("src", $a.attr("href"));
        $iframe.attr("frameborder", "1");
        $iframe.attr("allowfullscreen", "true");
        $iframe.css("padding", "4px 6px");
        $a.next().css("display", "block");
        $figure.appendTo($a.next().next());
        // $a.text("Hide Content")
        $('html, body').animate({
            scrollTop: $a.offset().top
        }, 1000);
    } else {
        $a.attr('value', 'show');
        $a.next().next().html("");
        // $a.text("Show Content")
    }

    $a.next().css("display", "inline");
}
</script>


<!-- TEXT BUTTON SCRIPT - INJECT -->
<script type="text/javascript">
  function showTextPopInject(event) {
    var $button = $(event.target);
    var txt = $button.attr("input");
    console.log(txt);
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $p = $("<p>");
        $p.html(txt);
        $button.next().css("display", "block");
        $p.appendTo($button.next());
        $button.text("Hide Content")
    } else {
        $button.attr('value', 'show');
        $button.next().html("");
        $button.text("Show Content")
    }

}
</script>

<!-- TEXT BUTTON SCRIPT - HIDDEN / HIDE / SHOW / HIDE/SHOW -->
<script type="text/javascript">
  function showTextPopHide(event) {
    var $button = $(event.target);
    // var txt = $button.attr("input");
    var txt = $button.text();
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $button.next().removeAttr("hidden");
        $button.text(txt + " - Hide Content");
    } else {
        $button.attr('value', 'show');
        $button.next().attr("hidden", "");
        $button.text(txt.replace(" - Hide Content",""));
    }
}
</script>

<!-- TEXT BUTTON SCRIPT - HIDDEN / HIDE / SHOW / HIDE/SHOW -->
<script type="text/javascript">
  function showText_withParent_PopHide(event) {
    var $button = $(event.target);
    var $parent = $button.parent();
    var txt = $button.text();
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $parent.next().removeAttr("hidden");
        $button.text(txt + " - Hide Content");
    } else {
        $button.attr('value', 'show');
        $parent.next().attr("hidden", "");
        $button.text(txt.replace(" - Hide Content",""));
    }
}
</script>

<!-- Print / Printing / printme -->
<!-- <script type="text/javascript">
i = 0

for (var i = 1; i < 6; i++) {
    var bodyContents = $(".bodyContents" + i);
    $("<p>").addClass("TOC1ul")  .appendTo(".TOC1");
    bodyContents.each(function(index, element) {
        var paragraph = $(element);
        $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC1ul");
         });
} 
</script>
 -->
 
</html>

