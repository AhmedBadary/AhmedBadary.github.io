<!DOCTYPE html>
<html lang="en-us">
  <head>
  <meta charset="UTF-8">
  <title>Ahmad Badary</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <link rel="stylesheet" href="/css/normalize.css">
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="/css/cayman.css">
  <link rel="stylesheet" href="/css/style.css">
  <link rel="shortcut icon" href="/main_files/favicon.ico" />
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="/css/customStyle.css">
  <title> » Ahmad Badary</title>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
</head>

  <body>
    <nav class="main-nav">
    <a href="https://ahmedbadary.github.io/" class="main-nav-logo">
        <img src="/main_files/logo.png">
    </a>
    <ul id="menu-main" class="main-nav-items">
        <li id="menu-item-1859" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-home menu-item-1859">
            <a href="/">Home</a>
        </li>
        <li id="menu-item-2869" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-2869">
            <a href="/work">Work</a>
        </li>
        <li id="menu-item-1892" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1892">
            <a href="/projects">Projects</a>
        </li>
        <li id="menu-item-1858" class="menu-item menu-item-type-post_type menu-item-object-page current_page_parent menu-item-1858">
            <a href="/blog">Blog</a>
        </li>
        <li id="menu-item-1862" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1862">
            <a href="/about">About</a>
        </li>
    </ul>
</nav>


<section class="page-header">
  <h1 class="project-name">Inference and Approximate Inference</h1>
  <h2 class="project-tagline"></h2>
  <a href="/#" class="btn">Home</a>
  <a href="/work" class="btn">Work-Space</a>
  <a href= /work_files/research/dl/concepts.html class="btn">Previous</a>
</section>

<!-- <div>
  <ul class="posts">
    
      <li><span>02 Jan 2014</span> &raquo; <a href="/2014/01/02/introducing-Ahmad/">Introducing Ahmad</a></li>
    
  </ul>
</div> -->


    <section class="main-content">
      
      <div class="TOC">
  <h1 id="table-of-contents">Table of Contents</h1>
  <ul class="TOC1">
    <li><a href="#content1">Inference and Approximate Inference</a></li>
  </ul>
  <ul class="TOC2">
    <li><a href="#content2">Variational Inference and Learning</a></li>
  </ul>
  <ul class="TOC3">
    <li><a href="#content3">Learned Approximate Inference</a></li>
  </ul>
  <ul class="TOC4">
    <li><a href="#content4">Mathematics of Approximate Inference Methods</a></li>
  </ul>
</div>

<hr />
<hr />

<p id="lst-p"><strong style="color: red">Resources:</strong></p>
<ul>
  <li><a href="http://bjlkeng.github.io/posts/variational-bayes-and-the-mean-field-approximation/">Variational Bayes and The Mean-Field Approximation (blog)</a></li>
  <li><a href="https://www.cs.cmu.edu/~epxing/Class/10708-17/notes-17/10708-scribe-lecture13.pdf">Variational Inference: Mean Field Approximation (Lecture Notes)</a></li>
  <li><a href="https://people.eecs.berkeley.edu/~wainwrig/Papers/WaiJor08_FTML.pdf">Graphical Models, Exponential Families, and Variational Inference (M Jordan)</a></li>
  <li><a href="https://www.reddit.com/r/MachineLearning/comments/7dd45h/d_a_cookbook_for_machine_learning_a_list_of_ml/dpyc13e/?context=8&amp;depth=9">Why is the Variational Bound Tight: the variational bound compared to the original error surface (reddit!)</a></li>
</ul>

<h2 id="content1">Inference and Approximate Inference</h2>

<ol>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents11">Inference:</strong><br />
 <strong>Inference</strong> usually refers to <span style="color: purple">computing the probability distribution over one set of variables given another</span>.</p>

    <p id="lst-p"><strong style="color: red">Goals:</strong></p>
    <ul>
      <li>Computing the likelihood of observed data (in models with latent variables).</li>
      <li>Computing the marginal distribution over a given subset of nodes in the model.</li>
      <li>Computing the conditional distribution over a subsets of nodes given a disjoint subset of nodes.</li>
      <li>Computing a mode of the density (for the above distributions).</li>
    </ul>

    <p id="lst-p"><strong style="color: red">Approaches:</strong></p>
    <ul>
      <li><strong>Exact inference algorithms:</strong>
        <ul>
          <li>Brute force</li>
          <li>The elimination algorithm</li>
          <li>Message passing (sum-product algorithm, belief propagation)</li>
          <li>Junction tree algorithm</li>
        </ul>
      </li>
      <li><strong>Approximate inference algorithms</strong>:
        <ul>
          <li>Loopy belief propagation</li>
          <li>Variational (Bayesian) inference \(+\) mean field approximations</li>
          <li>Stochastic simulation / sampling /  MCMC</li>
        </ul>
      </li>
    </ul>

    <p><strong style="color: red">Inference in Deep Learning - Formulation:</strong><br />
 In the context of <strong>Deep Learning</strong>, we usually have two <strong>sets of variables</strong>:<br />
 (1) Set of <em><strong>visible</strong></em> (<em><strong>observed</strong></em>) <strong>variables</strong>: \(\: \boldsymbol{v}\)<br />
 (2) Set of <em><strong>latent</strong></em> <strong>variables</strong>: \(\: \boldsymbol{h}\)</p>

    <p><strong>Inference</strong> in DL corresponds to <span style="color: goldenrod">computing the <em><strong>likelihood</strong></em> of <strong>observed data</strong> \(p(\boldsymbol{v})\)</span>.</p>

    <p>When training <strong>probabilistic models with <em>latent variables</em></strong>, we are usually interested in computing</p>
    <p>$$p(\boldsymbol{h} \vert \boldsymbol{v})$$</p>
    <p>where \(\boldsymbol{h}\) are the latent variables, and \(\boldsymbol{v}\) are the observed (visible) variables (data).<br />
 <br /></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents12">The Challenge of Inference:</strong><br />
 <strong style="color: red">Motivation - The Challenge of Inference:</strong><br />
 The <strong>challenge of inference</strong> usually refers to the difficult problem of computing \(p(\boldsymbol{h} \vert \boldsymbol{v})\) or taking expectations wrt it.<br />
 Such operations are often necessary for tasks like <strong>Maximum Likelihood Learning</strong>.</p>

    <p id="lst-p"><strong>Intractable Inference:</strong><br />
 In DL, intractable inference problems, usually, arise from <span style="color: purple">interactions between <em><strong>latent</strong></em> <strong>variables</strong> in a structured graphical model</span>.<br />
 These interactions are usually due to:</p>
    <ul>
      <li><strong>Directed Models</strong>: <em>“explaining away”</em> interactions between <em><strong>mutual ancestors</strong></em> of the <strong>same visible unit</strong>.</li>
      <li><strong>Undirected Models</strong>: direct interactions between the latent variables.</li>
    </ul>

    <p id="lst-p"><strong>In Models:</strong></p>
    <ul>
      <li><strong>Tractable Inference:</strong>
        <ul>
          <li>Many <em><strong>simple</strong></em> graphical models with only <span style="color: purple"><strong>one hidden layer</strong></span> have tractable inference.<br />
  E.g. <strong>RBMs</strong>, <strong>PPCA</strong>.</li>
        </ul>
      </li>
      <li><strong>Intractable Inference</strong>:
        <ul>
          <li>Most graphical models with <span style="color: purple"><strong>multiple hidden layers</strong></span> with <strong>hidden variables</strong> have intractable <em><strong>posterior distributions</strong></em>.<br />
  <strong>Exact inference</strong> requires an <strong>exponential time</strong>.<br />
  E.g. <strong>DBMs</strong>, <strong>DBNs</strong>.</li>
          <li>Even some models with only a <span style="color: goldenrod"><strong>single</strong> layer</span> can be intractable.<br />
  E.g. <strong>Sparse Coding</strong></li>
        </ul>
      </li>
    </ul>

    <p><button class="showText" value="show" onclick="showTextPopHide(event);">Interactions in Graphical Models</button>
 <img src="https://cdn.mathpix.com/snip/images/1GkbyQub5WitledsQpfv77ARrYUn67NzFhCFy_TMeUc.original.fullsize.png" alt="img" width="100%" hidden="" /></p>

    <p id="lst-p"><strong>Computing the Likelihood of Observed Data:</strong><br />
 We usually want to compute the likelihood of the observed data \(p(\boldsymbol{v})\), equivalently the log-likelihood \(\log p(\boldsymbol{v})\).<br />
 This usually requires marginalizing out \(\boldsymbol{h}\).<br />
 This problem is <strong>intractable</strong> (difficult) if it is <em>costly</em> to <strong>marginalize</strong> \(\boldsymbol{h}\).</p>
    <ul>
      <li><strong>Data Likelihood</strong>:  (<span style="color: purple">intractable</span>)<br />
  \(p_{\theta}(\boldsymbol{v})=\int_\boldsymbol{h} p_{\theta}(h) p_{\theta}(v \vert h) dh\)</li>
      <li><strong>Marginal Likelihood (evidence)</strong>: is the data likelihood \(p_{\theta}(\boldsymbol{v})\) (<span style="color: purple">intractable</span>)  <br />
  \(\int_\boldsymbol{h} p_{\theta}(h) p_{\theta}(v \vert h) dh\)</li>
      <li><strong>Prior</strong>:<br />
  \(p(\boldsymbol{h})\)</li>
      <li>(Conditional) <strong>Likelihood</strong>:<br />
  \(p_{\theta}(\boldsymbol{v} \vert h)\)</li>
      <li><strong>Joint</strong>:<br />
  \(p_{\theta}(\boldsymbol{v}, \boldsymbol{h})\)</li>
      <li><strong>Posterior</strong>: (<span style="color: purple">intractable</span>)  <br />
  \(p_{\theta}(\boldsymbol{h} \vert \boldsymbol{v})=\frac{p_{\theta}(\boldsymbol{v}, \boldsymbol{h})}{p_{\theta}(\boldsymbol{v})}=\frac{p_{\theta}(\boldsymbol{v} \vert h) p_{\theta}(h)}{\int_{\boldsymbol{h}} p_{\theta}(h) p_{\theta}(x \vert h) d h}\)</li>
    </ul>

    <p><br /></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents122">Approximate Inference:</strong><br />
<strong>Approximate Inference</strong> is an important and practical approach to confronting the <strong>challenge of (intractable) inference</strong>.<br />
It poses <strong>exact inference</strong> as an <strong>optimization problem</strong>, and aims to <em><strong>approximate</strong></em> the underlying optimization problem.<br />
<br /></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents13">Inference as Optimization:</strong><br />
 <strong>Exact inference</strong> can be described as an <strong>optimization problem</strong>.</p>

    <ul>
      <li><strong>Inference Problem:</strong>
        <ul>
          <li>Compute the <strong>log-likelihood</strong> of the <strong>observed data</strong>, \(\log p(\boldsymbol{v} ; \boldsymbol{\theta})\).<br />
  Can be intractable to marginalize \(\boldsymbol{h}\).</li>
        </ul>
      </li>
      <li><strong>Inference Problem as Optimization - Core Idea</strong>:
        <ul>
          <li>Choose a family of distributions over the <em><strong>latent</strong></em> <strong>variables</strong> \(\boldsymbol{h}\) with its own set of variational parameters \(\boldsymbol{v}\): \(q(\boldsymbol{h} \vert \boldsymbol{v})\).</li>
          <li>Find the setting of the parameters that makes our approximation closest to the posterior distribution over the latent variables \(p(\boldsymbol{h} \vert \boldsymbol{v})\).<br />
  I.E. <strong>Optimization</strong></li>
          <li>Use learned \(q\) in place of the posterior (as an approximation).</li>
        </ul>
      </li>
      <li><strong>Optimization - Fitting \(q\) to the posterior \(p\)</strong>:
        <ul>
          <li>Optimize \(q\) to approximate \(p(\boldsymbol{h} \vert \boldsymbol{v})\)</li>
          <li><strong>Similarity Measure:</strong> use the <em><strong>KL-Divergence</strong></em> as a similarity measure between the two distributions
            <p>$$D_{\mathrm{KL}}(q \| p) = \mathrm{E}_ {h \sim q}\left[\log \frac{q(h)}{p(h\vert {v})}\right] =\int_{h} q(h) \log \left(\frac{q(h)}{p(h\vert {v})}\right) dh$$</p>
          </li>
          <li><strong>Intractability:</strong> minimizing the KL Divergence (above) is an intractable problem.<br />
  Because the expression contains the intractable term \(p(\boldsymbol{h}\vert \boldsymbol{v})\) which we were trying to avoid.</li>
        </ul>
      </li>
      <li><strong>Evidence Lower Bound</strong>:
        <ul>
          <li>We rewrite the KL Divergence expression in terms of log-likelihood of the data:
            <p>$$\begin{aligned} D_{\mathrm{KL}}(q \| p) &amp;=\int_{\boldsymbol{h}} q(h) \log \frac{q(h)}{p(h \vert v)} dh \\ &amp;=\int_{\boldsymbol{h}} q(h) \log \frac{q(h)}{p(h, v)} dh+\int_{\boldsymbol{h}} q(h) \log p(v) dh \\ &amp;=\int_{\boldsymbol{h}} q(h) \log \frac{q(h)}{p(h, v)} dh+\log p(\boldsymbol{v}) \end{aligned}$$</p>
            <p>where we’re using Bayes theorem on the second line and the RHS integral simplifies because it’s simply integrating over the support of \(q\) and \(p\) is not a function of \(h\).<br />
  Thus,</p>
            <p>$$\log p(\boldsymbol{v}) = D_{\mathrm{KL}}(q \| p) - \int_{\boldsymbol{h}} q(h) \log \frac{q(h)}{p(h, v)} dh$$</p>
          </li>
          <li>Notice that since the KL-Divergence is <span style="color: purple"><em>Non-Negative</em></span>:
            <p>$$\begin{align}
      D_{\mathrm{KL}}(q \| p) &amp;\geq 0 \\
      D_{\mathrm{KL}}(q \| p) - \int_{\boldsymbol{h}} q(h) \log \frac{q(h)}{p(h, v)} dh &amp;\geq - \int_{\boldsymbol{h}} q(h) \log \frac{q(h)}{p(h, v)} dh \\
      \log p(\boldsymbol{v}) &amp;\geq - \int_{\boldsymbol{h}} q(h) \log \frac{q(h)}{p(h, v)} dh 
      \end{align}
      $$</p>
            <p>Thus, the term \(- \int_{\boldsymbol{h}} q(h) \log \frac{q(h)}{p(h, v)} dh\) provides a <strong style="color: goldenrod">lower-bound</strong> on the <strong>log likelihood of the data</strong>.</p>
          </li>
          <li>We rewrite the term as:
            <p>$$\mathcal{L}(\boldsymbol{v}, \boldsymbol{\theta}, q) = - \int_{\boldsymbol{h}} q(h) \log \frac{q(h)}{p(h, v)} dh$$</p>
            <p>the <strong style="color: goldenrod">Evidence Lower Bound (ELBO)</strong> AKA <span style="color: goldenrod">Variational Free Energy</span>.<br />
  Thus,</p>
            <p>$$\log p(\boldsymbol{v}) \geq \mathcal{L}(\boldsymbol{v}, \boldsymbol{\theta}, q)$$</p>
          </li>
          <li><strong>The Evidence Lower Bound</strong> can also be defined as:
            <p>$$\begin{align}
      \mathcal{L}(\boldsymbol{v}, \boldsymbol{\theta}, q) &amp;= - \int_{\boldsymbol{h}} q(h) \log \frac{q(h)}{p(h, v)} dh \\
      &amp;= \log p(\boldsymbol{v} ; \boldsymbol{\theta})-D_{\mathrm{KL}}(q(\boldsymbol{h} \vert \boldsymbol{v}) \| p(\boldsymbol{h} \vert \boldsymbol{v} ; \boldsymbol{\theta})) \\
      &amp;= \mathbb{E}_ {\mathbf{h} \sim q}[\log p(\boldsymbol{h}, \boldsymbol{v})]+H(q)  
      \end{align}
      $$ </p>
            <p>The latter being the <strong>canonical definition</strong> of the ELBO.</p>
          </li>
        </ul>
      </li>
      <li><strong>Inference with the Evidence Lower Bound</strong>:
        <ul>
          <li>For an appropriate choice of \(q, \mathcal{L}\) is <span style="color: purple"><strong>tractable</strong></span> to compute.</li>
          <li>For any choice of \(q, \mathcal{L}\) provides a lower bound on the likelihood</li>
          <li>For \(q(\boldsymbol{h} \vert \boldsymbol{v})\) that are better approximations of \(p(\boldsymbol{h} \vert \boldsymbol{v}),\) the lower bound \(\mathcal{L}\) will be tighter<br />
  I.E. closer to \(\log p(\boldsymbol{v})\).</li>
          <li>When \(q(\boldsymbol{h} \vert \boldsymbol{v})=p(\boldsymbol{h} \vert \boldsymbol{v}),\) the approximation is perfect, and \(\mathcal{L}(\boldsymbol{v}, \boldsymbol{\theta}, q)=\log p(\boldsymbol{v} ; \boldsymbol{\theta})\).</li>
          <li><span style="color: goldenrod">Maximizing the ELBO minimizes the KL-Divergence \(D_{\mathrm{KL}}(q \| p)\)</span>.</li>
        </ul>
      </li>
      <li><strong>Inference</strong>:<br />
  We can thus think of inference as the procedure for finding the \(q\) that maximizes \(\mathcal{L}\):
        <ul>
          <li><strong>Exact Inference</strong>: maximizes \(\mathcal{L}\) perfectly by searching over a family of functions \(q\) that includes \(p(\boldsymbol{h} \vert \boldsymbol{v})\).</li>
          <li><strong>Approximate Inference</strong>: approximate inference uses approximate optimization to find \(q\).<br />
  We can make the optimization procedure less expensive but approximate by:
            <ul>
              <li>Restricting the family of distributions \(q\) that the optimization is allowed to search over</li>
              <li>Using an imperfect optimization procedure that may not completely maximize \(\mathcal{L}\) but may merely increase it by a significant amount.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li><strong>Core Idea of Variational Inference</strong>:<br />
  We don’t need to explicitly compute the posterior (or the marginal likelihood), we can solve an optimization problem by finding the right distribution \(\)  that best fits the Evidence Lower Bound.</li>
    </ul>

    <p id="lst-p"><strong style="color: red">Learning and Inference wrt the ELBO - Summary:</strong><br />
 The <span style="color: goldenrod"><strong>ELBO</strong> \(\mathcal{L}(\boldsymbol{v}, \boldsymbol{\theta}, q)\) is a lower bound on \(\log p(\boldsymbol{v} ; \boldsymbol{\theta})\)</span>:</p>
    <ul>
      <li><strong>Inference</strong>: can be viewed as <span style="color: goldenrod">maximizing \(\mathcal{L}\) with respect to \(q\)</span>.</li>
      <li><strong>Learning</strong>: can be viewed as <span style="color: goldenrod">maximizing \(\mathcal{L}\) with respect to \(\theta\)</span>.</li>
    </ul>

    <p id="lst-p"><strong style="color: red">Notes:</strong></p>
    <ul>
      <li>The difference between the ELBO and the KL divergence is the log normalizer (i.e. the evidence), which is the quantity that the ELBO bounds.</li>
      <li>Maximizing the ELBO is equivalent to Minimizing the KL-Divergence.<br />
 <br /></li>
    </ul>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents14">Expectation Maximization:</strong><br />
 The <strong>Expectation-Maximization</strong> Algorithm is an iterative method to find <em>maximum likelihood</em> or <em>maximum a posteriori (MAP)</em> estimates of parameters in statistical models with <em>unobserved latent variables</em>.</p>

    <p>It is based on maximizing a lower bound \(\mathcal{L}\).<br />
 It is not an approach to <strong>approximate inference</strong>.<br />
 It is an approach to learning with an <em><strong>approximate</strong></em> <strong>posterior</strong>.</p>

    <p id="lst-p"><strong style="color: red">The EM Algorithm:</strong><br />
 The EM Algorithm consists of alternating between two steps until convergence:</p>
    <ul>
      <li>The <strong>E(xpectation)-step:</strong>
        <ul>
          <li>Let \(\theta^{(0)}\) denote the value of the parameters at the beginning of the step.</li>
          <li>Set \(q\left(\boldsymbol{h}^{(i)} \vert \boldsymbol{v}\right)=p\left(\boldsymbol{h}^{(i)} ; \boldsymbol{\theta}^{(0)}\right)\) for all indices \(i\) of the training examples \(\boldsymbol{v}^{(i)}\) we want to train on (both batch and minibatch variants are valid).<br />
  By this we mean \(q\) is defined in terms of the current parameter value of \(\boldsymbol{\theta}^{(0)}\);<br />
  if we vary \(\boldsymbol{\theta},\) then \(p(\boldsymbol{h} \vert \boldsymbol{v} ; \boldsymbol{\theta})\) will change, but \(q(\boldsymbol{h} \vert \boldsymbol{v})\) will remain equal to \(p\left(\boldsymbol{h} \vert \boldsymbol{v} ; \boldsymbol{\theta}^{(0)}\right)\).</li>
        </ul>
      </li>
      <li>The <strong>M(aximization)-step:</strong>
        <ul>
          <li>Completely or partially maximize
            <p>$$\sum_i \mathcal{L}\left(\boldsymbol{v}^{(i)}, \boldsymbol{\theta}, q\right)$$</p>
            <p>with respect to \(\boldsymbol{\theta}\) using your optimization algorithm of choice.</p>
          </li>
        </ul>
      </li>
    </ul>

    <p><strong style="color: red">Relation to Coordinate Ascent:</strong><br />
 The algorithm can be viewed as a <strong>Coordinate Ascent</strong> algorithm to maximize \(\mathcal{L}\).<br />
 On one step, we maximize \(\mathcal{L}\) with respect to \(q,\) and on the other, we maximize \(\mathcal{L}\) with respect to \(\boldsymbol{\theta}\).<br />
 <strong>Stochastic Gradient Ascent</strong> on <em>latent variable models</em> can be seen as a special case of the EM algorithm where the M-step consists of taking a single gradient step.</p>
    <blockquote>
      <p>Other variants of the EM algorithm can make much larger steps. For some model families, the M-step can even be performed analytically, jumping all the way to the optimal solution for \(\theta\) given the current \(q\).</p>
    </blockquote>

    <p><strong style="color: red">As Approximate Inference - Interpretation:</strong><br />
 Even though the E-step involves <em>exact inference</em>, the EM algorithm can be viewed as using <em>approximate inference</em>.<br />
 The M-step assumes that the same value of \(q\) can be used for all values of \(\theta\).<br />
 This will introduce a gap between \(\mathcal{L}\) and the true \(\log p(\boldsymbol{v})\) as the M-step moves further and further away from the value \(\boldsymbol{\theta}^{(0)}\) used in the E-step.<br />
 Fortunately, the E-step reduces the gap to zero again as we enter the loop for the next time.</p>

    <p id="lst-p"><strong style="color: red">Insights/Takeaways:</strong></p>
    <ol>
      <li>The <strong>Basic Structure of the Learning Process:</strong><br />
 We update the model parameters to improve the likelihood of a completed dataset, where all missing variables have their values provided by an estimate of the posterior distribution.
        <blockquote>
          <p>This particular insight is not unique to the EM algorithm. For example, using gradient descent to maximize the log-likelihood also has this same property; the log-likelihood gradient computations require taking expectations with respect to the posterior distribution over the hidden units.</p>
        </blockquote>
      </li>
      <li><strong>Reusing \(q\):</strong><br />
 We can continue to use one value of \(q\) even after we have moved to a different value of \(\theta\).<br />
 This particular insight is used throughout <em>classical machine learning</em> to derive large M-step updates.<br />
 In the context of deep learning, most models are too complex to admit a tractable solution for an optimal large M-step update, so this second insight, which is more unique to the EM algorithm, is rarely used.</li>
    </ol>

    <p id="lst-p"><strong style="color: red">Notes:</strong></p>
    <ul>
      <li><a href="http://bjlkeng.github.io/posts/the-expectation-maximization-algorithm/">The Expectation-Maximization Algorithm and Derivation (Blog!)</a></li>
      <li>The EM algorithm enables us to make large learning steps with a fixed \(q\)<br />
 <br /></li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents15">MAP Inference:</strong><br />
 <strong>MAP Inference</strong> is an alternative form of inference where we are interested in computing the single most likely value of the missing variables, rather than to infer the entire distribution over their possible values \(p(\boldsymbol{h} \vert \boldsymbol{v})\).<br />
 In the context of <strong>latent variable models</strong>, we compute:
    <p>$$\boldsymbol{h}^{* }=\underset{\boldsymbol{h}}{\arg \max } p(\boldsymbol{h} \vert \boldsymbol{v})$$</p>

    <p id="lst-p"><strong style="color: red">As Approximate Inference:</strong><br />
 It is <strong>not</strong> usually thought of as <strong>approximate inference</strong>, since it computes the <span style="color: purple">exact most likely value of \(\boldsymbol{h}^{* }\)</span>.<br />
 However, to develop a <a href="#bodyContents15lp"><strong>learning process</strong></a> wrt maximizing the lower bound \(\mathcal{L}(\boldsymbol{v}, \boldsymbol{h}, q),\) then it is helpful to think of MAP inference as a procedure that provides a value of \(q\).<br />
 In this sense, we can think of MAP inference as <strong>approximate inference</strong>, because it <span style="color: purple">does not provide the optimal \(q\)</span>.<br />
 We can <strong>derive</strong> MAP Inference as a form of approximate inference by <span style="color: goldenrod">restricting the family of distributions \(q\) may be drawn from</span>.<br />
 <strong>Derivation:</strong></p>
    <ul>
      <li>We require \(q\) to take on a <strong>Dirac distribution</strong>:
        <p>$$q(\boldsymbol{h} \vert \boldsymbol{v})=\delta(\boldsymbol{h}-\boldsymbol{\mu})$$</p>
      </li>
      <li>This means that we can now control \(q\) entirely via \(\boldsymbol{\mu}\).</li>
      <li>Dropping terms of \(\mathcal{L}\) that do not vary with \(\boldsymbol{\mu},\) we are left with the optimization problem:
        <p>$$\boldsymbol{\mu}^{* }=\underset{\mu}{\arg \max } \log p(\boldsymbol{h}=\boldsymbol{\mu}, \boldsymbol{v})$$</p>
      </li>
      <li>which is <em><strong>equivalent</strong></em> to the <strong>MAP inference problem</strong>:
        <p>$$\boldsymbol{h}^{* }=\underset{\boldsymbol{h}}{\arg \max } p(\boldsymbol{h} \vert \boldsymbol{v})$$</p>
      </li>
    </ul>

    <p id="lst-p"><strong style="color: red" id="bodyContents15lp">The Learning Procedure with MAP Inference:</strong><br />
 We can, thus, justify a learning procedure similar to <strong>EM</strong>, where we alternate between:</p>
    <ul>
      <li>Performing MAP inference to infer \(\boldsymbol{h}^{* }\), and</li>
      <li>Updating update \(\boldsymbol{\theta}\) to increase \(\log p\left(\boldsymbol{h}^{* }, \boldsymbol{v}\right)\).</li>
    </ul>

    <p><strong>As Coordinate Ascent:</strong><br />
 As with EM, this is a form of <strong>coordinate ascent</strong> on \(\mathcal{L},\) where we alternate between using inference to optimize \(\mathcal{L}\) with respect to \(q\) and using parameter updates to optimize \(\mathcal{L}\) with respect to \(\boldsymbol{\theta}\).</p>

    <p><strong>Lower Bound (ELBO) Justification:</strong><br />
 The procedure as a whole can be justified by the fact that \(\mathcal{L}\) is a lower bound on \(\log p(\boldsymbol{v})\).<br />
 In the case of MAP inference, this justification is rather <em><strong>vacuous</strong></em>, because the bound is <strong>infinitely loose</strong>, due to the <strong>Dirac distribution’s differential entropy of negative infinity</strong>.<br />
 <span style="color: goldenrod">Adding noise to \(\mu\) would make the bound meaningful again</span>.</p>

    <p><strong style="color: red">MAP Inference in Deep Learning - Applications:</strong><br />
 MAP Inference is commonly used in deep learning as both a <span style="color: purple"><strong>feature extractor</strong></span> and a <span style="color: purple"><strong>learning mechanism</strong></span>.<br />
 It is primarily used for <strong>sparse coding models</strong>.</p>

    <p><strong style="color: red" id="bodyContents15map_sc">MAP Inference in Sparse Coding Models:</strong><br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">Sparse Coding Discussion</button>
 <img src="https://cdn.mathpix.com/snip/images/xhMTHTt2HOc7OibASJ5OYwn0Dw0ck6ZFGalynC643GQ.original.fullsize.png" alt="img" width="100%" hidden="" /></p>

    <p><strong style="color: red">Summary:</strong><br />
 Learning algorithms based on MAP inference enable us to <span style="color: goldenrod"><strong>learn using a <em>point estimate</em></strong> of \(p(\boldsymbol{h} \vert \boldsymbol{v})\) rather than inferring the entire distribution</span>.<br />
 <br /></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents16">Variational Inference and Learning:</strong></p>

    <p><strong style="color: red">Main Idea - Restricting family of distributions \(q\):</strong><br />
 The core idea behind variational learning is that we can maximize \(\mathcal{L}\) over a restricted family of distributions \(q\).<br />
 This family should be chosen so that it is easy to compute \(\mathbb{E}_ {q} \log p(\boldsymbol{h}, \boldsymbol{v})\).<br />
 A typical way to do this is to introduce assumptions about how \(q\) factorizes.<br />
 Mainly, we make a <strong>Mean-Field Approximation</strong> to \(q\).</p>

    <p><strong style="color: red">Mean-Field Approximation:</strong><br />
 <strong>Mean-Field Approximation</strong> is a type of <em>Variational Bayesian Inference</em> where we assume that the unknown variables can be partitioned so that each partition is <span style="color: purple"><strong>independent</strong></span> of the others.<br />
 The Mean-Field Approximation assumes the variational distribution over the latent variables factorizes as:</p>
    <p>$$q(\boldsymbol{h} \vert \boldsymbol{v})=\prod_{i} q\left(h_{i} \vert \boldsymbol{v}\right)$$</p>
    <p>I.E. it imposes the restriction that \(q\) is a <strong>factorial distribution</strong>.</p>

    <p>More generally, we can impose any graphical model structure we choose on \(q,\) to flexibly determine how many interactions we want our approximation to capture.<br />
 This fully general graphical model approach is called <strong>structured variational inference</strong> <em>(Saul and Jordan, 1996)</em>.</p>

    <p id="lst-p"><strong style="color: red">The Optimal Probability Distribution \(q\):</strong><br />
 The beauty of the variational approach is that we do not need to specify a specific parametric form for \(q\).<br />
 We specify how it should factorize, but then <span style="color: purple">the optimization problem determines the <strong><em>optimal</em> probability distribution</strong> within those factorization constraints</span>.<br />
 <strong style="color: red">The Inference Optimization Problem:</strong></p>
    <ul>
      <li>For <strong><em>discrete</em> latent variables</strong>: we use traditional optimization techniques to optimize a finite number of variables describing the \(q\) distribution.</li>
      <li>For <strong><em>continuous</em> latent variables</strong>: we use <span style="color: purple"><strong>calculus of variations</strong></span> to perform optimization over a space of functions and actually determine which function should be used to represent \(q\).
        <ul>
          <li><strong>Calculus of Variations</strong> removes much of the responsibility from the human designer of the model, who now must specify only how \(q\) factorizes, rather than needing to guess how to design a specific \(q\) that can accurately approximate the posterior.</li>
        </ul>

        <blockquote>
          <p>Calculus of variations is the origin of the names “variational learning” and “variational inference”, but the names apply in both discrete and continuous cases.</p>
        </blockquote>
      </li>
    </ul>

    <p id="lst-p"><strong>KL-Divergence Optimization:</strong></p>
    <ul>
      <li>The Inference Optimization Problem boils down to <span style="color: purple">maximizing \(\mathcal{L}\) with respect to \(q\)</span>.</li>
      <li>This is equivalent to <span style="color: purple">minimizing \(D_{\mathrm{KL}}(q(\boldsymbol{h} \vert \boldsymbol{v}) \| p(\boldsymbol{h} \vert \boldsymbol{v}))\)</span>.</li>
      <li>Thus, we are <span style="color: goldenrod">fitting \(q\) to \(p\)</span>.</li>
      <li>However, we are doing so with the opposite direction of the KL-Divergence. We are, <em>unnaturally</em>, assuming that \(q\) is constant and \(p\) is varying.</li>
      <li>In the inference optimization problem, we choose to use \(D_{\mathrm{KL}}\left(q(\boldsymbol{h} \vert \boldsymbol{v}) \| p(\boldsymbol{h} \vert \boldsymbol{v})\right)\) for <em><strong>computational reasons</strong></em>.
        <ul>
          <li>Specifically, computing \(D_{\mathrm{KL}}\left(q(\boldsymbol{h} \vert \boldsymbol{v}) \| p(\boldsymbol{h} \vert \boldsymbol{v})\right)\) involves evaluating expectations with respect to \(q,\) so by designing \(q\) to be simple, we can simplify the required expectations.</li>
          <li>The opposite direction of the KL divergence would require computing expectations with respect to the true posterior.<br />
  Because the form of the true posterior is determined by the choice of model, we cannot design a reduced-cost approach to computing \(D_{\mathrm{KL}}(p(\boldsymbol{h} \vert \boldsymbol{v}) \| q(\boldsymbol{h} \vert \boldsymbol{v}))\) exactly.</li>
        </ul>
      </li>
      <li><strong>Three Cases for Optimization</strong>:
        <ul>
          <li>If \(q\) is high and \(p\) is high, then we are happy (i.e. low KL divergence).</li>
          <li>If \(q\) is high and \(p\) is low then we pay a price (i.e. high KL divergence).</li>
          <li>If \(q\) is low then we dont care (i.e. also low KL divergence, regardless of \(p\)).</li>
        </ul>
      </li>
      <li><strong>Optimization-based Inference vs Maximum Likelihood (ML) Learning</strong>:
        <ul>
          <li><strong>ML-Learning:</strong> fits a model to data by minimizing \(D_{\mathrm{KL}}\left(p_{\text {data }} \| p_{\text {model }}\right)\).<br />
  It encourages the <span style="color: purple"><strong>model</strong> to have <strong><em>high</em> probability</strong> everywhere that the <strong>data</strong> has <strong><em>high</em> probability</strong></span>,</li>
          <li><strong>Optimization-based Inference</strong>: <br />
  It encourages <span style="color: purple"><strong>\(q\)</strong> to have <strong><em>low</em> probability</strong> everywhere the <strong>true posterior</strong> has <strong><em>low</em> probability</strong></span>.</li>
        </ul>
      </li>
    </ul>

    <p><strong style="color: red">Variational (Bayesian) Inference:</strong><br />
 <strong>Variational Bayesian Inference</strong> AKA <strong>Variational Bayes</strong> is most often used to infer the <span style="color: purple"><em>conditional</em> distribution over the latent variables given the observations</span>  (and parameters).<br />
 This is also known as the <strong>posterior distribution over the <em>latent</em> variables</strong>:</p>
    <p>$$p(z \vert x, \alpha)=\frac{p(z, x \vert \alpha)}{\int_{z} p(z, x \vert \alpha)}$$</p>
    <p>which is usually <em><strong>intractable</strong></em>.</p>

    <p id="lst-p"><strong style="color: red">Notes:</strong></p>
    <ul>
      <li><strong>KL Divergence Optimization:</strong><br />
  Optimizing the KL-Divergence given by:
        <p>$$D_{\mathrm{KL}}(q \| p) = \mathrm{E}_ {z \sim q}\left[\log \frac{q(z)}{p(z\vert x)}\right] =\int_{z} q(z) \log \left(\frac{q(z)}{p(z\vert x)}\right) dz$$</p>
        <ul>
          <li><strong>Three Cases for Optimization</strong>:
            <ul>
              <li>If \(q\) is high and \(p\) is high, then we are happy (i.e. low KL divergence).</li>
              <li>If \(q\) is high and \(p\) is low then we pay a price (i.e. high KL divergence).</li>
              <li>If \(q\) is low then we dont care (i.e. also low KL divergence, regardless of \(p\)).
 <br /></li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<hr />

<h2 id="content2">Variational Inference and Learning</h2>

<!-- 1. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21} -->

<ol>
  <li><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents22">Variational Inference - Discrete Latent Variables:</strong><br />
 Variational Inference with Discrete Latent Variables is relatively straightforward.<br />
 <strong>Representing \(q\):</strong><br />
 We define a distribution \(q\) where each factor of \(q\) is just defined by a lookup table over discrete states.<br />
 In the simplest case, \(h\) is binary and we make the mean field assumption that \(q\) factorizes over each individual \(h_{i}\).<br />
 In this case we can parametrize \(q\) with a vector \(\hat{h}\) whose entries are probabilities.<br />
 Then \(q\left(h_{i}=1 \vert \boldsymbol{v}\right)=\hat{h}_ {i}\).<br />
 <strong>Optimizing \(q\):</strong><br />
 After determining how to represent \(q\) we simply <strong>optimize its parameters</strong>.<br />
 For <strong>discrete</strong> latent variables this is just a standard optimization problem e.g. <em><strong>gradient descent</strong></em>.<br />
 However, because this optimization must occur in the inner loop of a learning algorithm, it must be <strong>very fast</strong><sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>.<br />
 A popular choice is to <span style="color: purple"><strong>iterate fixed-point equations</strong></span>; to solve:
    <p>$$\frac{\partial}{\partial \hat{h}_ {i}} \mathcal{L}=0$$</p>
    <p>for \(\hat{h}_ {i}\).<br />
 We repeatedly update different elements of \(\hat{\boldsymbol{h}}\) until we satisfy a convergence criterion.</p>

    <p><strong style="color: red">Application - Binary Sparse Coding:</strong><br />
 <br /></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents23">Variational Inference - Continuous Latent Variables:</strong><br />
 Variational Inference and Learning with Continuous Latent Variables requires the use of the <a href="#bodyContents41"><strong>calculus of variations</strong></a> for maximizing \(\mathcal{L}\) with respect to \(q(\boldsymbol{h} \vert \boldsymbol{v})\).</p>

    <p>In most cases, practitioners need not solve any calculus of variations problems themselves. Instead, there is a <strong>general equation for the mean field fixed-point updates</strong>.</p>

    <p><strong style="color: red">The General Equation for Mean-Field Fixed-Point Updates:</strong><br />
 If we make the mean field approximation</p>
    <p>$$q(\boldsymbol{h} \vert \boldsymbol{v})=\prod_{i} q\left(h_{i} \vert \boldsymbol{v}\right)$$</p>
    <p>and fix \(q\left(h_{j} \vert \boldsymbol{v}\right)\) for all \(j \neq i,\) then the <span style="color: goldenrod">optimal \(q\left(h_{i} \vert \boldsymbol{v}\right)\) may be obtained by <strong>normalizing the unnormalized distribution</strong></span>:</p>
    <p>$$\tilde{q}\left(h_{i} \vert \boldsymbol{v}\right) = \exp \left(\mathbb{E}_{\mathbf{h}_{-i} \sim q\left(\mathbf{h}_ {-i} \vert \boldsymbol{v}\right)} \log \tilde{p}(\boldsymbol{v}, \boldsymbol{h})\right) = e^{\mathbb{E}_{\mathbf{h}_ {-i} \sim q\left(\mathbf{h}_ {-i} \vert \boldsymbol{v}\right)} \log \tilde{p}(\boldsymbol{v}, \boldsymbol{h})}$$</p>
    <p>as long as \(p\) does not assign \(0\) probability to any joint configuration of variables.<br />
 - Carrying out the expectation inside the equation will yield the correct functional form of \(q\left(h_{i} \vert \boldsymbol{v}\right)\). <br />
 - The General Equation yields the mean field approximation for any probabilistic model.<br />
 - Deriving functional forms of \(q\) directly using calculus of variations is only necessary if one wishes to develop a new form of variational learning.<br />
 - The General Equation is a <strong>fixed-point equation</strong>, designed to be iteratively applied for each value of \(i\) repeatedly until convergence.</p>

    <p><strong style="color: red">Functional Form of the Optimal Distribution/Solution:</strong><br />
 The General Equation tells us the <span style="color: purple"><strong>functional form</strong> that the <em>optimal solution</em> will take</span>, whether we arrive there by fixed-point equations or not.<br />
 <span style="color: goldenrod">This means we can take the functional form from that equation but regard some of the values that appear in it as <em><strong>parameters</strong></em>, which we can optimize with any optimization algorithm we like.</span><br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">Example - Application:</button>
 <img src="https://cdn.mathpix.com/snip/images/YFBRwnXeoFZ7iB-yu4PE3Pszt1U5A8BGnc5ZhATfijg.original.fullsize.png" alt="img" width="100%" hidden="" /></p>

    <p>For examples of real applications of variational learning with continuous variables in the context of deep learning, see <em>Goodfellow et al. (2013d)</em>.<br />
 <br /></p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents24">Interactions between Learning and Inference:</strong><br />
 <span style="color: purple">Using <strong>approximate inference</strong> as part of a <strong>learning algorithm</strong> affects the <strong>learning process</strong></span>, and this in turn <span style="color: purple">affects the <strong><em>accuracy</em> of the inference algorithm</strong></span>.<br />
 <strong>Analysis:</strong>
    <ul>
      <li>The training algorithm tends to adapt the model in a way that makes the approximating assumptions underlying the approximate inference algorithm become more true.</li>
      <li>When training the parameters, variational learning increases
        <p>$$\mathbb{E}_ {\mathbf{h} \sim q} \log p(\boldsymbol{v}, \boldsymbol{h})$$</p>
      </li>
      <li>For a specific \(v\) this:
        <ul>
          <li>increases \(p(\boldsymbol{h} \vert \boldsymbol{v})\) for values of \(\boldsymbol{h}\) that have high probability under \(q(\boldsymbol{h} \vert \boldsymbol{v})\) and</li>
          <li>decreases \(p(\boldsymbol{h} \vert \boldsymbol{v})\) for values of \(\boldsymbol{h}\) that have low probability under \(q(\boldsymbol{h} \vert \boldsymbol{v})\).</li>
        </ul>
      </li>
      <li>This behavior <span style="color: purple">causes our approximating assumptions to become <em><strong>self-fulfilling prophecies</strong></em></span>.<br />
  If we train the model with a unimodal approximate posterior, we will obtain a model with a true posterior that is far closer to unimodal than we would have obtained by training the model with exact inference.</li>
    </ul>

    <p id="lst-p"><strong style="color: red">Computing the Effect (Harm) of using Variational Inference:</strong><br />
 Computing the true amount of harm imposed on a model by a variational approximation is thus very difficult.</p>
    <ul>
      <li>There exist several methods for estimating \(\log p(\boldsymbol{v})\):<br />
  We often estimate \(\log p(\boldsymbol{v} ; \boldsymbol{\theta})\) after training the model and find that the gap with \(\mathcal{L}(\boldsymbol{v}, \boldsymbol{\theta}, q)\) is small.
        <ul>
          <li>From this, we can <strong>conclude</strong> that <span style="color: purple">our variational approximation is <strong>accurate for the specific value of \(\boldsymbol{\theta}\)</strong></span> that we obtained from the learning process.</li>
          <li>We should <em><strong>not</strong></em> <strong>conclude</strong> that <span style="color: purple">our variational approximation is <strong>accurate in general</strong></span> or that <span style="color: purple">the variational approximation <strong>did <em>little harm</em> to the learning process</strong></span>.</li>
        </ul>
      </li>
      <li>To measure the <em><strong>true amount of harm</strong> induced by the variational approximation</em>:
        <ul>
          <li>We would need to know \(\boldsymbol{\theta}^{* }=\max_{\boldsymbol{\theta}} \log p(\boldsymbol{v} ; \boldsymbol{\theta})\).</li>
          <li>It is possible for \(\mathcal{L}(\boldsymbol{v}, \boldsymbol{\theta}, q) \approx \log p(\boldsymbol{v} ; \boldsymbol{\theta})\) and \(\log p(\boldsymbol{v} ; \boldsymbol{\theta}) \ll \log p\left(\boldsymbol{v} ; \boldsymbol{\theta}^{* }\right)\) to hold simultaneously.</li>
          <li>If \(\max_{q} \mathcal{L}\left(\boldsymbol{v}, \boldsymbol{\theta}^{* }, q\right) \ll \log p\left(\boldsymbol{v} ; \boldsymbol{\theta}^{* }\right),\) because \(\boldsymbol{\theta}^{* }\) induces too complicated of a posterior distribution for our \(q\) family to capture, then the learning process will never approach \(\boldsymbol{\theta}^{* }\).</li>
          <li>Such a problem is very difficult to detect, because we can only know for sure that it happened if we have a superior learning algorithm that can find \(\boldsymbol{\theta}^{* }\) for comparison.<br />
 <br /></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents25">Learned Approximate Inference:</strong><br />
 <strong style="color: red">Motivation:</strong><br />
 Explicitly performing optimization via iterative procedures such as <em>fixed-point equations</em> or <em>gradient-based optimization</em> is often <strong>very expensive</strong> and <strong>time consuming</strong>.<br />
 Many approaches to inference avoid this expense by <span style="color: purple">learning to perform approximate inference</span>.</p>

    <p><strong>Learned Approximate Inference:</strong><br />
 Learns to perform approximate inference by viewing the (multistep iterative) optimization process as a function \(f\) that maps an input \(v\) to an approximate distribution \(q^{* }=\arg \max_{q} \mathcal{L}(\boldsymbol{v}, q)\), and then <span style="color: goldenrod">approximates this function with a <strong>neural network</strong></span> that implements an approximation \(f(\boldsymbol{v} ; \boldsymbol{\theta})\).</p>

    <p id="lst-p"><strong style="color: red">Wake-Sleep:</strong><br />
 <strong>Motivation</strong>:</p>
    <ul>
      <li>One of the main difficulties with training a model to infer \(h\) from \(v\) is that we do not have a supervised training set with which to train the model.</li>
      <li>Given a \(v\) we do not know the appropriate \(h\).</li>
      <li>The mapping from \(v\) to \(h\) depends on the choice of model family, and evolves throughout the learning process as \(\theta\) changes.</li>
    </ul>

    <p><strong>Wake-Sleep Algorithm</strong>:<br />
 The <strong>wake-sleep algorithm</strong> <em>(Hinton et al., 1995b; Frey et al., 1996)</em> resolves this problem by <span style="color: purple">drawing samples of both \(h\) and \(v\) <strong>from the <em>model distribution</em></strong></span>.</p>
    <ul>
      <li>For example, in a <strong>directed model</strong>, this can be done cheaply by performing <em><strong>ancestral sampling</strong></em> beginning at \(h\) and ending at \(v\).<br />
  The inference network can then be trained to perform the reverse mapping: predicting which \(h\) caused the present $\boldsymbol{v}$.</li>
    </ul>

    <p><strong>DrawBacks</strong>:<br />
 The main drawback to this approach is that we will only be able to train the inference network on values of \(\boldsymbol{v}\) that have high probability under the model.<br />
 Early in learning, the <span style="color: purple">model distribution will not resemble the data distribution</span>, so <span style="color: purple">the inference network will not have an opportunity to <em>learn on samples that resemble data</em></span>.</p>

    <p><strong>Relation to Biological Dreaming:</strong><br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">Discussion</button>
 <img src="https://cdn.mathpix.com/snip/images/gAcJcN1TtW7H23J0TssVMLYjOufszOjoEigvRExEVeo.original.fullsize.png" alt="img" width="100%" hidden="" /></p>

    <p><strong style="color: red">Generative Modeling - Application:</strong><br />
 Learned approximate inference has recently become one of the dominant approaches to generative modeling, in the form of the <strong>Variational AutoEncoder</strong> <em>(Kingma, 2013; Rezende et al., 2014)</em>.<br />
 In this elegant approach, there is <span style="color: purple">no need to <em>construct explicit targets</em> for the inference network</span>.<br />
 Instead, the <span style="color: purple">inference network is simply used to define \(\mathcal{L},\)</span> and then <span style="color: purple">the parameters of the inference network are adapted to increase \(\mathcal{L}\)</span>.</p>
  </li>
</ol>

<!-- 6. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents26}
7. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents27} -->

<hr />
<hr />

<h2 id="content4">Mathematics of Approximate Inference</h2>

<p><a href="http://tutorial.math.lamar.edu/Classes/CalcIII/DirectionalDeriv.aspx">Directional Derivative</a><br />
<a href="http://bjlkeng.github.io/posts/the-calculus-of-variations/#id1">The Calculus of Variations (Blog!)</a></p>

<ol>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents4" id="bodyContents41">Calculus of Variations:</strong></p>

    <p>Method for finding the <em><strong>stationary</strong></em> <strong>functions</strong> of a functional \(I[f]\) (function of functions) by solving a differential equation.</p>

    <p><strong>Formally,</strong> calculus of variations seeks to find the function \(y=f(x)\) such that the integral (functional):</p>
    <p>$$I[y]=\int_{x_{1}}^{x_{2}} L\left(x, y(x), y^{\prime}(x)\right) d x$$</p>
    <p>$$\begin{array}{l}{\text {where}}\\{x_{1}, x_{2} \text { are constants, }} \\ {y(x) \text { is twice continuously differentiable, }} \\ {y^{\prime}(x)=d y / d x} \\ {L\left(x, y(x), y^{\prime}(x)\right) \text { is twice continuously differentiable with respect to its arguments } x, y, y^{\prime}}\end{array}$$</p>
    <p>is <strong>stationary</strong>.</p>

    <p><strong style="color: red">Euler Lagrange Equation - Finding Extrema:</strong><br />
 Finding the extrema of functionals is similar to finding the maxima and minima of functions. The maxima and minima of a function may be located by finding the points where its derivative vanishes (i.e., is equal to zero). The extrema of functionals may be obtained by finding functions where the functional derivative is equal to zero. This leads to solving the associated Euler–Lagrange equation.</p>

    <p>The <strong>Euler Lagrange Equation</strong> is a second-order partial differential equation whose solutions are the functions for which a given functional is stationary:</p>
    <p>$$\frac{\partial L}{\partial f}-\frac{d}{d x} \frac{\partial L}{\partial f^{\prime}} = 0$$</p>
    <p>It is defined in terms of the <strong>functional derivative</strong>:</p>
    <p>$$\frac{\delta J}{\delta f(x)} = \frac{\partial L}{\partial f}-\frac{d}{d x} \frac{\partial L}{\partial f^{\prime}} = 0$$</p>

    <p><strong style="color: red">Shortest Path between Two Points:</strong><br />
 Find path such that the distance \(AB\) between two points is minimized.<br />
 Using the <em><strong>arc length</strong></em>, we define the following <strong>functional</strong>:</p>
    <p>$$\begin{align}
     I &amp;= \int_{A}^{B} dS \\
          &amp;= \int_{A}^{B} \sqrt{dx^2 + dy^2} \\
          &amp;= \int_{A}^{B} \sqrt{1 + \left(\dfrac{dy}{dx}\right)^2} dx \\ 
          &amp;= \int_{x_1}^{x_2} \sqrt{1 + \left(\dfrac{dy}{dx}\right)^2} dx
     \end{align}
     $$</p>
    <ul>
      <li>Now, we formulate the <strong>variational problem</strong>:<br />
  Find the extremal function \(y=f(x)\) between two points \(A=(x_1, y_1)\) and \(B=(x_2, y_2)\) such that the following integral is <strong>minimized</strong>:
        <p>$$I[y] = \int_{x_{1}}^{x_{2}} \sqrt{1+\left[y^{\prime}(x)\right]^{2}} d x$$</p>
        <p>where \(y^{\prime}(x)=\frac{d y}{d x}, y_{1}=f\left(x_{1}\right), y_{2}=f\left(x_{2}\right)\).</p>
      </li>
      <li><strong>Solution:</strong><br />
  We use the <strong>Euler-Lagrange Equation</strong> to find the extremal function \(f(x)\) that minimizes the functional \(I[y]\):
        <p>$$\frac{\partial L}{\partial f}-\frac{d}{d x} \frac{\partial L}{\partial f^{\prime}}=0$$</p>
        <p>where \(L=\sqrt{1+\left[f^{\prime}(x)\right]^{2}}\).</p>
        <ul>
          <li>Since \(f\) does not appear explicity in \(L,\) the first term in the Euler-Lagrange equation vanishes for all \(f(x)\)
            <p>$$\frac{\partial L}{\partial f} = 0$$</p>
          </li>
          <li>Thus,
            <p>$$\frac{d}{d x} \frac{\partial L}{\partial f^{\prime}}=0$$</p>
          </li>
          <li>Substituting for \(L\) and taking the derivative:
            <p>$$\frac{d}{d x} \frac{f^{\prime}(x)}{\sqrt{1+\left[f^{\prime}(x)\right]^{2}}}=0$$</p>
            <p>for some constant \(c\).</p>
          </li>
          <li>If the derivative \(\frac{d}{dx}\), above, is zero, then
            <p>$$\frac{f^{\prime}(x)}{\sqrt{1+\left[f^{\prime}(x)\right]^{2}}}=c$$</p>
            <p>for some constant \(c\).</p>
          </li>
          <li>Square both sides:
            <p>$$\frac{\left[f^{\prime}(x)\right]^{2}}{1+\left[f^{\prime}(x)\right]^{2}}=c^{2}$$</p>
            <p>where \(0 \leq c^{2}&lt;1\).</p>
          </li>
          <li>Solving:
            <p>$$\left[f^{\prime}(x)\right]^{2}=\frac{c^{2}}{1-c^{2}}$$</p>
            <p>\(\implies\)</p>
            <p>$$f^{\prime}(x)=m$$</p>
            <p>is a constant \(m\).</p>
          </li>
          <li>Integrating:
            <p>$$f(x)=m x+b$$</p>
            <p>is an <strong>equation of a (straight) line</strong>, where \(m=\frac{y_{2}-y_{1}}{x_{2}-x_{1}} \quad\) and \(\quad b=\frac{x_{2} y_{1}-x_{1} y_{2}}{x_{2}-x_{1}}\).</p>
          </li>
        </ul>

        <p>In other words, the shortest distance between two points is a straight line.</p>
        <div class="borderexample">
          <p><span style="color: purple">We have found the extremal function \(f(x)\) that minimizes the functional \(A[y]\) so that \(A[f]\) is a minimum.</span></p>
        </div>
      </li>
    </ul>

    <p><br /></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents4" id="bodyContents42">Mean Field Methods:</strong></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents4" id="bodyContents43">Mean Field Approximations:</strong></p>
  </li>
</ol>

<!-- 4. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents44}
5. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents45}
6. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents46}
7. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents47}
8. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents4 #bodyContents48} -->

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>To achieve this speed, we typically use special optimization algorithms that are designed to solve comparatively small and simple problems in few iterations. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>


      <footer class="site-footer">
    <!--   <span class="site-footer-owner"><a href="http://localhost:8880">Ahmad Badary</a> is maintained by <a href="http://ahmedbadary.ml">Ahmad Badary</a>.</span> -->
    
<!--  -->
    <div class="wrapper">
        <div class="footer-col-wrapper">
            <div class="footer-col footer-col-1">
            <span class="site-footer-owner"><a href="http://localhost:8880">Site</a> maintained by <a href="http://ahmedbadary.ml">Ahmad Badary</a>.</span>
    <span class="site-footer-credits">
        <p>
            &copy; 2017. All rights reserved.
        </p> 
    </span>
            </div>
            <div class="footer-col footer-col-2">
            <div><p>         </p></div>
            </div>
            <div class="footer-col footer-col-3">
                <ul class="social-media-list">
                    
                      <li>
                        <a href="https://github.com/AhmedBadary">
                          <i class="fa fa-github"></i> GitHub
                        </a>
                      </li>
                    
                    
                      <li>
                        <a href="https://linkedin.com/in/ahmad-badary-656098121/">
                          <i class="fa fa-linkedin"></i> LinkedIn
                        </a>
                      </li>
                    
                    
                      <li>
                        <a href="https://www.facebook.com/ahmed.thabet.94">
                          <i class="fa fa-facebook"></i> Facebook
                        </a>
                      </li>
                    
                </ul>
            </div>
        </div>
    </div>
<!--  -->
</footer>


    </section>

  </body>

<!-- Table of Content Script -->
<script type="text/javascript">
var bodyContents = $(".bodyContents1");
$("<ol>").addClass("TOC1ul").appendTo(".TOC1");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC1ul");
     });
// 
var bodyContents = $(".bodyContents2");
$("<ol>").addClass("TOC2ul").appendTo(".TOC2");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC2ul");
     });
// 
var bodyContents = $(".bodyContents3");
$("<ol>").addClass("TOC3ul").appendTo(".TOC3");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC3ul");
     });
//
var bodyContents = $(".bodyContents4");
$("<ol>").addClass("TOC4ul").appendTo(".TOC4");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC4ul");
     });
//
var bodyContents = $(".bodyContents5");
$("<ol>").addClass("TOC5ul").appendTo(".TOC5");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC5ul");
     });
//
var bodyContents = $(".bodyContents6");
$("<ol>").addClass("TOC6ul").appendTo(".TOC6");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC6ul");
     });
//
var bodyContents = $(".bodyContents7");
$("<ol>").addClass("TOC7ul").appendTo(".TOC7");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC7ul");
     });
//
var bodyContents = $(".bodyContents8");
$("<ol>").addClass("TOC8ul").appendTo(".TOC8");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC8ul");
     });
//
var bodyContents = $(".bodyContents9");
$("<ol>").addClass("TOC9ul").appendTo(".TOC9");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC9ul");
     });

</script>

<!-- VIDEO BUTTONS SCRIPT -->
<script type="text/javascript">
  function iframePopInject(event) {
    var $button = $(event.target);
    // console.log($button.parent().next());
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $figure = $("<div>").addClass("video_container");
        $iframe = $("<iframe>").appendTo($figure);
        $iframe.attr("src", $button.attr("src"));
        // $iframe.attr("frameborder", "0");
        $iframe.attr("allowfullscreen", "true");
        $iframe.css("padding", "4px 6px");
        $button.next().css("display", "block");
        $figure.appendTo($button.next());
        $button.text("Hide Video")
    } else {
        $button.attr('value', 'show');
        $button.next().html("");
        $button.text("Show Video")
    }
}
</script>

<!-- BUTTON TRY -->
<script type="text/javascript">
  function iframePopA(event) {
    event.preventDefault();
    var $a = $(event.target).parent();
    console.log($a);
    if ($a.attr('value') == 'show') {
        $a.attr('value', 'hide');
        $figure = $("<div>");
        $iframe = $("<iframe>").addClass("popup_website_container").appendTo($figure);
        $iframe.attr("src", $a.attr("href"));
        $iframe.attr("frameborder", "1");
        $iframe.attr("allowfullscreen", "true");
        $iframe.css("padding", "4px 6px");
        $a.next().css("display", "block");
        $figure.appendTo($a.next().next());
        // $a.text("Hide Content")
        $('html, body').animate({
            scrollTop: $a.offset().top
        }, 1000);
    } else {
        $a.attr('value', 'show');
        $a.next().next().html("");
        // $a.text("Show Content")
    }

    $a.next().css("display", "inline");
}
</script>


<!-- TEXT BUTTON SCRIPT - INJECT -->
<script type="text/javascript">
  function showTextPopInject(event) {
    var $button = $(event.target);
    var txt = $button.attr("input");
    console.log(txt);
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $p = $("<p>");
        $p.html(txt);
        $button.next().css("display", "block");
        $p.appendTo($button.next());
        $button.text("Hide Content")
    } else {
        $button.attr('value', 'show');
        $button.next().html("");
        $button.text("Show Content")
    }

}
</script>

<!-- TEXT BUTTON SCRIPT - HIDDEN / HIDE / SHOW / HIDE/SHOW -->
<script type="text/javascript">
  function showTextPopHide(event) {
    var $button = $(event.target);
    // var txt = $button.attr("input");
    var txt = $button.text();
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $button.next().removeAttr("hidden");
        $button.text(txt + " - Hide Content");
    } else {
        $button.attr('value', 'show');
        $button.next().attr("hidden", "");
        $button.text(txt.replace(" - Hide Content",""));
    }
}
</script>

<!-- TEXT BUTTON SCRIPT - HIDDEN / HIDE / SHOW / HIDE/SHOW -->
<script type="text/javascript">
  function showText_withParent_PopHide(event) {
    var $button = $(event.target);
    var $parent = $button.parent();
    var txt = $button.text();
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $parent.next().removeAttr("hidden");
        $button.text(txt + " - Hide Content");
    } else {
        $button.attr('value', 'show');
        $parent.next().attr("hidden", "");
        $button.text(txt.replace(" - Hide Content",""));
    }
}
</script>

<!-- Print / Printing / printme -->
<!-- <script type="text/javascript">
i = 0

for (var i = 1; i < 6; i++) {
    var bodyContents = $(".bodyContents" + i);
    $("<p>").addClass("TOC1ul")  .appendTo(".TOC1");
    bodyContents.each(function(index, element) {
        var paragraph = $(element);
        $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC1ul");
         });
} 
</script>
 -->
 
</html>

