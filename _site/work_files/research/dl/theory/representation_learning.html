<!DOCTYPE html>
<html lang="en-us">
  <head>
  <meta charset="UTF-8">
  <title>Ahmad Badary</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <link rel="stylesheet" href="/css/normalize.css">
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="/css/cayman.css">
  <link rel="stylesheet" href="/css/style.css">
  <link rel="shortcut icon" href="/main_files/favicon.ico" />
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="/css/customStyle.css">
  <title> » Ahmad Badary</title>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
</head>

  <body>
    <nav class="main-nav">
    <a href="https://ahmedbadary.github.io/" class="main-nav-logo">
        <img src="/main_files/logo.png">
    </a>
    <ul id="menu-main" class="main-nav-items">
        <li id="menu-item-1859" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-home menu-item-1859">
            <a href="/">Home</a>
        </li>
        <li id="menu-item-2869" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-2869">
            <a href="/work">Work</a>
        </li>
        <li id="menu-item-1892" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1892">
            <a href="/projects">Projects</a>
        </li>
        <li id="menu-item-1858" class="menu-item menu-item-type-post_type menu-item-object-page current_page_parent menu-item-1858">
            <a href="/blog">Blog</a>
        </li>
        <li id="menu-item-1862" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1862">
            <a href="/about">About</a>
        </li>
    </ul>
</nav>


<section class="page-header">
  <h1 class="project-name">Representation Learning</h1>
  <h2 class="project-tagline"></h2>
  <a href="/#" class="btn">Home</a>
  <a href="/work" class="btn">Work-Space</a>
  <a href= /work_files/research/dl/theory.html class="btn">Previous</a>
</section>

<!-- <div>
  <ul class="posts">
    
      <li><span>02 Jan 2014</span> &raquo; <a href="/2014/01/02/introducing-Ahmad/">Introducing Ahmad</a></li>
    
  </ul>
</div> -->


    <section class="main-content">
      
      <div class="TOC">
  <h1 id="table-of-contents">Table of Contents</h1>

  <ul class="TOC1">
    <li><a href="#content1">Representation Learning</a></li>
  </ul>
  <ul class="TOC2">
    <li><a href="#content2">Unsupervised Representation Learning</a></li>
  </ul>
  <ul class="TOC3">
    <li><a href="#content3">Supervised Representation Learning</a></li>
  </ul>
  <ul class="TOC4">
    <li><a href="#content4">Transfer Learning and Domain Adaptation</a></li>
  </ul>
  <ul class="TOC5">
    <li><a href="#content5">Causal Factor Learning</a></li>
  </ul>
  <p><!--     * [SIXTH](#content6)
  {: .TOC6} --></p>
</div>

<hr />
<hr />

<ul>
  <li>
    <p><a href="https://docs.google.com/presentation/d/1asNbolA4VgTdhp7x84hfj5-GB2M8yApJylc01WhiOo0/edit#slide=id.gc7d6ca60f0_0_0">My Presentation on Representation Learning</a></p>
  </li>
  <li><a href="https://www.youtube.com/watch?v=Yr1mOzC93xs">From Deep Learning of Disentangled Representations to Higher-level Cognition (Bengio Lec)</a></li>
  <li><a href="https://www.youtube.com/watch?v=754vWvIimPo">Representation Learning (CMU Lec!)</a></li>
  <li><a href="https://www.youtube.com/watch?v=O6itYc2nnnM">Representation Learning and Deep Learning (Bengio Talk)</a></li>
  <li><a href="https://www.youtube.com/watch?v=7kAlBa7yhDM">Deep Learning and Representation Learning (Hinton Talk)</a></li>
  <li><a href="https://www.inference.vc/goals-and-principles-of-representation-learning/">Goals and Principles of Representation Learning (inFERENCe!)</a></li>
  <li><a href="https://www.youtube.com/playlist?list=PL-tWvTpyd1VAlbzhCpljlREd76Nlo1pOo">DALI Goals and Principles of Representation Learning (vids!)</a></li>
  <li><a href="https://arxiv.org/pdf/1305.0445.pdf">Deep Learning of Representations: Looking Forward (Bengio paper!)</a></li>
  <li><a href="https://blog.ml.cmu.edu/2019/09/13/on-learning-invariant-representations-for-domain-adaptation/">On Learning Invariant Representations for Domain Adaptation (blog!)</a></li>
  <li><a href="http://www.offconvex.org/2019/03/19/CURL/">Contrastive Unsupervised Learning of Semantic Representations: A Theoretical Framework (Blog!)</a></li>
</ul>

<p id="lst-p"><strong style="color: red">Notes (Move Inside):</strong></p>
<ul>
  <li>Representation Learning (Feature Learning) is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data.<br />
  This replaces manual feature engineering and allows a machine to both learn the features and use them to perform a specific task.</li>
  <li>Hypothesis - Main Idea:<br />
  The core hypothesis for representation learning is that the unlabeled data can be used to learn a good representation.</li>
  <li>Types:<br />
  Representation learning can be either supervised or unsupervised.<br />
<br /></li>
</ul>

<h2 id="content1">Representation Learning</h2>

<ol>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents11">Representation Learning:</strong><br />
 <strong>Representation Learning</strong> (<strong>Feature Learning</strong>) is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data.<br />
 This replaces manual feature engineering and allows a machine to both learn the features and use them to perform a specific task.</p>

    <p><strong style="color: red">Hypothesis - Main Idea:</strong><br />
 The core <strong>hypothesis</strong> for representation learning is that <span style="color: goldenrod">the <em>unlabeled</em> data can be used to learn a <em>good</em> representation</span>.</p>

    <p><strong style="color: red">Types:</strong><br />
 Representation learning can be either <a href="#bodyContents14"><strong>supervised</strong></a> or <a href="#bodyContents13"><strong>unsupervised</strong></a>.</p>

    <p id="lst-p"><strong style="color: red">Representation Learning Approaches:</strong><br />
 There are various ways of learning different representations:</p>
    <ul>
      <li><strong>Probabilistic Models</strong>: the goal is to learn a representation that captures the probability distribution of the underlying explanatory features for the observed input. Such a learnt representation can then be used for prediction.</li>
      <li><strong>Deep Learning</strong>: the representations are formed by composition of multiple non-linear transformations of the input data with the goal of yielding abstract and useful representations for tasks like classification, prediction etc.</li>
    </ul>

    <p><strong style="color: red">Representation Learning Tradeoff:</strong><br />
 Most representation learning problems face a tradeoff between <span style="color: purple">preserving as much information about the input</span> as possible and <span style="color: purple">attaining nice properties</span> (such as independence).</p>

    <p><strong style="color: red">The Problem of Data (Semi-Supervised Learning*):</strong><br />
 We often have very large amounts of unlabeled training data and relatively little labeled training data. Training with supervised learning techniques on the labeled subset often results in severe overfitting. Semi-supervised learning offers the chance to resolve this overfitting problem by also learning from the unlabeled data. Specifically, we can learn good representations for the unlabeled data, and then use these representations to solve the supervised learning task.</p>

    <p><strong style="color: red">Learning from Limited Data:</strong><br />
 Humans and animals are able to learn from very few labeled examples. <br />
 Many factors could explain improved human performance — for example, the brain may use <span style="color: purple">very large ensembles of classifiers</span> or <span style="color: purple">Bayesian inference</span> techniques.<br />
 One popular hypothesis is that the brain is able to <span style="color: purple">leverage unsupervised or semi-supervised learning</span>.</p>

    <p id="lst-p"><strong style="color: red">Motivation/Applications:</strong></p>
    <ol>
      <li>ML tasks such as <em>classification</em> often require input that is mathematically and computationally convenient to process.<br />
 However, real-world data such as images, video, and sensor data has not yielded to attempts to algorithmically define specific features.</li>
      <li>Learning <a href="">good representations</a> enables us to perform certain (specific) tasks in a more optimal manner.
        <ul>
          <li>E.g. linked lists \(\implies\) \(\mathcal{O}(n)\) insertion | red-black tree \(\implies\) \(\mathcal{O}(\log n)\) insertion.</li>
          <li><button class="showText" value="show" onclick="showTextPopHide(event);">Ex: Learning Language Representations</button>
            <ul hidden="">
              <li>Goal: Learn Portuguese</li>
              <li>For 1 month you listen to Portuguese on the radio (this is unlabeled data)</li>
              <li>You develop an intuition for the language, phrases, and grammar (a model in your head)</li>
              <li>It is easier to learn now from a tutor because you have a better (higher representation) of the data/language</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Representation Learning is particularly interesting because it <span style="color: purple">provides (one) way to perform <strong>unsupervised</strong> and <strong>semi-supervised learning</strong></span>.</li>
      <li><strong>Feature Engineering</strong> is hard. Representation Learning allows us to avoid having to engineer features, manually.</li>
      <li>In general, representation learning can allow us to <span style="color: purple">achieve <strong>multi-task learning</strong>, <strong>transfer learning</strong>, and <strong>domain adaptation</strong></span> through <span style="color: goldenrod">shared representations</span>.</li>
    </ol>

    <p><strong style="color: red">The Quality of Representations:</strong><br />
 Generally speaking, a good representation is one that makes a subsequent learning task easier.<br />
 The choice of representation will usually depend on the choice of the subsequent learning task.</p>

    <p id="lst-p"><strong style="color: red">Success of Representation Learning:</strong><br />
 The success of representation learning can be attributed to many factors, including:</p>
    <ul>
      <li>Theoretical advantages of <strong><em>distributed</em> representations</strong> <em>(Hinton et al., 1986)</em></li>
      <li>Theoretical advantages of <strong><em>deep</em> representations</strong> <em>(Hinton et al., 1986)</em></li>
      <li>The <strong>Causal Factors Hypothesis</strong>: a general idea of underlying assumptions about the data generating process, in particular about underlying causes of the observed data.</li>
    </ul>

    <p id="lst-p"><strong style="color: red">Representation Learning Domain Applications:</strong></p>
    <ul>
      <li><strong>Computer Vision</strong>: CNNs.</li>
      <li><strong>Natural Language Processing</strong>: Word-Embeddings.</li>
      <li><strong>Speech Recognition</strong>: Speech-Embeddings.</li>
    </ul>

    <p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);"><strong>Representation Quality</strong></button></p>
    <ul hidden="">
      <li><span style="color: purple"><strong>“What is a <em>good</em> representation?”</strong></span>
        <ul>
          <li>Generally speaking, a <strong>good representation</strong> is one that makes a subsequent learning task easier.<br />
  The choice of representation will usually depend on the choice of the subsequent learning task.</li>
        </ul>
      </li>
      <li><span style="color: purple"><strong>“What makes one representation better than another?”</strong></span>
        <ul>
          <li><strong>Causal Factors Hypothesis:</strong><br />
  An <strong>ideal representation</strong> is one in which <span style="color: purple">the <strong>features</strong> within the representation <em>correspond</em> to the underlying <strong>causes</strong> of the observed data</span>, with <strong><em>separate</em> features</strong> or <strong>directions</strong> in <em>feature space</em> corresponding to <strong><em>different</em> causes</strong>, so that <span style="color: purple">the <strong>representation</strong> <em><strong>disentangles</strong></em> the <strong>causes</strong> from one another</span>.
            <ul>
              <li><strong>Why</strong>:
                <ul>
                  <li><strong>Ease of Modeling:</strong> A representation that <strong>cleanly separates the underlying causal factors</strong> is, also, one that is <strong>easy to model</strong>.
                    <ul>
                      <li>For <em><strong>many</strong></em> <strong>AI tasks</strong> the two properties <strong>coincide</strong>: once we are able to <span style="color: purple">obtain the underlying explanations for the observations</span>, it generally becomes <span style="color: purple">easy to isolate individual attributes</span> from the others.</li>
                      <li>Specifically, <strong>if</strong> a <span style="color: purple"><strong>representation \(\boldsymbol{h}\)</strong> <em>represents</em> many of the <em><strong>underlying causes</strong></em> of the <strong>observed \(\boldsymbol{x}\)</strong></span>, <strong>and</strong> the <span style="color: purple"><strong>outputs \(\boldsymbol{y}\)</strong> are among the <strong>most <em>salient causes</em></strong></span>, <strong>then</strong> it is <span style="color: purple">easy to <strong>predict</strong> \(\boldsymbol{y}\) from \(\boldsymbol{h}\)</span>.</li>
                    </ul>
                  </li>
                </ul>
              </li>
            </ul>
          </li>
          <li><strong>Summary</strong> of the <em>Causal Factors Hypothesis</em>:<br />
  An <strong>ideal representation</strong> is one in which <span style="color: purple">the <strong>features</strong> within the representation <em>correspond</em> to the underlying <strong>causes</strong> of the observed data</span>, with <strong><em>separate</em> features</strong> or <strong>directions</strong> in <em>feature space</em> corresponding to <strong><em>different</em> causes</strong>, so that <span style="color: purple">the <strong>representation</strong> <em><strong>disentangles</strong></em> the <strong>causes</strong> from one another</span>, especially those factors that are relevant to our applications.</li>
        </ul>
      </li>
      <li><span style="color: purple"><strong>“What is a <em>“salient factor”</em>?”</strong></span>
        <ul>
          <li>A <strong><em>“salient factor”</em></strong> is a causal factor (latent variable) that explains, <em><strong>well</strong></em>, the observed variations in \(X\).
            <ul>
              <li><button class="showText" value="show" onclick="showTextPopHide(event);">Illustration: Statistical Saliency</button>
  <img src="https://cdn.mathpix.com/snip/images/TTMVZWgnTfvYTrRbqLCW5VGoOZSUPC21oMaPDhijp-c.original.fullsize.png" alt="img" width="100%" hidden="" /></li>
              <li>What makes a feature <em>“salient”</em> for humans?<br />
  It could be something really simple like <strong>correlation</strong> or <strong>predictive power</strong>.<br />
  Ears are a salient feature of Humans because in a majority of cases, presence of one implies presence of another.</li>
              <li>Discriminative features as salient features:<br />
  Note that in object detection case, the predictive power is only measured in:<br />
  (ear \(\rightarrow\) person) direction, not (person \(\rightarrow\) ear) direction.<br />
  E.g. if your task was to discriminate between males and females, presence of ears would not be a useful feature even though all humans have ears. Compare this to the pimples case: in human vs dog classification, pimples are a really good predictor of ‘human’, even though they are not a salient feature of Humans.<br />
  Basically I think discriminative =/= salient</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>

    <p id="lst-p"><strong style="color: red">Notes:</strong></p>
    <ul>
      <li>Representation Learning can be done with both, <strong>generative</strong> and <strong>discriminative</strong> models.</li>
      <li>In DL, representation learning uses a composition of <strong>transformations</strong> of the input data (features) to create learned features.<br />
 <br /></li>
    </ul>

    <p><!-- 2. **Shared Representations:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents12} --></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents17">Distributed Representation:</strong><br />
 <strong>Distributed Representations</strong> of concepts are representations composed of many elements that can be set separately from each other.</p>

    <p id="lst-p"><strong>Distributed representations of concepts</strong> are one of the most important tools for <strong>representation learning</strong>:</p>
    <ul>
      <li>Distributed representations are <strong>powerful</strong> because <span style="color: purple">they can use \(n\) <strong>features</strong> with \(k\) <strong>values</strong> to <em><strong>describe</strong></em> \(k^{n}\) <strong>different concepts</strong></span>.</li>
      <li>Both <strong>neural networks with multiple hidden units</strong> and <strong>probabilistic models with multiple latent variables</strong> make use of the strategy of distributed representation.</li>
      <li><strong>Motivation for using Distributed Representations:</strong> <br />
  Many <strong>deep learning algorithms</strong> are motivated by the assumption that the <span style="color: goldenrod">hidden units can <em>learn to represent</em> the underlying <strong>causal factors</strong> that <em>explain</em> the data</span>.<br />
  Distributed representations are natural for this approach, because each direction in representation space can correspond to the value of a different underlying configuration variable.</li>
      <li><strong>Distributed vs Symbolic Representations</strong>:
        <ul>
          <li><strong>Number of “Representable” Configurations - by example</strong>:
            <ul>
              <li>An example of a <strong>distributed representation</strong> is a <span style="color: purple">vector of \(n\) binary features</span>.<br />
  It can take \(2^{n}\) configurations, each potentially corresponding to a different region in input space.<br />
  <button class="showText" value="show" onclick="showTextPopHide(event);">Illustration</button>
  <img src="https://cdn.mathpix.com/snip/images/aSpsvuJ06k0ls64IxpaAcv31EdfpEubzymZhuD6H5ZE.original.fullsize.png" alt="img" width="100%" hidden="" /></li>
              <li>An example of a <strong>symbolic representation</strong>, is <span style="color: purple">one-hot representation</span><sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">1</a></sup> where the input is associated with a single symbol or category.<br />
  If there are \(n\) symbols in the dictionary, one can imagine \(n\) feature detectors, each corresponding to the detection of the presence of the associated category.<br />
  In that case only \(n\) different configurations of the representation space are possible, carving \(n\) different regions in input space.<br />
  <button class="showText" value="show" onclick="showTextPopHide(event);">Illustration</button>
  <img src="https://cdn.mathpix.com/snip/images/LENWWJMJ5SNBNMLyIzxdYGLIMVqSEk9fGtkv0XQoi5s.original.fullsize.png" alt="img" width="100%" hidden="" /><br />
  A symbolic representation is a specific example of the broader class of non-distributed representations, which are representations that may contain many entries but without significant meaningful separate control over each entry.</li>
            </ul>
          </li>
          <li><strong>Generalization</strong>:<br />
  An important related concept that distinguishes a distributed representation from a symbolic one is that <span style="color: purple"><strong>generalization</strong> arises due to <em><strong>shared attributes between different concepts</strong></em></span>.
            <ul>
              <li><button class="showText" value="show" onclick="showTextPopHide(event);">Discussion/Analysis</button>
                <ul hidden="">
                  <li>As pure symbols, “cat” and “dog” are as far from each other as any other two symbols.<br />
  However, if one associates them with a meaningful distributed representation, then many of the things that can be said about cats can generalize to dogs and vice-versa.
                    <ul>
                      <li>For example, our distributed representation may contain entries such as “has_fur” or “number_of_legs” that have the same value for the embedding of both “cat ” and “dog.”<br />
  <strong>Neural language models</strong> that operate on distributed representations of words generalize much better than other models that operate directly on one-hot representations of words <em>(section 12.4)</em>.<br />
  <span style="color: purple">Distributed representations induce a rich similarity space, in which semantically close concepts (or inputs) are close in distance, a property that is absent from purely symbolic representations.</span></li>
                    </ul>
                  </li>
                </ul>
                <p><span style="color: purple">Distributed representations induce a rich similarity space, in which semantically close concepts (or inputs) are close in distance, a property that is absent from purely symbolic representations.</span><br />
  <a href="#bodyContents17gen"><strong>[Analysis: Generalization of Distributed Representations]</strong></a></p>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li><button class="showText" value="show" onclick="showTextPopHide(event);">Examples of <strong>learning algorithms</strong> based on <strong>non-distributed representations</strong>:</button>
        <ul hidden="">
          <li><strong>Clustering methods, including the \(k\)-means algorithm</strong>: each input point is assigned to exactly one cluster.</li>
          <li><strong>k-nearest neighbors algorithms</strong>: one or a few templates or prototype examples are associated with a given input. In the case of \(k&gt;1\), there are multiple values describing each input, but they can not be controlled separately from each other, so this does not qualify as a true distributed representation.</li>
          <li><strong>Decision trees</strong>: only one leaf (and the nodes on the path from root to leaf) is activated when an input is given.</li>
          <li><strong>Gaussian mixtures and mixtures of experts</strong>: the templates (cluster centers) or experts are now associated with a degree of activation. As with the k-nearest neighbors algorithm, each input is represented with multiple values, but those values cannot readily be controlled separately from each other.</li>
          <li><strong>Kernel machines with a Gaussian kernel (or other similarly local kernel)</strong>: although the degree of activation of each “support vector” or template example is now continuous-valued, the same issue arises as with Gaussian mixtures.</li>
          <li><strong>Language or translation models based on n-grams</strong>: The set of contexts (sequences of symbols) is partitioned according to a tree structure of suffixes. A leaf may correspond to the last two words being w1 and w2, for example. Separate parameters are estimated for each leaf of the tree (with some sharing being possible).</li>
        </ul>
        <blockquote>
          <p>For some of these non-distributed algorithms, the output is not constant by parts but instead interpolates between neighboring regions. The relationship between the number of parameters (or examples) and the number of regions they can define remains linear.</p>
        </blockquote>
      </li>
    </ul>

    <p><strong style="color: red" id="bodyContents17gen">Generalization of Distributed Representations:</strong><br />
 We know that for <strong>distributed representations</strong>, <span style="color: purple"><strong>Generalization</strong> arises due to <strong><em>shared attributes</em> between different concepts</strong></span>.</p>

    <p id="lst-p">But an important question is:<br />
 <span style="color: purple"><strong>“When and why can there be a statistical advantage from using a distributed representation as part of a learning algorithm?”</strong></span></p>
    <ul>
      <li>Distributed representations can have a <strong>statistical advantage</strong> when an <span style="color: goldenrod"><strong>apparently complicated structure</strong> can be <em><strong>compactly</strong></em> <strong>represented</strong> using a <strong><em>small number</em> of parameters</strong></span>.</li>
      <li>Some traditional nondistributed learning algorithms generalize only due to the <strong>smoothness assumption</strong>, which states that if \(u \approx v,\) then the target function \(f\) to be learned has the property that \(f(u) \approx f(v),\) in general.<br />
  There are many ways of formalizing such an assumption, but the end result is that if we have an example \((x, y)\) for which we know that \(f(x) \approx y,\) then we choose an estimator \(\hat{f}\) that approximately satisfies these constraints while changing as little as possible when we move to a nearby input \(x+\epsilon\).
        <ul>
          <li>This assumption is clearly very useful, but it <em>suffers</em> from the <strong>curse of dimensionality</strong>: in order to learn a target function that increases and decreases many times in many different regions,1 we may need a number of examples that is at least as large as the number of distinguishable regions.<br />
  One can think of each of these regions as a category or symbol: by having a separate degree of freedom for each symbol (or region), we can learn an arbitrary decoder mapping from symbol to value.<br />
  However, this does not allow us to generalize to new symbols for new regions.</li>
        </ul>
      </li>
      <li>If we are lucky, there may be some <strong><em>regularity</em></strong> in the <strong>target function</strong>, besides being <em>smooth</em>.<br />
  For example, a <strong>convolutional network</strong> with <strong>max-pooling</strong> can recognize an object regardless of its location in the image, even though spatial translation of the object may not correspond to smooth transformations in the input space.</li>
    </ul>

    <p id="lst-p"><strong style="color: red">Justifying Generalization in distributed representations:</strong></p>
    <ul>
      <li><strong>Geometric justification (by analyzing binary, linear feature extractors (units)):</strong><br />
  Let us examine a special case of a distributed representation learning algorithm, that extracts <strong>binary features</strong> by thresholding <strong>linear functions</strong> of the input: <br />
  <button class="showText" value="show" onclick="showTextPopHide(event);">Discussion</button>
        <ul hidden="">
          <li>Each binary feature in this representation <em><strong>divides</strong></em> \(\mathbb{R}^{d}\) into a <strong>pair</strong> of <strong>half-spaces</strong>.</li>
          <li>The <em><strong>exponentially large</strong></em> <strong>number</strong> of <strong>intersections</strong> of \(n\) of the corresponding half-spaces <em>determines</em> the <span style="color: purple">number of regions this distributed representation learner can distinguish</span>.</li>
          <li>The <strong>number of regions generated by an arrangement of \(n\) hyperplanes in \(\mathbb{R}^{d}\)</strong>:<br />
  By applying a general result concerning the intersection of hyperplanes <em>(Zaslavsky, } 1975)</em>, one can show <em>(Pascanu et al, 2014b)</em> that the number of regions this binary feature representation can distinguish is:
            <p>$$\sum_{j=0}^{d}\left(\begin{array}{l}{n} \\ {j}\end{array}\right)=O\left(n^{d}\right)$$</p>
          </li>
          <li>Therefore, we see a <span style="color: purple"><strong>growth</strong> that is <em><strong>exponential</strong></em> in the <strong>input size</strong> and <em><strong>polynomial</strong></em> in the <strong>number of hidden units</strong></span>.</li>
          <li>This provides a <strong>geometric argument</strong> to <em>explain</em> the <span style="color: purple">generalization power of distributed representation</span>:<br />
  with \(\mathcal{O}(n d)\) parameters (for \(n\) linear-threshold features in \(\mathbb{R}^{d}\)) we can distinctly represent \(\mathcal{O}\left(n^{d}\right)\) regions in input space.
            <ul>
              <li>If instead we made no assumption at all about the data, and used a representation with unique symbol for each region, and separate parameters for each symbol to recognize its corresponding portion of \(\mathbb{R}^{d},\) then,<br />
  specifying \(\mathcal{O}\left(n^{d}\right)\) regions would require \(\mathcal{O}\left(n^{d}\right)\) examples.</li>
            </ul>
          </li>
          <li>More generally, the argument in favor of the distributed representation could be extended to the case where instead of using linear threshold units we use <strong>nonlinear</strong>, possibly <strong>continuous</strong>, <strong>feature extractors</strong> for each of the attributes in the distributed representation.<br />
  The argument in this case is that if a parametric transformation with \(k\) parameters can learn about \(r\) regions in input space, with \(k \ll r,\) and if obtaining such a representation was useful to the task of interest, then we could potentially generalize much better in this way than in a non-distributed setting where we would need \(\mathcal{O}(r)\) examples to obtain the same features and associated partitioning of the input space into \(r\) regions.<br />
  Using fewer parameters to represent the model means that we have fewer parameters to fit, and thus require far fewer training examples to generalize well.</li>
        </ul>
      </li>
      <li><strong>VC-Theory justification - Fixed Capacity</strong>:<br />
  <button class="showText" value="show" onclick="showTextPopHide(event);">Discussion</button>
        <div hidden="">
          <p>The <strong>capacity</strong> remains <em><strong>limited</strong></em> despite being able to distinctly encode so many different regions.<br />
  For example, the <strong>VC-dimension</strong> of a <strong>neural network of linear threshold units</strong> is only \(\mathcal{O}(w \log w),\) where \(w\) is the number of weights <em>(Sontag, 1998</em>.</p>

          <p id="lst-p">This limitation arises because, while we can assign very many unique codes to representation space, we <strong>cannot</strong>:</p>
          <ul>
            <li>Use absolutely all of the code space</li>
            <li>Learn arbitrary functions mapping from the representation space \(h\) to the output \(y\) using a linear classifier.</li>
          </ul>

          <p>The <span style="color: purple">use of a <strong>distributed representation</strong> <em>combined</em> with a <strong>linear classifier</strong></span> thus <strong><em>expresses</em> a prior belief</strong> that <span style="color: purple">the classes to be recognized are <strong>linearly separable</strong> as a <strong>function</strong> of the underlying <strong>causal factors</strong> captured by \(h\)</span>.</p>

          <blockquote>
            <p>We will typically want to learn categories such as the set of all images of all green objects or the set of all images of cars, but not categories that require nonlinear, \(\mathrm{XOR}\) logic. For example, we typically do not want to partition the data into the set of all red cars and green trucks as one class and the set of all green cars and red trucks as another class.</p>
          </blockquote>
        </div>
      </li>
      <li><strong>Experimental justification</strong>:<br />
  <button class="showText" value="show" onclick="showTextPopHide(event);">Discussion</button>
  Though the above ideas have been abstract, they may be experimentally validated:
        <ul hidden="">
          <li><em>Zhou et al. (2015)</em> find that <strong>hidden units in a deep convolutional network</strong> trained on the ImageNet and Places benchmark datasets <span style="color: purple">learn features that are very often interpretable, corresponding to a label that humans would naturally assign</span>.<br />
  In practice it is certainly not always the case that hidden units learn something that has a simple linguistic name, but it is interesting to see this emerge near the top levels of the best computer vision deep networks. What such features have in common is that one could imagine learning about each of them without having to see all the configurations of all the others.</li>
          <li><em>Radford et al. (2015)</em> demonstrated that a <strong>generative model</strong> can <span style="color: purple">learn a representation of images of faces, with separate directions in representation space capturing different underlying factors of variation</span>.<br />
  The following illustration demonstrates that one direction in representation space corresponds to whether the person is male or female, while another corresponds to whether the person is wearing glasses.<br />
  <button class="showText" value="show" onclick="showTextPopHide(event);">Illustration: linear structure of latent variables</button>
  <img src="https://cdn.mathpix.com/snip/images/zCy3LJD1DqrdXwzCvbZeuG1DyuuPwR6xdHZoClbwN4o.original.fullsize.png" alt="img" width="100%" hidden="" /><br />
  These features were discovered automatically, not fixed a priori.<br />
  There is no need to have labels for the hidden unit classifiers: gradient descent on an objective function of interest naturally learns semantically interesting features, so long as the task requires such features.<br />
  We can learn about the distinction between male and female, or about the presence or absence of glasses, without having to characterize all of the configurations of the \(n − 1\) other features by examples covering all of these combinations of values.<br />
  <span style="color: goldenrod">This form of <strong>statistical separability</strong> is what allows one to generalize to new configurations of a person’s features that have never been seen during training.</span></li>
        </ul>
      </li>
    </ul>

    <p id="lst-p"><strong style="color: red">Notes:</strong></p>
    <ul>
      <li><code class="language-plaintext highlighter-rouge">page 542</code> As a counter-example, recent research from DeepMind (<a href="https://arxiv.org/abs/1803.06959">Morcos et al., 2018</a>) suggests that while some hidden units might appear to learn an interpretable feature, ‘these interpretable neurons are no more important than confusing neurons with difficult-to-interpret activity’.<br />
  Moreover, ‘networks which generalise well are much less reliant on single directions [ie. hidden units] than those which memorise’. See more in the DeepMind <a href="https://deepmind.com/blog/understanding-deep-learning-through-neuron-deletion/">blog post</a>.</li>
      <li><span style="color: goldenrod">Distributed representations based on latent variables can obtain all of the advantages of representation learning that we have seen with deep feedforward and recurrent networks.</span></li>
      <li><strong>Food for Thought (F2T):</strong><br />
  <em>“since feature engineering was made obsolete by deep learning, algorithm engineering will be made obsolete by meta-learning”</em> - Sohl-Dickstein<br />
 <br /></li>
    </ul>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents18">Deep Representations - Exponential Gain from Depth:</strong></p>

    <p><strong style="color: red">Exponential Gain in MLPs:</strong><br />
 We have seen in (section 6.4.1) that multilayer perceptrons are <strong>universal approximators</strong>, and that some functions can be represented by <strong>exponentially smaller <em>deep</em> networks</strong> compared to <em><strong>shallow</strong></em> <strong>networks</strong>.<br />
 This <strong>decrease in model size</strong> leads to <em>improved</em> <strong>statistical efficiency</strong>.</p>

    <p>Similar results apply, more generally, to other kinds of <strong>models with</strong> <span style="color: purple"><strong><em>distributed</em> hidden representations</strong></span>.</p>

    <p><strong style="color: red">Justification/Motivation:</strong><br />
 In this and other AI tasks, <span style="color: purple">the <strong>factors</strong> that can be <em>chosen almost <strong>independently</strong> from each other</em> yet still <em>correspond to <strong>meaningful inputs</strong></em> are more likely to be <strong><em>very high-level</em></strong> and <strong>related</strong> in <strong><em>highly nonlinear ways</em></strong> to the <strong>input</strong></span>.<br />
 <em>Goodfellow et al.</em> argue that this <span style="color: purple">demands <strong>deep distributed representations</strong></span>, where the <strong>higher level features</strong> (seen as functions of the input) or <strong>factors</strong> (seen as generative causes) are <em>obtained through the <strong>composition</strong> of many <strong>nonlinearities</strong></em>.</p>
    <blockquote>
      <p>E.g. the example of a generative model that learned about the explanatory factors underlying images of faces, including the person’s gender and whether they are wearing glasses.<br />
     It would not be reasonable to expect a shallow network, such as a linear network, to learn the complicated relationship between these abstract explanatory factors and the pixels in the image.</p>
    </blockquote>

    <p id="lst-p"><strong style="color: red">Universal Approximation property in Models (from Depth):</strong></p>
    <ul>
      <li>It has been proven in many different settings that <span style="color: purple"><strong>organizing computation through the composition of many nonlinearities</strong> and a <strong>hierarchy of reused features</strong> can give an <em><strong>exponential boost</strong></em> to <strong>statistical efficiency</strong></span>, on top of the <em><strong>exponential boost</strong></em> given by using a <strong>distributed representation</strong>.</li>
      <li>Many kinds of networks (e.g., with saturating nonlinearities, Boolean gates, sum/products, or RBF units) with a <strong>single hidden layer</strong> can be shown to be <strong>universal approximators</strong>.<br />
  A model family that is a universal approximator can approximate a large class of functions (including all continuous functions) up to any non-zero tolerance level, given enough hidden units.<br />
  However, the required number of hidden units may be very large.</li>
      <li>Theoretical results concerning the <strong>expressive power of deep architectures</strong> state that <span style="color: purple">there are families of functions that can be represented efficiently by an architecture of depth \(k\)</span>, but would require an <em><strong>exponential number</strong></em> of <strong>hidden units</strong> (wrt. <strong>input size</strong>) with <em><strong>insufficient</strong></em> <strong>depth</strong> (depth \(2\) or depth \(k − 1\)).</li>
    </ul>

    <p id="lst-p"><strong style="color: red">Exponential Gains in Structured Probabilistic Models:</strong></p>
    <ul>
      <li><strong>PGMs as Universal Approximators</strong>:
        <ul>
          <li>Just like <strong>deterministic feedforward networks</strong> are <strong>universal approximators</strong> of <strong style="color: goldenrod"><em>functions</em></strong>.<br />
  Many <strong>structured probabilistic models</strong> with a <strong>single hidden layer</strong> of <strong>latent variables</strong>, including restricted Boltzmann machines and deep belief networks, are <strong>universal approximators</strong> of <strong style="color: goldenrod"><em>probability distributions</em></strong> <em>(Le Roux and Bengio, 2008, 2010; Montúfar and Ay, 2011; Montúfar, 2014; Krause et al., 2013)</em>.</li>
        </ul>
      </li>
      <li><strong>Exponential Gain from Depth in PGMs</strong>:
        <ul>
          <li>Just like a <em>sufficiently</em> <em><strong>deep</strong></em> <strong>feedforward network</strong> can have an <em><strong>exponential</strong></em> <strong>advantage</strong> over a network that is too <em><strong>shallow</strong></em>.  <br />
  Such results can also be obtained for other models such as <strong>probabilistic models</strong>.
            <ul>
              <li>E.g. The <strong>sum-product network (SPN)</strong> <em>(Poon and Domingos, 2011)</em>.<br />
  These models use polynomial circuits to compute the probability distribution over a set of random variables.
                <ul>
                  <li><em>Delalleau and Bengio (2011)</em> showed that there exist probability distributions for which a minimum depth of SPN is required to avoid needing an exponentially large model.</li>
                  <li>Later, <em>Martens and Medabalimi (2014)</em> showed that there are significant differences between every two finite depths of SPN, and that some of the constraints used to make SPNs tractable may limit their representational power.</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>

    <p><strong style="color: red">Expressiveness of Convolutional Networks:</strong><br />
 Another interesting development is a set of theoretical results for the expressive power of families of deep circuits related to convolutional nets:<br />
 They highlight an <em><strong>exponential advantage</strong></em> for the deep circuit even when the <span style="color: purple">shallow circuit is allowed to <em>only</em> <strong>approximate</strong> the function computed by the deep circuit</span>  (Cohen et al., 2015).<br />
 By comparison, previous theoretical work made claims regarding only the case where the shallow circuit must exactly replicate particular functions.</p>

    <p id="lst-p"><strong style="color: red">Notes:</strong></p>
    <ul>
      <li><a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem">Universal Approximation Theorem (wiki)</a></li>
      <li><a href="https://en.wikipedia.org/wiki/Stone–Weierstrass_theorem">Stone–Weierstrass Approximation Theorem</a><br />
 <br /></li>
    </ul>
  </li>
</ol>

<hr />

<h2 id="content2">Unsupervised Representation Learning</h2>

<ol>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents211">Unsupervised Representation Learning:</strong><br />
In <strong>Unsupervised feature learning</strong>, features are learned with <em><strong>unlabeled</strong></em> <strong>data</strong>.</p>

    <p>The <strong>Goal</strong> of unsupervised feature learning is often to <span style="color: purple">discover low-dimensional features that captures some structure underlying the high-dimensional input data</span>.</p>

    <p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Examples:</button></p>
    <div hidden="">
      <ul>
        <li>(Unsupervised) Dictionary Learning</li>
        <li>ICA/PCA</li>
        <li>AutoEncoders</li>
        <li>Matrix Factorization</li>
        <li>Clustering Algorithms</li>
      </ul>
    </div>

    <p><strong style="color: red">Learning:</strong><br />
Unsupervised deep learning algorithms have a main training objective but also <span style="color: purple"><strong>learn a representation</strong> as a <em><strong>side effect</strong></em></span>.</p>

    <p><strong style="color: red">Unsupervised Learning for Semisupervised Learning:</strong>  xw
When the feature learning is performed in an unsupervised way, it enables a form of <strong>semisupervised learning</strong> where features learned from an unlabeled dataset are then employed to improve performance in a supervised setting with labeled data.<br />
<br /></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents21">Greedy Layer-Wise Unsupervised Pretraining:</strong><br />
 <strong>(Greedy Layer-Wise) Unsupervised Pretraining</strong></p>

    <ul id="lst-p">
      <li><strong>Greedy</strong>: it is a <strong>greedy algorithm</strong>.<br />
  It optimizes each piece of the solution independently, one piece at a time, rather than jointly optimizing all pieces.</li>
      <li><strong>Layer-Wise</strong>: the independent pieces are the layers of the network<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">2</a></sup>.</li>
      <li><strong>Unsupervised</strong>: each layer is trained with an unsupervised representation learning algorithm.</li>
      <li><strong>Pretraining</strong><sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">3</a></sup>: it is supposed to be only a first step before a joint training algorithm is applied to fine-tune all the layers together.</li>
    </ul>

    <p>This procedure is a canonical example of how a representation learned for one task (unsupervised learning, trying to capture the shape of the input distribution) can sometimes be useful for another task (supervised learning with the same input domain).</p>

    <p id="lst-p"><strong style="color: red">Algorithm/Procedure:</strong><br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">Algorithm</button>
 <img src="https://cdn.mathpix.com/snip/images/1wrL7C3u0dxHDgsAnbJi7tXljo56azdawSlLh7m1tk4.original.fullsize.png" alt="img" width="100%" hidden="" /></p>
    <ul>
      <li><strong>Supervised Learning Phase:</strong><br />
  It may involve:
        <ol>
          <li>Training a simple classifier on top of the features learned in the pretraining phase.</li>
          <li>Supervised <strong>fine-tuning</strong> of the entire network learned in the pretraining phase.</li>
        </ol>
      </li>
    </ul>

    <p id="lst-p"><strong style="color: red">Interpretation in Supervised Settings:</strong><br />
 In the context of a <strong>supervised learning</strong> task, the procedure can be viewed as:</p>
    <ul>
      <li>A <strong>Regularizer</strong>.<br />
  In some experiments, pretraining decreases test error without decreasing training error.</li>
      <li>A form of <strong>Parameter Initialization</strong>.</li>
    </ul>

    <p id="lst-p"><strong style="color: red">Applications:</strong></p>
    <ul>
      <li><strong>Training Deep Models</strong>:<br />
  Greedy layer-wise training procedures based on unsupervised criteria have long been used to sidestep the difficulty of jointly training the layers of a deep neural net for a supervised task.<br />
  The deep learning renaissance of 2006 began with the discovery that this greedy learning procedure could be used to find a good initialization for a joint learning procedure over all the layers, and that this approach could be used to successfully train even fully connected architectures.<br />
  Prior to this discovery, only convolutional deep networks or networks whose depth resulted from recurrence were regarded as feasible to train.</li>
      <li><strong>Parameter Initialization</strong>:<br />
  THey can also be used as initialization for other unsupervised learning algorithms, such as:
        <ul>
          <li><strong>Deep Autoencoders</strong> <em>(Hinton and Salakhutdinov, 2006)</em></li>
          <li><strong>Probabilistic mModels</strong> with <strong><em>many layers of latent variables</em></strong>:<br />
  E.g. <strong>deep belief networks (DBNs)</strong> <em>(Hinton et al., 2006)</em> and <strong>deep Boltzmann machines (DBMs)</strong> <em>(Salakhutdinov and Hinton, 2009a)</em>.</li>
        </ul>
      </li>
    </ul>

    <p><br /></p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents22">Clustering | K-Means:</strong></li>
  <li><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents23">Local Linear Embeddings:</strong></li>
  <li><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents24">Principal Components Analysis (PCA):</strong></li>
  <li><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents25">Independent Components Analysis (ICA):</strong></li>
  <li><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents26">(Unsupervised) Dictionary Learning:</strong></li>
</ol>

<!-- 7. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents27}
8. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents28} -->

<hr />

<h2 id="content3">Supervised Representation Learning</h2>

<ol>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents311">Supervised Representation Learning:</strong><br />
In <strong>Supervised feature learning</strong>, features are learned using <em><strong>labeled</strong></em> <strong>data</strong>.</p>

    <p><strong style="color: red">Learning:</strong><br />
The data label allows the system to compute an error term, the degree to which the system fails to produce the label, which can then be used as feedback to correct the learning process (reduce/minimize the error).</p>

    <p id="lst-p"><strong>Examples:</strong></p>
    <ul>
      <li>Supervised Neural Networks</li>
      <li>Supervised Dictionary Learning</li>
    </ul>

    <p><strong style="color: red">FFNs as Representation Learning Algorithms:</strong><br />
<button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Discussion</button></p>
    <ul hidden="">
      <li>We can think of <strong>Feed-Forward Neural Networks</strong> trained by <em>supervised learning</em> as performing a kind of representation learning.</li>
      <li>All the layers except the last layer (usually a linear classifier), are basically producing representations (featurizing) of the input.</li>
      <li>Training with a supervised criterion naturally leads to the representation at every hidden layer (but more so near the top hidden layer) taking on properties that make the classification task easier:<br />
  E.g. Making classes linearly separable in the latent space.</li>
      <li>The features in the penultimate layer should learn different properties depending on the type of the last layer.</li>
      <li>Supervised training of feedforward networks does not involve explicitly imposing any condition on the learned intermediate features.</li>
      <li>We can, however, explicitly impose certain desirable conditions.<br />
  <button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Example: Learning Independent Representations</button>
        <p hidden="">
  Suppose we want to learn a representation that makes density estimation easier. Distributions with more independences are easier to model, so we could design an objective function that encourages the elements of the representation vector \(\boldsymbol{h}\) to be independent.<br />
  </p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents31">Greedy Layer-Wise Supervised Pretraining:</strong><br />
 As discussed in section <strong>8.7.4</strong>, it is also possible to have greedy layer-wise supervised pretraining.<br />
 This builds on the <strong>premise</strong> that <span style="color: goldenrod">training a shallow network is easier than training a deep one</span>, which seems to have been validated in several contexts (Erhan et al., 2010).<br /></p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents32">Neural Networks:</strong></li>
  <li><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents33">Supervised Dictionary Learning:</strong></li>
</ol>

<!-- 4. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents34}
5. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents35} -->

<hr />

<h2 id="content4">Transfer Learning and Domain Adaptation</h2>

<p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Resources</button></p>
<ul hidden="">
  <li><a href="https://towardsdatascience.com/icml-2018-advances-in-transfer-multitask-and-semi-supervised-learning-2a15ef7208ec">ICML 2018: Advances in transfer, multitask, and semi-supervised learning (blog)</a></li>
  <li><a href="http://ruder.io/transfer-learning/">Transfer Learning (Ruder Blog!)</a></li>
  <li><a href="http://ruder.io/multi-task-learning-nlp/">Multi-Task Learning Objectives for Natural Language Processing (Ruder Blog)</a></li>
  <li><a href="http://ruder.io/multi-task/index.html">An Overview of Multi-Task Learning in Deep Neural Networks (Ruder Blog!)</a></li>
  <li><a href="http://ftp.cs.wisc.edu/machine-learning/shavlik-group/torrey.handbook09.pdf">Transfer Learning Overview (paper!)</a></li>
  <li><a href="https://machinelearningmastery.com/transfer-learning-for-deep-learning/">Transfer Learning for Deep Learning (Blog! - Resources!)</a></li>
  <li><a href="https://arxiv.org/abs/1411.1792">How transferable are features in deep neural networks? (paper)</a></li>
  <li><a href="https://blog.ml.cmu.edu/2019/09/13/on-learning-invariant-representations-for-domain-adaptation/">On Learning Invariant Representations for Domain Adaptation (blog!)</a></li>
  <li><a href="https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a">A Comprehensive Hands-on Guide to Transfer Learning with Real-World Applications in Deep Learning (blog)</a></li>
</ul>

<p><img src="https://cdn.mathpix.com/snip/images/Qk1gQN3lvxom7rIa4o9ZUDHnlPfJZCVDiM6pAvkiq3s.original.fullsize.png" alt="img" width="50%" /></p>

<ol>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents4" id="bodyContents41">Introduction - Transfer Learning and Domain Adaptation:</strong><br />
 <strong>Transfer Learning</strong> and <strong>Domain Adaptation</strong> refer to the situation where what has been learned in one setting (i.e., distribution \(P_{1}\)) is exploited to improve generalization in another setting (say distribution \(P_{2}\)).</p>

    <p>This is a generalization of <strong>unsupervised pretraining</strong>, where we transferred representations between an unsupervised learning task and a supervised learning task.</p>

    <p>In <strong>Supervised Learning</strong>: <strong>transfer learning</strong>, <strong>domain adaptation</strong>, and <strong>concept drift</strong> can be viewed as particular forms of <strong>Multi-Task Learning</strong>.</p>
    <blockquote>
      <p>However, <strong>Transfer Learning</strong> is a more general term that applies to both <strong>Supervised</strong> and <strong>Unsupervised Learning</strong>, as well as, <strong>Reinforcement Learning</strong>.</p>
    </blockquote>

    <p><strong style="color: red">Goal/Objective and Relation to Representation Learning:</strong><br />
 In the cases of <strong>Transfer Learning</strong>, <strong>Multi-Task Learning</strong>, and <strong>Domain Adaptation</strong>: 
 The <strong>Objective/Goal</strong> is to <span style="color: purple">take advantage of data from the first setting to extract information that may be useful when learning or even when directly making predictions in the second setting</span>.</p>

    <p>The <strong>core idea</strong> of <strong>Representation Learning</strong> is that <span style="color: purple">the same representation may be useful in both settings</span>.</p>

    <p>Thus, we can use <span style="color: goldenrod">shared representations</span> to accomplish Transfer Learning etc.<br />
 <strong>Shared Representations</strong> are useful to handle multiple modalities or domains, or to transfer learned knowledge to tasks for which few or no examples are given but a task representation exists.</p>

    <p><button class="showText" value="show" onclick="showTextPopHide(event);">Transfer Learning</button>
 <img src="https://cdn.mathpix.com/snip/images/VNQ1uX35tE4SKhEHGuTA53A03GOb9Ttb_LWI6QdOjkg.original.fullsize.png" alt="img" width="80%" /><br />
 <br /></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents4" id="bodyContents42">Transfer Learning:</strong><br />
 <strong>Transfer Learning</strong> (in ML) is the problem of storing knowledge gained while solving one problem and applying it to a different but related problem.</p>

    <p id="lst-p"><strong>Definition:</strong><br />
 <strong>Formally,</strong> the definition of transfer learning is given in terms of:</p>
    <ul>
      <li>A <strong>Domain \(\mathcal{D}=\{\mathcal{X}, P(X)\}\)</strong>, \(\:\:\) consisting of:
        <ul>
          <li><strong>Feature Space \(\mathcal{X}\)</strong></li>
          <li><strong>Marginal Probability Distribution \(P(X)\)</strong>,<br />
  where \(X=\left\{x_{1}, \ldots, x_{n}\right\} \in \mathcal{X}\).</li>
        </ul>
      </li>
      <li>A <strong>Task \(\mathcal{T}=\{\mathcal{Y}, f(\cdot)\}\)</strong>,<br />
  (given a specific domain \(\mathcal{D}=\{\mathcal{X}, P(X)\}\)) consisting of:
        <ul>
          <li>A <strong>label space \(\mathcal{Y}\)</strong></li>
          <li>An <strong>objective predictive function \(f(\cdot)\)</strong><br />
  It is learned from the training data, which consist of pairs \(\left\{x_ {i}, y_{i}\right\}\), where \(x_{i} \in X\) and \(y_{i} \in \mathcal{Y}\).<br />
  It can be used to predict the corresponding label, \(f(x)\), of a new instance \(x\).</li>
        </ul>
      </li>
    </ul>

    <p>Given a source domain \(\mathcal{D}_ {S}\) and learning task \(\mathcal{T}_ {S}\), a target domain \(\mathcal{D}_ {T}\) and learning task \(\mathcal{T}_ {T}\), <strong>transfer learning</strong> aims to help improve the learning of the target predictive function \(f_ {T}(\cdot)\) in \(\mathcal{D}_ {T}\) using the knowledge in \(\mathcal{D}_ {S}\) and \(\mathcal{T}_ {S}\), where \(\mathcal{D}_ {S} \neq \mathcal{D}_ {T}\), or \(\mathcal{T}_ {S} \neq \mathcal{T}_ {T}\).<br />
 <br /></p>

    <p>In Transfer Learning, the learner must perform two or more different tasks, but we assume that many of the factors that explain the variations in \(P_1\) are relevant to the variations that need to be captured for learning \(P_2\). This is typically understood in a supervised learning context, where the input is the same but the target may be of a different nature.<br />
 <button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Example: visual features</button></p>
    <div hidden="">
      <p>We may learn about one set of visual categories, such as cats and dogs, in the first setting, then learn about a different set of visual categories, such as ants and wasps, in the second setting.<br />
 If there is significantly more data in the first setting (sampled from \(P_1\)), then that may help to learn representations that are useful to quickly generalize from only very few examples drawn from \(P_2\).<br />
 Many visual categories share low-level notions of edges and visual shapes, the effects of geometric changes, changes in lighting, etc.</p>
    </div>

    <p id="lst-p"><strong style="color: red">Types of Transfer Learning:</strong></p>
    <ul>
      <li><strong>Inductive Transfer Learning</strong>:<br />
  \(\mathcal{D}_ {S} = \mathcal{D}_ {T} \:\:\: \text{  and  }\:\:\: \mathcal{T}_ {S} \neq \mathcal{T}_ {T}\)<br />
  <strong>e.g.</strong> \(\left(\mathcal{D}_ {S} = \text{ Wikipedia } = \mathcal{D}_ {T}\right) \:\: \text{  and  } \:\: \left(\mathcal{T}_ {S} = \text{ Skip-Gram }\right) \neq \left(\mathcal{T}_ {T} = \text{ Classification }\right)\)</li>
      <li><strong>Transductive Transfer Learning (Domain Adaptation)</strong>:<br />
  \(\mathcal{D}_ {S} \neq \mathcal{D}_ {T} \:\:\: \text{  and  }\:\:\: \mathcal{T}_ {S} = \mathcal{T}_ {T}\)<br />
  <strong>e.g.</strong> \(\left(\mathcal{D}_ {S} = \text{ Reviews }\right) \neq \left(\mathcal{D}_ {T} = \text{ Tweets }\right) \:\: \text{  and  } \:\: \left(\mathcal{T}_ {S} = \text{ Sentiment Analysis } = \mathcal{T}_ {T}\right)\)</li>
      <li><strong>Unsupervised Transfer Learning</strong>:<br />
  \(\mathcal{D}_ {S} \neq \mathcal{D}_ {T} \:\:\: \text{  and  }\:\:\: \mathcal{T}_ {S} \neq \mathcal{T}_ {T}\)<br />
  <strong>e.g.</strong> \(\left(\mathcal{D}_ {S} = \text{ Animals}\right) \neq \left(\mathcal{D}_ {T} = \text{ Cars}\right) \: \text{  and  } \: \left(\mathcal{T}_ {S} = \text{ Recog.}\right) \neq \left(\mathcal{T}_ {T} = \text{ Detection}\right)\)</li>
      <li><button class="showText" value="show" onclick="showTextPopHide(event);">Transfer Learning</button>
  <img src="https://cdn.mathpix.com/snip/images/VNQ1uX35tE4SKhEHGuTA53A03GOb9Ttb_LWI6QdOjkg.original.fullsize.png" alt="img" width="60%" hidden="" /></li>
    </ul>

    <p><strong style="color: red">Concept Drift:</strong><br />
 <strong>Concept Drift</strong> is a phenomena where the statistical properties of the target variable, which the model is trying to predict, change over time in unforeseen ways. This causes problems because the predictions become less accurate as time passes.</p>

    <p>It can be viewed as a form of <strong>transfer learning</strong> due to <span style="color: purple">gradual changes in the data distribution</span> over time.</p>

    <p><button class="showText" value="show" onclick="showTextPopHide(event);">Concept Drift in <strong>RL</strong></button></p>
    <div hidden="">
      <p>Another example is in reinforcement learning. Since the agent’s policy affects the environment, the agent learning and updating its policy directly results in a changing environment with shifting data distribution.</p>
    </div>

    <p><strong style="color: red">Unsupervised Deep Learning for Transfer Learning:</strong><br />
 <button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Discussion</button></p>
    <div hidden="">
      <p id="lst-p">Unsupervised Deep Learning for Transfer Learning has seen success in some machine learning competitions <em>(Mesnil et al., 2011; Goodfellow et al., 2011)</em>.<br />
 In the first of these competitions, the experimental setup is the following:</p>
      <ul>
        <li>Each participant is first given a dataset from the first setting (from distribution \(P_1\)), illustrating examples of some set of categories.</li>
        <li>The participants must use this to learn a good feature space (mapping the raw input to some representation), such that when we apply this learned transformation to inputs from the transfer setting (distribution \(P_2\) ), a linear classifier can be trained and generalize well from very few labeled examples.</li>
      </ul>

      <p>One of the most striking results found in this competition is that as an architecture makes use of deeper and deeper representations (learned in a purely unsupervised way from data collected in the first setting, \(P_1\)), the learning curve on the new categories of the second (transfer) setting \(P_2\) becomes much better.<br />
 For <strong>deep representations</strong>, <em>fewer labeled examples</em> of the <em>transfer tasks</em> are necessary to achieve the apparently <strong>asymptotic generalization</strong> performance.</p>
    </div>
    <p><br /></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents4" id="bodyContents43">Domain Adaptation:</strong><br />
 <strong>Domain Adaptation</strong> is a form of <em><strong>transfer learning</strong></em> where we aim at learning from a source data distribution a well performing model on a different (but related) target data distribution.</p>

    <p>It is a <em><strong>sequential</strong></em> process.</p>

    <p>In <strong>domain adaptation</strong>, the task (and the optimal input-to output mapping) remains the same between each setting, but the input distribution is slightly different.<br />
 <button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Example: Sentiment Analysis</button></p>
    <div hidden="">
      <p>Consider the task of sentiment analysis, which consists of determining whether a comment expresses positive or negative sentiment. Comments posted on the web come from many categories. A domain adaptation scenario can arise when a sentiment predictor trained on customer reviews of media content such as books, videos and music is later used to analyze comments about consumer electronics such as televisions or smartphones. <br />
 One can imagine that there is an underlying function that tells whether any statement is positive, neutral or negative, but of course the vocabulary and style may vary from one domain to another, making it more difficult to generalize across domains.<br />
 Simple unsupervised pretraining (with denoising autoencoders) has been found to be very successful for sentiment analysis with domain adaptation <em>(Glorot et al., 2011b)</em>.</p>
    </div>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents4" id="bodyContents44">Multitask Learning:</strong><br />
 <strong>Multitask Learning</strong> is a <em><strong>transfer learning</strong></em> where multiple learning tasks are solved at the same time, while exploiting commonalities and differences across tasks.</p>

    <p>In particular, it is an approach to <strong>inductive transfer</strong> that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better.</p>

    <p>It is a <em><strong>parallel</strong></em> process.</p>

    <p><strong style="color: red">Multitask vs Transfer Learning:</strong><br />
 <img src="https://cdn.mathpix.com/snip/images/Qk1gQN3lvxom7rIa4o9ZUDHnlPfJZCVDiM6pAvkiq3s.original.fullsize.png" alt="img" width="50%" /></p>
    <ol>
      <li><strong>Multi-Task Learning</strong>: general term for training on multiple tasks
        <ol>
          <li><strong>Joint Learning:</strong> by choosing mini-batches from two different tasks simultaneously/alternately</li>
          <li><strong>Pre-Training:</strong> first train on one task, then train on another<br />
 widely used for <strong>word embeddings</strong>.</li>
        </ol>
      </li>
      <li><strong>Transfer Learning</strong>:<br />
 a type of multi-task learning where we are focused on one task; by learning on another task then applying those models to our main task<br />
 <br /></li>
    </ol>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents4" id="bodyContents45">Representation Learning for the Transfer of Knowledge:</strong><br />
 We can use <strong>Representation Learning</strong> to achieve <strong>Multi-Task Learning</strong>, <strong>Transfer Learning</strong>, and <strong>Domain Adaptation</strong>.</p>

    <p id="lst-p">In general, <strong>Representation Learning</strong> can be used to achieve <strong>Multi-Task Learning</strong>, <strong>Transfer Learning</strong>, and <strong>Domain Adaptation</strong>, <span style="color: purple">when there exist <strong>features</strong> that are <em>useful for the different settings or tasks</em>, corresponding to <strong>underlying factors</strong> that <em>appear in more than one setting</em></span>.  <br />
 This applies in two cases:</p>
    <ul>
      <li><strong>Shared <em>Input</em> Semantics</strong>: <br />
  <button class="showText" value="show" onclick="showTextPopHide(event);">E.g. Shared Visual Features</button>
        <div hidden="">
          <p>We may learn about one set of visual categories, such as cats and dogs, in the first setting, then learn about a different set of visual categories, such as ants and wasps, in the second setting.<br />
  If there is significantly more data in the first setting (sampled from \(P_1\)), then that may help to learn representations that are useful to quickly generalize from only very few examples drawn from \(P_2\).<br />
  Many visual categories share low-level notions of edges and visual shapes, the effects of geometric changes, changes in lighting, etc.</p>
        </div>
        <p>In this case, we <strong>share</strong> the <strong><em>lower</em> layers</strong>, and have a <strong>task-dependent</strong> <strong><em>upper</em> layers</strong>.<br />
  <button class="showText" value="show" onclick="showTextPopHide(event);">Illustration:</button>
  <img src="https://cdn.mathpix.com/snip/images/717fBbHzwKW71RWB9Lc6gA8gdFmVzymEWs_klK6t-w4.original.fullsize.png" alt="img" width="100%" hidden="" /></p>
      </li>
      <li><strong>Shared <em>Output</em> Semantics</strong>:<br />
  <button class="showText" value="show" onclick="showTextPopHide(event);">E.g. <strong>Speech Recognition Systems</strong></button>
        <div hidden="">
          <p>A speech recognition system needs to produce valid sentences at the output layer, but the earlier layers near the input may need to recognize very different versions of the same phonemes or sub-phonemic vocalizations depending on which person is speaking.</p>
        </div>
        <p>In cases like these, it makes more sense to <strong>share</strong> the <strong><em>upper</em> layers</strong> (near the output) of the neural network, and have a <strong>task-specific</strong> <em><strong>preprocessing</strong></em>.<br />
  <button class="showText" value="show" onclick="showTextPopHide(event);">Illustration:</button>
  <img src="https://cdn.mathpix.com/snip/images/pFAT7AVYii3xvvOrZvGwn_lcnTZV4EVqCacUebAegYc.original.fullsize.png" alt="img" width="100%" hidden="" />
 <br /></p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents4" id="bodyContents46">K-Shot Learning:</strong><br />
 <strong>K-Shot (Few-Shot) Learning</strong> is a <em>supervised</em> learning setting (problem) where the goal is to learn from an extremely small number \(k\) of labeled examples (called <strong>shots</strong>).</p>

    <p><strong>General Setting:</strong><br />
 We first train a model on a <strong><em>large</em> dataset</strong> \(\mathcal{D}=\left\{\widetilde{\mathbf{x}}_ {i}, \widetilde{\gamma}_ {i}\right\}_ {i=1}^{N}\) of <strong>inputs</strong> \(\widetilde{\mathbf{x}}_ {i}\) and <strong>labels</strong> \(\widetilde{y}_ {i} \in\{1, \ldots, \widetilde{C}\}\) that indicate which of the \(\widetilde{C}\) <strong>classes</strong> each input belongs to.<br />
 Then, using knowledge from the model trained on the large dataset, we perform \(\mathrm{k}\)-shot learning with a <strong><em>small</em> dataset</strong> \(\mathcal{D}=\left\{\mathbf{x}_ {i}, y_ {i}\right\}_ {i=1}^{N}\) with \(C\) <strong><em>new</em> classes</strong>, labels \(y_ {i} \in\{\widetilde{C}+1, \widetilde{C}+C\}\) and <strong><em>\(k\)</em> examples (inputs)</strong> from each new class.<br />
 During test time we classify unseen examples (inputs) \(\mathbf{x}^{* }\) from the new classes \(C\) and evaluate the predictions against ground truth labels \(y^{* }\).</p>

    <p><strong style="color: red">Comparison to alternative Learning Paradigms:</strong><br />
 <img src="https://cdn.mathpix.com/snip/images/UcWrNybNWHVrfmFVHkf69cc6UiIivyatymeq-e99MSI.original.fullsize.png" alt="img" width="60%" /></p>

    <p><strong style="color: red">As Transfer Learning:</strong><br />
 Two extreme forms of <strong>transfer learning</strong> are <strong>One-Shot Learning</strong> and <strong>Zero-Shot Learning</strong>; they provide only <em><strong>one</strong></em> and <em><strong>zero</strong></em> labeled examples of the transfer task, respectively.</p>

    <p><strong style="color: red">One-Shot Learning:</strong><br />
 <strong>One-Shot Learning</strong> <em>(Fei-Fei et al., 2006)</em> is a form of <strong>k-shot learning</strong> where \(k=1\).</p>

    <p>It is possible because the representation learns to cleanly separate the underlying classes during the first stage.<br />
 During the transfer learning stage, only one labeled example is needed to infer the label of many possible test examples that all cluster around the same point in representation space.<br />
 This works to the extent that the factors of variation corresponding to these invariances have been cleanly separated from the other factors, in the learned representation space, and we have somehow learned which factors do and do not matter when discriminating objects of certain categories.</p>

    <p><strong style="color: red">Zero-Shot Learning:</strong><br />
 <strong>Zero-Shot Learning</strong> <em>(Palatucci et al., 2009; Socher et al., 2013b)</em> or <strong>Zero-data learning</strong> <em>(Larochelle et al., 2008)</em> is a form of <strong>k-shot learning</strong> where \(k=0\).</p>

    <p><strong>Example: Zero-Shot Learning Setting</strong><br />
 Consider the problem of having a learner read a large collection of text and then solve object recognition problems.<br />
 It may be possible to recognize a specific object class even without having seen an image of that object, if the text describes the object well enough.<br />
 For example, having read that a cat has four legs and pointy ears, the learner might be able to guess that an image is a cat, without having seen a cat before.</p>

    <p><strong>Justification and Interpretation:</strong><br />
 Zero-Shot Learning is only possible because <span style="color: purple"><strong>additional information</strong> has been exploited during training</span>.</p>

    <p id="lst-p">We can think of think of the zero-data learning scenario as including <strong><em>three</em> random variables</strong>:</p>
    <ol>
      <li>(Traditional) <strong>Inputs</strong> \(x\)</li>
      <li>(Traditional) <strong>Outputs</strong> or <strong>Targets</strong> \(\boldsymbol{y}\)</li>
      <li>(Additional) <strong>Random Variable <em>describing the task</em></strong>, \(T\)</li>
    </ol>

    <p id="lst-p">The model is trained to estimate the conditional distribution \(p(\boldsymbol{y} \vert \boldsymbol{x}, T)\).</p>
    <ul>
      <li><button class="showText" value="show" onclick="showTextPopHide(event);">Example: <em>updated</em> zero-shot learning setting</button>
        <div hidden="">
          <p>In the example of recognizing cats after having read about cats, the output is a binary variable \(y\) with \(y=1\) indicating “yes” and \(y=0\) indicating “no”.<br />
  The task variable \(T\) then represents questions to be answered such as “Is there a cat in this image?”.<br />
  If we have a training set containing unsupervised examples of objects that live in the Same space as \(T\), we may be able to infer the meaning of unseen instances of \(T\).<br />
  In our example of recognizing cats without having seen an image of the cat, it is important that we have had unlabeled text data containing sentences such as “cats have four legs” or “cats have pointy ears”.</p>
        </div>
      </li>
    </ul>

    <p><strong>Representing the task \(T\):</strong><br />
 Zero-shot learning requires \(T\) to be represented in a way that <span style="color: purple">allows some sort of <strong>generalization</strong></span>.<br />
 For example, \(T\) cannot be just a <em>one-hot code</em> indicating an object category.</p>
    <blockquote>
      <p><em>Socher et al. (2013 b)</em> provide instead a distributed representation of object categories by using a learned word embedding for the word associated with each category.</p>
    </blockquote>

    <p><strong style="color: red">Representation Learning for Zero-Shot Learning:</strong><br />
 The principle, underlying <strong>zero-shot learning</strong> as a form of <strong>transfer learning</strong>: <span style="color: purple">capturing a <strong>representation</strong> in <strong><em>one</em> modality</strong></span>, <span style="color: purple">a <strong>representation</strong> in <strong><em>another</em> modality</strong></span>, and the <span style="color: purple"><strong>relationship</strong></span> (in general a <strong>joint distribution</strong>) <span style="color: purple">between pairs \((\boldsymbol{x}, \boldsymbol{y})\)</span> consisting of <em>one observation \(\boldsymbol{x}\) in one modality</em> and <em>another observation \(\boldsymbol{y}\) in the other modality</em>, <em>(Srivastava and Salakhutdinov, 2012)</em>. <br />
 By learning all three sets of parameters (from \(\boldsymbol{x}\) to its representation, from \(\boldsymbol{y}\) to its representation, and the relationship between the two representations), concepts in one representation are anchored in the other, and vice-versa, allowing one to meaningfully generalize to new pairs.</p>

    <p>In particular, <span style="color: goldenrod">Transfer learning between two domains \(x\) and \(y\) enables zero-shot learning</span>.</p>

    <p><button class="showText" value="show" onclick="showTextPopHide(event);">Illustration</button>
 <img src="https://cdn.mathpix.com/snip/images/FhmatT_1_acHctjXuwq9kRjPlpqPCwLyOnMP0aSHzXI.original.fullsize.png" alt="img" width="100%" hidden="" /></p>

    <p><strong>Zero-Shot Learning in Machine Translation:</strong><br />
 <button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Discussion - Example</button></p>
    <div hidden="">
      <p>A similar phenomenon happens in machine translation <em>(Klementiev et al., 2012; Mikolov et al., 2013b; Gouws et al., 2014)</em>:<br />
 we have words in one language, and the relationships between words can be learned from unilingual corpora; on the other hand, we have translated sentences which relate words in one language with words in the other. Even though we may not have labeled examples translating word \(A\) in language \(X\) to word \(B\) in language \(Y\), we can generalize and guess a translation for word \(A\) because we have learned a distributed representation for words in language \(X\), a distributed representation for words in language \(Y\), and created a link (possibly two-way) relating the two spaces, via training examples consisting of matched pairs of sentences in both languages.<br />
 This transfer will be most successful if all three ingredients (the two representations and the relations between them) are learned jointly.</p>
    </div>

    <p><strong>Relation to Multi-modal Learning:</strong><br />
 Zero-Shot Learning can be performed using <strong>Multi-model Learning</strong>, and vice-versa.<br />
 The same principle of <strong>transfer learning</strong> with <strong>representation learning</strong> explain how one can perform either tasks.</p>

    <p id="lst-p"><strong style="color: red">Notes:</strong></p>
    <ul>
      <li><a href="https://pdfs.semanticscholar.org/8fc6/4f04a94033704453255c905796872e16f284.pdf?_ga=2.177121317.1868310785.1568811982-246730594.1555910960">K-Shot Learning (Thesis!)</a></li>
      <li><a href="https://sorenbouma.github.io/blog/oneshot/">One Shot Learning and Siamese Networks in Keras (Code - Tutorial)</a></li>
      <li><strong>Zero-Shot Learning</strong>: is a form of extending supervised learning to a setting of solving for example a classification problem when not enough labeled examples are available for all classes.
        <blockquote>
          <p>“Zero-shot learning is being able to solve a task despite not having received any training examples of that task.” - Goodfellow</p>
        </blockquote>
      </li>
      <li><strong>Detecting <em>Gravitational Waves</em></strong> is a form of <strong>Zero-Shot Learning</strong></li>
      <li>Few-shot, one-shot or zero-shot learning are encompassed in a recently emerging field known as <strong>meta-learning</strong>.<br />
  While traditionally including mainly classification, recent works in meta-learning have included regression and reinforcement learning (<a href="https://arxiv.org/abs/1606.04080">Vinyals et al., 2016</a>) (<a href="https://arxiv.org/abs/1606.04474">Andrychowicz et al., 2016</a>) (<a href="https://openreview.net/forum?id=rJY0-Kcll">Ravi &amp; Larochelle, 2017</a>) (<a href="https://arxiv.org/abs/1611.02779">Duan et al., 2017</a>) (<a href="https://arxiv.org/pdf/1703.03400.pdf">Finn et al., 2017</a>).<br />
  Works in this area seems to be primarily motivated by the notion of human-level AI, since humans appear to be able to require far fewer training data than most deep learning models.<br />
 <br /></li>
    </ul>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents4" id="bodyContents47">Multi-Modal Learning:</strong><br />
 <strong>Multi-Modal Learning</strong></p>

    <p><strong style="color: red">Representation Learning for Multi-modal Learning:</strong><br />
 The same principle, underlying <strong>zero-shot learning</strong> as a form of <strong>transfer learning</strong>, explains how one can perform multi-modal learning; capturing a representation in one modality, a representation in the other, and the relationship (in general a joint distribution) between pairs \((\boldsymbol{x}, \boldsymbol{y})\) consisting of one observation \(\boldsymbol{x}\) in one modality and another observation \(\boldsymbol{y}\) in the other modality <em>(Srivastava and Salakhutdinov, 2012)</em>. <br />
 By learning all three sets of parameters (from \(\boldsymbol{x}\) to its representation, from \(\boldsymbol{y}\) to its representation, and the relationship between the two representations), concepts in one representation are anchored in the other, and vice-versa, allowing one to meaningfully generalize to new pairs.</p>
  </li>
</ol>

<hr />

<h2 id="content5">Causal Factor Learning</h2>

<!-- 1. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents51}
2. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents52}
 -->

<ol>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents5" id="bodyContents53">Semi-Supervised Disentangling of Causal Factors:</strong></p>

    <p id="lst-p"><strong style="color: red">Quality of Representations:</strong><br />
 - An important question in Representation Learning is:<br />
 <span style="color: purple"><em>“what makes one representation better than another?”</em></span></p>
    <ol>
      <li>One answer to that is the <strong>Causal Factors Hypothesis:</strong><br />
 An <strong>ideal representation</strong> is one in which <span style="color: purple">the <strong>features</strong> within the representation <em>correspond</em> to the underlying <strong>causes</strong> of the observed data</span>, with <strong><em>separate</em> features</strong> or <strong>directions</strong> in <em>feature space</em> corresponding to <strong><em>different</em> causes</strong>, so that <span style="color: purple">the <strong>representation</strong> <em><strong>disentangles</strong></em> the <strong>causes</strong> from one another</span>.
        <ul>
          <li>This hypothesis <em>motivates</em> approaches in which we first <span style="color: purple">seek a <strong><em>good</em> representation</strong> for \(p(\boldsymbol{x})\)</span>.<br />
  This representation may also be a <em>good</em> representation for <strong>computing \(p(\boldsymbol{y} \vert \boldsymbol{x})\)</strong> if \(\boldsymbol{y}\) is among the <strong>most <em>salient</em></strong> <strong>causes</strong> of \(\boldsymbol{x}\)<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">4</a></sup> <sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">5</a></sup>.</li>
        </ul>
      </li>
      <li><strong>Ease of Modeling:</strong><br />
 In many approaches to representation learning, we are often concerned with a representation that is <strong>easy to model</strong> (e.g. sparse entries, independent entries etc.).<br />
 It is not directly observed, however, that <span style="color: purple">a representation that <strong>cleanly separates the underlying causal factors</strong> is, also, one that is <strong>easy to model</strong></span>.<br />
 The answer to that is an <strong><em>extension</em></strong> of the <strong>Causal Factor Hypothesis:</strong><br />
 For <em><strong>many</strong></em> <strong>AI tasks</strong> the two properties <strong>coincide</strong>: once we are able to <span style="color: purple">obtain the underlying explanations for the observations</span>, it generally becomes <span style="color: purple">easy to isolate individual attributes</span> from the others.<br />
 Specifically, <strong>if</strong> a <span style="color: purple"><strong>representation \(\boldsymbol{h}\)</strong> <em>represents</em> many of the <em><strong>underlying causes</strong></em> of the <strong>observed \(\boldsymbol{x}\)</strong></span>, <strong>and</strong> the <span style="color: purple"><strong>outputs \(\boldsymbol{y}\)</strong> are among the <strong>most <em>salient causes</em></strong></span>, <strong>then</strong> it is <span style="color: purple">easy to <strong>predict</strong> \(\boldsymbol{y}\) from \(\boldsymbol{h}\)</span>.</li>
    </ol>

    <div class="borderexample">
      <p><span> The complete <strong>Causal Factors Hypothesis</strong> <span style="color: goldenrod"><em>motivates</em> <strong>Semi-Supervised Learning</strong> via <strong>Unsupervised Representation Learning</strong></span>.</span></p>
    </div>

    <p><strong style="color: red">Analysis - When does Semi-Supervised Learning Work:</strong><br />
 <button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Full Analysis</button></p>
    <div hidden="">
      <ul>
        <li><strong>When does Semi-Supervised Disentangling of Causal Factors <em>Work</em>?</strong><br />
  Let’s start by considering two scenarios where Semi-Supervised Learning via Unsupervised Representation Learning Fails and Succeeds:
          <ul>
            <li><button class="showText" value="show" onclick="showTextPopHide(event);">Failure</button>
              <div hidden="">
                <p>Let us see how semi-supervised learning can fail because <span style="color: purple"><strong>unsupervised learning</strong> of \(p(\mathbf{x})\) is of <strong><em>no help</em> to learn</strong> \(p(\mathbf{y} \vert \mathbf{x})\)</span>.<br />
  Consider the case where \(p(\mathbf{x})\) is <strong>uniformly distributed</strong> and we want to learn \(f(\boldsymbol{x})=\mathbb{E}[\mathbf{y} \vert \boldsymbol{x}]\).<br />
  Clearly, <strong>observing a training set</strong> of \(\boldsymbol{x}\) values <em><strong>alone</strong></em> gives us <strong>no information</strong> about \(p(\mathbf{y} \vert \mathbf{x})\).</p>
              </div>
            </li>
            <li><button class="showText" value="show" onclick="showTextPopHide(event);">Success</button>
              <div hidden="">
                <p>Consider the case where \(x\) arises from a mixture, with one mixture component per value of \(y\).<br />
  If the mixture components are <strong>well-separated</strong>, then modeling \(p(x)\) reveals precisely where each component is, and <span style="color: purple">a <strong>single labeled example</strong> of <em>each class</em> will then be <em><strong>enough</strong></em> to perfectly <strong>learn</strong> \(p(\mathbf{y} \vert \mathbf{x})\)</span>.  <br />
  <button class="showText" value="show" onclick="showTextPopHide(event);">Illustration: Well-Separated Mixture</button>
  <img src="https://cdn.mathpix.com/snip/images/ctuu5i3mZUhHsIdzpRMp4SJXHz_iWb_5_YnCfWXuCZg.original.fullsize.png" alt="img" width="100%" hidden="" /></p>
              </div>
            </li>
          </ul>

          <div class="borderexample">
            <p><span>Thus, we conclude that semi-supervised learning works <span style="color: goldenrod">when \(p(\mathbf{y} \vert \mathbf{x})\) and \(p(\mathbf{x})\) are <strong>tied</strong> together</span>.</span></p>
          </div>
        </li>
        <li><strong>When are \(p(\mathbf{y} \vert \mathbf{x})\) and \(p(\mathbf{x})\) tied?</strong><br />
  This happens when \(\mathbf{y}\) is closely associated with one of the causal factors of \(\mathbf{x}\), then \(p(\mathbf{x})\) and \(p(\mathbf{y} \vert \mathbf{x})\) will be strongly tied.
          <ul>
            <li>Thus, unsupervised representation learning that tries to disentangle the underlying factors of variation is likely to be useful as a semi-supervised learning strategy.</li>
          </ul>
        </li>
      </ul>

      <p id="lst-p">Now, Consider the assumption that \(\mathbf{y}\) is one of the causal factors of \(\mathbf{x}\), and let \(\mathbf{h}\) represent all those factors:</p>
      <ul>
        <li>The <strong>“true” generative process</strong> can be conceived as <span style="color: purple"><em><strong>structured</strong></em> according to this <strong>directed graphical model</strong></span>, with \(\mathbf{h}\) as the <strong>parent</strong> of \(\mathbf{x}\):
          <p>$$p(\mathbf{h}, \mathbf{x})=p(\mathbf{x} \vert \mathbf{h}) p(\mathbf{h})$$</p>
          <ul>
            <li>As a consequence, the <strong>data</strong> has <strong>marginal probability</strong>:
              <p>$$p(\boldsymbol{x})=\mathbb{E}_ {\mathbf{h}} p(\boldsymbol{x} \vert \boldsymbol{h})$$</p>
            </li>
          </ul>

          <p>From this straightforward observation, we <strong>conclude</strong> that:</p>
          <div class="borderexample">
            <p><span>The <span style="color: goldenrod"><strong>best possible model</strong> of \(\mathbf{x}\) (wrt. <strong>generalization</strong>) is the one that <em><strong>uncovers</strong></em> the above <strong>“true” structure</strong>, with \(\boldsymbol{h}\) as a <strong>latent variable</strong> that <em><strong>explains</strong></em> the <strong>observed variations</strong> in \(\boldsymbol{x}\)</span>.</span></p>
          </div>
          <p>I.E. The <strong>“ideal” representation learning</strong> discussed above should thus <strong>recover these latent factors</strong>.<br />
  If \(\mathbf{y}\) is one of these (or closely related to one of them), then it will be very easy to learn to predict \(\mathbf{y}\) from such a representation.</p>
        </li>
        <li>We also see that the <strong>conditional distribution</strong> of \(\mathbf{y}\) given \(\mathbf{x}\) is <span style="color: purple">tied by <em>Bayes’ rule</em> to the <strong>components in the above equation</strong></span>:
          <p>$$p(\mathbf{y} \vert \mathbf{x})=\frac{p(\mathbf{x} \vert \mathbf{y}) p(\mathbf{y})}{p(\mathbf{x})}$$</p>

          <div class="borderexample">
            <p><span>Thus the <span style="color: purple"><strong>marginal</strong> \(p(\mathbf{x})\) is <em><strong>intimately tied</strong></em> to the <strong>conditional</strong> \(p(\mathbf{y} \vert \mathbf{x})\), and knowledge of the structure of the former should be helpful to learn the latter</span>.</span></p>
          </div>
          <p><br /></p>
        </li>
      </ul>
      <div class="borderexample">
        <p><span>Therefore, <strong>in situations respecting these assumptions, semi-supervised learning should improve performance</strong>.</span></p>
      </div>
    </div>

    <p id="lst-p"><strong>Justifying the setting where Semi-Supervised Learning Works:</strong></p>
    <ul>
      <li><strong>Semi-Supervised Learning<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">6</a></sup> <em>Works</em></strong> when: <span style="color: goldenrod">\(p(\mathbf{y} \vert \mathbf{x})\) and \(p(\mathbf{x})\) are <em><strong>tied together</strong></em></span>.</li>
      <li><strong>\(p(\mathbf{y} \vert \mathbf{x})\) and \(p(\mathbf{x})\) are <em>Tied</em></strong> when: \(\mathbf{y}\) is <em><strong>closely associated</strong></em> with one of the <strong>causal factors</strong> of \(\mathbf{x}\), or it is a <strong>causal factor itself</strong>.
        <ul>
          <li>Let \(\mathbf{h}\) represent all the <strong>causal factors</strong> of \(\mathbf{x}\), and let \(\mathbf{y} \in \mathbf{h}\) (be a <strong>causal factor</strong> of \(\mathbf{x}\)), then:<br />
  The <strong>“true” generative process</strong> can be conceived as <span style="color: purple"><em><strong>structured</strong></em> according to this <strong>directed graphical model</strong></span>, with \(\mathbf{h}\) as the <strong>parent</strong> of \(\mathbf{x}\):
            <p>$$p(\mathbf{h}, \mathbf{x})=p(\mathbf{x} \vert \mathbf{h}) p(\mathbf{h})$$</p>
            <ul>
              <li><strong>Thus</strong>, the <strong>Marginal Probability of the Data \(p(\mathbf{x})\)</strong> is:
                <ol>
                  <li><span style="color: purple"><em><strong>Tied</strong></em> to the <strong>conditional</strong> \(p(\mathbf{x} \vert \mathbf{h})\)</span> as:
                    <p>$$p(\boldsymbol{x})=\mathbb{E}_ {\mathbf{h}} p(\boldsymbol{x} \vert \boldsymbol{h})$$</p>
                    <p>\(\implies\)</p>
                    <ul>
                      <li>The <span style="color: goldenrod"><strong>best possible model</strong> of \(\mathbf{x}\) (wrt. <strong>generalization</strong>)</span> is the one that <span style="color: goldenrod"><em><strong>uncovers</strong></em> the above <strong>“true” structure</strong></span>, with <span style="color: goldenrod">\(\boldsymbol{h}\) as a <strong>latent variable</strong> that <em><strong>explains</strong></em> the <strong>observed variations</strong> in \(\boldsymbol{x}\)</span>.<br />
  I.E. The <strong>“ideal” representation learning</strong> discussed above <strong>should</strong> thus <strong><em>recover</em></strong> these <strong>latent factors</strong>.</li>
                    </ul>
                  </li>
                  <li><span style="color: purple"><em><strong>(intimately) Tied</strong></em> to the <strong>conditional</strong> \(p(\mathbf{y} \vert \mathbf{x})\)</span> (by <strong>Bayes’ rule</strong>) as:
                    <p>$$p(\mathbf{y} \vert \mathbf{x})=\frac{p(\mathbf{x} \vert \mathbf{y}) p(\mathbf{y})}{p(\mathbf{x})}$$</p>
                  </li>
                </ol>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>

    <div class="borderexample">
      <p><span>Therefore, <strong>in situations respecting these assumptions, semi-supervised learning should improve performance</strong>.</span></p>
    </div>

    <p id="lst-p"><strong style="color: red">Encoding/Learning Causal Factors:</strong></p>
    <ul>
      <li><strong style="color: DarkRed">Problem - Number of Causal Factors:</strong><br />
  An important research problem regards the fact that <span style="color: purple">most observations are formed by an <em>extremely</em> <strong><em>large number</em> of underlying causes</strong></span>.
        <ul>
          <li>Suppose \(\mathbf{y}=\mathrm{h}_ {i}\), but the unsupervised learner does not know which \(\mathrm{h}_ {i}\):
            <ul>
              <li>The <strong>brute force solution</strong> is for an unsupervised learner to <span style="color: purple">learn a representation that <strong>captures <em>all</em> the reasonably salient generative factors</strong> \(\mathrm{h}_ {j}\) and disentangles them from each other</span>, thus making it easy to predict \(\mathbf{y}\) from \(\mathbf{h}\), regardless of which \(\mathrm{h}_ {i}\) is associated with \(\mathbf{y}\).
                <ul>
                  <li>In practice, the brute force solution is <strong>not feasible</strong> because it is <em>not possible to capture all or most of the factors of variation that influence an observation</em>.<br />
  For example, in a visual scene, should the representation always encode all of the smallest objects in the background?<br />
  It is a well-documented psychological phenomenon that human beings fail to perceive changes in their environment that are not immediately relevant to the task they are performing <em>Simons and Levin (1998)</em>.</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li><strong style="color: DarkRed">Solution - Determining which causal factor to encode/learn:</strong> <br />
  An important research frontier in semi-supervised learning is determining <span style="color: purple"><em>“what to encode in each situation”</em></span>.
        <ul>
          <li>Currently, there are <strong>two main strategies</strong> for <em>dealing with a large number of underlying causes</em>:
            <ol>
              <li>Use a <strong>supervised learning signal</strong> at the same time as the <strong>(<em>“plus”</em>) unsupervised learning signal</strong>,<br />
 so that the model will choose to capture the most relevant factors of variation.</li>
              <li>Use <strong>much larger representations</strong> if using <em>purely unsupervised learning</em>.</li>
            </ol>
          </li>
          <li><strong>New (Emerging) Strategy</strong> for <strong>unsupervised learning</strong>:<br />
  <span style="color: goldenrod">Redefining the <em>definition</em> of “<strong>salient</strong>” factors</span>.</li>
        </ul>
      </li>
    </ul>

    <p><!-- __Modifying the definition of "*Salient*" Factors:__{: style="color: red"}   --></p>

    <p id="lst-p"><strong style="color: red">The definition of “<em>Salient</em>“:</strong></p>
    <ul>
      <li>The <em>current</em> <strong>definition of <em>“salient”</em> factors:</strong><br />
  In practice, we <em>encode</em> the definition of <em>“salient”</em> by using the <strong>objective criterion</strong> (e.g. MSE).
        <blockquote>
          <p>Historically, autoencoders and generative models have been trained to optimize a fixed criterion, often similar to MSE.</p>
        </blockquote>

        <ul>
          <li><strong>Problem with current definition</strong>:<br />
  Since these fixed criteria determine which causes are considered salient, they will be emphasizing different factors depending on their e.g. effects on the error:
            <ul>
              <li>E.g. MSE applied to the pixels of an image implicitly specifies that an underlying cause is only salient if it significantly changes the brightness of a large number of pixels.<br />
  This can be problematic if the task we wish to solve involves interacting with small objects.<br />
  <button class="showText" value="show" onclick="showTextPopHide(event);">Illustration: AutoEncoder w/ MSE</button>
  <img src="https://cdn.mathpix.com/snip/images/mEJX8pk5A1QENzLNvH4WLOzEzWaH61y-wpq6iIraGA0.original.fullsize.png" alt="img" width="100%" hidden="" /></li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        <p><strong>Learned (pattern-based) “Saliency”</strong>:<br />
  Certain factors could be considered <em>“salient”</em> if they follow a <strong><em>highly recognizable</em> pattern</strong>.<br />
  E.g. if a group of pixels follow a highly recognizable pattern, even if that pattern does not involve extreme brightness or darkness, then that pattern could be considered extremely salient.</p>

        <ul>
          <li>This definition is <em>implemented</em> by <strong>Generative Adversarial Networks (GANs)</strong>.<br />
  In this approach, a generative model is trained to fool a feedforward classifier. The feedforward classifier attempts to recognize all samples from the generative model as being fake, and all samples from the training set as being real.<br />
  In this framework, <span style="color: purple">any <em><strong>structured pattern</strong></em> that the feedforward network can <em>recognize</em> is <strong>highly salient</strong></span>.<br />
  They <span style="color: goldenrod"><em><strong>learn</strong></em> how to determine what is salient</span>.<br />
  <button class="showText" value="show" onclick="showTextPopHide(event);">Example: Advantages of Adversarial Framework in Learning Ears</button>
            <div hidden="">
              <p><em>Lotter et al. (2015)</em> showed that models trained to generate images of human heads will often neglect to generate the ears when trained with mean squared error, but will successfully generate the ears when trained with the adversarial framework.<br />
  Because the ears are not extremely bright or dark compared to the surrounding skin, they are not especially salient according to mean squared error loss, but their highly recognizable shape and consistent position means that a feedforward network can easily learn to detect them, making them highly salient under the generative adversarial framework.  <br />
  <button class="showText" value="show" onclick="showTextPopHide(event);">Illustration: Generating Faces w/ Ears</button>
  <img src="https://cdn.mathpix.com/snip/images/f23z_MZr459udxGiWj0dqEV7ai-QcU9ru7a67utkDJ8.original.fullsize.png" alt="img" width="100%" hidden="" /></p>
            </div>
          </li>
        </ul>
      </li>
    </ul>

    <p>Generative adversarial networks are only one step toward <strong>determining <em>which factors should be represented</em></strong>.<br />
 We expect that <strong style="color: goldenrod">future research</strong> will discover <span style="color: purple">better ways of determining <strong>which factors to represent</strong></span>, and develop <span style="color: purple">mechanisms for <strong><em>representing</em> different factors</strong> <em>depending on the task</em></span>.</p>

    <p><strong style="color: red">Robustness to Change - Causal Invariance:</strong><br />
 A <strong>benefit</strong> of learning the underlying causal factors <em>(Schölkopf et al. (2012))</em> is that:<br />
 if the <strong>true generative process</strong> has <span style="color: purple">\(\mathbf{x}\) as an <em><strong>effect</strong></em></span> and <span style="color: purple">\(\mathbf{y}\) as a <em><strong>cause</strong></em></span>, then <strong>modeling \(p(\mathbf{x} \vert \mathbf{y})\)</strong> is <span style="color: goldenrod">robust to changes in \(p(\mathbf{y})\)</span>.</p>
    <blockquote>
      <p>If the cause-effect relationship was reversed, this would not be true, since by Bayes’ rule, \(p(\mathbf{x} \vert \mathbf{y})\) would be sensitive to changes in \(p(\mathbf{y})\).</p>
    </blockquote>

    <p>Very often, when we consider changes in distribution due to <strong>different domains</strong>, <strong>temporal non-stationarity</strong>, or <strong>changes in the nature of the task</strong>, <strong style="color: goldenrod"><em>the causal mechanisms remain invariant</em></strong> (the laws of the universe are constant) while the <strong>marginal distribution over the underlying causes</strong> can <em><strong>change</strong></em>.<br />
 Hence, better <strong>generalization and robustness</strong> to all kinds of changes can be expected via <strong style="color: goldenrod">learning a generative model that attempts to recover the causal factors</strong> \(\mathbf{h}\) and \(p(\mathbf{x} \vert \mathbf{h})\).<br />
 <br /></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents5" id="bodyContents54">Providing Clues to Discover Underlying Causes:</strong><br />
 <strong style="color: red">Quality of Representations:</strong><br />
 The answer to the following question:<br />
 <span style="color: purple"><em>“what makes one representation better than another?”</em></span> <br />
 was the <strong>Causal Factors Hypothesis:</strong><br />
 An <strong>ideal representation</strong> is one in which <span style="color: purple">the <strong>features</strong> within the representation <em>correspond</em> to the underlying <strong>causes</strong> of the observed data</span>, with <strong><em>separate</em> features</strong> or <strong>directions</strong> in <em>feature space</em> corresponding to <strong><em>different</em> causes</strong>, so that <span style="color: purple">the <strong>representation</strong> <em><strong>disentangles</strong></em> the <strong>causes</strong> from one another</span>, especially those factors that are relevant to our applications.</p>

    <p><strong style="color: red">Clues for Finding the Causal Factors of Variation:</strong><br />
 Most strategies for representation learning are based on:<br />
 <span style="color: purple">Introducing clues that help the learning to find these underlying factors of variations</span>.<br />
 The clues can help the learner separate these observed factors from the others.</p>

    <p><strong>Supervised learning</strong> provides a <em><strong>very strong</strong></em> <strong>clue</strong>: a <strong>label</strong> \(\boldsymbol{y},\) presented with each \(\boldsymbol{x},\) that usually specifies the value of at least one of the factors of variation directly.</p>

    <p>More generally, to make use of abundant unlabeled data, representation learning makes use of other, less direct, hints about the underlying factors.<br />
 These hints take the form of <strong>implicit prior beliefs</strong> that we, the designers of the learning algorithm, impose in order to <span style="color: purple">guide the learner</span>.</p>

    <p><strong style="color: red">Clues in the form of Regularization:</strong><br />
 Results such as the <strong>no free lunch theorem</strong> show that <span style="color: purple"><strong>regularization</strong> strategies are necessary to obtain <strong>good generalization</strong></span>.<br />
 While it is impossible to find a universally superior regularization strategy, one goal of deep learning is to find a <strong>set of fairly generic regularization strategies</strong> that are <em>applicable to a wide variety of AI tasks</em>, similar to the tasks that people and animals are able to solve.</p>

    <p id="lst-p">We can <span style="color: goldenrod">use <em>generic</em> <strong>regularization strategies</strong> to <em>encourage</em> learning algorithms to discover <strong>features</strong> that <em><strong>correspond</strong></em> to <strong>underlying factors</strong></span>, E.G. <em>(Bengio et al. (2013d))</em>:</p>
    <ul>
      <li><strong>Smoothness</strong>: This is the assumption that \(f(\boldsymbol{x}+\epsilon \boldsymbol{d}) \approx f(\boldsymbol{x})\) for unit \(\boldsymbol{d}\) and small \(\epsilon\). This assumption allows the learner to generalize from training examples to nearby points in input space. Many machine learning algorithms leverage this idea, but it is insufficient to overcome the curse of dimensionality.</li>
      <li><strong>Linearity</strong>: Many learning algorithms assume that relationships between some variables are linear. This allows the algorithm to make predictions even very far from the observed data, but can sometimes lead to overly extreme predictions. Most simple machine learning algorithms that do not make the smoothness assumption instead make the linearity assumption. These are in fact different assumptions—<span style="color: goldenrod">linear functions with large weights applied to high-dimensional spaces may not be very smooth</span><sup id="fnref:7" role="doc-noteref"><a href="#fn:7" class="footnote" rel="footnote">7</a></sup>.</li>
      <li><strong>Multiple explanatory factors</strong>: Many representation learning algorithms are motivated by the assumption that the data is generated by multiple underlying explanatory factors, and that most tasks can be solved easily given the state of each of these factors. Section 15.3 describes how this view motivates semisupervised learning via representation learning. Learning the structure of \(p(\boldsymbol{x})\) requires learning some of the same features that are useful for modeling \(p(\boldsymbol{y} \vert \boldsymbol{x})\) because both refer to the same underlying explanatory factors. Section 15.4 describes how this view motivates the use of distributed representations, with separate directions in representation space corresponding to separate factors of variation.</li>
      <li><strong>Causal factors</strong>: the model is constructed in such a way that it treats the factors of variation described by the learned representation \(\boldsymbol{h}\) as the causes of the observed data \(\boldsymbol{x}\), and not vice-versa. As discussed in section 15.3, this is advantageous for semi-supervised learning and makes the learned model more robust when the distribution over the underlying causes changes or when we use the model for a new task.</li>
      <li><strong>Depth, or a hierarchical organization of explanatory factors</strong>: High-level, abstract concepts can be defined in terms of simple concepts, forming a hierarchy. From another point of view, the use of a deep architecture expresses our belief that the task should be accomplished via a multi-step program, with each step referring back to the output of the processing accomplished via previous steps.</li>
      <li><strong>Shared factors across tasks</strong>: In the context where we have many tasks, corresponding to different \(y_{i}\) variables sharing the same input \(\mathbf{x}\) or where each task is associated with a subset or a function \(f^{(i)}(\mathbf{x})\) of a global input \(\mathbf{x},\) the assumption is that each \(\mathbf{y}_ {i}\) is associated with a different subset from a common pool of relevant factors \(\mathbf{h}\). Because these subsets overlap, learning all the \(P\left(y_{i} \vert \mathbf{x}\right)\) via a shared intermediate representation \(P(\mathbf{h} \vert \mathbf{x})\) allows sharing of statistical strength between the tasks.</li>
      <li><strong>Manifolds</strong>: Probability mass concentrates, and the regions in which it concentrates are locally connected and occupy a tiny volume. In the continuous case, these regions can be approximated by low-dimensional manifolds with a much smaller dimensionality than the original space where the data lives. Many machine learning algorithms behave sensibly only on this manifold (Goodfellow et al., 2014b). Some machine learning algorithms, especially autoencoders, attempt to explicitly learn the structure of the manifold.</li>
      <li><strong>Natural clustering</strong>: Many machine learning algorithms assume that each connected manifold in the input space may be assigned to a single class. The data may lie on many disconnected manifolds, but the class remains constant within each one of these. This assumption motivates a variety of learning algorithms, including tangent propagation, double backprop, the manifold tangent classifier and adversarial training.</li>
      <li><strong>Temporal and spatial coherence</strong>: Slow feature analysis and related algorithms make the assumption that the most important explanatory factors change slowly over time, or at least that it is easier to predict the true underlying explanatory factors than to predict raw observations such as pixel values. See section 13.3 for further description of this approach.</li>
      <li><strong>Sparsity</strong>: Most features should presumably not be relevant to describing most inputs—there is no need to use a feature that detects elephant trunks when representing an image of a cat. It is therefore reasonable to impose a prior that any feature that can be interpreted as “present” or “absent” should be absent most of the time.</li>
      <li><strong>Simplicity of Factor Dependencies</strong>: In good high-level representations, the factors are related to each other through simple dependencies. The simplest possible is marginal independence, \(P(\mathbf{h})=\prod_{i} P\left(\mathbf{h}_ {i}\right)\), but linear dependencies or those captured by a shallow autoencoder are also reasonable assumptions. This can be seen in many laws of physics, and is assumed when plugging a linear predictor or a factorized prior on top of a learned representation.
        <ul>
          <li><span style="color: goldenrod"><em>Consciousness Prior</em></span>:
            <ul>
              <li><button class="showText" value="show" onclick="showTextPopHide(event);">Discussion</button>
                <ul hidden="">
                  <li><strong>Key Ideas:</strong><br />
  (1) Seek Objective Functions defined purely in abstract space (no decoders) <br />
  (2) <em>“Conscious”</em> thoughts are <em><strong>low-dimensional</strong></em>.
                    <ul>
                      <li>Conscious thoughts are very <em><strong>low-dimensional</strong></em> objects compared to the full state of the (unconscious) brain</li>
                      <li>Yet they have unexpected predictive value or usefulness<br />
  \(\rightarrow\) strong constraint or prior on the underlying representation
                        <blockquote>
                          <p>e.g. we can plan our lives by only thinking of simple/short sentences at a time, that can be expressed with few variables (words); short-term memory is only 7 words (underutilization? no, rather, <strong>prior</strong>).</p>
                        </blockquote>

                        <ul>
                          <li><strong>Thought</strong>: composition of few selected factors / concepts (key/value) at the highest level of abstraction of our brain.</li>
                          <li>Richer than but closely associated with short verbal expression such as a <strong>sentence</strong> or phrase, a <strong>rule</strong> or <strong>fact</strong> (link to classical symbolic Al &amp; knowledge representation)</li>
                        </ul>
                      </li>
                      <li>Thus, <span style="color: goldenrod"><em><strong>true</strong></em> <strong>statements</strong> about the <strong>very complex</strong> world, could be conveyed with very <em><strong>low-dimensional</strong></em> representations</span>.</li>
                    </ul>
                  </li>
                  <li><strong>How to select a few <em>relevant</em> abstract concepts making a thought</strong>:<br />
  <span style="color: goldenrod">Content-based <strong>Attention</strong></span>.
                    <ul>
                      <li>Thus, <span style="color: purple"><strong>Abstraction</strong> is <em>related to</em> <strong>Attention</strong>:</span><br />
  <button class="showText" value="show" onclick="showTextPopHide(event);">Relation between <strong>Abstraction</strong> and <strong>Attention</strong></button>
  <img src="https://cdn.mathpix.com/snip/images/f-kwoXKJhtC3oGzLt8HQfdcl3-x8T9-aULdanqZaqnI.original.fullsize.png" alt="img" width="100%" hidden="" /></li>
                    </ul>
                  </li>
                  <li><strong>Two Levels of Representations</strong>:
                    <ul>
                      <li>High-dimensional abstract representation space (all known concepts and factors) \(h\)</li>
                      <li>Low-dimensional conscious thought \(c,\) extracted from \(h\)
                        <ul>
                          <li>\(c\) includes names (keys) and values of factors<br />
  <img src="https://cdn.mathpix.com/snip/images/TwwbkSLzeI_DT6oFP1Y1DWkIQXjQOsHGs9bcT3f4cfE.original.fullsize.png" alt="img" width="50%" /></li>
                        </ul>
                      </li>
                      <li>The <strong>Goal</strong> of <span style="color: purple">using attention on the unconscious states</span>:<br />
  is to put pressure (constraint) on the <em>mapping between input and representations (Encoder)</em> and the <em>unconscious states representations \(h\)</em> such that the Encoder is encouraged to learn representations that have the property that <span style="color: purple">that if I pick just a few elements of it, I can make a true statement or very highly probable statement about the world, (e.g. a highly probable prediction)</span>.</li>
                    </ul>
                  </li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li><strong>Causal/Mechanism Independence</strong>:
        <ul>
          <li><span style="color: purple"><em>Controllable Factors</em></span>.</li>
        </ul>
      </li>
    </ul>

    <p>The concept of representation learning ties together all of the many forms of deep learning.<br />
 Feedforward and recurrent networks, autoencoders and deep probabilistic models all learn and exploit representations. Learning the best possible representation remains an exciting avenue of research.<br />
 <br /></p>
  </li>
</ol>

<!-- 4. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents54}
5. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents55}
 -->

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:6" role="doc-endnote">
      <p>It is also called a one-hot representation, since it can be captured by a binary vector with \(n\) bits that are mutually exclusive (only one of them can be active). <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:1" role="doc-endnote">
      <p>It proceeds one layer at a time, training the k -th layer while keeping the previous ones fixed. In particular, the lower layers (which are trained first) are not adapted after the upper layers are introduced. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>Commonly, “pretraining” to refer not only to the pretraining stage itself but to the entire two phase protocol that combines the pretraining phase and a supervised learning phase. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>This idea has guided a large amount of deep learning research since at least the 1990s <em>(Becker and Hinton, 1992; Hinton and Sejnowski, 1999)</em>, in more detail. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>For other arguments about when <span style="color: purple"><strong>semi-supervised</strong> learning can outperform pure <strong>supervised</strong> learning</span>, we refer the reader to <em>section 1.2</em> of <em>Chapelle et al. (2006)</em>. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p>Using <strong>unsupervised representation learning</strong> that tries to <em><strong>disentangle</strong></em> the <strong>underlying factors of variation</strong>. <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:7" role="doc-endnote">
      <p>See <em>Goodfellow et al. (2014b)</em> for a further discussion of the limitations of the linearity assumption. <a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>


      <footer class="site-footer">
    <!--   <span class="site-footer-owner"><a href="http://localhost:8880">Ahmad Badary</a> is maintained by <a href="http://ahmedbadary.ml">Ahmad Badary</a>.</span> -->
    
<!--  -->
    <div class="wrapper">
        <div class="footer-col-wrapper">
            <div class="footer-col footer-col-1">
            <span class="site-footer-owner"><a href="http://localhost:8880">Site</a> maintained by <a href="http://ahmedbadary.ml">Ahmad Badary</a>.</span>
    <span class="site-footer-credits">
        <p>
            &copy; 2017. All rights reserved.
        </p> 
    </span>
            </div>
            <div class="footer-col footer-col-2">
            <div><p>         </p></div>
            </div>
            <div class="footer-col footer-col-3">
                <ul class="social-media-list">
                    
                      <li>
                        <a href="https://github.com/AhmedBadary">
                          <i class="fa fa-github"></i> GitHub
                        </a>
                      </li>
                    
                    
                      <li>
                        <a href="https://linkedin.com/in/ahmad-badary-656098121/">
                          <i class="fa fa-linkedin"></i> LinkedIn
                        </a>
                      </li>
                    
                    
                      <li>
                        <a href="https://www.facebook.com/ahmed.thabet.94">
                          <i class="fa fa-facebook"></i> Facebook
                        </a>
                      </li>
                    
                </ul>
            </div>
        </div>
    </div>
<!--  -->
</footer>


    </section>

  </body>

<!-- Table of Content Script -->
<script type="text/javascript">
var bodyContents = $(".bodyContents1");
$("<ol>").addClass("TOC1ul").appendTo(".TOC1");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC1ul");
     });
// 
var bodyContents = $(".bodyContents2");
$("<ol>").addClass("TOC2ul").appendTo(".TOC2");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC2ul");
     });
// 
var bodyContents = $(".bodyContents3");
$("<ol>").addClass("TOC3ul").appendTo(".TOC3");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC3ul");
     });
//
var bodyContents = $(".bodyContents4");
$("<ol>").addClass("TOC4ul").appendTo(".TOC4");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC4ul");
     });
//
var bodyContents = $(".bodyContents5");
$("<ol>").addClass("TOC5ul").appendTo(".TOC5");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC5ul");
     });
//
var bodyContents = $(".bodyContents6");
$("<ol>").addClass("TOC6ul").appendTo(".TOC6");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC6ul");
     });
//
var bodyContents = $(".bodyContents7");
$("<ol>").addClass("TOC7ul").appendTo(".TOC7");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC7ul");
     });
//
var bodyContents = $(".bodyContents8");
$("<ol>").addClass("TOC8ul").appendTo(".TOC8");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC8ul");
     });
//
var bodyContents = $(".bodyContents9");
$("<ol>").addClass("TOC9ul").appendTo(".TOC9");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC9ul");
     });

</script>

<!-- VIDEO BUTTONS SCRIPT -->
<script type="text/javascript">
  function iframePopInject(event) {
    var $button = $(event.target);
    // console.log($button.parent().next());
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $figure = $("<div>").addClass("video_container");
        $iframe = $("<iframe>").appendTo($figure);
        $iframe.attr("src", $button.attr("src"));
        // $iframe.attr("frameborder", "0");
        $iframe.attr("allowfullscreen", "true");
        $iframe.css("padding", "4px 6px");
        $button.next().css("display", "block");
        $figure.appendTo($button.next());
        $button.text("Hide Video")
    } else {
        $button.attr('value', 'show');
        $button.next().html("");
        $button.text("Show Video")
    }
}
</script>

<!-- BUTTON TRY -->
<script type="text/javascript">
  function iframePopA(event) {
    event.preventDefault();
    var $a = $(event.target).parent();
    console.log($a);
    if ($a.attr('value') == 'show') {
        $a.attr('value', 'hide');
        $figure = $("<div>");
        $iframe = $("<iframe>").addClass("popup_website_container").appendTo($figure);
        $iframe.attr("src", $a.attr("href"));
        $iframe.attr("frameborder", "1");
        $iframe.attr("allowfullscreen", "true");
        $iframe.css("padding", "4px 6px");
        $a.next().css("display", "block");
        $figure.appendTo($a.next().next());
        // $a.text("Hide Content")
        $('html, body').animate({
            scrollTop: $a.offset().top
        }, 1000);
    } else {
        $a.attr('value', 'show');
        $a.next().next().html("");
        // $a.text("Show Content")
    }

    $a.next().css("display", "inline");
}
</script>


<!-- TEXT BUTTON SCRIPT - INJECT -->
<script type="text/javascript">
  function showTextPopInject(event) {
    var $button = $(event.target);
    var txt = $button.attr("input");
    console.log(txt);
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $p = $("<p>");
        $p.html(txt);
        $button.next().css("display", "block");
        $p.appendTo($button.next());
        $button.text("Hide Content")
    } else {
        $button.attr('value', 'show');
        $button.next().html("");
        $button.text("Show Content")
    }

}
</script>

<!-- TEXT BUTTON SCRIPT - HIDDEN / HIDE / SHOW / HIDE/SHOW -->
<script type="text/javascript">
  function showTextPopHide(event) {
    var $button = $(event.target);
    // var txt = $button.attr("input");
    var txt = $button.text();
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $button.next().removeAttr("hidden");
        $button.text(txt + " - Hide Content");
    } else {
        $button.attr('value', 'show');
        $button.next().attr("hidden", "");
        $button.text(txt.replace(" - Hide Content",""));
    }
}
</script>

<!-- TEXT BUTTON SCRIPT - HIDDEN / HIDE / SHOW / HIDE/SHOW -->
<script type="text/javascript">
  function showText_withParent_PopHide(event) {
    var $button = $(event.target);
    var $parent = $button.parent();
    var txt = $button.text();
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $parent.next().removeAttr("hidden");
        $button.text(txt + " - Hide Content");
    } else {
        $button.attr('value', 'show');
        $parent.next().attr("hidden", "");
        $button.text(txt.replace(" - Hide Content",""));
    }
}
</script>

<!-- Print / Printing / printme -->
<!-- <script type="text/javascript">
i = 0

for (var i = 1; i < 6; i++) {
    var bodyContents = $(".bodyContents" + i);
    $("<p>").addClass("TOC1ul")  .appendTo(".TOC1");
    bodyContents.each(function(index, element) {
        var paragraph = $(element);
        $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC1ul");
         });
} 
</script>
 -->
 
</html>

