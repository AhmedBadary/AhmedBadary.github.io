<!DOCTYPE html>
<html lang="en-us">
  <head>
  <meta charset="UTF-8">
  <title>Ahmad Badary</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <link rel="stylesheet" href="/css/normalize.css">
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="/css/cayman.css">
  <link rel="stylesheet" href="/css/style.css">
  <link rel="shortcut icon" href="/main_files/favicon.ico" />
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="/css/customStyle.css">
  <title> » Ahmad Badary</title>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
</head>

  <body>
    <nav class="main-nav">
    <a href="https://ahmedbadary.github.io/" class="main-nav-logo">
        <img src="/main_files/logo.png">
    </a>
    <ul id="menu-main" class="main-nav-items">
        <li id="menu-item-1859" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-home menu-item-1859">
            <a href="/">Home</a>
        </li>
        <li id="menu-item-2869" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-2869">
            <a href="/work">Work</a>
        </li>
        <li id="menu-item-1892" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1892">
            <a href="/projects">Projects</a>
        </li>
        <li id="menu-item-1858" class="menu-item menu-item-type-post_type menu-item-object-page current_page_parent menu-item-1858">
            <a href="/blog">Blog</a>
        </li>
        <li id="menu-item-1862" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1862">
            <a href="/about">About</a>
        </li>
    </ul>
</nav>


<section class="page-header">
  <h1 class="project-name">Statistical Learning Theory</h1>
  <h2 class="project-tagline"></h2>
  <a href="/#" class="btn">Home</a>
  <a href="/work" class="btn">Work-Space</a>
  <a href= /work_files/research/dl/theory.html class="btn">Previous</a>
</section>

<!-- <div>
  <ul class="posts">
    
      <li><span>02 Jan 2014</span> &raquo; <a href="/2014/01/02/introducing-Ahmad/">Introducing Ahmad</a></li>
    
  </ul>
</div> -->


    <section class="main-content">
      
      <div class="TOC">
  <h1 id="table-of-contents">Table of Contents</h1>

  <ul class="TOC1">
    <li><a href="#content1">Statistical Learning Theory</a></li>
  </ul>
  <ul class="TOC2">
    <li><a href="#content2">The Vapnik-Chervonenkis (VC) Theory</a></li>
  </ul>
  <ul class="TOC3">
    <li><a href="#content3">The Bias-Variance Decomposition Theory</a></li>
  </ul>
  <!--  * [FOURTH](#content4)
  {: .TOC4}
  * [FIFTH](#content5)
  {: .TOC5}
  * [SIXTH](#content6)
  {: .TOC6} -->
</div>

<hr />
<hr />

<ul>
  <li><a href="http://papers.nips.cc/paper/506-principles-of-risk-minimization-for-learning-theory.pdf">Principles of Risk Minimization for Learning Theory (original papers)</a></li>
  <li><a href="http://www.tml.cs.uni-tuebingen.de/team/luxburg/publications/StatisticalLearningTheory.pdf">Statistical Learning Theory from scratch (paper)</a></li>
  <li><a href="https://www.youtube.com/watch?v=pFWiauHOFpY">The learning dynamics behind generalization and overfitting in Deep Networks</a></li>
  <li><a href="http://maxim.ece.illinois.edu/teaching/SLT/SLT.pdf">Notes on SLT</a></li>
  <li><a href="http://web.mit.edu/9.s915/www/classes/dealing_with_data.pdf">Mathematics of Learning (w/ proofs &amp; necessary+sufficient conditions for learning)</a></li>
  <li><a href="https://courses.cs.washington.edu/courses/cse522/11wi/scribes/lecture4.pdf">Generalization Bounds for Hypothesis Spaces</a></li>
  <li><a href="https://mostafa-samir.github.io/ml-theory-pt2/">Generalization Bound Derivation</a></li>
  <li><a href="http://mlexplained.com/2018/04/24/overfitting-isnt-simple-overfitting-re-explained-with-priors-biases-and-no-free-lunch/">Overfitting isn’t simple: Overfitting Re-explained with Priors, Biases, and No Free Lunch</a></li>
</ul>

<h2 id="content1">Statistical Learning Theory</h2>

<ol>
  <li><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents11">Statistical Learning Theory:</strong><br />
 <strong>Statistical Learning Theory</strong> is a framework for machine learning drawing from the fields of statistics and functional analysis. Under certain assumptions, this framework allows us to study the question:
    <blockquote>
      <p><strong style="color: blue">How can we affect performance on the test set when we can only observe the training set?</strong></p>
    </blockquote>

    <p>It is a <em>statistical</em> approach to <strong>Computational Learning Theory</strong>.</p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents12">Formal Definition:</strong><br />
 Let:
    <ul>
      <li><script type="math/tex">X</script>: <script type="math/tex">\:</script> the vector space of all possible <strong>inputs</strong></li>
      <li><script type="math/tex">Y</script>: <script type="math/tex">\:</script> the vector space of all possible <strong>outputs</strong></li>
      <li><script type="math/tex">Z = X \times Y</script>: <script type="math/tex">\:</script> the <strong>product space</strong> of (input,output) pairs</li>
      <li><script type="math/tex">n</script>: <script type="math/tex">\:</script> the number of <strong>samples</strong> in the <strong>training set</strong></li>
      <li><script type="math/tex">S=\left\{\left(\vec{x}_{1}, y_{1}\right), \ldots,\left(\vec{x}_{n}, y_{n}\right)\right\}=\left\{\vec{z}_{1}, \ldots, \vec{z}_{n}\right\}</script>: <script type="math/tex">\:</script> the <strong>training set</strong></li>
      <li><script type="math/tex">\mathcal{H} = f : X \rightarrow Y</script>: <script type="math/tex">\:</script> the <strong>hypothesis space</strong> of all functions</li>
      <li><script type="math/tex">V(f(\vec{x}), y)</script>: <script type="math/tex">\:</script> an <strong>error/loss function</strong></li>
    </ul>

    <p id="lst-p"><strong>Assumptions:</strong></p>
    <ul>
      <li>The training and test data are generated by an <em><strong>unknown, joint</strong></em> <strong>probability distribution over datasets</strong> (over the product space <script type="math/tex">Z</script>, denoted: <script type="math/tex">p_{\text{data}}(z)=p(\vec{x}, y)</script>) called the <strong>data-generating process</strong>.
        <ul>
          <li><script type="math/tex">p_{\text{data}}</script> is a <strong>joint distribution</strong> so that it allows us to model <em>uncertainty in predictions</em> (e.g. from noise in data) because <script type="math/tex">y</script> is not a deterministic function of <script type="math/tex">\vec{x}</script>, but rather a <em>random variable</em> with <strong>conditional distribution</strong> <script type="math/tex">p(y \vert \vec{x})</script> for a fixed <script type="math/tex">\vec{x}</script>.</li>
        </ul>
      </li>
      <li>The <strong>i.i.d. assumptions:</strong>
        <ul>
          <li>The examples in each dataset are <strong>independent</strong> from each other</li>
          <li>The <em>training set</em> and <em>test set</em> are <strong>identically distributed</strong> (drawn from the same probability distribution as each other)</li>
        </ul>

        <blockquote>
          <p>A collection of random variables is <strong>independent and identically distributed</strong> if each random variable has the same probability distribution as the others and all are mutually independent.<br />
<em>Informally,</em> it says that all the variables provide the same kind of information independently of each other.</p>
          <ul>
            <li><a href="https://stats.stackexchange.com/questions/213464/on-the-importance-of-the-i-i-d-assumption-in-statistical-learning/214220">Discussion on the importance if i.i.d assumptions</a></li>
          </ul>
        </blockquote>
      </li>
    </ul>

    <p><strong style="color: red">The Inference Problem</strong>  <br />
 Finding a function <script type="math/tex">f : X \rightarrow Y</script> such that <script type="math/tex">f(\vec{x}) \sim y</script>.</p>

    <p><strong>The Expected Risk:</strong></p>
    <p>$$I[f]=\mathbf{E}[V(f(\vec{x}), y)]=\int_{X \times Y} V(f(\vec{x}), y) p(\vec{x}, y) d \vec{x} d y$$</p>

    <p><strong>The Target Function:</strong><br />
 is the best possible function <script type="math/tex">f</script> that can be chosen, is given by:</p>
    <p>$$f=\inf_{h \in \mathcal{H}} I[h]$$</p>

    <p><strong>The Empirical Risk:</strong><br />
 Is a <em><strong>proxy measure</strong></em> for the <strong>expected risk</strong>, based on the training set.<br />
 It is necessary because the probability distribution <script type="math/tex">p(\vec{x}, y)</script> is <em>unknown</em>.</p>
    <p>$$I_{S}[f]=\frac{1}{n} \sum_{i=1}^{n} V\left(f\left(\vec{x}_{i}\right), y_{i}\right)$$</p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents13">Empirical risk minimization:</strong><br />
 <strong>Empirical Risk Minimization (ERM)</strong> is a principle in <em>statistical learning theory</em> that is based on approximating the <strong>Generalization Error (True Risk)</strong> by measuring the <strong>Training Error (Empirical Risk)</strong>, i.e. the performance on training data.</p>

    <p>A <em>learning algorithm</em> that chooses the function <script type="math/tex">f_{S}</script> that minimizes the <em>empirical risk</em> is called <strong>empirical risk minimization</strong>:</p>
    <p>$$R_{\mathrm{emp}}(h) = I_{S}[f]=\frac{1}{n} \sum_{i=1}^{n} V\left(f\left(\vec{x}_{i}\right), y_{i}\right)$$</p>
    <p>$$f_{S} = \hat{h} = \arg \min _{h \in \mathcal{H}} R_{\mathrm{emp}}(h)$$</p>

    <p><strong style="color: red">Complexity:</strong><br />
 <button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Show</button></p>
    <ul hidden="">
      <li>Empirical risk minimization for a classification problem with a <em>0-1 loss function</em> is known to be an <strong>NP-hard</strong> problem even for such a relatively simple class of functions as linear classifiers.
        <ul>
          <li><a href="https://arxiv.org/abs/1012.0729">Paper Proof</a></li>
        </ul>
      </li>
      <li>Though, it can be solved efficiently when the minimal empirical risk is zero, i.e. data is linearly separable.</li>
      <li><strong>Coping with Hardness:</strong>
        <ul>
          <li>Employing a <strong>convex approximation</strong> to the 0-1 loss: <em>Hinge Loss</em>, <em>SVM</em></li>
          <li>Imposing <strong>Assumptions on the data-generating distribution</strong> and thus, stop being an <strong>agnostic learning algorithm</strong>.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents14">Definitions:</strong>
    <ul>
      <li><strong style="color: red">Generalization Error:</strong><br />
  AKA: <strong>Expected Risk/Error</strong>, <strong>Out-of-Sample Error</strong><sup id="fnref:2"><a href="#fn:2" class="footnote">1</a></sup>, <strong><script type="math/tex">E_{\text{out}}</script></strong> <br />
  It is a measure of how accurately an algorithm is able to predict outcome values for previously unseen data.</li>
      <li>
        <p><strong style="color: red">Generalization Gap:</strong><br />
  It is a measure of how accurately an algorithm is able to predict outcome values for previously unseen data.</p>

        <p><strong>Formally:</strong><br />
  The generalization gap is the <strong>difference between the expected and empirical error</strong>:</p>
        <p>$$G =I\left[f_{n}\right]-I_{S}\left[f_{n}\right]$$</p>

        <p>An Algorithm is said to <strong>Generalize</strong> (achieve <strong>Generalization</strong>) if:</p>
        <p>$$\lim _{n \rightarrow \infty} G_n = \lim _{n \rightarrow \infty} I\left[f_{n}\right]-I_{S}\left[f_{n}\right]=0$$</p>
        <p>Equivalently:</p>
        <p>$$E_{\text { out }}(g) \approx E_{\text { in }}(g)$$</p>
        <p>or</p>
        <p>$$I\left[f_{n}\right] \approx I_{S}\left[f_{n}\right]$$</p>

        <p><strong>Computing the Generalization Gap:</strong><br />
  Since <script type="math/tex">I\left[f_{n}\right]</script> cannot be computed for an unknown distribution, the generalization gap <strong>cannot be computed</strong> either.<br />
  Instead the goal of <strong>statistical learning theory</strong> is to <em>bound</em> or <em>characterize</em> the generalization gap in probability:</p>
        <p>$$P_{G}=P\left(I\left[f_{n}\right]-I_{S}\left[f_{n}\right] \leq \epsilon\right) \geq 1-\delta_{n}$$</p>
        <p>That is, the goal is to characterize the probability <script type="math/tex">{\displaystyle 1-\delta _{n}}</script> that the generalization gap is less than some error bound <script type="math/tex">{\displaystyle \epsilon }</script> (known as the <strong>learning rate</strong> and generally dependent on <script type="math/tex">{\displaystyle \delta }</script> and <script type="math/tex">{\displaystyle n}</script>).</p>
      </li>
      <li><strong style="color: red">The Empirical Distribution:</strong><br />
  <em>AKA <strong>Data-Generating Distribution</strong></em><br />
  is the <strong>discrete</strong> uniform distribution over the <em>sample points</em>.</li>
      <li><strong style="color: red">The Approximation-Generalization Tradeoff:</strong>
        <ul>
          <li><strong>Goal</strong>:<br />
  Small <script type="math/tex">E_{\text{out}}</script>: Good approximation of <script type="math/tex">f</script> <em><strong>out of sample</strong></em> (not in-sample).</li>
          <li>The tradeoff is characterized by the <strong>complexity</strong> of the <strong>hypothesis space <script type="math/tex">\mathcal{H}</script></strong>:
            <ul>
              <li><strong>More Complex <script type="math/tex">\mathcal{H}</script></strong>: Better chance of approximating <script type="math/tex">f</script></li>
              <li><strong>Less Complex <script type="math/tex">\mathcal{H}</script></strong>: Better chance of generalizing out-of-sample</li>
            </ul>
          </li>
          <li><a href="https://www.youtube.com/embed/zrEyxfl2-a8?start=358" value="show" onclick="iframePopA(event)"><strong>Abu-Mostafa</strong></a>
  <a href="https://www.youtube.com/embed/zrEyxfl2-a8?start=358"></a>
            <div></div>
          </li>
          <li><a href="https://mdav.ece.gatech.edu/ece-6254-spring2017/notes/13-bias-variance-marked.pdf">Lecture-Slides on Approximation-Generalization</a></li>
        </ul>
      </li>
      <li><strong style="color: red">Excess Risk (Generalization-Gap) Decomposition | Estimation-Approximation Tradeoff:</strong><br />
  <strong>Excess Risk</strong> is defined as the difference between the expected-risk/generalization-error of any function <script type="math/tex">\hat{f} = g^{\mathcal{D}}</script> that we learn from the data (exactly just bias-variance), and the expected-risk of the <strong>target function</strong> <script type="math/tex">f</script> (known as the <strong>bayes optimal predictor</strong>)
        <ul>
          <li><a href="https://www.youtube.com/embed/YA_CE9jat4I" value="show" onclick="iframePopA(event)"><strong>Excess Risk Decomposition Video</strong></a>
  <a href="https://www.youtube.com/embed/YA_CE9jat4I"></a>
            <div></div>
          </li>
          <li><a href="https://www.ics.uci.edu/~smyth/courses/cs274/readings/xing_singh_CMU_bias_variance.pdf">Excess Risk &amp; Bias-Variance Lecture Slides</a><br />
 <br /></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents18">Notes:</strong>
    <ul>
      <li><strong>Choices of Loss Functions</strong>:
        <ul>
          <li><strong>Regression</strong>:
            <ul>
              <li><strong>MSE</strong>: <script type="math/tex">\: V(f(\vec{x}), y)=(y-f(\vec{x}))^{2}</script></li>
              <li><strong>MAE</strong>: <script type="math/tex">\: V(f(\vec{x}), y)=\vert{y-f(\vec{x})}\vert</script></li>
            </ul>
          </li>
          <li><strong>Classification</strong>:
            <ul>
              <li><strong>Binary</strong>: <script type="math/tex">\: V(f(\vec{x}), y)=\theta(-y f(\vec{x}))</script><br />
  where <script type="math/tex">\theta</script> is the <em>Heaviside Step Function</em>.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li><strong>Training Data, Errors, and Risk</strong>:
        <ul>
          <li><strong>Training-Error</strong> is the <strong>Empirical Risk</strong>
            <ul>
              <li>It is a <strong>proxy</strong> for the <strong>Generalization Error/Expected Risk</strong></li>
              <li>This is what we minimize</li>
            </ul>
          </li>
          <li><strong>Test-Error</strong> is an <em><strong>approximation</strong></em> to the <strong>Generalization Error/Expected Risk</strong>
            <ul>
              <li>This is what we (can) compute to ensure that minimizing Training-Err/Empirical-Risk (ERM) also minimized the Generalization-Err/Expected-Risk (which we can’t compute directly)</li>
            </ul>
          </li>
        </ul>
      </li>
      <li><strong>Why the goal is NOT to minimize <script type="math/tex">E_{\text{in}}</script> completely (intuition)</strong>:<br />
  Basically, if you have noise in the data; then fitting the (finite) training-data completely; i.e. minimizing the in-sample-err completely will underestimate the out-of-sample-err.<br />
  Since, if noise existed AND you fit training-data completely <script type="math/tex">E_{\text{in}} = 0</script> THEN you inherently have fitted the noise AND your performance on out-sample will be lower.</li>
    </ul>
  </li>
</ol>

<hr />

<h2 id="content2">The Vapnik-Chervonenkis (VC) Theory</h2>
<!-- 
1. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents21}

2. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents22}

3. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents23}

4. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents24}

5. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents25}

6. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents26}

7. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents27}

8. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents28} -->

<hr />

<h2 id="content3">The Bias-Variance Decomposition Theory</h2>

<ol>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents311">The Bias-Variance Decomposition Theory:</strong>  <br />
<strong>The Bias-Variance Decomposition Theory</strong> is a way to quantify the <strong>Approximation-Generalization Tradeoff</strong>.</p>

    <p id="lst-p"><strong>Assumptions:</strong></p>
    <ul>
      <li>The analysis is done over the <strong>entire data-distribution</strong></li>
      <li>The target function <script type="math/tex">f</script> is already <strong>known</strong>; and you’re trying to answer the question:<br />
  “How can <script type="math/tex">\mathcal{H}</script> approximate <script type="math/tex">f</script> over all? not just on your sample.”</li>
      <li>Applies to <strong>real-valued targets</strong> (can be extended)</li>
      <li>Use <strong>Square Error</strong> (can be extended)<br />
<br /></li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents31">The Bias-Variance Decomposition:</strong>  <br />
 The <strong>Bias-Variance Decomposition</strong> is a way of analyzing a learning algorithm’s <em><strong>expected out-of-sample error</strong></em><sup id="fnref:1"><a href="#fn:1" class="footnote">2</a></sup> as a <em>sum of three terms:</em>
    <ul>
      <li><strong>Bias:</strong> is an error from erroneous assumptions in the learning algorithm.</li>
      <li><strong>Variance:</strong> is an error from sensitivity to small fluctuations in the training set.</li>
      <li><strong>Irreducible Error</strong> (resulting from noise in the problem itself)</li>
    </ul>

    <p>Equivalently, <strong>Bias</strong> and <strong>Variance</strong> measure <em>two different sources of errors in an estimator:</em></p>
    <ul>
      <li><strong>Bias:</strong> measures the expected deviation from the true value of the function or parameter.
        <blockquote>
          <p>AKA: <strong>Approximation Error</strong><sup id="fnref:3"><a href="#fn:3" class="footnote">3</a></sup>  (statistics)  How well can <script type="math/tex">\mathcal{H}</script> approximate the target function ‘<script type="math/tex">f</script>’</p>
        </blockquote>
      </li>
      <li><strong>Variance:</strong> measures the deviation from the expected estimator value that any particular sampling of the data is likely to cause.
        <blockquote>
          <p>AKA: <strong>Estimation (Generalization) Error</strong> (statistics) How well we can zoom in on a good <script type="math/tex">h \in \mathcal{H}</script></p>
        </blockquote>
      </li>
    </ul>

    <p><strong>Bias-Variance Decomposition Formula:</strong><br />
 For any function <script type="math/tex">\hat{f} = g^{\mathcal{D}}</script> we select, we can decompose its <em><strong>expected (out-of-sample) error</strong></em> on an <em>unseen sample <script type="math/tex">x</script></em> as:</p>
    <p>$$\mathrm{E}\left[(y-\hat{f}(x))^{2}\right]=(\operatorname{Bias}[\hat{f}(x)])^{2}+\operatorname{Var}[\hat{f}(x)]+\sigma^{2}$$</p>
    <p>Where:</p>
    <ul>
      <li><strong>Bias</strong>:
        <p>$$\operatorname{Bias}[\hat{f}(x)]=\mathrm{E}[\hat{f}(x)]-f(x)$$</p>
      </li>
      <li><strong>Variance</strong>:
        <p>$$\operatorname{Var}[\hat{f}(x)]=\mathrm{E}\left[\hat{f}(x)^{2}\right]-\mathrm{E}[\hat{f}(x)]^{2}$$</p>
        <p>and the expectation ranges over different realizations of the training set <script type="math/tex">\mathcal{D}</script>.</p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents32">The Bias-Variance Tradeoff:</strong><br />
 is the property of a set of predictive models whereby, models with a <em>lower bias</em> (in parameter estimation) have a <em>higher variance</em> (of the parameter estimates across samples) and vice-versa.</p>

    <p id="lst-p"><strong style="color: black">Effects of Bias:</strong></p>
    <ul>
      <li><strong>High Bias</strong>: simple models, lead to <em style="color: red"><strong>underfitting</strong></em>.</li>
      <li><strong>Low Bias</strong>: complex models, lead to <em style="color: red"><strong>overfitting</strong></em>.</li>
    </ul>

    <p id="lst-p"><strong style="color: black">Effects of Variance:</strong></p>
    <ul>
      <li><strong>High Variance</strong>: complex models, lead to <em style="color: red"><strong>overfitting</strong></em>.</li>
      <li><strong>Low Variance</strong>: simple models, lead to <em style="color: red"><strong>underfitting</strong></em>.<br />
 <br /></li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents33">Derivation:</strong>
    <p>$${\displaystyle {\begin{aligned}\mathbb{E}_{\mathcal{D}} {\big [}I[g^{(\mathcal{D})}]{\big ]}&amp;=\mathbb{E}_{\mathcal{D}} {\big [}\mathbb{E}_{x}{\big [}(g^{(\mathcal{D})}-y)^{2}{\big ]}{\big ]}\\
 &amp;=\mathbb{E}_{x} {\big [}\mathbb{E}_{\mathcal{D}}{\big [}(g^{(\mathcal{D})}-y)^{2}{\big ]}{\big ]}\\
 &amp;=\mathbb{E}_{x}{\big [}\mathbb{E}_{\mathcal{D}}{\big [}(g^{(\mathcal{D})}- f -\varepsilon)^{2}{\big ]}{\big ]}
 \\&amp;=\mathbb{E}_{x}{\big [}\mathbb{E}_{\mathcal{D}} {\big [}(f+\varepsilon -g^{(\mathcal{D})}+\bar{g}-\bar{g})^{2}{\big ]}{\big ]}\\
 &amp;=\mathbb{E}_{x}{\big [}\mathbb{E}_{\mathcal{D}} {\big [}(\bar{g}-f)^{2}{\big ]}+\mathbb{E}_{\mathcal{D}} [\varepsilon ^{2}]+\mathbb{E}_{\mathcal{D}} {\big [}(g^{(\mathcal{D})} - \bar{g})^{2}{\big ]}+2\: \mathbb{E}_{\mathcal{D}} {\big [}(\bar{g}-f)\varepsilon {\big ]}+2\: \mathbb{E}_{\mathcal{D}} {\big [}\varepsilon (g^{(\mathcal{D})}-\bar{g}){\big ]}+2\: \mathbb{E}_{\mathcal{D}} {\big [}(g^{(\mathcal{D})} - \bar{g})(\bar{g}-f){\big ]}{\big ]}\\
 &amp;=\mathbb{E}_{x}{\big [}(\bar{g}-f)^{2}+\mathbb{E}_{\mathcal{D}} [\varepsilon ^{2}]+\mathbb{E}_{\mathcal{D}} {\big [}(g^{(\mathcal{D})} - \bar{g})^{2}{\big ]}+2(\bar{g}-f)\mathbb{E}_{\mathcal{D}} [\varepsilon ]\: +2\: \mathbb{E}_{\mathcal{D}} [\varepsilon ]\: \mathbb{E}_{\mathcal{D}} {\big [}g^{(\mathcal{D})}-\bar{g}{\big ]}+2\: \mathbb{E}_{\mathcal{D}} {\big [}g^{(\mathcal{D})}-\bar{g}{\big ]}(\bar{g}-f){\big ]}\\
 &amp;=\mathbb{E}_{x}{\big [}(\bar{g}-f)^{2}+\mathbb{E}_{\mathcal{D}} [\varepsilon ^{2}]+\mathbb{E}_{\mathcal{D}} {\big [}(g^{(\mathcal{D})} - \bar{g})^{2}{\big ]}{\big ]}\\
 &amp;=\mathbb{E}_{x}{\big [}(\bar{g}-f)^{2}+\operatorname {Var} [y]+\operatorname {Var} {\big [}g^{(\mathcal{D})}{\big ]}{\big ]}\\
 &amp;=\mathbb{E}_{x}{\big [}\operatorname {Bias} [g^{(\mathcal{D})}]^{2}+\operatorname {Var} [y]+\operatorname {Var} {\big [}g^{(\mathcal{D})}{\big ]}{\big ]}\\
 &amp;=\mathbb{E}_{x}{\big [}\operatorname {Bias} [g^{(\mathcal{D})}]^{2}+\sigma ^{2}+\operatorname {Var} {\big [}g^{(\mathcal{D})}{\big ]}{\big ]}\\
\end{aligned}}}$$</p>
    <p>where:<br />
 <script type="math/tex">\overline{g}(\mathbf{x})=\mathbb{E}_{\mathcal{D}}\left[g^{(\mathcal{D})}(\mathbf{x})\right]</script> is the <strong>average hypothesis</strong> over all realization of <script type="math/tex">N</script> data-points <script type="math/tex">\mathcal{D}_ i</script>.</p>

    <p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Derivation with Wikipedia Notation</button></p>
    <p hidden="">$${\displaystyle {\begin{aligned}\operatorname {E}_ {\mathcal{D}} {\big [}(y-{\hat {f}})^{2}{\big ]}&amp;=\operatorname {E} {\big [}(f+\varepsilon -{\hat {f}})^{2}{\big ]}\\&amp;=\operatorname {E} {\big [}(f+\varepsilon -{\hat {f}}+\operatorname {E} [{\hat {f}}]-\operatorname {E} [{\hat {f}}])^{2}{\big ]}\\&amp;=\operatorname {E} {\big [}(f-\operatorname {E} [{\hat {f}}])^{2}{\big ]}+\operatorname {E} [\varepsilon ^{2}]+\operatorname {E} {\big [}(\operatorname {E} [{\hat {f}}]-{\hat {f}})^{2}{\big ]}+2\operatorname {E} {\big [}(f-\operatorname {E} [{\hat {f}}])\varepsilon {\big ]}+2\operatorname {E} {\big [}\varepsilon (\operatorname {E} [{\hat {f}}]-{\hat {f}}){\big ]}+2\operatorname {E} {\big [}(\operatorname {E} [{\hat {f}}]-{\hat {f}})(f-\operatorname {E} [{\hat {f}}]){\big ]}\\&amp;=(f-\operatorname {E} [{\hat {f}}])^{2}+\operatorname {E} [\varepsilon ^{2}]+\operatorname {E} {\big [}(\operatorname {E} [{\hat {f}}]-{\hat {f}})^{2}{\big ]}+2(f-\operatorname {E} [{\hat {f}}])\operatorname {E} [\varepsilon ]+2\operatorname {E} [\varepsilon ]\operatorname {E} {\big [}\operatorname {E} [{\hat {f}}]-{\hat {f}}{\big ]}+2\operatorname {E} {\big [}\operatorname {E} [{\hat {f}}]-{\hat {f}}{\big ]}(f-\operatorname {E} [{\hat {f}}])\\&amp;=(f-\operatorname {E} [{\hat {f}}])^{2}+\operatorname {E} [\varepsilon ^{2}]+\operatorname {E} {\big [}(\operatorname {E} [{\hat {f}}]-{\hat {f}})^{2}{\big ]}\\&amp;=(f-\operatorname {E} [{\hat {f}}])^{2}+\operatorname {Var} [y]+\operatorname {Var} {\big [}{\hat {f}}{\big ]}\\&amp;=\operatorname {Bias} [{\hat {f}}]^{2}+\operatorname {Var} [y]+\operatorname {Var} {\big [}{\hat {f}}{\big ]}\\&amp;=\operatorname {Bias} [{\hat {f}}]^{2}+\sigma ^{2}+\operatorname {Var} {\big [}{\hat {f}}{\big ]}\\\end{aligned}}}$$</p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents34">Results and Takeaways of the Decomposition:</strong><br />
 Match the <strong>“Model Complexity”</strong> to the <em><strong>Data Resources</strong></em>, NOT to the <em>Target Complexity</em>.<br />
 <img src="/main_files/dl/theory/stat_lern_thry/1.png" alt="img" width="80%" /></p>

    <p><button class="showText" value="show" onclick="showText_withParent_PopHide(event);">Analogy to the Approximation-Generalization Tradeoff</button></p>
    <p hidden="">Pretty much like I'm sitting in my office, and I want a document of some kind, an old letter. Someone has asked me for a letter of recommendation, and I don't want to rewrite it from scratch. So I want to take the older letter, and just see what I wrote, and then add the update to that.  <br />
 Before everything was archived in the computers, it used to be a piece of paper. So I know the letter of recommendation is somewhere. Now I face the question, should I write the letter of recommendation from scratch? Or should I look for the letter of recommendation? The recommendation is there. It's much easier when I find it. However, finding it is a big deal. So the question is not that the target function is there. The question is, can I find it?<br />  
 (Therefore, when I give you 100 examples, you choose the hypothesis set to match the 100 examples. If the 100 examples are terribly noisy, that's even worse. Because their information to guide you is worse.)  <br />
 <strong style="color: red">The data resources you have is, "what do you have in order to navigate the hypothesis set?". Let's pick a hypothesis set that we can afford to navigate. That is the game in learning. Done with the bias and variance.</strong></p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents35">Measuring the Bias and Variance:</strong>
    <ul>
      <li><strong>Training Error</strong>: reflects Bias, NOT variance</li>
      <li><strong>Test Error</strong>: reflects Both</li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents36">Reducing the Bias and Variance, and Irreducible Err:</strong>
    <ul>
      <li><strong>Adding Good Feature</strong>:
        <ul>
          <li>Decrease Bias</li>
        </ul>
      </li>
      <li><strong>Adding Bad Feature</strong>:
        <ul>
          <li>Doesn’t affect (increase) Bias much</li>
        </ul>
      </li>
      <li><strong>Adding ANY Feature</strong>:
        <ul>
          <li>Increases Variance</li>
        </ul>
      </li>
      <li><strong>Adding more Data</strong>:
        <ul>
          <li>Decreases Variance</li>
          <li>(May) Decreases Bias: if <script type="math/tex">h</script> can fit <script type="math/tex">f</script> exactly.</li>
        </ul>
      </li>
      <li><strong>Noise in Test Set</strong>:
        <ul>
          <li>Affects ONLY Irreducible Err</li>
        </ul>
      </li>
      <li><strong>Noise in Training Set</strong>:
        <ul>
          <li>Affects BOTH and ONLY Bias and Variance</li>
        </ul>
      </li>
      <li><strong>Dimensionality Reduction</strong>:
        <ul>
          <li>Decrease Variance (by simplifying models)</li>
        </ul>
      </li>
      <li><strong>Feature Selection</strong>:
        <ul>
          <li>Decrease Variance (by simplifying models)</li>
        </ul>
      </li>
      <li><strong>Regularization</strong>:
        <ul>
          <li>Increase Bias</li>
          <li>Decrease Variance</li>
        </ul>
      </li>
      <li><strong>Increasing # of Hidden Units in ANNs</strong>:
        <ul>
          <li>Decrease Bias</li>
          <li>Increase Variance</li>
        </ul>
      </li>
      <li><strong>Increasing # of Hidden Layers in ANNs</strong>:
        <ul>
          <li>Decrease Bias</li>
          <li>Increase Variance</li>
        </ul>
      </li>
      <li><strong>Increasing <script type="math/tex">k</script> in K-NN</strong>:
        <ul>
          <li>Increase Bias</li>
          <li>Decrease Variance</li>
        </ul>
      </li>
      <li><strong>Increasing Depth in Decision-Trees</strong>:
        <ul>
          <li>Increase Variance</li>
        </ul>
      </li>
      <li><strong>Boosting</strong>:
        <ul>
          <li>Decreases Bias</li>
        </ul>
      </li>
      <li><strong>Bagging</strong>:
        <ul>
          <li>Reduces Variance</li>
        </ul>
      </li>
      <li>We <strong>Cannot Reduce</strong> the <strong>Irreducible Err</strong></li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents37">Application of the Decomposition to Classification:</strong><br />
 A similar decomposition exists for:
    <ul>
      <li>Classification w/ <script type="math/tex">0-1</script> loss</li>
      <li>Probabilistic Classification w/ Squared Error</li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents38">Bias-Variance Decomposition and Risk (Excess Risk Decomposition):</strong><br />
 The <strong>Bias-Variance Decomposition</strong> analyzes the behavior of the <em><strong>Expected Risk/Generalization Error</strong></em> for any function <script type="math/tex">\hat{f}</script>:
    <p>$$R(\hat{f}) = \mathrm{E}\left[(y-\hat{f}(x))^{2}\right]=(\operatorname{Bias}[\hat{f}(x)])^{2}+\operatorname{Var}[\hat{f}(x)]+\sigma^{2}$$</p>
    <p>Assuming that <script type="math/tex">y = f(x) + \epsilon</script>.</p>

    <p>The <strong>Bayes Optimal Predictor</strong> is <script type="math/tex">f(x) = \mathrm{E}[Y\vert X=x]</script>.</p>

    <p>The <strong>Excess Risk</strong> is:</p>
    <p>$$\text{ExcessRisk}(\hat{f}) = R(\hat{f}) - R(f)$$</p>

    <p><strong style="color: red">Excess Risk Decomposition:</strong><br />
 We add and subtract the <strong>target function <script type="math/tex">f_{\text{target}}=\inf_{h \in \mathcal{H}} I[h]</script></strong> that minimizes the <strong>(true) expected risk</strong>:</p>
    <p>$$\text{ExcessRisk}(\hat{f}) = \underbrace{\left(R(\hat{f}) - R(f_{\text{target}})\right)}_ {\text { estimation error }} + \underbrace{\left(R(f_{\text{target}}) - R(f)\right)}_ {\text { approximation error }}$$</p>

    <p>The <strong>Bias-Variance Decomposition</strong> for <em><strong>Excess Risk:</strong></em></p>
    <ul>
      <li>Re-Writing <strong>Excess Risk</strong>:
        <p>$$\text{ExcessRisk}(\hat{f}) = R(\hat{f}) - R(f) = \mathrm{E}\left[(y-\hat{f}(x))^{2}\right] - \mathrm{E}\left[(y-f(x))^{2}\right]$$</p>
        <p>which is equal to:</p>
        <p>$$R(\hat{f}) - R(f) = \mathrm{E}\left[(f(x)-\hat{f}(x))^{2}\right]$$</p>
      </li>
    </ul>
    <p>$$\mathrm{E}\left[(f(x)-\hat{f}(x))^{2}\right] = (\operatorname{Bias}[\hat{f}(x)])^{2}+\operatorname{Var}[\hat{f}(x)]$$</p>
  </li>
</ol>

<p>[<br />
if you dont want to mess with stat-jargon; lemme rephrase:<br />
is the minimizer <script type="math/tex">{\displaystyle f=\inf_{h \in \mathcal{H}} I[h]}</script> where <script type="math/tex">I[h]</script> is the expected-risk/generalization-error (assume MSE);<br />
is it <script type="math/tex">\overline{f}(\mathbf{x})=\mathbb{E}_{\mathcal{D}}\left[f^{(\mathcal{D})}(\mathbf{x})\right]</script> the average hypothesis over all realizations of <script type="math/tex">N</script> data-points <script type="math/tex">\mathcal{D}_i</script>??<br />
]</p>
<div class="footnotes">
  <ol>
    <li id="fn:2">
      <p>Note that Abu-Mostafa defines <em>out-sample error <script type="math/tex">E_{\text{out}}</script></em> as the <em>expected error/risk <script type="math/tex">I[f]</script></em>; thus making <script type="math/tex">G = E_{\text{out}} - E_{\text{in}}</script>. <a href="#fnref:2" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:1">
      <p>with respect to a particular problem. <a href="#fnref:1" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>can be viewed as a measure of the <strong>average network approximation error</strong> <em>over all possible training data sets <script type="math/tex">\mathcal{D}</script></em> <a href="#fnref:3" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>


      <footer class="site-footer">
    <!--   <span class="site-footer-owner"><a href="http://localhost:8889">Ahmad Badary</a> is maintained by <a href="http://ahmedbadary.ml">Ahmad Badary</a>.</span> -->
    
<!--  -->
    <div class="wrapper">
        <div class="footer-col-wrapper">
            <div class="footer-col footer-col-1">
            <span class="site-footer-owner"><a href="http://localhost:8889">Site</a> maintained by <a href="http://ahmedbadary.ml">Ahmad Badary</a>.</span>
    <span class="site-footer-credits">
        <p>
            &copy; 2017. All rights reserved.
        </p> 
    </span>
            </div>
            <div class="footer-col footer-col-2">
            <div><p>         </p></div>
            </div>
            <div class="footer-col footer-col-3">
                <ul class="social-media-list">
                    
                      <li>
                        <a href="https://github.com/AhmedBadary">
                          <i class="fa fa-github"></i> GitHub
                        </a>
                      </li>
                    
                    
                      <li>
                        <a href="https://linkedin.com/in/ahmad-badary-656098121/">
                          <i class="fa fa-linkedin"></i> LinkedIn
                        </a>
                      </li>
                    
                    
                      <li>
                        <a href="https://www.facebook.com/ahmed.thabet.94">
                          <i class="fa fa-facebook"></i> Facebook
                        </a>
                      </li>
                    
                </ul>
            </div>
        </div>
    </div>
<!--  -->
</footer>


    </section>

  </body>

<!-- Table of Content Script -->
<script type="text/javascript">
var bodyContents = $(".bodyContents1");
$("<ol>").addClass("TOC1ul").appendTo(".TOC1");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC1ul");
     });
// 
var bodyContents = $(".bodyContents2");
$("<ol>").addClass("TOC2ul").appendTo(".TOC2");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC2ul");
     });
// 
var bodyContents = $(".bodyContents3");
$("<ol>").addClass("TOC3ul").appendTo(".TOC3");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC3ul");
     });
//
var bodyContents = $(".bodyContents4");
$("<ol>").addClass("TOC4ul").appendTo(".TOC4");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC4ul");
     });
//
var bodyContents = $(".bodyContents5");
$("<ol>").addClass("TOC5ul").appendTo(".TOC5");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC5ul");
     });
//
var bodyContents = $(".bodyContents6");
$("<ol>").addClass("TOC6ul").appendTo(".TOC6");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC6ul");
     });
//
var bodyContents = $(".bodyContents7");
$("<ol>").addClass("TOC7ul").appendTo(".TOC7");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC7ul");
     });
//
var bodyContents = $(".bodyContents8");
$("<ol>").addClass("TOC8ul").appendTo(".TOC8");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC8ul");
     });
//
var bodyContents = $(".bodyContents9");
$("<ol>").addClass("TOC9ul").appendTo(".TOC9");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC9ul");
     });

</script>

<!-- VIDEO BUTTONS SCRIPT -->
<script type="text/javascript">
  function iframePopInject(event) {
    var $button = $(event.target);
    // console.log($button.parent().next());
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $figure = $("<div>").addClass("video_container");
        $iframe = $("<iframe>").appendTo($figure);
        $iframe.attr("src", $button.attr("src"));
        // $iframe.attr("frameborder", "0");
        $iframe.attr("allowfullscreen", "true");
        $iframe.css("padding", "4px 6px");
        $button.next().css("display", "block");
        $figure.appendTo($button.next());
        $button.text("Hide Video")
    } else {
        $button.attr('value', 'show');
        $button.next().html("");
        $button.text("Show Video")
    }
}
</script>

<!-- BUTTON TRY -->
<script type="text/javascript">
  function iframePopA(event) {
    event.preventDefault();
    var $a = $(event.target).parent();
    console.log($a);
    if ($a.attr('value') == 'show') {
        $a.attr('value', 'hide');
        $figure = $("<div>");
        $iframe = $("<iframe>").addClass("popup_website_container").appendTo($figure);
        $iframe.attr("src", $a.attr("href"));
        $iframe.attr("frameborder", "1");
        $iframe.attr("allowfullscreen", "true");
        $iframe.css("padding", "4px 6px");
        $a.next().css("display", "block");
        $figure.appendTo($a.next().next());
        // $a.text("Hide Content")
        $('html, body').animate({
            scrollTop: $a.offset().top
        }, 1000);
    } else {
        $a.attr('value', 'show');
        $a.next().next().html("");
        // $a.text("Show Content")
    }

    $a.next().css("display", "inline");
}
</script>


<!-- TEXT BUTTON SCRIPT - INJECT -->
<script type="text/javascript">
  function showTextPopInject(event) {
    var $button = $(event.target);
    var txt = $button.attr("input");
    console.log(txt);
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $p = $("<p>");
        $p.html(txt);
        $button.next().css("display", "block");
        $p.appendTo($button.next());
        $button.text("Hide Content")
    } else {
        $button.attr('value', 'show');
        $button.next().html("");
        $button.text("Show Content")
    }

}
</script>

<!-- TEXT BUTTON SCRIPT - HIDDEN / HIDE / SHOW / HIDE/SHOW -->
<script type="text/javascript">
  function showTextPopHide(event) {
    var $button = $(event.target);
    // var txt = $button.attr("input");
    var txt = $button.text();
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $button.next().removeAttr("hidden");
        $button.text(txt + " - Hide Content");
    } else {
        $button.attr('value', 'show');
        $button.next().attr("hidden", "");
        $button.text(txt.replace(" - Hide Content",""));
    }
}
</script>

<!-- TEXT BUTTON SCRIPT - HIDDEN / HIDE / SHOW / HIDE/SHOW -->
<script type="text/javascript">
  function showText_withParent_PopHide(event) {
    var $button = $(event.target);
    var $parent = $button.parent();
    var txt = $button.text();
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $parent.next().removeAttr("hidden");
        $button.text(txt + " - Hide Content");
    } else {
        $button.attr('value', 'show');
        $parent.next().attr("hidden", "");
        $button.text(txt.replace(" - Hide Content",""));
    }
}
</script>

<!-- Print / Printing / printme -->
<!-- <script type="text/javascript">
i = 0

for (var i = 1; i < 6; i++) {
    var bodyContents = $(".bodyContents" + i);
    $("<p>").addClass("TOC1ul")  .appendTo(".TOC1");
    bodyContents.each(function(index, element) {
        var paragraph = $(element);
        $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC1ul");
         });
} 
</script>
 -->
 
</html>

