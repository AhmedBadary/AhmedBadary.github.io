<!DOCTYPE html>
<html lang="en-us">
  <head>
  <meta charset="UTF-8">
  <title>Ahmad Badary</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <link rel="stylesheet" href="/css/normalize.css">
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="/css/cayman.css">
  <link rel="stylesheet" href="/css/style.css">
  <link rel="shortcut icon" href="/main_files/favicon.ico" />
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="/css/customStyle.css">
  <title> » Ahmad Badary</title>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
</head>

  <body>
    <nav class="main-nav">
    <a href="https://ahmedbadary.github.io/" class="main-nav-logo">
        <img src="/main_files/logo.png">
    </a>
    <ul id="menu-main" class="main-nav-items">
        <li id="menu-item-1859" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-home menu-item-1859">
            <a href="/">Home</a>
        </li>
        <li id="menu-item-2869" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-2869">
            <a href="/work">Work</a>
        </li>
        <li id="menu-item-1892" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1892">
            <a href="/projects">Projects</a>
        </li>
        <li id="menu-item-1858" class="menu-item menu-item-type-post_type menu-item-object-page current_page_parent menu-item-1858">
            <a href="/blog">Blog</a>
        </li>
        <li id="menu-item-1862" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1862">
            <a href="/about">About</a>
        </li>
    </ul>
</nav>


<section class="page-header">
  <h1 class="project-name">Introduction and Basics of Deep Learning</h1>
  <h2 class="project-tagline"></h2>
  <a href="/#" class="btn">Home</a>
  <a href="/work" class="btn">Work-Space</a>
  <a href= /work_files/research/dl/theory.html class="btn">Previous</a>
</section>

<!-- <div>
  <ul class="posts">
    
      <li><span>02 Jan 2014</span> &raquo; <a href="/2014/01/02/introducing-Ahmad/">Introducing Ahmad</a></li>
    
  </ul>
</div> -->


    <section class="main-content">
      
      <div class="TOC">
  <h1 id="table-of-contents">Table of Contents</h1>

  <ul class="TOC1">
    <li><a href="#content1">Introduction: Deep Feedforward Neural Networks</a></li>
  </ul>
  <ul class="TOC2">
    <li><a href="#content2">Gradient-Based Learning</a></li>
  </ul>
  <ul class="TOC3">
    <li><a href="#content3">Output Units</a></li>
  </ul>
  <!--     * [FOURTH](#content4)
  {: .TOC4}
  * [FIFTH](#content5)
  {: .TOC5}
  * [SIXTH](#content6)
  {: .TOC6} -->
</div>

<hr />
<hr />

<h2 id="content1">Introduction: Deep Feedforward Neural Networks</h2>

<ol>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents11">(Deep) Feedforward neural networks:</strong><br />
 Given, for a classifier, <script type="math/tex">y=f^{\ast}(\boldsymbol{x})</script> maps an input <script type="math/tex">\boldsymbol{x}</script> to a category <script type="math/tex">y</script>.<br />
 An <strong>FFN</strong> defines a mapping <script type="math/tex">\boldsymbol{y}=f(\boldsymbol{x} ; \boldsymbol{\theta})</script>  and learns the value of the parameters <script type="math/tex">\boldsymbol{\theta}</script>  that result in the best function approximation.</p>

    <ul>
      <li>FFNs are called <strong>networks</strong> because they are typically represented by composing together many different functions</li>
      <li>The model is associated with a <strong>DAG</strong> describing how the functions are composed together.</li>
      <li>Functions connected in a <strong>chain structure</strong> are the most commonly used structure of neural networks.
        <blockquote>
          <p>E.g. we might have three functions <script type="math/tex">f^{(1)}, f^{(2)},</script> and <script type="math/tex">f^{(3)}</script> connected in a chain, to form <script type="math/tex">f(\boldsymbol{x})=f^{(3)}\left(f^{(2)}\left(f^{(1)}(\boldsymbol{x})\right)\right)</script>; being called the <script type="math/tex">n</script>-th <strong>Layer</strong> respectively.</p>
        </blockquote>
      </li>
      <li>The overall length of the chain is the <strong>depth</strong> of the model.</li>
      <li>During training, we drive <script type="math/tex">f(\boldsymbol{x})</script> to match <script type="math/tex">f^{\ast}(\boldsymbol{x})</script>. <br />
  The training data provides us with noisy, approximate examples of <script type="math/tex">f^{\ast}(\boldsymbol{x})</script> evaluated at different training points.</li>
    </ul>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents12">FFNs from Linear Models:</strong><br />
 Consider linear models biggest limitation: model capacity is limited to linear functions.<br />
 To <strong>extend</strong> linear models to represent non-linear functions of <script type="math/tex">\boldsymbol{x}</script> we can:</p>
    <ul>
      <li>Apply the linear model not to <script type="math/tex">\boldsymbol{x}</script> itself but <em>to a transformed input</em> <script type="math/tex">\phi(\boldsymbol{x})</script>, where <script type="math/tex">\phi</script> is a nonlinear transformation.</li>
      <li>Equivalently, apply the <strong>kernel trick</strong> to obtain nonlinear learning algorithm based on implicitly applying the <script type="math/tex">\phi</script> mapping.</li>
    </ul>

    <p>We can think of <script type="math/tex">\phi</script> as providing a set of features describing <script type="math/tex">\boldsymbol{x}</script>, or as providing a new representation for <script type="math/tex">\boldsymbol{x}</script>.<br />
 <strong>Choosing the mapping <script type="math/tex">\phi</script>:</strong></p>
    <ol>
      <li>Use a very generic <script type="math/tex">\phi</script>, s.a. infinite-dimensional (RBF) kernel.<br />
 If <script type="math/tex">\phi(\boldsymbol{x})</script> is of <em>high enough dimension</em>, we can <em>always have enough capacity</em> to fit the training set, but <em>generalization</em> to the test set often <em>remains poor</em>.<br />
 Very generic feature mappings are usually <em>based only</em> on the <em>principle of local smoothness</em> and do not encode enough prior information to solve advanced problems.</li>
      <li>Manually Engineer <script type="math/tex">\phi</script>.<br />
 Requires decades of human effort and the results are usually poor and non-scalable.</li>
      <li>The <em>strategy</em> of <em>deep learning</em> is to <strong>learn <script type="math/tex">\phi</script></strong>. We have a model:
        <p>$$y=f(\boldsymbol{x} ; \boldsymbol{\theta}, \boldsymbol{w})=\phi(\boldsymbol{x} ; \boldsymbol{\theta})^{\top} \boldsymbol{w}$$</p>
        <p>We now have parameters <script type="math/tex">\theta</script> that we use to learn <script type="math/tex">\phi</script> from a broad class of functions, and parameters <script type="math/tex">\boldsymbol{w}</script> that map from <script type="math/tex">\phi(\boldsymbol{x})</script> to the desired output.</p>
        <blockquote>
          <p>This is an example of a <em>deep FFN</em>, with <script type="math/tex">\phi</script> defining a <em>hidden layer</em>.</p>
        </blockquote>

        <p>This approach is the <em>only one</em> of the three that <em>gives up</em> on the <em>convexity</em> of the training problem, but the <em>benefits outweigh the harms</em>.<br />
 In this approach, we parametrize the representation as <script type="math/tex">\phi(\boldsymbol{x}; \theta)</script> and use the optimization algorithm to find the <script type="math/tex">\theta</script> that corresponds to a good representation.</p>

        <ul>
          <li>Capturing the benefit of the first approach:<br />
  by being highly generic — we do so by using a very broad family <script type="math/tex">\phi(\boldsymbol{x};\theta)</script>.</li>
          <li>Capturing the benefit of the second approach:<br />
  Human practitioners can encode their knowledge to help generalization by designing families <script type="math/tex">\phi(\boldsymbol{x}; \theta)</script> that they expect will perform well.<br />
  The <strong>advantage</strong> is that the human designer only needs to find the right general function family rather than finding precisely the right function.</li>
        </ul>
      </li>
    </ol>
  </li>
</ol>

<!-- 3. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents13}

4. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents14}

5. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents15}

6. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents16}

7. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents17}

8. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents18}
 -->

<hr />

<h2 id="content2">Gradient-Based Learning</h2>

<ol>
  <li><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents21">Stochastic Gradient Descent and FFNs:</strong><br />
 <strong>Stochastic Gradient Descent</strong> applied to <em>nonconvex</em> loss functions has <em>no</em> such <em>convergence guarantee</em> and is <em>sensitive</em> to the <em>values</em> of the <em>initial parameters</em>.<br />
 Thus, for FFNs (since they have nonconvex loss functions):
    <ul>
      <li><em><strong>Initialize all weights to small random values</strong></em>.</li>
      <li>The <em><strong>biases</strong></em> may be <em><strong>initialized to zero or to small positive values</strong></em>.</li>
    </ul>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents22">Learning Conditional Distributions with Maximum Likelihood:</strong><br />
 When <strong>Training</strong> using <strong>Maximum Likelihood</strong>:<br />
 The <em><strong>cost function</strong></em> is, simply, the <em><strong>negative log-likelihood</strong></em>.<br />
 Equivalently, the <em><strong>cross-entropy</strong></em> <em>between</em> the <em>training data</em> and the <em>model distribution</em>.</p>

    <script type="math/tex; mode=display">J(\boldsymbol{\theta})=-\mathbb{E}_{\mathbf{x}, \mathbf{y} \sim \hat{p}_{\text {data }}} \log p_{\text {model }}(\boldsymbol{y} | \boldsymbol{x})  \tag{6.12}</script>

    <ul>
      <li>The specific form of the cost function changes from model to model, depending on the specific form of <script type="math/tex">\log p_{\text {model}}</script>.</li>
      <li>The expansion of the above equation typically yields some terms that do not depend on the model parameters and may be discarded.</li>
    </ul>

    <p><strong>Maximum Likelihood and MSE:</strong><br />
 The equivalence between <em>maximum likelihood estimation with an output distribution</em> and <em>minimization of mean squared error</em> holds not just for a linear model, but in fact, the equivalence holds regardless of the <script type="math/tex">f(\boldsymbol{x} ; \boldsymbol{\theta})</script> used to predict the mean of the Gaussian.<br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">Example: MSE from MLE with gaussian distr.</button>
     <img src="/main_files/dl_book/22.png" alt="img" hidden="" /></p>

    <p hidden="">If $$p_{\text {model }}(\boldsymbol{y} | \boldsymbol{x})=\mathcal{N}(\boldsymbol{y} ; f(\boldsymbol{x} ; \boldsymbol{\theta}), \boldsymbol{I})$$ , then we recover the mean squared error cost, </p>
    <p hidden="">$$J(\theta)=\frac{1}{2} \mathbb{E}_{\mathbf{x}, \mathbf{y} \sim \hat{p}_{\text {data }}}\|\boldsymbol{y}-f(\boldsymbol{x} ; \boldsymbol{\theta})\|^{2}+\mathrm{const} \tag{6.13}$$ </p>
    <p hidden="">up to a scaling factor of $$1/2$$ and a term that does not depend on $$\theta$$.  
 The discarded constant is based on the variance of the Gaussian distribution, which in this case we chose not to parametrize.</p>

    <p><strong>Why derive the cost function from Maximum Likelihood?</strong><br />
 It removes the burden of designing cost functions for each model.<br />
 Specifying a model <script type="math/tex">p(\boldsymbol{y} | \boldsymbol{x})</script> automatically determines a cost function <script type="math/tex">\log p(\boldsymbol{y} | \boldsymbol{x})</script>.</p>

    <p><strong>Cost Function Design - Desirable Properties:</strong></p>
    <ul>
      <li>The <em><strong>gradient</strong></em> of the cost function must be <em><strong>large</strong></em> and <em><strong>predictable</strong></em> enough to serve as a good guide.<br />
  Functions that <em><strong>saturate</strong></em> (become very flat) undermine this objective because they make the gradient become very small. In many cases this happens because the activation functions used to produce the output of the hidden units or the output units saturate.<br />
  The negative log-likelihood helps to avoid this problem for many models. Several output units involve an exp function that can saturate when its argument is very negative. The log function in the negative log-likelihood cost function undoes the exp of some output units.</li>
    </ul>

    <p><button class="showText" value="show" onclick="showTextPopHide(event);">Making cross-entropy-cost-based training coherent</button>
     <img src="/main_files/dl_book/11.png" alt="img" hidden="" /></p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents23">Learning Conditional Statistics:</strong><br />
 Instead of learning a full probability distribution <script type="math/tex">p(\boldsymbol{y} | \boldsymbol{x} ; \boldsymbol{\theta})</script>, we often want to learn just one conditional statistic of <script type="math/tex">\boldsymbol{y}</script> given <script type="math/tex">\boldsymbol{x}</script>.
    <blockquote>
      <p>For example, we may have a predictor <script type="math/tex">f(\boldsymbol{x} ; \boldsymbol{\theta})</script>  that we wish to employ to predict the mean of <script type="math/tex">\boldsymbol{y}</script>.</p>
    </blockquote>

    <p><strong>The Cost Function as a <em>Functional:</em></strong><br />
 If we use a sufficiently powerful neural network, we can think of the neural network as being able to represent any function <script type="math/tex">f</script> from a wide class of functions, with this class being limited only by features such as continuity and boundedness rather than by having a specific parametric form. From this point of view, we can view the cost function as being a <strong>functional</strong> rather than just a function.<br />
 A <strong>Functional</strong> is a mapping from functions to real numbers.<br />
 We can thus think of <em>learning as choosing a function</em> rather than merely choosing a set of parameters.<br />
 We can design our cost functional to have its minimum occur at some specific function we desire.</p>
    <blockquote>
      <p>For example, we can design the cost functional to have its minimum lie on the function that maps <script type="math/tex">\boldsymbol{x}</script>  to the expected value of <script type="math/tex">\boldsymbol{y}</script> given <script type="math/tex">\boldsymbol{x}</script>.</p>
    </blockquote>

    <p>Solving an optimization problem with respect to a function requires a mathematical tool called <strong>calculus of variations</strong>, described in <em>section 19.4.2.</em></p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents24">Important Results in Optimization:</strong><br />
 The <strong>calculus of variations</strong> can be used to derive the following two important results in Optimization:
    <ol>
      <li>Solving the optimization problem
        <p>$$f^{\ast}=\underset{f}{\arg \min } \: \mathbb{E}_{\mathbf{x}, \mathbf{y} \sim p_{\text {data }}}\|\boldsymbol{y}-f(\boldsymbol{x})\|^{2} \tag{6.14}$$</p>
        <p>yields</p>
        <p>$$f^{\ast}(\boldsymbol{x})=\mathbb{E}_{\mathbf{y} \sim p_{\text {data }}(\boldsymbol{y} | \boldsymbol{x})}[\boldsymbol{y}] \tag{6.15}$$</p>
        <p>so long as this function lies within the class we optimize over.<br />
 In words: if we could train on infinitely many samples from the true data distribution, <em><strong>minimizing the MSE cost function</strong></em> would give a <strong><em>function</em> that <em>predicts</em> the <em>mean of <script type="math/tex">\boldsymbol{y}</script></em> for <em>each</em> value of <em><script type="math/tex">\boldsymbol{x}</script></em></strong>.</p>
        <blockquote>
          <p>Different cost functions give different statistics.</p>
        </blockquote>
      </li>
      <li>Solving the optimization problem (commonly known as <strong>Mean Absolute Error</strong>)
        <p>$$f^{\ast}=\underset{f}{\arg \min } \: \mathbb{E}_ {\mathbf{x}, \mathbf{y} \sim p_{\mathrm{data}}}\|\boldsymbol{y}-f(\boldsymbol{x})\|_ {1} \tag{6.16}$$</p>
        <p>yields a <strong><em>function</em> that predicts the <em>median</em> value of <em><script type="math/tex">\boldsymbol{y}</script></em> for each <script type="math/tex">\boldsymbol{x}</script></strong>, as long as such a function may be described by the family of functions we optimize over.</p>
      </li>
    </ol>

    <ul>
      <li><a href="http://www.stat.cmu.edu/~larry/=stat401/lecture-01.pdf">Derivations (linear? prob not)</a></li>
    </ul>

    <p><strong>Drawbacks of MSE and MAE (mean absolute error):</strong><br />
 They often lead to poor results when used with <em><strong>gradient-based</strong></em> optimization.<br />
 Some output units that saturate produce very small gradients when combined with these cost functions.<br />
 This is one reason that the <strong>cross-entropy</strong> cost function is <em>more popular</em> than <strong>MSE</strong> or <strong>MAE</strong>, even when it is not necessary to estimate an entire distribution <script type="math/tex">p(\boldsymbol{y} | \boldsymbol{x})</script>.</p>
  </li>
</ol>

<!-- 5. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents25}

6. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents26}

7. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents27}

8. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents28}
 -->

<hr />

<h2 id="content3">Output Units</h2>

<ol>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents31">Introduction:</strong><br />
 The choice of cost function is tightly coupled with the choice of output unit. Most of the time, we simply use the cross-entropy between the data distribution and the model distribution.<br />
 Thus, the choice of <em>how to represent the output</em> then determines the form of the cross-entropy function.</p>

    <p>Throughout this analysis, we suppose that:<br />
 <em>The FFN provides a set of hidden features defined by</em> <script type="math/tex">\boldsymbol{h}=f(\boldsymbol{x} ; \boldsymbol{\theta})</script>.<br />
 The <em>role of the output layer</em>, thus, is to <em>provide some additional transformation from the features to complete the task</em> the FFN is tasked with.</p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents32">Linear Units:</strong><br />
 <strong>Linear Units</strong> are a simple kind of output units, based on an <em>affine transformation</em> with no non-linearity.<br />
 <strong>Mathematically,</strong> given features <script type="math/tex">\boldsymbol{h}</script>, a layer of linear output units produces a vector <script type="math/tex">\hat{\boldsymbol{y}}=\boldsymbol{W}^{\top} \boldsymbol{h}+\boldsymbol{b}</script>.</p>

    <p><strong>Application:</strong> used for <em><strong>Gaussian Output Distributions</strong></em>.<br />
 Linear output layers are often used to <strong>produce the mean of a conditional Gaussian Distributions</strong>:</p>
    <p>$$p(\boldsymbol{y} | \boldsymbol{x})=\mathcal{N}(\boldsymbol{y} ; \hat{\boldsymbol{y}}, \boldsymbol{I}) \tag{6.17}$$</p>
    <p>In this case, <em>maximizing the log-likelihood</em> is equivalent to <em>minimizing the MSE</em>.</p>

    <p><strong>Learning the Covariance of the Gaussian:</strong><br />
 The MLE framework makes it straightforward to:</p>
    <ul>
      <li>Learn the covariance of the Gaussian too</li>
      <li>Make the covariance of the Gaussian be a function of the input<br />
 However, the covariance must be constrained to be a <em>positive definite matrix</em> for all inputs.
        <blockquote>
          <p>It is difficult to satisfy such constraints with a linear output layer, so typically other output units are used to parametrize the covariance.</p>
          <blockquote>
            <p>Approaches to modeling the covariance are described shortly, in <em>section 6.2.2.4.</em></p>
          </blockquote>
        </blockquote>
      </li>
    </ul>

    <p><strong>Saturation:</strong><br />
 Because linear units do not saturate, they pose little difficulty for gradient- based optimization algorithms and may be used with a wide variety of optimization algorithms.</p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents33">Sigmoid Units:</strong><br />
 <strong>Sigmoid Units</strong></p>

    <p><strong>Binary Classification:</strong> is a classification problem over two classes. It requires predicting the value of a <em>binary variable <script type="math/tex">y</script></em>. It is one of many tasks requiring that.<br />
          <strong>The MLE approach</strong> is to define a <em><strong>Bernoulli distribution</strong></em> over <script type="math/tex">y</script> conditioned on <script type="math/tex">\boldsymbol{x}</script>.<br />
 A <strong>Bernoulli distribution</strong> is defined by just <em>1 single</em> number.</p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents34">Asynchronous:</strong></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents35">Asynchronous:</strong></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents36">Asynchronous:</strong></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents37">Asynchronous:</strong></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents38">Asynchronous:</strong></p>
  </li>
</ol>



      <footer class="site-footer">
    <!--   <span class="site-footer-owner"><a href="http://localhost:8889">Ahmad Badary</a> is maintained by <a href="http://ahmedbadary.ml">Ahmad Badary</a>.</span> -->
    
<!--  -->
    <div class="wrapper">
        <div class="footer-col-wrapper">
            <div class="footer-col footer-col-1">
            <span class="site-footer-owner"><a href="http://localhost:8889">Site</a> maintained by <a href="http://ahmedbadary.ml">Ahmad Badary</a>.</span>
    <span class="site-footer-credits">
        <p>
            &copy; 2017. All rights reserved.
        </p> 
    </span>
            </div>
            <div class="footer-col footer-col-2">
            <div><p>         </p></div>
            </div>
            <div class="footer-col footer-col-3">
                <ul class="social-media-list">
                    
                      <li>
                        <a href="https://github.com/AhmedBadary">
                          <i class="fa fa-github"></i> GitHub
                        </a>
                      </li>
                    
                    
                      <li>
                        <a href="https://linkedin.com/in/ahmad-badary-656098121/">
                          <i class="fa fa-linkedin"></i> LinkedIn
                        </a>
                      </li>
                    
                    
                      <li>
                        <a href="https://www.facebook.com/ahmed.thabet.94">
                          <i class="fa fa-facebook"></i> Facebook
                        </a>
                      </li>
                    
                </ul>
            </div>
        </div>
    </div>
<!--  -->
</footer>


    </section>

  </body>

<!-- Table of Content Script -->
<script type="text/javascript">
var bodyContents = $(".bodyContents1");
$("<ol>").addClass("TOC1ul").appendTo(".TOC1");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC1ul");
     });
// 
var bodyContents = $(".bodyContents2");
$("<ol>").addClass("TOC2ul").appendTo(".TOC2");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC2ul");
     });
// 
var bodyContents = $(".bodyContents3");
$("<ol>").addClass("TOC3ul").appendTo(".TOC3");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC3ul");
     });
//
var bodyContents = $(".bodyContents4");
$("<ol>").addClass("TOC4ul").appendTo(".TOC4");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC4ul");
     });
//
var bodyContents = $(".bodyContents5");
$("<ol>").addClass("TOC5ul").appendTo(".TOC5");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC5ul");
     });
//
var bodyContents = $(".bodyContents6");
$("<ol>").addClass("TOC6ul").appendTo(".TOC6");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC6ul");
     });
//
var bodyContents = $(".bodyContents7");
$("<ol>").addClass("TOC7ul").appendTo(".TOC7");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC7ul");
     });
//
var bodyContents = $(".bodyContents8");
$("<ol>").addClass("TOC8ul").appendTo(".TOC8");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC8ul");
     });
//
var bodyContents = $(".bodyContents9");
$("<ol>").addClass("TOC9ul").appendTo(".TOC9");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC9ul");
     });

</script>

<!-- VIDEO BUTTONS SCRIPT -->
<script type="text/javascript">
  function iframePopInject(event) {
    var $button = $(event.target);
    // console.log($button.parent().next());
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $figure = $("<div>").addClass("video_container");
        $iframe = $("<iframe>").appendTo($figure);
        $iframe.attr("src", $button.attr("src"));
        // $iframe.attr("frameborder", "0");
        $iframe.attr("allowfullscreen", "true");
        $iframe.css("padding", "4px 6px");
        $button.next().css("display", "block");
        $figure.appendTo($button.next());
        $button.text("Hide Video")
    } else {
        $button.attr('value', 'show');
        $button.next().html("");
        $button.text("Show Video")
    }
}
</script>

<!-- BUTTON TRY -->
<script type="text/javascript">
  function iframePopA(event) {
    event.preventDefault();
    var $a = $(event.target).parent();
    console.log($a);
    if ($a.attr('value') == 'show') {
        $a.attr('value', 'hide');
        $figure = $("<div>");
        $iframe = $("<iframe>").addClass("popup_website_container").appendTo($figure);
        $iframe.attr("src", $a.attr("href"));
        $iframe.attr("frameborder", "1");
        $iframe.attr("allowfullscreen", "true");
        $iframe.css("padding", "4px 6px");
        $a.next().css("display", "block");
        $figure.appendTo($a.next().next());
        // $a.text("Hide Content")
        $('html, body').animate({
            scrollTop: $a.offset().top
        }, 1000);
    } else {
        $a.attr('value', 'show');
        $a.next().next().html("");
        // $a.text("Show Content")
    }

    $a.next().css("display", "inline");
}
</script>


<!-- TEXT BUTTON SCRIPT - INJECT -->
<script type="text/javascript">
  function showTextPopInject(event) {
    var $button = $(event.target);
    var txt = $button.attr("input");
    console.log(txt);
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $p = $("<p>");
        $p.html(txt);
        $button.next().css("display", "block");
        $p.appendTo($button.next());
        $button.text("Hide Content")
    } else {
        $button.attr('value', 'show');
        $button.next().html("");
        $button.text("Show Content")
    }

}
</script>

<!-- TEXT BUTTON SCRIPT - HIDDEN / HIDE / SHOW / HIDE/SHOW -->
<script type="text/javascript">
  function showTextPopHide(event) {
    var $button = $(event.target);
    // var txt = $button.attr("input");
    var txt = $button.text();
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $button.next().removeAttr("hidden");
        $button.text(txt + " - Hide Content");
    } else {
        $button.attr('value', 'show');
        $button.next().attr("hidden", "");
        $button.text(txt.replace(" - Hide Content",""));
    }
}
</script>

<!-- TEXT BUTTON SCRIPT - HIDDEN / HIDE / SHOW / HIDE/SHOW -->
<script type="text/javascript">
  function showText_withParent_PopHide(event) {
    var $button = $(event.target);
    var $parent = $button.parent();
    var txt = $button.text();
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $parent.next().removeAttr("hidden");
        $button.text(txt + " - Hide Content");
    } else {
        $button.attr('value', 'show');
        $parent.next().attr("hidden", "");
        $button.text(txt.replace(" - Hide Content",""));
    }
}
</script>

<!-- Print / Printing / printme -->
<!-- <script type="text/javascript">
i = 0

for (var i = 1; i < 6; i++) {
    var bodyContents = $(".bodyContents" + i);
    $("<p>").addClass("TOC1ul")  .appendTo(".TOC1");
    bodyContents.each(function(index, element) {
        var paragraph = $(element);
        $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC1ul");
         });
} 
</script>
 -->
 
</html>

