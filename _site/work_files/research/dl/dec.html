<!DOCTYPE html>
<html lang="en-us">
  <head>
  <meta charset="UTF-8">
  <title>Ahmad Badary</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <link rel="stylesheet" href="/css/normalize.css">
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="/css/cayman.css">
  <link rel="stylesheet" href="/css/style.css">
  <link rel="shortcut icon" href="/main_files/favicon.ico" />
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="/css/customStyle.css">
  <title> » Ahmad Badary</title>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
</head>

  <body>
    <nav class="main-nav">
    <a href="https://ahmedbadary.github.io/" class="main-nav-logo">
        <img src="/main_files/logo.png">
    </a>
    <ul id="menu-main" class="main-nav-items">
        <li id="menu-item-1859" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-home menu-item-1859">
            <a href="/">Home</a>
        </li>
        <li id="menu-item-2869" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-2869">
            <a href="/work">Work</a>
        </li>
        <li id="menu-item-1892" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1892">
            <a href="/projects">Projects</a>
        </li>
        <li id="menu-item-1858" class="menu-item menu-item-type-post_type menu-item-object-page current_page_parent menu-item-1858">
            <a href="/blog">Blog</a>
        </li>
        <li id="menu-item-1862" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1862">
            <a href="/about">About</a>
        </li>
    </ul>
</nav>


<section class="page-header">
  <h1 class="project-name">Object Detection <br /> with Deep Learning</h1>
  <h2 class="project-tagline"></h2>
  <a href="/#" class="btn">Home</a>
  <a href="/work" class="btn">Work-Space</a>
  <a href= /work_files/research/dl/cv.html class="btn">Previous</a>
</section>

<!-- <div>
  <ul class="posts">
    
      <li><span>02 Jan 2014</span> &raquo; <a href="/2014/01/02/introducing-Ahmad/">Introducing Ahmad</a></li>
    
  </ul>
</div> -->


    <section class="main-content">
      
      <div class="TOC">
  <h1 id="table-of-contents">Table of Contents</h1>

  <ul class="TOC1">
    <li><a href="#content1">Object Detection</a></li>
  </ul>
  <ul class="TOC2">
    <li><a href="#content2">Approaches (The Pre-DeepLearning Era)</a></li>
  </ul>
  <ul class="TOC3">
    <li><a href="#content3">Approaches (The Deep Learning Era)</a></li>
  </ul>
  <ul class="TOC4">
    <li><a href="#content4">Methods, Approaches and Algorithms in Training DL Models</a></li>
  </ul>
</div>

<hr />
<hr />

<h2 id="content1">Object Detection</h2>

<ol>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents11">Object Detection:</strong></dt>
      <dd><strong>Object Detection</strong> is the process of finding multiple instances of real-world objects such as faces, vehicles, and animals in images.</dd>
      <dd><img src="/main_files/cs231n/11_3/1.png" alt="img" width="28%" /></dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents12">The Structure:</strong></dt>
      <dd>
        <ul>
          <li><strong>Input</strong>: Image</li>
          <li><strong>Output</strong>: A pair of (box-co-ords, class) of all the objects in a fixed number of classes that appear in the image.</li>
        </ul>
      </dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents13">Properties:</strong></dt>
      <dd>In the problem of object detection, we normally do not know the number of objects that we need to detect.<br />
This leads to a problem when trying to model the problem as a <strong>regression</strong> problem due to the undefined number of coordinates of boxes.</dd>
      <dd>Thus, this problem is mainly modeled as a classification problem.</dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents14">Applications:</strong></dt>
      <dd>
        <ul>
          <li>Image Retrieval</li>
          <li>Surveillance</li>
          <li>Face Detection</li>
          <li>Face Recognition</li>
          <li>Pedestrian Detection</li>
          <li>Self-Driving Cars</li>
        </ul>
      </dd>
    </dl>
  </li>
</ol>

<hr />

<h2 id="content2">Approaches (The Pre-DeepLearning Era)</h2>

<ol>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents21">Semantic Texton Forests:</strong></dt>
      <dd>This approach consists of ensembles of decision trees that act directly on image pixels.</dd>
      <dd>Semantic Texton Forests (STFs) 
are randomized decision forests that use only simple pixel comparisons on local image patches, performing both an
implicit hierarchical clustering into semantic textons and an explicit local classification of the patch category.</dd>
      <dd>STFs allow us to build powerful texton codebooks without computing expensive filter-banks or descriptors, and without performing costly k-means clustering and nearest-neighbor assignment.</dd>
      <dd><em>Semantic Texton Forests for Image Categorization and Segmentation, Shawton et al. (2008)</em></dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents22">Region Proposals:</strong></dt>
      <dd></dd>
      <dd>
        <ul>
          <li><strong>Algorithm</strong>:
            <ul>
              <li>Find “blobby” image regions that are likely to contain objects</li>
            </ul>
          </li>
        </ul>
      </dd>
      <dd>These are relatively fast algorithms.</dd>
      <dd><em>Alexe et al, “Measuring the objectness of image windows”, TPAMI 2012</em><br />
<em>Uijlings et al, “Selective Search for Object Recognition”, IJCV 2013</em><br />
<em>Cheng et al, “BING: Binarized normed gradients for objectness estimation at 300fps”, CVPR 2014</em><br />
<em>Zitnick and Dollar, “Edge boxes: Locating object proposals from edges”, ECCV 2014</em></dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents23">Conditional Random Fields:</strong></dt>
      <dd>CRFs provide a probabilistic framework for labeling and segmenting structured data.</dd>
      <dd>They try to model the relationship between pixels, e.g.:
        <ol>
          <li>nearby pixels more likely to have same label</li>
          <li>pixels with similar color more likely to have same label</li>
          <li>the pixels above the pixels “chair” more likely to be “person” instead of “plane”</li>
          <li>refine results by iterations</li>
        </ol>
      </dd>
      <dd><em>W. Wu, A. Y. C. Chen, L. Zhao and J. J. Corso (2014): “Brain Tumor detection and segmentation in a CRF framework with pixel-pairwise affinity and super pixel-level features”</em></dd>
      <dd><em>Plath et al. (2009): “Multi-class image segmentation using conditional random fields and global classification”</em></dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents24">SuperPixel Segmentation:</strong></dt>
      <dd>The concept of superpixels was first introduced by Xiaofeng Ren and Jitendra Malik in 2003.</dd>
      <dd><strong>Superpixel</strong> is a group of connected pixels with similar colors or gray levels.<br />
They produce an image patch which is better aligned with intensity edges than a rectangular patch.</dd>
      <dd><strong>Superpixel segmentation</strong> is the idea of dividing an image into hundreds of non-overlapping superpixels.<br />
Then, these can be fed into a segmentation algorithm, such as <strong>Conditional Random Fields</strong> or <strong>Graph Cuts</strong>, for the purpose of segmentation.</dd>
      <dd><em>Efficient graph-based image segmentation, Felzenszwalb, P.F. and Huttenlocher, D.P. International Journal of Computer Vision, 2004</em></dd>
      <dd><em>Quick shift and kernel methods for mode seeking, Vedaldi, A. and Soatto, S. European Conference on Computer Vision, 2008</em></dd>
      <dd><em>Peer Neubert &amp; Peter Protzel (2014). Compact Watershed and Preemptive</em></dd>
    </dl>
  </li>
</ol>

<hr />

<h2 id="content3">Approaches (The Deep Learning Era)</h2>

<ol>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents31">The Sliding Window Approach:</strong></dt>
      <dd>We utilize <em>classification</em> for <em>detection</em> purposes.</dd>
      <dd>
        <ul>
          <li><strong>Algorithm</strong>:
            <ul>
              <li>We break up the input image into tiny “crops” of the input image.</li>
              <li>Use Classification+Localization to find the class of the center pixel of the crop, or classify it as background.
                <blockquote>
                  <p>Using the same machinery for classification+Localization.</p>
                </blockquote>
              </li>
              <li>Slide the window and look at more “crops”</li>
            </ul>
          </li>
        </ul>
      </dd>
      <dd>Basically, we do classification+Localization on each crop of the image.</dd>
      <dd>
        <ul>
          <li><strong>DrawBacks:</strong>
            <ul>
              <li>Very Inefficient and Expensive:<br />
  We need to apply a CNN to a huge number of locations and scales.</li>
            </ul>
          </li>
        </ul>
      </dd>
      <dd><em>Sermant et. al 2013: “OverFeat”</em></dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents32">Region Proposal Networks (R-CNNs):</strong></dt>
      <dd>A framework for object detection, that utilizes Region Proposals (Regions of Interest (ROIs)), consisting of three separate architectures.</dd>
      <dd><img src="/main_files/cs231n/11_3/3.png" alt="img" width="82%" /></dd>
      <dd>
        <ul>
          <li><strong>Structure</strong>:
            <ul>
              <li><em>Input</em>: Image vector</li>
              <li><em>Output</em>: A vector of bounding boxes coordinates and a class prediction for each box</li>
            </ul>
          </li>
        </ul>
      </dd>
      <dd>
        <ul>
          <li><strong>Strategy</strong>:<br />
  Propose a number of “bounding boxes”, then check if any of them, actually, corresponds to an object.
            <blockquote>
              <p>The bounding boxes are created using <strong>Selective Search</strong>.</p>
            </blockquote>
          </li>
        </ul>
      </dd>
      <dd>
        <ul>
          <li><strong>Selective Search</strong>: A method that looks at the image through windows of different sizes, and for each size tries to group together adjacent pixels by texture, color, or intensity to identify objects.  <br />
  <img src="/main_files/cs231n/11_3/2.png" alt="img" width="100%" /></li>
        </ul>
      </dd>
      <dd>
        <ul>
          <li><strong>Key Insights</strong>:
            <ol>
              <li>One can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects</li>
              <li>When labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost.</li>
            </ol>
          </li>
        </ul>
      </dd>
      <dd>
        <ul>
          <li><strong>Algorithm</strong>:
            <ul>
              <li>Create <em>Region Proposals (Regions of Interest (ROIs))</em> of bounding boxes</li>
              <li>Warp the regions to a standard square size to fit the “cnn classification models”, due to the FCNs</li>
              <li>Pass the warped images to a modified version of <em>AlexNet</em> to <em>extract image features</em></li>
              <li>Pass the <em>image features</em> to an SVM to <em>classify the image regions</em> into a <em>class</em> or <em>background</em></li>
              <li>Run the bounding box coordinates in a <strong>Linear Regression</strong> model to “tighten” the bounding boxes
                <ul>
                  <li><em><strong>Linear Regression</strong></em>:
                    <ul>
                      <li><strong>Structure</strong>:
                        <ul>
                          <li><em>Input</em>: sub-regions of the image corresponding to objects</li>
                          <li><em>Output</em>: New bounding box coordinates for the object in the sub-region.</li>
                        </ul>
                      </li>
                    </ul>
                  </li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </dd>
      <dd>
        <ul>
          <li><strong>Issues</strong>:
            <ul>
              <li>Ad hoc training objectives:
                <ul>
                  <li>Fine-tune network with softmax classifier (log loss)</li>
                  <li>Train post-hoc linear SVMs (hinge loss)</li>
                  <li>Train post-hoc bounding-box regressions (least squares)</li>
                </ul>
              </li>
              <li>Training is slow (84h), takes a lot of disk space</li>
              <li>Inference (detection) is slow
                <ul>
                  <li>47s / image with VGG16 [Simonyan &amp; Zisserman. ICLR15]</li>
                  <li>Fixed by SPP-net [He et al. ECCV14]</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </dd>
      <dd>R-CNN is slow because it performs a ConvNet forward pass for each region proposal, without sharing computation.</dd>
      <dd><em>R. Girshick, J. Donahue, T. Darrell, J. Malik. (2014): “Rich feature hierarchies for accurate object detection and semantic segmentation”</em></dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents33">Fast R-CNNs:</strong></dt>
      <dd>A single, end-to-end, architecture for object detection based on R-CNNs, that vastly improves on its speed and accuracy by utilizing shared computations of features.</dd>
      <dd><img src="/main_files/cs231n/11_3/4.png" alt="img" width="82%" /></dd>
      <dd>
        <ul>
          <li><strong>Structure</strong>:
            <ul>
              <li><em>Input</em>: Image vector</li>
              <li><em>Output</em>: A vector of bounding boxes coordinates and a class prediction for each box</li>
            </ul>
          </li>
        </ul>
      </dd>
      <dd>
        <ul>
          <li><strong>Key Insights</strong>:
            <ol>
              <li>Instead of running the ConvNet on <strong>each region proposal separately</strong>, we run the ConvNet on the <strong>entire image</strong>.</li>
              <li>Instead of taking <strong>crops of the original image</strong>, we <strong>project the regions of interest</strong> onto the <strong>ConvNet Feature Map</strong>, corresponding to each RoI, and then use the <strong>projected regions in the feature map</strong> <em>for classification</em>.<br />
 This allows us to <strong>reuse</strong> a lot of the expensive computation of the features.</li>
              <li>Jointly train the CNN, classifier, and bounding box regressor in a single model. Where earlier we had different models to extract image features (CNN), classify (SVM), and tighten bounding boxes (regressor), Fast R-CNN instead used a single network to compute all three.</li>
            </ol>
          </li>
        </ul>
      </dd>
      <dd>
        <ul>
          <li><strong>Algorithm</strong>:
            <ul>
              <li>Create <em>Region Proposals (Regions of Interest (ROIs))</em> of bounding boxes</li>
              <li>Pass the entire image to a modified version of <em>AlexNet</em> to <em>extract image features</em> by creating an <em>image feature map</em> for the <strong>entire image</strong>.</li>
              <li>Project each <em>RoI</em> to the <em>feature map</em> and crop each respective projected region</li>
              <li>Apply <strong>RoI Pooling</strong> to the <em>regions extracted from the feature map</em> to a standard square size to fit the “cnn classification models”, due to the FCNs</li>
              <li>Pass the <em>image features</em> to an SVM to <em>classify the image regions</em> into a <em>class</em> or <em>background</em></li>
              <li>Run the bounding box coordinates in a <strong>Linear Regression</strong> model to “tighten” the bounding boxes
                <ul>
                  <li><em><strong>Linear Regression</strong></em>:
                    <ul>
                      <li><strong>Structure</strong>:
                        <ul>
                          <li><em>Input</em>: sub-regions of the image corresponding to objects</li>
                          <li><em>Output</em>: New bounding box coordinates for the object in the sub-region.</li>
                        </ul>
                      </li>
                    </ul>
                  </li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </dd>
      <dd>
        <ul>
          <li><strong>RoI Pooling</strong>: is a pooling technique aimed to perform max pooling on inputs of nonuniform sizes to obtain fixed-size feature maps (e.g. 7×7).
            <ul>
              <li><em><strong>Structure</strong></em>:
                <ul>
                  <li><em>Input</em>: A fixed-size feature map obtained from a deep convolutional network with several convolutions and max pooling layers.</li>
                  <li><em>Output</em>: An N x 5 matrix of representing a list of regions of interest, where N is a number of RoIs. The first column represents the image index and the remaining four are the coordinates of the top left and bottom right corners of the region.<br />
  For every region of interest from the input list, it takes a section of the input feature map that corresponds to it and scales it to some pre-defined size.</li>
                </ul>
              </li>
              <li><em><strong>Scaling</strong></em>:
                <ol>
                  <li>Divide the RoI into equal-sized sections (the number of which is the same as the dimension of the output)</li>
                  <li>Find the largest value in each section</li>
                  <li>Copy these max values to the output buffer<br />
  The <strong>dimension of the output</strong> is determined solely by <em>the number of sections we divide the proposal</em> into. <br />
  <img src="/main_files/cs231n/11_3/5.png" alt="img" width="90%" /></li>
                </ol>
              </li>
            </ul>
          </li>
        </ul>
      </dd>
      <dd>
        <ul>
          <li><strong>The Bottleneck</strong>: <br />
  It appears that Fast R-CNNs are capable of object detection at test time in:
            <ul>
              <li><em>Including RoIs</em>: 2.3s</li>
              <li><em>Excluding RoIs</em>: 0.3s<br />
  Thus, <strong>the bottleneck</strong> for the speed seems to be the method of creating the RoIs, <em>Selective Search</em></li>
            </ul>
          </li>
        </ul>
      </dd>
      <dd><em>[1] Girshick, Ross (2015). “Fast R-CNN”</em></dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents34">Faster R-CNNs:</strong></dt>
      <dd>A single, end-to-end, architecture for object detection based on Fast R-CNNs, that tackles the bottleneck in speed (i.e. computing RoIs) by introducing <strong>Region Proposal Networks (RPNs)</strong> to make a CNN predict proposals from features.   <br />
Region Proposal Networks share full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. <br />
The network is jointly trained with 4 losses:
    1. RPN classify object / not object
    2. RPN regress box coordinates
    3. Final classification score (object classes)
    4. Final box coordinates</dd>
      <dd><img src="/main_files/cs231n/11_3/6.png" alt="img" width="60%" /></dd>
      <dd>
        <ul>
          <li><strong>Region Proposal Network (RPN)</strong>: is an, end-to-end, fully convolutional network that simultaneously predicts object bounds and objectness scores at each position.<br />
RPNs work by passing a sliding window over the CNN feature map and at each window, outputting k potential bounding boxes and scores for how good each of those boxes is expected to be.</li>
        </ul>
      </dd>
      <dd>
        <ul>
          <li><strong>Structure</strong>:
            <ul>
              <li><em>Input</em>: Image vector</li>
              <li><em>Output</em>: A vector of bounding boxes coordinates and a class prediction for each box</li>
            </ul>
          </li>
        </ul>
      </dd>
      <dd>
        <ul>
          <li><strong>Key Insights</strong>:
            <ol>
              <li>Replace <strong>Selective Search</strong> for finding RoIs by a <strong>Region Proposal Network</strong> that shares the features, and thus reduces the computation and time, of the pipeline.</li>
            </ol>
          </li>
        </ul>
      </dd>
      <dd>
        <ul>
          <li><strong>Algorithm</strong>:
            <ul>
              <li>Pass the entire image to a modified version of <em>AlexNet</em> to <em>extract image features</em> by creating an <em>image feature map</em> for the <strong>entire image</strong>.</li>
              <li>Pass the <strong>CNN Feature Map</strong> to the RPN to generate bounding boxes and a score for each bounding box</li>
              <li>Pass each such bounding box that is likely to be an object into Fast R-CNN to generate a classification and tightened bounding boxes.</li>
            </ul>
          </li>
        </ul>
      </dd>
      <dd><em>Ren et al, (2015). “Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks”</em></dd>
    </dl>
  </li>
</ol>

<hr />

<h2 id="content4">Methods, Approaches and Algorithms in Training DL Models</h2>


      <footer class="site-footer">
    <!--   <span class="site-footer-owner"><a href="http://localhost:8889">Ahmad Badary</a> is maintained by <a href="https://ahmedbadary.github.io/">Ahmad Badary</a>.</span> -->
    
<!--  -->
    <div class="wrapper">
        <div class="footer-col-wrapper">
            <div class="footer-col footer-col-1">
            <span class="site-footer-owner"><a href="http://localhost:8889">Site</a> maintained by <a href="https://ahmedbadary.github.io/">Ahmad Badary</a>.</span>
    <span class="site-footer-credits">
        <p>
            &copy; 2017. All rights reserved.
        </p> 
    </span>
            </div>
            <div class="footer-col footer-col-2">
            <div><p>         </p></div>
            </div>
            <div class="footer-col footer-col-3">
                <ul class="social-media-list">
                    
                      <li>
                        <a href="https://github.com/AhmedBadary">
                          <i class="fa fa-github"></i> GitHub
                        </a>
                      </li>
                    
                    
                      <li>
                        <a href="https://linkedin.com/in/ahmad-badary-656098121/">
                          <i class="fa fa-linkedin"></i> LinkedIn
                        </a>
                      </li>
                    
                    
                      <li>
                        <a href="https://www.facebook.com/ahmed.thabet.94">
                          <i class="fa fa-facebook"></i> Facebook
                        </a>
                      </li>
                    
                </ul>
            </div>
        </div>
    </div>
<!--  -->
</footer>


    </section>

  </body>

<!-- Table of Content Script -->
<script type="text/javascript">
var bodyContents = $(".bodyContents1");
$("<ol>").addClass("TOC1ul").appendTo(".TOC1");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC1ul");
     });
// 
var bodyContents = $(".bodyContents2");
$("<ol>").addClass("TOC2ul").appendTo(".TOC2");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC2ul");
     });
// 
var bodyContents = $(".bodyContents3");
$("<ol>").addClass("TOC3ul").appendTo(".TOC3");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC3ul");
     });
//
var bodyContents = $(".bodyContents4");
$("<ol>").addClass("TOC4ul").appendTo(".TOC4");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC4ul");
     });
//
var bodyContents = $(".bodyContents5");
$("<ol>").addClass("TOC5ul").appendTo(".TOC5");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC5ul");
     });
//
var bodyContents = $(".bodyContents6");
$("<ol>").addClass("TOC6ul").appendTo(".TOC6");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC6ul");
     });
//
var bodyContents = $(".bodyContents7");
$("<ol>").addClass("TOC7ul").appendTo(".TOC7");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC7ul");
     });
//
var bodyContents = $(".bodyContents8");
$("<ol>").addClass("TOC8ul").appendTo(".TOC8");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC8ul");
     });
//
var bodyContents = $(".bodyContents9");
$("<ol>").addClass("TOC9ul").appendTo(".TOC9");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC9ul");
     });

</script>

<!-- VIDEO BUTTONS SCRIPT -->
<script type="text/javascript">
  function iframePopInject(event) {
    var $button = $(event.target);
    // console.log($button.parent().next());
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $figure = $("<div>").addClass("video_container");
        $iframe = $("<iframe>").appendTo($figure);
        $iframe.attr("src", $button.attr("src"));
        // $iframe.attr("frameborder", "0");
        $iframe.attr("allowfullscreen", "true");
        $iframe.css("padding", "4px 6px");
        $button.next().css("display", "block");
        $figure.appendTo($button.next());
        $button.text("Hide Video")
    } else {
        $button.attr('value', 'show');
        $button.next().html("");
        $button.text("Show Video")
    }
}
</script>

<!-- BUTTON TRY -->
<script type="text/javascript">
  function iframePopA(event) {
    event.preventDefault();
    var $a = $(event.target).parent();
    console.log($a);
    if ($a.attr('value') == 'show') {
        $a.attr('value', 'hide');
        $figure = $("<div>");
        $iframe = $("<iframe>").addClass("popup_website_container").appendTo($figure);
        $iframe.attr("src", $a.attr("href"));
        $iframe.attr("frameborder", "1");
        $iframe.attr("allowfullscreen", "true");
        $iframe.css("padding", "4px 6px");
        $a.next().css("display", "block");
        $figure.appendTo($a.next().next());
        // $a.text("Hide Content")
        $('html, body').animate({
            scrollTop: $a.offset().top
        }, 1000);
    } else {
        $a.attr('value', 'show');
        $a.next().next().html("");
        // $a.text("Show Content")
    }

    $a.next().css("display", "inline");
}
</script>


<!-- TEXT BUTTON SCRIPT - INJECT -->
<script type="text/javascript">
  function showTextPopInject(event) {
    var $button = $(event.target);
    var txt = $button.attr("input");
    console.log(txt);
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $p = $("<p>");
        $p.html(txt);
        $button.next().css("display", "block");
        $p.appendTo($button.next());
        $button.text("Hide Content")
    } else {
        $button.attr('value', 'show');
        $button.next().html("");
        $button.text("Show Content")
    }

}
</script>

<!-- TEXT BUTTON SCRIPT - HIDDEN / HIDE / SHOW / HIDE/SHOW -->
<script type="text/javascript">
  function showTextPopHide(event) {
    var $button = $(event.target);
    // var txt = $button.attr("input");
    var txt = $button.text();
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $button.next().removeAttr("hidden");
        $button.text(txt + " - Hide Content");
    } else {
        $button.attr('value', 'show');
        $button.next().attr("hidden", "");
        $button.text(txt.replace(" - Hide Content",""));
    }
}
</script>

<!-- TEXT BUTTON SCRIPT - HIDDEN / HIDE / SHOW / HIDE/SHOW -->
<script type="text/javascript">
  function showText_withParent_PopHide(event) {
    var $button = $(event.target);
    var $parent = $button.parent();
    var txt = $button.text();
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $parent.next().removeAttr("hidden");
        $button.text(txt + " - Hide Content");
    } else {
        $button.attr('value', 'show');
        $parent.next().attr("hidden", "");
        $button.text(txt.replace(" - Hide Content",""));
    }
}
</script>

<!-- Print / Printing / printme -->
<!-- <script type="text/javascript">
i = 0

for (var i = 1; i < 6; i++) {
    var bodyContents = $(".bodyContents" + i);
    $("<p>").addClass("TOC1ul")  .appendTo(".TOC1");
    bodyContents.each(function(index, element) {
        var paragraph = $(element);
        $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC1ul");
         });
} 
</script>
 -->
 
</html>

