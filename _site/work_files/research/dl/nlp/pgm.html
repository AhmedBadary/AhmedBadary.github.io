<!DOCTYPE html>
<html lang="en-us">
  <head>
  <meta charset="UTF-8">
  <title>Ahmad Badary</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <link rel="stylesheet" href="/css/normalize.css">
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="/css/cayman.css">
  <link rel="stylesheet" href="/css/style.css">
  <link rel="shortcut icon" href="/main_files/favicon.ico" />
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="/css/customStyle.css">
  <title> » Ahmad Badary</title>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
</head>

  <body>
    <nav class="main-nav">
    <a href="https://ahmedbadary.github.io/" class="main-nav-logo">
        <img src="/main_files/logo.png">
    </a>
    <ul id="menu-main" class="main-nav-items">
        <li id="menu-item-1859" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-home menu-item-1859">
            <a href="/">Home</a>
        </li>
        <li id="menu-item-2869" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-2869">
            <a href="/work">Work</a>
        </li>
        <li id="menu-item-1892" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1892">
            <a href="/projects">Projects</a>
        </li>
        <li id="menu-item-1858" class="menu-item menu-item-type-post_type menu-item-object-page current_page_parent menu-item-1858">
            <a href="/blog">Blog</a>
        </li>
        <li id="menu-item-1862" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1862">
            <a href="/about">About</a>
        </li>
    </ul>
</nav>


<section class="page-header">
  <h1 class="project-name">ASR <br /> Automatic Speech Recognition</h1>
  <h2 class="project-tagline"></h2>
  <a href="/#" class="btn">Home</a>
  <a href="/work" class="btn">Work-Space</a>
  <a href= /work_files/research/dl/nlp.html class="btn">Previous</a>
</section>

<!-- <div>
  <ul class="posts">
    
      <li><span>02 Jan 2014</span> &raquo; <a href="/2014/01/02/introducing-Ahmad/">Introducing Ahmad</a></li>
    
  </ul>
</div> -->


    <section class="main-content">
      
      <div class="TOC">
  <h1 id="table-of-contents">Table of Contents</h1>

  <ul class="TOC1">
    <li><a href="#content1">FIRST</a></li>
  </ul>
  <ul class="TOC2">
    <li><a href="#content2">SECOND</a></li>
  </ul>
  <ul class="TOC3">
    <li><a href="#content3">THIRD</a></li>
  </ul>
</div>

<hr />
<hr />

<h2 id="content1">FIRST</h2>

<ol>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents11">Uncertainty in General Systems and the need for a Probabilistic Framework:</strong></dt>
      <dd>
        <ol>
          <li><strong>Inherent stochasticity in the system being modeled:</strong><br />
 Take Quantum Mechanics, most interpretations of quantum mechanics describe the dynamics of sub-atomic particles as being probabilistic.</li>
          <li><strong>Incomplete observability</strong>:<br />
 Deterministic systems can appear stochastic even when we cannot observe all the variables that drive the behavior of the system.
            <blockquote>
              <p>i.e. Point-of-View determinism (Monty-Hall)</p>
            </blockquote>
          </li>
          <li><strong>Incomplete modeling</strong>:<br />
 Building a system that makes strong assumptions about the problem and discards (observed) information result in uncertainty in the predictions.</li>
        </ol>
      </dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents12">Bayesian Probabilities and Frequentist Probabilities:</strong></dt>
      <dd><strong>Frequentist Probabilities</strong> describe the predicted number of times that a <strong>repeatable</strong> process will result in a given output in an absolute scale.<br />
<strong>Bayesian Probabilities</strong> describe the <em>degree of belief</em> that a certain <strong>non-repeatable</strong> event is going to result in a given output, in an absolute scale.</dd>
      <dd>We assume that <strong>Bayesian Probabilities</strong> behaves in exactly the same way as <strong>Frequentist Probabilities</strong>.<br />
This assumption is derived from a set of <em>“common sense”</em> arguments that end in the logical conclusion that both approaches to probabilities must behave the same way - <a href="https://socialsciences.mcmaster.ca/econ/ugcm/3ll3/ramseyfp/ramsess.pdf">Truth and probability (Ramsey 1926)</a>.</dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents13">Probability as an extension of Logic:</strong></dt>
      <dd>“Probability can be seen as the extension of logic to deal with uncertainty. Logic provides a set of formal rules for determining what propositions are implied to be true or false given the assumption that some other set of propositions is true or false. Probability theory provides a set of formal rules for determining the likelihood of a proposition being true given the likelihood of other propositions.” - deeplearningbook p.54</dd>
    </dl>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents14">Asynchronous:</strong></p>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents15">Asynchronous:</strong></dt>
      <dd></dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents16">Asynchronous:</strong></dt>
      <dd></dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents17">Asynchronous:</strong></dt>
      <dd></dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents18">Asynchronous:</strong></dt>
      <dd></dd>
    </dl>
  </li>
</ol>

<hr />

<h2 id="content2">SECOND</h2>

<ol>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyxContents2" id="bodyContents21">Random Variables:</strong></dt>
      <dd>A <strong>Random Variable</strong> is a variable that can take on different values randomly.<br />
Precisely, it is a <em>function</em> that maps outcomes to numerical quantities (labels), typically real numbers.</dd>
      <dd>
        <ul>
          <li><strong>Types</strong>:
            <ul>
              <li><em><strong>Discrete</strong></em>: is a variable that has a finite or countably infinite number of states</li>
              <li><em><strong>Continuous</strong></em>: is a variable that is a real value</li>
            </ul>
          </li>
        </ul>
      </dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents22">Probability Distributions:</strong></dt>
      <dd>A <strong>Probability Distribution</strong> is a function that describes the likelihood that a random variable (or a set of r.v.) will take on each of its possible states.<br />
Probability Distributions are defined in terms of the <strong>Sample Space</strong>.</dd>
      <dd>
        <ul>
          <li><strong>Classes</strong>:
            <ul>
              <li><em><strong>Discrete Probability Distribution</strong></em>: is encoded by a discrete list of the probabilities of the outcomes, known as a <strong>Probability Mass Function (PMF)</strong>.</li>
              <li><em><strong>Continuous Probability Distribution</strong></em>: is described by a <strong>Probability Density Function (PDF)</strong>.</li>
            </ul>
          </li>
        </ul>
      </dd>
      <dd>
        <ul>
          <li><strong>Types</strong>:
            <ul>
              <li><em><strong>Univariate Distributions</strong></em>: are those whose sample space is <script type="math/tex">\mathrm{R}</script>.<br />
  They give the probabilities of a single random variable taking on various alternative values</li>
              <li><em><strong>Multivariate Distributions</strong></em> (also known as <em><strong>Joint Probability distributions</strong></em>):  are those whose sample space is a vector space. <br />
  They give the probabilities of a random vector taking on various combinations of values.</li>
            </ul>
          </li>
        </ul>
      </dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents23">Probability Mass Function:</strong></dt>
      <dd>A <strong>Probability Mass Function (PMF)</strong> is a function (probability distribution) that gives the probability that a discrete random variable is exactly equal to some value.</dd>
      <dd><strong>Mathematical Definition</strong>:<br />
Suppose that <script type="math/tex">X: S \rightarrow A (A {\displaystyle \subseteq }  R)</script> is a discrete random variable defined on a sample space <script type="math/tex">S</script>. Then the probability mass function <script type="math/tex">f_X: A \rightarrow [0, 1]</script> for <script type="math/tex">X</script> is defined as</dd>
      <dd>
        <script type="math/tex; mode=display">f_{X}(x)=\Pr(X=x)=\Pr(\{s\in S:X(s)=x\})</script>
      </dd>
      <dd>The total probability for all hypothetical outcomes <script type="math/tex">x</script> is always conserved:</dd>
      <dd>
        <script type="math/tex; mode=display">\sum _{x\in A}f_{X}(x)=1</script>
      </dd>
      <dd><strong>Joint Probability Distribution</strong> is a PMF over many variables, denoted <script type="math/tex">P(\mathrm{x} = x, \mathrm{y} = y)</script> or <script type="math/tex">P(x, y)</script>.</dd>
      <dd>A <strong>PMF</strong> must satisfy these properties:
        <ul>
          <li>The domain of <script type="math/tex">P</script> must be the set of all possible states of <script type="math/tex">x</script>.</li>
          <li><script type="math/tex">\forall x \in \mathrm{x}, \: 0 \leq P(x) \leq 1</script>. Impossible events has probability <script type="math/tex">0</script>. Guaranteed events have probability <script type="math/tex">1</script>.</li>
          <li><script type="math/tex">\sum_{x \in \mathrm{x}} P(x) = 1</script>, i.e. the PMF must be normalized.</li>
        </ul>
      </dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents24">Probability Density Function:</strong></dt>
      <dd>A <strong>Probability Density Function (PDF)</strong> is a function (probability distribution) whose value at any given sample (or point) in the sample space can be interpreted as providing a relative likelihood that the value of the random variable would equal that sample.</dd>
      <dd>A Probability Density Function <script type="math/tex">p(x)</script> does not give the probability of a specific state directly; instead the probability of landing inside an infinitesimal region with volume <script type="math/tex">\delta x</script> is given by <script type="math/tex">p(x)\delta x</script>.<br />
     We can integrate the density function to find the actual probability mass of a set of points. Specifically, the probability that <script type="math/tex">x</script> lies in some set <script type="math/tex">S</script> is given by the integral of <script type="math/tex">p(x)</script> over that set.<br />
     &gt; In the <strong>Univariate</strong> example, the probability that <script type="math/tex">x</script> lies in the interval <script type="math/tex">[a, b]</script> is given by <script type="math/tex">\int_{[a, b]} p(x)dx</script></dd>
      <dd>A <strong>PDF</strong> must satisfy these properties:
        <ul>
          <li>The domain of <script type="math/tex">P</script> must be the set of all possible states of <script type="math/tex">x</script>.</li>
          <li><script type="math/tex">\forall x \in \mathrm{x}, \: 0 \leq P(x)</script>. Impossible events has probability <script type="math/tex">0</script>. Guaranteed events have probability <script type="math/tex">1</script>.</li>
          <li><script type="math/tex">\int p(x)dx = 1</script>, i.e. the integral of the PDF must be normalized.</li>
        </ul>
      </dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents25">Marginal Probability:</strong></dt>
      <dd>The <strong>Marginal Distribution</strong> of a subset of a collection of random variables is the probability distribution of the variables contained in the subset.</dd>
      <dd><strong>Two-variable Case</strong>:<br />
Given two random variables <script type="math/tex">X</script> and <script type="math/tex">Y</script> whose joint distribution is known, the marginal distribution of <script type="math/tex">X</script> is simply the probability distribution of <script type="math/tex">X</script> averaging over information about <script type="math/tex">Y</script>.</dd>
      <dd>
        <ul>
          <li><strong>Discrete</strong>:</li>
        </ul>
      </dd>
      <dd>
        <script type="math/tex; mode=display">{\displaystyle \Pr(X=x)=\sum _{y}\Pr(X=x,Y=y)=\sum _{y}\Pr(X=x\mid Y=y)\Pr(Y=y)}</script>
      </dd>
      <dd>
        <ul>
          <li><strong>Continuous</strong>:</li>
        </ul>
      </dd>
      <dd>
        <script type="math/tex; mode=display">{\displaystyle p_{X}(x)=\int _{y}p_{X,Y}(x,y)\,\mathrm {d} y=\int _{y}p_{X\mid Y}(x\mid y)\,p_{Y}(y)\,\mathrm {d} y}</script>
      </dd>
      <dd>
        <ul>
          <li><em><strong>Marginal Probability as Expectation</strong></em>:</li>
        </ul>
      </dd>
      <dd>
        <script type="math/tex; mode=display">{\displaystyle p_{X}(x)=\int _{y}p_{X\mid Y}(x\mid y)\,p_{Y}(y)\,\mathrm {d} y=\mathbb {E} _{Y}[p_{X\mid Y}(x\mid y)]}</script>
      </dd>
      <dd><button class="showText" value="show" onclick="showTextPopHide(event);">Intuitive Explanation</button>
 <img src="/main_files/math/prob/1.png" alt="img" width="100%" hidden="" /></dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents26">Conditional Probability:</strong></dt>
      <dd><strong>Conditional Probability</strong> is a measure of the probability of an event given that another event has occurred.<br />
Conditional Probability is only defined when <script type="math/tex">P(x) > 0</script> - We cannot computethe conditional probability conditioned on an event that never happens.</dd>
      <dd><strong>Definition</strong>:</dd>
      <dd>
        <script type="math/tex; mode=display">P(A|B)={\frac {P(A\cap B)}{P(B)}} = {\frac {P(A, B)}{P(B)}}</script>
      </dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents27">The Chain Rule of Conditional Probability:</strong></dt>
      <dd>Any joint probability distribution over many random variables may be decomposed into conditional distributions over only one variable.<br />
The chain rule permits the calculation of any member of the joint distribution of a set of random variables using only conditional probabilities:</dd>
      <dd>
        <script type="math/tex; mode=display">\mathrm {P} \left(\bigcap _{k=1}^{n}A_{k}\right)=\prod _{k=1}^{n}\mathrm {P} \left(A_{k}\,{\Bigg |}\,\bigcap _{j=1}^{k-1}A_{j}\right)</script>
      </dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents28">Independence and Conditional Independence:</strong></dt>
      <dd>Two random variables <script type="math/tex">x</script> and <script type="math/tex">y</script> are <strong>independent</strong> if their probability distribution can be expressed as a product of two factors, one involving only <script type="math/tex">x</script> and one involving only <script type="math/tex">y</script>:</dd>
      <dd>
        <script type="math/tex; mode=display">\mathrm{P}(A \cap B) = \mathrm{P}(A)\mathrm{P}(B)</script>
      </dd>
      <dd>Two random variables <script type="math/tex">x</script> and <script type="math/tex">y</script> are conditionally independent given a random variable <script type="math/tex">z</script> if the conditional probability distribution over <script type="math/tex">x</script> and <script type="math/tex">y</script> factorizes in this way for every value of <script type="math/tex">z</script>:</dd>
      <dd>
        <script type="math/tex; mode=display">\Pr(A\cap B\mid Y)=\Pr(A\mid Y)\Pr(B\mid Y)</script>
      </dd>
      <dd>or equivalently,</dd>
      <dd>
        <script type="math/tex; mode=display">\Pr(A\mid B\cap Y)=\Pr(A\mid Y)</script>
      </dd>
      <dd><strong>Notation:</strong>
        <ul>
          <li><strong><script type="math/tex">x</script> is Independent from <script type="math/tex">y</script></strong>:  <script type="math/tex">x{\perp}y</script></li>
          <li><strong><script type="math/tex">x</script> and <script type="math/tex">y</script> are conditionally Independent given <script type="math/tex">z</script></strong>:  <script type="math/tex">x{\perp}y \:\vert z</script></li>
        </ul>
      </dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents29">Expectation:</strong></dt>
      <dd>The <strong>expectation</strong>, or <strong>expected value</strong>, of some function <script type="math/tex">f(x)</script> with respect to a probability distribution <script type="math/tex">P(x)</script> is the average, or mean value, that <script type="math/tex">f</script> takes on when <script type="math/tex">x</script> is drawn from <script type="math/tex">P</script>.</dd>
      <dd>
        <ul>
          <li><strong>Discrete case</strong>:</li>
        </ul>
      </dd>
      <dd>
        <script type="math/tex; mode=display">{\displaystyle \operatorname {E}_{x \sim P} [f(X)]=x_{1}p_{1}+x_{2}p_{2}+\cdots +x_{k}p_{k}} = \sum_x P(x)f(x)</script>
      </dd>
      <dd>
        <ul>
          <li><strong>Continuous case</strong>:</li>
        </ul>
      </dd>
      <dd>
        <script type="math/tex; mode=display">{\displaystyle \operatorname {E}_{x \sim P} [f(X)] = \int p(x)f(x)dx}</script>
      </dd>
      <dd><strong>Linearity of Expectation:</strong></dd>
      <dd>
        <script type="math/tex; mode=display">% <![CDATA[
{\displaystyle {\begin{aligned}\operatorname {E} [X+Y]&=\operatorname {E} [X]+\operatorname {E} [Y],\\[6pt]\operatorname {E} [aX]&=a\operatorname {E} [X],\end{aligned}}} %]]></script>
      </dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents210">Variance:</strong></dt>
      <dd><strong>Variance</strong> is the expectation of the squared deviation of a random variable from its mean.<br />
It gives a measure of how much the values of a function of a random variable <script type="math/tex">x</script> vary as we sample different values of <script type="math/tex">x</script> from its probability distribution:</dd>
      <dd>
        <script type="math/tex; mode=display">\operatorname {Var} (f(x))=\operatorname {E} \left[(f(x)-\mu )^{2}\right]</script>
      </dd>
      <dd><strong>Variance expanded</strong>:</dd>
      <dd>
        <script type="math/tex; mode=display">% <![CDATA[
{\displaystyle {\begin{aligned}\operatorname {Var} (X)&=\operatorname {E} \left[(X-\operatorname {E} [X])^{2}\right]\\&=\operatorname {E} \left[X^{2}-2X\operatorname {E} [X]+\operatorname {E} [X]^{2}\right]\\&=\operatorname {E} \left[X^{2}\right]-2\operatorname {E} [X]\operatorname {E} [X]+\operatorname {E} [X]^{2}\\&=\operatorname {E} \left[X^{2}\right]-\operatorname {E} [X]^{2}\end{aligned}}} %]]></script>
      </dd>
      <dd><strong>Variance as Covariance</strong>: 
    Variance can be expressed as the covariance of a random variable with itself:</dd>
      <dd>
        <script type="math/tex; mode=display">\operatorname {Var} (X)=\operatorname {Cov} (X,X)</script>
      </dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents211">Standard Deviation:</strong></dt>
      <dd>The <strong>Standard Deviation</strong> is a measure that is used to quantify the amount of variation or dispersion of a set of data values.<br />
It is defined as the square root of the variance:</dd>
      <dd>
        <script type="math/tex; mode=display">% <![CDATA[
{\displaystyle {\begin{aligned}\sigma &={\sqrt {\operatorname {E} [(X-\mu )^{2}]}}\\&={\sqrt {\operatorname {E} [X^{2}]+\operatorname {E} [-2\mu X]+\operatorname {E} [\mu ^{2}]}}\\&={\sqrt {\operatorname {E} [X^{2}]-2\mu \operatorname {E} [X]+\mu ^{2}}}\\&={\sqrt {\operatorname {E} [X^{2}]-2\mu ^{2}+\mu ^{2}}}\\&={\sqrt {\operatorname {E} [X^{2}]-\mu ^{2}}}\\&={\sqrt {\operatorname {E} [X^{2}]-(\operatorname {E} [X])^{2}}}\end{aligned}}} %]]></script>
      </dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents212">Covariance:</strong></dt>
      <dd><strong>Covariance</strong> is a measure of the joint variability of two random variables.<br />
It gives some sense of how much two values are linearly related to each other, as well as the scale of these variables:</dd>
      <dd>
        <script type="math/tex; mode=display">\operatorname {cov} (X,Y)=\operatorname {E} { {\big[ }(X-\operatorname {E} [X])(Y-\operatorname {E} [Y]){ \big] } }</script>
      </dd>
      <dd><strong>Covariance expanded:</strong></dd>
      <dd>
        <script type="math/tex; mode=display">% <![CDATA[
{\displaystyle {\begin{aligned}\operatorname {cov} (X,Y)&=\operatorname {E} \left[\left(X-\operatorname {E} \left[X\right]\right)\left(Y-\operatorname {E} \left[Y\right]\right)\right]\\&=\operatorname {E} \left[XY-X\operatorname {E} \left[Y\right]-\operatorname {E} \left[X\right]Y+\operatorname {E} \left[X\right]\operatorname {E} \left[Y\right]\right]\\&=\operatorname {E} \left[XY\right]-\operatorname {E} \left[X\right]\operatorname {E} \left[Y\right]-\operatorname {E} \left[X\right]\operatorname {E} \left[Y\right]+\operatorname {E} \left[X\right]\operatorname {E} \left[Y\right]\\&=\operatorname {E} \left[XY\right]-\operatorname {E} \left[X\right]\operatorname {E} \left[Y\right].\end{aligned}}} %]]></script>
      </dd>
      <dd>
        <blockquote>
          <p>when <script type="math/tex">{\displaystyle \operatorname {E} [XY]\approx \operatorname {E} [X]\operatorname {E} [Y]}</script>, this last equation is prone to catastrophic cancellation when computed with floating point arithmetic and thus should be avoided in computer programs when the data has not been centered before.</p>
        </blockquote>
      </dd>
      <dd><strong>Covariance of Random Vectors</strong>:</dd>
      <dd>
        <script type="math/tex; mode=display">% <![CDATA[
{\begin{aligned}\operatorname {cov} (\mathbf {X} ,\mathbf {Y} )&=\operatorname {E} \left[(\mathbf {X} -\operatorname {E} [\mathbf {X} ])(\mathbf {Y} -\operatorname {E} [\mathbf {Y} ])^{\mathrm {T} }\right]\\&=\operatorname {E} \left[\mathbf {X} \mathbf {Y} ^{\mathrm {T} }\right]-\operatorname {E} [\mathbf {X} ]\operatorname {E} [\mathbf {Y} ]^{\mathrm {T} },\end{aligned}} %]]></script>
      </dd>
      <dd><strong>The Covariance Matrix</strong> of a random vector <script type="math/tex">x \in \mathbb{R}^n</script> is an <script type="math/tex">n \times n</script> matrix, such that:</dd>
      <dd>
        <script type="math/tex; mode=display">\operatorname {cov} (X)_{i,j} = \operatorname {cov}(x_i, x_j) \\
\operatorname {cov}(x_i, x_j) = \operatorname {Var} (x_i)</script>
      </dd>
      <dd><strong>Interpretations</strong>:
        <ul>
          <li>High absolute values of the covariance mean that the values change very much and are both far from their respective means at the same time.</li>
          <li><strong>The sign of the covariance</strong>: <br />
  The sign of the covariance shows the tendency in the linear relationship between the variables:
            <ul>
              <li><em><strong>Positive</strong></em>:<br />
  the variables tend to show similar behavior</li>
              <li><em><strong>Negative</strong></em>:<br />
  the variables tend to show opposite behavior</li>
              <li><strong>Reason</strong>:<br />
  If the greater values of one variable mainly correspond with the greater values of the other variable, and the same holds for the lesser values, (i.e., the variables tend to show similar behavior), the covariance is positive. In the opposite case, when the greater values of one variable mainly correspond to the lesser values of the other, (i.e., the variables tend to show opposite behavior), the covariance is negative.</li>
            </ul>
          </li>
        </ul>
      </dd>
      <dd><strong>Covariance and Independence:</strong>
        <ul>
          <li>Independence <script type="math/tex">\Rightarrow</script> Zero Covariance</li>
          <li>Zero Covariance <script type="math/tex">\nRightarrow</script> Independence</li>
        </ul>
      </dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents213">Mixtures of Distributions:</strong></dt>
      <dd>It is also common to define probability distributions by combining other simpler probability distributions. One common way of combining distributions is to construct a <strong>mixture distribution</strong>.</dd>
      <dd>A <strong>Mixture Distribution</strong> is the probability distribution of a random variable that is derived from a collection of other random variables as follows: first, a random variable is selected by chance from the collection according to given probabilities of selection, and then the value of the selected random variable is realized.<br />
On each trial, the choice of which component distribution should generate the sample is determined by sampling a component identity from a multinoulli distribution:</dd>
      <dd>
        <script type="math/tex; mode=display">P(x) = \sum_i P(x=i)P(x \vert c=i)</script>
      </dd>
      <dd>where <script type="math/tex">P(c)</script> is the multinoulli distribution over component identities.</dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents214">Bayes’ Rule:</strong></dt>
      <dd><strong>Bayes’ Rule</strong> describes the probability of an event, based on prior knowledge of conditions that might be related to the event.</dd>
      <dd>
        <script type="math/tex; mode=display">{\displaystyle P(A\mid B)={\frac {P(B\mid A)\,P(A)}{P(B)}}}</script>
      </dd>
      <dd>where,</dd>
      <dd>
        <script type="math/tex; mode=display">P(B) =\sum_A P(B \vert A) P(A)</script>
      </dd>
    </dl>
  </li>
</ol>

<hr />

<h2 id="content9">Discrete Distributions</h2>

<ol>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents9" id="bodyContents91">Uniform Distribution:</strong></dt>
      <dd></dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents9" id="bodyContents92">Asynchronous:</strong></dt>
      <dd></dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents9" id="bodyContents93">Asynchronous:</strong></dt>
      <dd></dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents9" id="bodyContents94">Asynchronous:</strong></dt>
      <dd></dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents9" id="bodyContents95">Asynchronous:</strong></dt>
      <dd></dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents9" id="bodyContents96">Asynchronous:</strong></dt>
      <dd></dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents9" id="bodyContents97">Asynchronous:</strong></dt>
      <dd></dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents9" id="bodyContents98">Asynchronous:</strong></dt>
      <dd></dd>
    </dl>
  </li>
</ol>

<hr />

<h2 id="content99">Continuous Distributions</h2>

<ol>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents99" id="bodyContents991">Asynchronous:</strong></dt>
      <dd></dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents99" id="bodyContents992">Asynchronous:</strong></dt>
      <dd></dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents99" id="bodyContents993">Asynchronous:</strong></dt>
      <dd></dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents99" id="bodyContents994">Asynchronous:</strong></dt>
      <dd></dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents99" id="bodyContents995">Asynchronous:</strong></dt>
      <dd></dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents99" id="bodyContents996">Asynchronous:</strong></dt>
      <dd></dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents99" id="bodyContents997">Asynchronous:</strong></dt>
      <dd></dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents99" id="bodyContents998">Asynchronous:</strong></dt>
      <dd>
        <hr />
      </dd>
    </dl>
  </li>
</ol>

<h2 id="content10">Tips and Tricks</h2>

<ul>
  <li>It is more practical to use a simple but uncertain rule rather than a complex but certain one, even if the true rule is deterministic and our modeling system has the ﬁdelity to accommodate a complex rule.<br />
  For example, the simple rule “Most birds ﬂy” is cheap to develop and is broadly useful, while a rule of the form, “Birds ﬂy, except for very young birds that have not yet learned to ﬂy, sick or injured birds that have lost the ability to ﬂy, ﬂightless species of birds including the cassowary, ostrich and kiwi. . .” is expensive to develop, maintain and communicate and, after all this effort, is still brittle and prone to failure.</li>
</ul>



      <footer class="site-footer">
    <!--   <span class="site-footer-owner"><a href="http://localhost:8889">Ahmad Badary</a> is maintained by <a href="http://ahmedbadary.ml">Ahmad Badary</a>.</span> -->
    
<!--  -->
    <div class="wrapper">
        <div class="footer-col-wrapper">
            <div class="footer-col footer-col-1">
            <span class="site-footer-owner"><a href="http://localhost:8889">Site</a> maintained by <a href="http://ahmedbadary.ml">Ahmad Badary</a>.</span>
    <span class="site-footer-credits">
        <p>
            &copy; 2017. All rights reserved.
        </p> 
    </span>
            </div>
            <div class="footer-col footer-col-2">
            <div><p>         </p></div>
            </div>
            <div class="footer-col footer-col-3">
                <ul class="social-media-list">
                    
                      <li>
                        <a href="https://github.com/AhmedBadary">
                          <i class="fa fa-github"></i> GitHub
                        </a>
                      </li>
                    
                    
                      <li>
                        <a href="https://linkedin.com/in/ahmad-badary-656098121/">
                          <i class="fa fa-linkedin"></i> LinkedIn
                        </a>
                      </li>
                    
                    
                      <li>
                        <a href="https://www.facebook.com/ahmed.thabet.94">
                          <i class="fa fa-facebook"></i> Facebook
                        </a>
                      </li>
                    
                </ul>
            </div>
        </div>
    </div>
<!--  -->
</footer>


    </section>

  </body>

<!-- Table of Content Script -->
<script type="text/javascript">
var bodyContents = $(".bodyContents1");
$("<ol>").addClass("TOC1ul").appendTo(".TOC1");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC1ul");
     });
// 
var bodyContents = $(".bodyContents2");
$("<ol>").addClass("TOC2ul").appendTo(".TOC2");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC2ul");
     });
// 
var bodyContents = $(".bodyContents3");
$("<ol>").addClass("TOC3ul").appendTo(".TOC3");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC3ul");
     });
//
var bodyContents = $(".bodyContents4");
$("<ol>").addClass("TOC4ul").appendTo(".TOC4");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC4ul");
     });
//
var bodyContents = $(".bodyContents5");
$("<ol>").addClass("TOC5ul").appendTo(".TOC5");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC5ul");
     });
//
var bodyContents = $(".bodyContents6");
$("<ol>").addClass("TOC6ul").appendTo(".TOC6");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC6ul");
     });
//
var bodyContents = $(".bodyContents7");
$("<ol>").addClass("TOC7ul").appendTo(".TOC7");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC7ul");
     });
//
var bodyContents = $(".bodyContents8");
$("<ol>").addClass("TOC8ul").appendTo(".TOC8");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC8ul");
     });
//
var bodyContents = $(".bodyContents9");
$("<ol>").addClass("TOC9ul").appendTo(".TOC9");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC9ul");
     });

</script>

<!-- VIDEO BUTTONS SCRIPT -->
<script type="text/javascript">
  function iframePopInject(event) {
    var $button = $(event.target);
    // console.log($button.parent().next());
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $figure = $("<div>").addClass("video_container");
        $iframe = $("<iframe>").appendTo($figure);
        $iframe.attr("src", $button.attr("src"));
        // $iframe.attr("frameborder", "0");
        $iframe.attr("allowfullscreen", "true");
        $iframe.css("padding", "4px 6px");
        $button.next().css("display", "block");
        $figure.appendTo($button.next());
        $button.text("Hide Video")
    } else {
        $button.attr('value', 'show');
        $button.next().html("");
        $button.text("Show Video")
    }
}
</script>

<!-- BUTTON TRY -->
<script type="text/javascript">
  function iframePopA(event) {
    event.preventDefault();
    var $a = $(event.target).parent();
    console.log($a);
    if ($a.attr('value') == 'show') {
        $a.attr('value', 'hide');
        $figure = $("<div>");
        $iframe = $("<iframe>").addClass("popup_website_container").appendTo($figure);
        $iframe.attr("src", $a.attr("href"));
        $iframe.attr("frameborder", "1");
        $iframe.attr("allowfullscreen", "true");
        $iframe.css("padding", "4px 6px");
        $a.next().css("display", "block");
        $figure.appendTo($a.next().next());
        // $a.text("Hide Content")
        $('html, body').animate({
            scrollTop: $a.offset().top
        }, 1000);
    } else {
        $a.attr('value', 'show');
        $a.next().next().html("");
        // $a.text("Show Content")
    }

    $a.next().css("display", "inline");
}
</script>


<!-- TEXT BUTTON SCRIPT - INJECT -->
<script type="text/javascript">
  function showTextPopInject(event) {
    var $button = $(event.target);
    var txt = $button.attr("input");
    console.log(txt);
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $p = $("<p>");
        $p.html(txt);
        $button.next().css("display", "block");
        $p.appendTo($button.next());
        $button.text("Hide Content")
    } else {
        $button.attr('value', 'show');
        $button.next().html("");
        $button.text("Show Content")
    }

}
</script>

<!-- TEXT BUTTON SCRIPT - HIDDEN / HIDE / SHOW / HIDE/SHOW -->
<script type="text/javascript">
  function showTextPopHide(event) {
    var $button = $(event.target);
    var txt = $button.attr("input");
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $button.next().removeAttr("hidden");
        $button.text("Hide Content");
    } else {
        $button.attr('value', 'show');
        $button.next().attr("hidden", "");
        $button.text("Show Content");
    }
}
</script>

<!-- TEXT BUTTON SCRIPT - HIDDEN / HIDE / SHOW / HIDE/SHOW -->
<script type="text/javascript">
  function showText_withParent_PopHide(event) {
    var $button = $(event.target);
    var $parent = $button.parent();
    var txt = $button.attr("input");
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $parent.next().removeAttr("hidden");
        $button.text("Hide Content");
    } else {
        $button.attr('value', 'show');
        $parent.next().attr("hidden", "");
        $button.text("Show Content");
    }
}
</script>

<!-- Print / Printing / printme -->
<!-- <script type="text/javascript">
i = 0

for (var i = 1; i < 6; i++) {
    var bodyContents = $(".bodyContents" + i);
    $("<p>").addClass("TOC1ul")  .appendTo(".TOC1");
    bodyContents.each(function(index, element) {
        var paragraph = $(element);
        $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC1ul");
         });
} 
</script>
 -->
 
</html>

