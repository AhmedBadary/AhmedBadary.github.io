<!DOCTYPE html>
<html lang="en-us">
  <head>
  <meta charset="UTF-8">
  <title>Ahmad Badary</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <link rel="stylesheet" href="/css/normalize.css">
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="/css/cayman.css">
  <link rel="stylesheet" href="/css/style.css">
  <link rel="shortcut icon" href="/main_files/favicon.ico" />
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="/css/customStyle.css">
  <title> » Ahmad Badary</title>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
</head>

  <body>
    <nav class="main-nav">
    <a href="https://ahmedbadary.github.io/" class="main-nav-logo">
        <img src="/main_files/logo.png">
    </a>
    <ul id="menu-main" class="main-nav-items">
        <li id="menu-item-1859" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-home menu-item-1859">
            <a href="/">Home</a>
        </li>
        <li id="menu-item-2869" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-2869">
            <a href="/work">Work</a>
        </li>
        <li id="menu-item-1892" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1892">
            <a href="/projects">Projects</a>
        </li>
        <li id="menu-item-1858" class="menu-item menu-item-type-post_type menu-item-object-page current_page_parent menu-item-1858">
            <a href="/blog">Blog</a>
        </li>
        <li id="menu-item-1862" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1862">
            <a href="/about">About</a>
        </li>
    </ul>
</nav>


<section class="page-header">
  <h1 class="project-name">Contextual Word Representations and Pretraining</h1>
  <h2 class="project-tagline"></h2>
  <a href="/#" class="btn">Home</a>
  <a href="/work" class="btn">Work-Space</a>
  <a href= /work_files/research/dl/nlp.html class="btn">Previous</a>
</section>

<!-- <div>
  <ul class="posts">
    
      <li><span>02 Jan 2014</span> &raquo; <a href="/2014/01/02/introducing-Ahmad/">Introducing Ahmad</a></li>
    
  </ul>
</div> -->


    <section class="main-content">
      
      <div class="TOC">
  <h1 id="table-of-contents">Table of Contents</h1>

  <ul class="TOC1">
    <li><a href="#content1">Word Representations and their progress</a></li>
  </ul>
  <ul class="TOC2">
    <li><a href="#content2">Transformers</a></li>
  </ul>
  <!--   * [THIRD](#content3)
  {: .TOC3} -->
</div>

<hr />
<hr />

<ul>
  <li><a href="https://arxiv.org/pdf/1902.06006.pdf">Paper on Contextual Word Representations</a></li>
  <li><a href="https://lilianweng.github.io/lil-log/2020/04/07/the-transformer-family.html">The Transformer Family (Blog)</a></li>
</ul>

<h2 id="content1">Word Representations and their progress</h2>

<p id="lst-p"><strong style="color: red">Summary of Progress:</strong></p>
<ul>
  <li><strong>2011-13s</strong>: Learning Unsupervised Representations for words (Pre-Trained Word Vectors) is crucial for making Supervised Learning work (e.g. for NERs, POS-Tagging, etc.)</li>
  <li><strong>2014-18s</strong>: Pre-Trained Word Vectors are not actually necessary for <em>good performance</em> of supervised methods.
    <ul>
      <li>The reason is due to advances in training supervised methods: <strong>regularization</strong>, <strong>non-linearities</strong>, etc.</li>
      <li>They can boost the performance by ~ \(1\%\) on average. <br />
<br /></li>
    </ul>
  </li>
</ul>

<ol>
  <li><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents11">Word Representations (Accepted Methods):</strong><br />
 The current accepted methods provide one representation of words:
    <ol>
      <li><strong>Word2Vec</strong></li>
      <li><strong>GloVe</strong></li>
      <li><strong>FastText</strong></li>
    </ol>

    <p><button class="showText" value="show" onclick="showTextPopHide(event);">Early Day Results</button>
 <img src="/main_files/dl/nlp/ctxt_word_repr/2.png" alt="img" width="80%" hidden="" /></p>

    <p id="lst-p"><strong>Problems:</strong></p>
    <ul>
      <li><strong>Word Senses</strong>: Always the same representation for a <strong>word type</strong> regardless of the context in which a <strong>word token</strong> occurs
        <ul>
          <li>We might want very fine-grained word sense disambiguation (e.g. not just ‘holywood star’ and ‘astronomical star’; but also ‘rock star’, ‘star student’ etc.)</li>
        </ul>
      </li>
      <li>We just have <strong>one representation</strong> for a word, but words have <strong>different aspects</strong>: including <strong>semantics</strong>, <strong>syntactic behavior</strong>, and <strong>register/connotations</strong> (e.g. when is it appropriate to use ‘bathroom’ vs ‘shithole’ etc.; ‘can’-noun vs ‘can’-verb have same vector)</li>
    </ul>

    <p id="lst-p"><strong>Possible Solution (that we always had?):</strong></p>
    <ul>
      <li>In a NLM, LSTM layers are trained to predict the next word, producing hidden/state vectors, that are basically <strong>context-specific</strong> word representations, at each position<br />
  <button class="showText" value="show" onclick="showTextPopHide(event);">LSTM Representations</button>
  <img src="https://cdn.mathpix.com/snip/images/wWkGdRomPB3JlxbnRWCNrAX8nfHerQyAymnxmTU6--Q.original.fullsize.png" alt="img" width="100%" hidden="" /><br />
 <br /></li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents12">TagLM (Peters et al. 2017) — Pre-Elmo:</strong><br />
 <strong>Idea:</strong>
    <ul>
      <li>Want meaning of word in context, but standardly learn task RNN only on small task-labeled data (e.g. NER).</li>
      <li>Do <strong>semi-supervised</strong> approach where we train NLM on large unlabeled corpus, rather than just word vectors.</li>
      <li>Run a BiRNN-LM and concatenate the For and Back representations</li>
      <li>Also, train a traditional word-embedding (w2v) on the word and concatenate with Bi-LM repr.</li>
      <li>Also, train a Char-CNN/RNN to get character level embedding and concatenate all of them together</li>
    </ul>

    <p id="lst-p"><strong>Details:</strong></p>
    <ul>
      <li>Language model is trained on 800 million training words of “Billion word benchmark”</li>
      <li><strong>Language model observations</strong>:
        <ul>
          <li>An LM trained on supervised data does not help</li>
          <li>Having a bidirectional LM helps over only forward, by about 0.2</li>
          <li>Having a huge LM design (ppl 30) helps over a smaller model (ppl 48) by about 0.3</li>
        </ul>
      </li>
      <li><strong>Task-specific BiLSTM observations</strong>:
        <ul>
          <li>Using just the LM embeddings to predict isn’t great: 88.17 F1
            <ul>
              <li>Well below just using an BiLSTM tagger on labeled data</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>

    <p><button class="showText" value="show" onclick="showTextPopHide(event);">Tag LM Detailed</button>
 <img src="/main_files/dl/nlp/ctxt_word_repr/3.png" alt="img" width="100%" hidden="" /><br />
 <button class="showText" value="show" onclick="showTextPopHide(event);">TagLM step-by-step Overview</button>
 <img src="https://cdn.mathpix.com/snip/images/LZid2lHBZj9P1EQAOhHzDYxk6o_I0kh-gtfIBDbI6Rg.original.fullsize.png" alt="img" width="100%" hidden="" /><br />
 <br /></p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents13">Cove - Pre-Elmo:</strong>
    <ul>
      <li>Also has idea of using a trained sequence model to provide context to other NLP models</li>
      <li>Idea: Machine translation is meant to preserve meaning, so maybe that’s a good objective?</li>
      <li>Use a 2-layer bi-LSTM that is the encoder of seq2seq + attention NMT system as the context provider</li>
      <li>The resulting CoVe vectors do outperform GloVe vectors on various tasks</li>
      <li>But, the results aren’t as strong as the simpler NLM training described in the rest of these slides so seems abandoned
        <ul>
          <li>Maybe NMT is just harder than language modeling?</li>
          <li>Maybe someday this idea will return?</li>
        </ul>
      </li>
    </ul>

    <p><br /></p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents14">Elmo - Embeddings from Language Models (Peters et al. 2018):</strong><br />
 <strong>Idea:</strong>
    <ul>
      <li>Train a bidirectional LM</li>
      <li>Aim at performant but not overly large LM:
        <ul>
          <li>Use 2 biLSTM layers</li>
          <li>Use character CNN to build initial word representation (only)
            <ul>
              <li>2048 char n-gram filters and 2 highway layers, 512 dim projection</li>
            </ul>
          </li>
          <li>Use 4096 dim hidden/cell LSTM states with 512 dim projections to next input</li>
          <li>Use a residual connection</li>
          <li>Tie parameters of token input and output (softmax) and tie these between forward and backward LMs</li>
        </ul>
      </li>
    </ul>

    <p id="lst-p"><strong>Key Results:</strong></p>
    <ul>
      <li>ELMo learns task-specific combination of BiLM representations</li>
      <li>This is an innovation that improves on just using top layer of LSTM stack</li>
    </ul>
    <p>$$\begin{aligned} R_{k} &amp;=\left\{\mathbf{x}_{k}^{L M}, \overrightarrow{\mathbf{h}}_{k, j}^{L M}, \mathbf{h}_{k, j}^{L M} | j=1, \ldots, L\right\} \\ &amp;=\left\{\mathbf{h}_{k, j}^{L M} | j=0, \ldots, L\right\} \end{aligned}$$</p>
    <p>$$\mathbf{E} \mathbf{L} \mathbf{M} \mathbf{o}_{k}^{t a s k}=E\left(R_{k} ; \Theta^{t a s k}\right)=\gamma^{task} \sum_{j=0}^{L} s_{j}^{task} \mathbf{h}_ {k, j}^{L M}$$</p>
    <ul>
      <li>\(\gamma^{\text { task }}\) scales overall usefulness of ELMo to task;</li>
      <li>\(s^{\text { task }}\) are softmax-normalized mixture model weights
        <blockquote>
          <p>Possibly this is a way of saying different semantic and syntactic meanings of a word are represented in different layers; and by doing a weighted average of those, in a task-specific manner, we can leverage the appropriate kind of information for each task.</p>
        </blockquote>
      </li>
    </ul>

    <p id="lst-p"><strong>Using ELMo with a Task:</strong></p>
    <ul>
      <li>First run biLM to get representations for each word</li>
      <li>Then let (whatever) end-task model use them
        <ul>
          <li>Freeze weights of ELMo for purposes of supervised model</li>
          <li>Concatenate ELMo weights into task-specific model
            <ul>
              <li>Details depend on task
                <ul>
                  <li>Concatenating into intermediate layer as for TagLM is typical</li>
                  <li>Can provide ELMo representations again when producing outputs, as in a question answering system</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>

    <p id="lst-p"><strong>Weighting of Layers:</strong></p>
    <ul>
      <li>The two Bi-LSTM NLP Layers have differentiated uses/meanings
        <ul>
          <li>Lower layer is better for lower-level syntax, etc.
            <ul>
              <li>POS-Tagging, Syntactic Dependencies, NER</li>
            </ul>
          </li>
          <li>Higher layer is better for higher-level semantics
            <ul>
              <li>Sentiment, Semantic role labeling, QA, SNLI</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>

    <p><strong>Reason for Excitement:</strong><br />
 ELMo proved to be great for <strong>all NLP tasks</strong> (even tho the core of the idea was in <em>TagLM</em>)</p>
    <ul>
      <li><button class="showText" value="show" onclick="showTextPopHide(event);">ELMo Results</button>
  <img src="/main_files/dl/nlp/ctxt_word_repr/5.png" alt="img" width="70%" hidden="" /><br />
 <br /></li>
    </ul>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents15">ULMfit - Universal Language Model Fine-Tuning (Howard and Ruder 2018):</strong><br />
 ULMfit - Universal Language Model Fine-Tuning for Text Classification:<br />
 <img src="/main_files/dl/nlp/ctxt_word_repr/4.png" alt="img" width="90%" /></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents16">BERT (Devlin et al. 2018):</strong><br />
 BERT - Bidirectional Encoder Representations from Transformers:<br />
 <strong>Idea:</strong> Pre-training of Deep Bidirectional Transformers for Language Understanding.</p>

    <p id="lst-p"><strong>Model Architecture:</strong></p>
    <ul>
      <li>Transformer Encoder</li>
      <li>Self-attention –&gt; no locality bias
        <ul>
          <li>Long-distance context has “equal opportunity”</li>
        </ul>
      </li>
      <li>Single multiplication per layer –&gt; efficiency on GPU/TPU</li>
      <li><strong>Architectures</strong>:
        <ul>
          <li><strong>BERT-Base</strong>: 12 layer, 768-hidden, 12-head</li>
          <li><strong>BERT-Large</strong>: 24 layer, 1024 hudden, 16 heads</li>
        </ul>
      </li>
    </ul>

    <p id="lst-p"><strong>Model Training:</strong></p>
    <ul>
      <li>Train on Wikipedia + BookCorpus</li>
      <li>Train 2 model sizes:
        <ul>
          <li><strong>BERT-Base</strong></li>
          <li><strong>BERT-Large</strong></li>
        </ul>
      </li>
      <li>Trained on \(4\times 4\)  or \(8\times 8\) TPU slice for 4 days</li>
    </ul>

    <p id="lst-p"><strong>Model Fine-Tuning:</strong></p>
    <ul>
      <li>Simply learn a classifier built on top layer for each task that you fine-tune for.</li>
    </ul>

    <p id="lst-p"><strong>Problem with Unidirectional and Bidirectional LMs:</strong></p>
    <ul>
      <li><strong>Uni:</strong> build representation incrementally; not enough context from the sentence</li>
      <li><strong>Bi:</strong>  Cross-Talk; words can “see themselves”</li>
    </ul>

    <p id="lst-p"><strong>Solution:</strong></p>
    <ul>
      <li>Mask out \(k\%\) of the input words, and then predict the masked words
        <ul>
          <li>They always use \(k=15%\)
            <blockquote>
              <p>Ex: “The man went to the <em>[MASK]</em> to buy a <em>[MASK]</em> of milk.”</p>
            </blockquote>
          </li>
          <li><strong>Too little Masking</strong>:  Too Expensive to train</li>
          <li><strong>Too much Masking</strong>:  Not enough context</li>
        </ul>
      </li>
      <li><strong>Other Benefits</strong>:
        <ul>
          <li>In ELMo, bidirectional training is done independently for each direction and then concatenated. No joint-context in the model during the building of contextual-reprs.</li>
          <li>In GPT, there is only unidirectional context.</li>
        </ul>
      </li>
    </ul>

    <p><strong>Another Objective - Next Sentence Prediction:</strong><br />
 To learn <em>relationships</em> between sentences, predict whether sentence B is actual sentence that proceeeds sentence A, or a random sentence (for QA, NLU, etc.).</p>

    <p><strong>Results:</strong><br />
 Beats every other architecture in every <strong>GLUE</strong> task (NL-Inference).</p>

    <ul>
      <li><a href="http://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/">BERT Word Embeddings Tutorial</a></li>
    </ul>
  </li>
</ol>

<p id="lst-p"><strong style="color: red">Notes:</strong></p>
<ul>
  <li><strong>Tips for unknown words with word vectors:</strong>
    <ul>
      <li>Simplest and common solution:</li>
      <li>Train time: Vocab is \(\{\text { words occurring, say, } \geq 5 \text { times }\} \cup\{&lt;UNK&gt;\}\)</li>
      <li>Map <strong>all</strong> rarer \((&lt;5)\) words to \(&lt;UNK&gt;\), train a word vector for it</li>
      <li>
        <p>Runtime: use \(&lt;UNK&gt;\) when out-of-vocabulary (OOV) words occur</p>
      </li>
      <li><strong>Problems:</strong>
        <ul>
          <li>No way to distinguish different \(UNK\) words, either for identity or meaning</li>
        </ul>
      </li>
      <li><strong>Solutions:</strong>
        <ol>
          <li>Hey, we just learned about char-level models to build vectors! Let’s do that!
            <ul>
              <li>Especially in applications like question answering
                <ul>
                  <li>Where it is important to match on word identity, even for words outside your word vector vocabulary</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>
            <p>Try these tips (from Dhingra, Liu, Salakhutdinov, Cohen 2017)<br />
 a. If the <UNK> word at test time appears in your unsupervised word embeddings, use that vector as is at test time.
 b. Additionally, for other words, just assign them a random vector, adding them to your vocabulary</UNK></p>

            <p>a. definitely helps a lot; b. may help a little more</p>
          </li>
          <li>Another thing you can try:
            <ul>
              <li>Collapsing things to word classes (like unknown number, capitalized thing, etc. and having an <UNK-class> for each</UNK-class></li>
            </ul>
          </li>
        </ol>
      </li>
    </ul>
  </li>
</ul>

<!-- 7. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents17}

8. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents1 #bodyContents18}
 -->

<hr />

<h2 id="content2">The Transformer</h2>

<ol>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents21">Self-Attention:</strong><br />
 <strong>Computational Complexity Comparison</strong>:<br />
 <img src="/main_files/dl/nlp/ctxt_word_repr/1.png" alt="img" width="70%" /><br />
 It is favorable when the <strong>sequence-length</strong> \(&lt;&lt;\) <strong>dimension-representations</strong>.</p>

    <p id="lst-p"><strong>Self-Attention/Relative-Attention Interpretations</strong>:</p>
    <ul>
      <li>Can achieve <strong>Translational Equivariance</strong> (like convs) (by removing pos-encoding).</li>
      <li>Can model <strong>similarity graphs</strong>.</li>
      <li>Connected to <strong>message-passing NNs</strong>: Can think of self-attention as <em>passing messages between pairs of nodes in graph</em>; equivalently, <em>imposing a complete bipartite graph</em> and you’re passing messages between nodes.<br />
  Mathematically, the difference is message-passing NNs impose condition that messages pass ONLY bet pairs of nodes; while self-attention uses softmax and thus passes messages between all nodes.</li>
    </ul>

    <p id="lst-p"><strong>Self-Attention Summary/Properties</strong>:</p>
    <ul>
      <li><span style="color: purple"><strong>Constant</strong> path-length</span> between any two positions</li>
      <li>Unbounded memory (i.e no fixed size h-state)</li>
      <li>Gating/multiplicative interactions<br />
  Because you multiply attention probabilities w/ activations. PixelCNN needed those interactions too.</li>
      <li>Trivial to parallelize (per layer): just matmuls</li>
      <li>Models <strong>self-similarity</strong></li>
      <li>Relative attention provides <strong>expressive timing</strong>, <strong>equivariance</strong>, and extends naturally to graphs</li>
      <li>(Without <strong>positional encoding</strong>) It’s <span style="color: purple">Permutation-Invariant and Translation-Equivariant</span><br />
  It can learn to <strong>copy</strong> well.</li>
    </ul>

    <p id="lst-p"><strong>Current Issues</strong>:</p>
    <ul>
      <li><strong>Slow Generation</strong>:<br />
  Mainly due to <strong>Auto-Regressive</strong> generation, which is necessary to break the multi-modality of generation. Multi-modality prohibits naive parallel generation.<br />
  Multi-modality refers to the fact that there are multiple different sentences in german that are considered a correct translation of a sentence in english, and they all depend on the word that was generated first (ie no parallelization).</li>
      <li><strong>Active Area of Research</strong>:<br />
  <span style="color: purple"><strong>Non Auto-Regressive Transformers</strong></span>.
        <ul>
          <li>Papers:
            <ul>
              <li><em>Non autoregressive transformer (Gu and Bradbury et al., 2018)</em></li>
              <li><em>Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement (Lee, Manismov, and Cho, 2018)</em></li>
              <li><em>Fast Decoding in Sequence Models Using Discrete Latent Variables (ICML 2018) Kaiser, Roy, Vaswani, Pamar, Bengio, Uszkoreit, Shazeer</em></li>
              <li><em>Towards a Better Understanding of Vector Quantized Autoencoders Roy, Vaswani, Parmar, Neelakantan, 2018</em></li>
              <li><em>Blockwise Parallel Decoding For Deep Autogressive Models (NeurIPS 2019) Stern, Shazeer, Uszkoreit</em></li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>

    <p id="lst-p"><strong style="color: red">Notes:</strong></p>
    <ul>
      <li><strong>Self-Similarity:</strong>
        <ul>
          <li>Use Encoder-Self-Attention and replace word-embeddings with image-patches: you compute a notion of content-based similarity between the elements (patches), then - based on this content-based similarity - it computes a convex combination that brings the patches together.
            <ul>
              <li>You can think about it as a <span style="color: purple"><em><strong>differentiable</strong></em> way to perform <strong>non-local means</strong></span>.</li>
              <li><strong>Issue - Computational Problem:</strong><br />
  Attention is Cheap only if <em>length</em> \(&lt;&lt;\) <em>dim</em>.<br />
  Length for images is \(32\times 32\times 3 = 3072\): <br />
  <button class="showText" value="show" onclick="showTextPopHide(event);">Diagram</button>
  <img src="https://cdn.mathpix.com/snip/images/zvEcczfCg3TLiJ76y0QohmJNP2V-OxYsYFCQTaHBy40.original.fullsize.png" alt="img" width="100%" hidden="" /></li>
              <li><strong>Solution - Combining Locality with Self-Attention</strong>:<br />
  Restrict the attention windows to be local neighborhoods.<br />
  Good assumption for images because of spatial locality.<br />
 <br /></li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<!-- 2. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents22}

3. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents23}

4. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents24}

5. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents25}

6. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents26}

7. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents27}

8. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents2 #bodyContents28}

***

## THIRD
{: #content3}

1. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents31}

2. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents32}

3. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents33}

4. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents34}

5. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents35}

6. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents36}

7. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents37}

8. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents3 #bodyContents38}

 -->


      <footer class="site-footer">
    <!--   <span class="site-footer-owner"><a href="http://localhost:8889">Ahmad Badary</a> is maintained by <a href="https://ahmedbadary.github.io/">Ahmad Badary</a>.</span> -->
    
<!--  -->
    <div class="wrapper">
        <div class="footer-col-wrapper">
            <div class="footer-col footer-col-1">
            <span class="site-footer-owner"><a href="http://localhost:8889">Site</a> maintained by <a href="https://ahmedbadary.github.io/">Ahmad Badary</a>.</span>
    <span class="site-footer-credits">
        <p>
            &copy; 2017. All rights reserved.
        </p> 
    </span>
            </div>
            <div class="footer-col footer-col-2">
            <div><p>         </p></div>
            </div>
            <div class="footer-col footer-col-3">
                <ul class="social-media-list">
                    
                      <li>
                        <a href="https://github.com/AhmedBadary">
                          <i class="fa fa-github"></i> GitHub
                        </a>
                      </li>
                    
                    
                      <li>
                        <a href="https://linkedin.com/in/ahmad-badary-656098121/">
                          <i class="fa fa-linkedin"></i> LinkedIn
                        </a>
                      </li>
                    
                    
                      <li>
                        <a href="https://www.facebook.com/ahmed.thabet.94">
                          <i class="fa fa-facebook"></i> Facebook
                        </a>
                      </li>
                    
                </ul>
            </div>
        </div>
    </div>
<!--  -->
</footer>


    </section>

  </body>

<!-- Table of Content Script -->
<script type="text/javascript">
var bodyContents = $(".bodyContents1");
$("<ol>").addClass("TOC1ul").appendTo(".TOC1");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC1ul");
     });
// 
var bodyContents = $(".bodyContents2");
$("<ol>").addClass("TOC2ul").appendTo(".TOC2");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC2ul");
     });
// 
var bodyContents = $(".bodyContents3");
$("<ol>").addClass("TOC3ul").appendTo(".TOC3");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC3ul");
     });
//
var bodyContents = $(".bodyContents4");
$("<ol>").addClass("TOC4ul").appendTo(".TOC4");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC4ul");
     });
//
var bodyContents = $(".bodyContents5");
$("<ol>").addClass("TOC5ul").appendTo(".TOC5");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC5ul");
     });
//
var bodyContents = $(".bodyContents6");
$("<ol>").addClass("TOC6ul").appendTo(".TOC6");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC6ul");
     });
//
var bodyContents = $(".bodyContents7");
$("<ol>").addClass("TOC7ul").appendTo(".TOC7");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC7ul");
     });
//
var bodyContents = $(".bodyContents8");
$("<ol>").addClass("TOC8ul").appendTo(".TOC8");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC8ul");
     });
//
var bodyContents = $(".bodyContents9");
$("<ol>").addClass("TOC9ul").appendTo(".TOC9");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC9ul");
     });

</script>

<!-- VIDEO BUTTONS SCRIPT -->
<script type="text/javascript">
  function iframePopInject(event) {
    var $button = $(event.target);
    // console.log($button.parent().next());
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $figure = $("<div>").addClass("video_container");
        $iframe = $("<iframe>").appendTo($figure);
        $iframe.attr("src", $button.attr("src"));
        // $iframe.attr("frameborder", "0");
        $iframe.attr("allowfullscreen", "true");
        $iframe.css("padding", "4px 6px");
        $button.next().css("display", "block");
        $figure.appendTo($button.next());
        $button.text("Hide Video")
    } else {
        $button.attr('value', 'show');
        $button.next().html("");
        $button.text("Show Video")
    }
}
</script>

<!-- BUTTON TRY -->
<script type="text/javascript">
  function iframePopA(event) {
    event.preventDefault();
    var $a = $(event.target).parent();
    console.log($a);
    if ($a.attr('value') == 'show') {
        $a.attr('value', 'hide');
        $figure = $("<div>");
        $iframe = $("<iframe>").addClass("popup_website_container").appendTo($figure);
        $iframe.attr("src", $a.attr("href"));
        $iframe.attr("frameborder", "1");
        $iframe.attr("allowfullscreen", "true");
        $iframe.css("padding", "4px 6px");
        $a.next().css("display", "block");
        $figure.appendTo($a.next().next());
        // $a.text("Hide Content")
        $('html, body').animate({
            scrollTop: $a.offset().top
        }, 1000);
    } else {
        $a.attr('value', 'show');
        $a.next().next().html("");
        // $a.text("Show Content")
    }

    $a.next().css("display", "inline");
}
</script>


<!-- TEXT BUTTON SCRIPT - INJECT -->
<script type="text/javascript">
  function showTextPopInject(event) {
    var $button = $(event.target);
    var txt = $button.attr("input");
    console.log(txt);
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $p = $("<p>");
        $p.html(txt);
        $button.next().css("display", "block");
        $p.appendTo($button.next());
        $button.text("Hide Content")
    } else {
        $button.attr('value', 'show');
        $button.next().html("");
        $button.text("Show Content")
    }

}
</script>

<!-- TEXT BUTTON SCRIPT - HIDDEN / HIDE / SHOW / HIDE/SHOW -->
<script type="text/javascript">
  function showTextPopHide(event) {
    var $button = $(event.target);
    // var txt = $button.attr("input");
    var txt = $button.text();
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $button.next().removeAttr("hidden");
        $button.text(txt + " - Hide Content");
    } else {
        $button.attr('value', 'show');
        $button.next().attr("hidden", "");
        $button.text(txt.replace(" - Hide Content",""));
    }
}
</script>

<!-- TEXT BUTTON SCRIPT - HIDDEN / HIDE / SHOW / HIDE/SHOW -->
<script type="text/javascript">
  function showText_withParent_PopHide(event) {
    var $button = $(event.target);
    var $parent = $button.parent();
    var txt = $button.text();
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $parent.next().removeAttr("hidden");
        $button.text(txt + " - Hide Content");
    } else {
        $button.attr('value', 'show');
        $parent.next().attr("hidden", "");
        $button.text(txt.replace(" - Hide Content",""));
    }
}
</script>

<!-- Print / Printing / printme -->
<!-- <script type="text/javascript">
i = 0

for (var i = 1; i < 6; i++) {
    var bodyContents = $(".bodyContents" + i);
    $("<p>").addClass("TOC1ul")  .appendTo(".TOC1");
    bodyContents.each(function(index, element) {
        var paragraph = $(element);
        $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC1ul");
         });
} 
</script>
 -->
 
</html>

