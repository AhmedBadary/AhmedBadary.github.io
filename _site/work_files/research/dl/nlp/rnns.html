<!DOCTYPE html>
<html lang="en-us">
  <head>
  <meta charset="UTF-8">
  <title>Ahmad Badary</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <link rel="stylesheet" href="/css/normalize.css">
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="/css/cayman.css">
  <link rel="stylesheet" href="/css/style.css">
  <link rel="shortcut icon" href="/main_files/favicon.ico" />
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="/css/customStyle.css">
  <title> » Ahmad Badary</title>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
</head>

  <body>
    <nav class="main-nav">
    <a href="https://ahmedbadary.github.io/" class="main-nav-logo">
        <img src="/main_files/logo.png">
    </a>
    <ul id="menu-main" class="main-nav-items">
        <li id="menu-item-1859" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-home menu-item-1859">
            <a href="/">Home</a>
        </li>
        <li id="menu-item-2869" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-2869">
            <a href="/work">Work</a>
        </li>
        <li id="menu-item-1892" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1892">
            <a href="/projects">Projects</a>
        </li>
        <li id="menu-item-1858" class="menu-item menu-item-type-post_type menu-item-object-page current_page_parent menu-item-1858">
            <a href="/blog">Blog</a>
        </li>
        <li id="menu-item-1862" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1862">
            <a href="/about">About</a>
        </li>
    </ul>
</nav>


<section class="page-header">
  <h1 class="project-name">Language Modeling  <br /> Recurrent Neural Networks (RNNs)</h1>
  <h2 class="project-tagline"></h2>
  <a href="/#" class="btn">Home</a>
  <a href="/work" class="btn">Work-Space</a>
  <a href= /work_files/research/dl/nlp.html class="btn">Previous</a>
</section>

<!-- <div>
  <ul class="posts">
    
      <li><span>02 Jan 2014</span> &raquo; <a href="/2014/01/02/introducing-Ahmad/">Introducing Ahmad</a></li>
    
  </ul>
</div> -->


    <section class="main-content">
      
      <div class="TOC">
  <h1 id="table-of-contents">Table of Contents</h1>

  <ul class="TOC1">
    <li><a href="#content1">Introduction to and History of Language Models</a></li>
  </ul>
  <ul class="TOC2">
    <li><a href="#content2">Recurrent Neural Networks</a></li>
  </ul>
  <ul class="TOC3">
    <li><a href="#content3">RNN Language Models</a></li>
  </ul>
  <ul class="TOC4">
    <li><a href="#content4">Training RNNs</a></li>
  </ul>
  <ul class="TOC4">
    <li><a href="#content4">RNNs in Sequence Modeling</a></li>
  </ul>
  <ul class="TOC4">
    <li><a href="#content4">Bidirectional and Deep RNNs</a></li>
  </ul>
</div>

<hr />
<hr />

<p><a href="https://www.youtube.com/watch?v=nfyE8oF23yQ&amp;list=PL613dYIGMXoZBtZhbyiBqb0QtgK6oJbpm&amp;index=6&amp;t=0s">Language Modeling and RNNS I (Oxford)</a></p>
<blockquote>
  <p>Note: 25:00 (important problem not captured w/ newer models about smoothing and language distribution as Heaps law)</p>
</blockquote>

<p><a href="https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes05-LM_RNN.pdf">LMs Stanford Notes</a></p>

<h2 id="content1">Introduction to and History of Language Models</h2>

<ol>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents11">Language Models:</strong><br />
 A <strong>Language Model</strong> is a statistical model that computes a <em>probability distribution</em> over sequences of words.</p>

    <p>It is a <strong>time-series prediction</strong> problem in which we must be <em>very careful</em> to <em><strong>train on the past</strong></em> and <em><strong>test on the future</strong></em>.</p>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents12">Applications:</strong></dt>
      <dd>
        <ul>
          <li><strong>Machine Translation (MT)</strong>:
            <ul>
              <li>Word Ordering:<br />
  p(“the cat is small”) &gt; p(“small the cat is”)</li>
              <li>Word Choice:<br />
  p(“walking home after school”) &gt; p(“walking house after school”)</li>
            </ul>
          </li>
          <li><strong>Speech Recognition</strong>:
            <ul>
              <li>Word Disambiguation:<br />
  p(“The listeners <em>recognize speech</em>”) &gt; p(“The listeners <em>wreck a nice beach</em>”)</li>
            </ul>
          </li>
          <li><strong>Information Retrieval</strong>:
            <ul>
              <li>Used in <em>query likelihood model</em></li>
            </ul>
          </li>
        </ul>
      </dd>
    </dl>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents13">Traditional Language Models:</strong>
    <ul>
      <li>Most language models employ the chain rule to decompose the <em>joint probability</em> into a <em>sequence of conditional probabilities</em>:
        <p>$$\begin{array}{c}{P\left(w_{1}, w_{2}, w_{3}, \ldots, w_{N}\right)=} \\ {P\left(w_{1}\right) P\left(w_{2} | w_{1}\right) P\left(w_{3} | w_{1}, w_{2}\right) \times \ldots \times P\left(w_{N} | w_{1}, w_{2}, \ldots w_{N-1}\right)}\end{array}$$</p>
        <p>Note that this decomposition is exact and allows us to model complex joint distributions by learning conditional distributions over the next word \((w_n)\) given the history of words observed \(\left(w_{1}, \dots, w_{n-1}\right)\). <br />
  Thus, the <strong>Goal</strong> of the <strong>LM-Task</strong> is to find <em><strong>good conditional distributions</strong></em> that we can <em>multiply</em> to get the <em><strong>Joint Distribution</strong></em>.</p>
        <blockquote>
          <p>Allows you to predict the first word, then the second word <em>given the first word</em>, then the third given the first two, etc..</p>
        </blockquote>
      </li>
      <li>The Probability is usually conditioned on window of \(n\) previous words
        <ul>
          <li>An incorrect but necessary Markovian assumption:
            <p>$$P(w_1, \ldots, w_m) = \prod_{i=1}^m P(w_i | w_1, \ldots, w_{i-1}) \approx \prod_{i=1}^m P(w_i | w_{i-(n-1)}, \ldots, w_{i-1})$$</p>
            <ul>
              <li>Only previous history matters</li>
              <li><strong>Limited Memory</strong>: only last \(n-1\) words are included in history
                <blockquote>
                  <p>E.g. \(2-\)gram LM (only looks at the <em><strong>previous word</strong></em>):</p>
                </blockquote>
              </li>
            </ul>
            <p>$$\begin{aligned} p\left(w_{1}, w_{2}, w_{3},\right.&amp; \ldots &amp;, w_{n} ) \\ &amp;=p\left(w_{1}\right) p\left(w_{2} | w_{1}\right) p\left(w_{3} | w_{1}, w_{2}\right) \times \ldots \\ &amp; \times p\left(w_{n} | w_{1}, w_{2}, \ldots w_{n-1}\right) \\ &amp; \approx p\left(w_{1}\right) p\left(w_{2} | w_{1}\right) p\left(w_{3} | w_{2}\right) \times \ldots \times p\left(w_{n} | w_{n-1}\right) \end{aligned}$$</p>
            <p>The conditioning context, \(w_{i-1}\), is called the <strong>history</strong>.</p>
          </li>
        </ul>
      </li>
      <li>The <strong>MLE</strong> estimate for probabilities, compute for
        <ul>
          <li>Bi-grams:
            <p>$$P(w_2 \| w_1) = \dfrac{\text{count}(w_1, w_2)}{\text{count}(w_1)}$$</p>
          </li>
          <li>Tri-grams:
            <p>$$P(w_3 \| w_1, w_2) = \dfrac{\text{count}(w_1, w_2, w_3)}{\text{count}(w_1, w_2)}$$</p>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents14">Issues with the Traditional Approaches:</strong><br />
 To improve performance we need to:
    <ul>
      <li>Keep higher n-gram counts</li>
      <li>Use Smoothing</li>
      <li>Use Backoff (trying n-gram, (n-1)-gram, (n-2)-grams, ect.)<br />
  When? If you never saw a 3-gram b4, try 2-gram, 1-gram etc.<br />
 However,</li>
      <li>There are <strong>A LOT</strong> of n-grams
        <ul>
          <li>\(\implies\) Gigantic RAM requirements</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents15">NLP Tasks as LM Tasks:</strong><br />
 Much of Natural Language Processing can be structured as <em><strong>(conditional) Language Modeling</strong></em>:
    <ul>
      <li><strong>Translation</strong>:
        <p>$$p_{\mathrm{LM}}(\text { Les chiens aiment les os }\| \| \text { Dogs love bones) }$$</p>
      </li>
      <li><strong>QA</strong>:
        <p>$$p_{\mathrm{LM}}(\text { What do dogs love? }\| \| \text { bones } . | \beta)$$</p>
      </li>
      <li><strong>Dialog</strong>:
        <p>$$p_{\mathrm{LM}}(\text { How are you? }\| \| \text { Fine thanks. And you? } | \beta)$$</p>
        <blockquote>
          <p>where \(\| \|\) means “concatenation”, and \(\beta\) is an observed data (e.g. news article) to be conditioned on.</p>
        </blockquote>
      </li>
    </ul>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents16">Analyzing the LM Tasks:</strong><br />
 The simple objective of <em>modeling the next word given observed history</em> contains much of the complexity of <strong>natural language understanding (NLU)</strong> (e.g. reasoning, intelligence, etc.).</p>

    <p>Consider predicting the extension of the utterance:</p>
    <p>$$p(\cdot | \text { There she built a) }$$</p>
    <blockquote>
      <p>The distribution of what word to predict right now is quite flat; you dont know where <em>“there”</em> is, you dont know who <em>“she”</em> is, you dont know what she would want to <em>“build”</em>.</p>
    </blockquote>

    <p>However, With more context we are able to use our knowledge of both language and the world to heavily constrain the distribution over the next word.</p>
    <p>$$p(\cdot | \color{red} {\text { Alice }} \text {went to the} \color{blue} {\text { beach. } } \color{blue} {\text {There}} \color{red} {\text { she}} \text { built a})$$</p>
    <blockquote>
      <p>At this point your distributions getting <em>very peaked</em> about what could come next and the reason is because you understand language you understand that in the second utterance “she” is “Alice” and “There” is “Beach” so you’ve resolved those Co references and you can do that because you understand the syntactic structure of the first utterance; you understand we have a subject and object, where the verb phrase is, all of these things you do automatically and then, using the semantics that “at a beach you build things like sandcastles or boats” and so you can <strong>constrict your distribution</strong>.</p>
    </blockquote>

    <p>If we can get a automatically trained machine to do that then we’ve come a long way to solving AI.</p>
    <blockquote>
      <p>“The diversity of tasks the model is able to perform in a zero-shot setting suggests that high-capacity models trained to <em>maximize the likelihood of a sufficiently varied text corpus</em> begin to learn how to perform a surprising amount of tasks without the need for explicit supervision” - GPT 2</p>
    </blockquote>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents17">Evaluating a Language Model | The Loss:</strong><br />
 For a probabilistic model, it makes sense to evaluate how well the “learned” distribution matches the real distribution of the data (of real utterances). A good model assigns real utterances \(w_{1}^{N}\)  from a language a high probability. This can be measured with <strong>Cross-Entropy</strong>:
    <p>$$H\left(w_{1}^{N}\right)=-\frac{1}{N} \log _{2} p\left(w_{1}^{N}\right)$$</p>
    <p><strong>Why Cross-Entropy:</strong> It is a measure of <em>how many bits are need to encode text with our model</em> (bits you would need to represent the distribution).<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></p>
    <blockquote>
      <p>Commonly used for <strong>character-level</strong>.</p>
    </blockquote>

    <p>Alternatively, people tend to use <strong>Perplexity</strong>:</p>
    <p>$$\text { perplexity }\left(w_{1}^{N}\right)=2^{H\left(w_{1}^{N}\right)}$$</p>
    <p><strong>Why Perplexity:</strong>  It is a measure of how <em>surprised our model is on seeing each word</em>.</p>
    <blockquote>
      <p>If <strong>no surprise</strong>, the perplexity \(= 1\).  <br />
Commonly used for <strong>word-level</strong>.<br />
 <br /></p>
    </blockquote>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents18">Language Modeling Data:</strong><br />
 Language modelling is a time series prediction problem in which we must be careful to train on the past and test on the future.<br />
 If the corpus is composed of articles, it is best to ensure the test data is drawn from a disjoint set of articles to the training data.</p>

    <p>Two popular data sets for language modeling evaluation are a preprocessed version of the Penn Treebank,1 and the Billion Word Corpus.2 Both are <strong>flawed</strong>:</p>
    <ul>
      <li>The PTB is very small and has been heavily processed. As such it is not representative of natural language.</li>
      <li>The Billion Word corpus was extracted by first randomly permuting sentences in news articles and then splitting into training and test sets. As such train and test sentences come from the same articles and overlap in time</li>
    </ul>

    <p>The recently introduced <strong>WikiText datasets</strong> are a better option.</p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents19">Three Approaches to Parameterizing Language Models:</strong>
    <ol>
      <li><strong>Count-Based N-gram models</strong>: we approximate the history of observed words with just the previous \(n\) words.<br />
 They capture <strong>Multinomial distributions</strong>.</li>
      <li><strong>Neural N-gram models</strong>: embed the same fixed n-gram history in a <em>continuous space</em> and thus better capture <em>correlations between histories</em>.<br />
 Replace the <em>Multinomial distributions</em> with an <strong>FFN</strong>.</li>
      <li><strong>RNNs</strong>: drop the fixed n-gram history and <em>compress the entire history in a fixed length vector</em>, enabling <em>long range correlations</em> to be captured. <br />
 Replace the <strong>finite</strong> history, captured by the conditioning context \(w_{i-1}\), with an <strong>infinite</strong> history, captured by the (previous) hidden state \(h_{n-1}\) (but also \(w_{n=1})\).<br />
 <br /></li>
    </ol>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents110">Bias vs Variance in LM Approximations:</strong><br />
The main issue in language modeling is compressing the history (a string). This is useful beyond language modeling in classification and representation tasks.
    <ul>
      <li>With n-gram models we approximate the history with only the last n words</li>
      <li>With recurrent models (RNNs, next) we compress the unbounded history into a fixed sized vector</li>
    </ul>

    <p>We can view this progression as the classic <strong>Bias vs. Variance tradeoff</strong> in ML:</p>
    <ul>
      <li><strong>N-gram models</strong>: are biased but low variance.<br />
  No matter how much data (infinite) they will always be wrong/biased.</li>
      <li><strong>RNNs:</strong> decrease the bias considerably, hopefully at a small cost to variance.</li>
    </ul>

    <p>Consider predicting the probability of a sentence by how many times you have seen it before. This is an <em>unbiased estimator with (extremely) high variance</em>.</p>
    <ul>
      <li>In the limit of infinite data, gives true distribution.<br />
<br /></li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents111">Scaling Language Models (Large Vocabularies):</strong><br />
<strong>Bottleneck:</strong><br />
Much of the computational cost of a Neural LM is a function of the <strong>size of the vocabulary</strong> and is dominated by calculating the softmax:
    <p>$$\hat{p}_{n}=\operatorname{softmax}\left(W h_{n}+b\right)$$</p>

    <p><strong>Solutions:</strong></p>
    <ul>
      <li><strong>Short-Lists</strong>: use the neural LM for the most frequent words, and a traditional <em>n-gram</em> LM for the rest.<br />
  While easy to implement, this nullifies the Neural LMs main advantage, i.e. generalization to rare events.</li>
      <li><strong>Batch local short-lists</strong>: approximate the full partition function for data instances from a segment for the data with a subset of vocabulary chosen for that segment.</li>
      <li><strong>Approximate the gradient/change the objective</strong>:  if we did not have to sum over the vocabulary to normalize during training, it would be much faster. It is tempting to consider maximizing likelihood by making the log partition function an independent parameter \(c\), but this leads to an ill defined objective:
        <p>$$\hat{p}_{n} \equiv \exp \left(W h_{n}+b\right) \times \exp (c)$$</p>
        <blockquote>
          <p>What does the Softmax layer do?<br />
The idea of the Softmax is to say: at each time step look at the word we want to predict and the whole vocab; where we try to <strong>maximize the probability of the word we want to predict</strong> and <strong>minimize the probability of ALL THE OTHER WORDS</strong>.</p>
        </blockquote>

        <p>So, The better solution is to try to approximate what the softmax does using:</p>
        <ul>
          <li><strong>Noise Contrastive Estimation (NCE)</strong>: this amounts to learning a binary classifier to distinguish data samples from \((k)\) samples from a noise distribution (a unigram is a good choice):</li>
        </ul>
        <p>$$p\left(\text { Data }=1 | \hat{p}_{n}\right)=\frac{\hat{p}_{n}}{\hat{p}_{n}+k p_{\text { noise }}\left(w_{n}\right)}$$</p>
        <p>Now parametrizing the log partition function as \(c\) does not degenerate. This is very effective for <em>speeding up training</em> but has no effect on <em>testing</em>.</p>
        <ul>
          <li><strong>Importance Sampling (IS)</strong>: similar to NCE but defines a multiclass classification problem between the true word and noise samples, with a Softmax and cross entropy loss.</li>
          <li><a href="http://ruder.io/word-embeddings-softmax/index.html" value="show" onclick="iframePopA(event)"><strong>(more on) Approximating the Softmax</strong></a>
  <a href="http://ruder.io/word-embeddings-softmax/index.html"></a>
            <div></div>
          </li>
        </ul>
      </li>
      <li><strong>Factorize the output vocabulary</strong>: the idea is to decompose the (one big) softmax into a series of softmaxes (2 in this case). We map words to a set of classes, then we, first, predict which class the word is in, and then we predict the right word from the words in that class.<br />
  One level factorization works well (Brown clustering is a good choice, frequency binning is not):
        <p>$$p\left(w_{n} | \hat{p}_{n}^{\text { class }}, \hat{p}_{n}^{\text { word }}\right)=p\left(\operatorname{class}\left(w_{n}\right) | \hat{p}_{n}^{\text { class }}\right) \times p\left(w_{n} | \operatorname{class}\left(w_{n}\right), \hat{p}_ {n}^{\text { word }}\right)$$</p>
        <p>where the function \(\text{ class}(\cdot)\) maps each word to one class. Assuming balanced classes, this gives a quadratic, \(\root{V}\) speedup.</p>
        <ul>
          <li><a href="https://www.youtube.com/embed/eDUaRvMDs-s?start=2818" value="show" onclick="iframePopA(event)"><strong>Binary Tree Factorization for $$\log{V} Speedup</strong></a>
  <a href="https://www.youtube.com/embed/eDUaRvMDs-s?start=2818"></a>
            <div></div>
          </li>
        </ul>
      </li>
    </ul>

    <p><strong>Complexity Comparison of the different solutions:</strong><br />
<img src="/main_files/dl/nlp/rnn/8.png" alt="img" width="70%" /><br />
<br /></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents112">Sub-Word Level Language Models:</strong><br />
Could be viewed as an alternative to changing the softmax by changing the input granularity and model text at the <strong>morpheme</strong> or <strong>character</strong> level.<br />
This results in a much smaller softmax and no unknown words, but the downsides are longer sequences and longer dependencies; moreover, a lot of the structure in a language is in the words and we want to learn correlations amongst the words but since the model doesn’t get the words as a unit, it will have to <em>learn what/where a is</em> before it can learn its correlation with other sequences; which effecitely means that we made the learning problem harder and more non-linear   <br />
This, also, allows the model to capture subword structure and morphology: e.g. “disunited” &lt;-&gt; “disinherited” &lt;-&gt; “disinterested”.<br />
Character LMs <strong>lag</strong> behind word-based models in perplexity, but are clearly the future of language modeling.</p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents113">Conditional Language Models:</strong><br />
A <strong>Conditional LM</strong> assigns probabilities to sequences of words given some conditioning context \(x\). It models “What is the probability of the next word, given the history of previously generated words AND conditioning context \(x\)?”.<br />
The probability, decomposed w/ chain rule:
    <p>$$p(\boldsymbol{w} | \boldsymbol{x})=\prod_{t=1}^{\ell} p\left(w_{t} | \boldsymbol{x}, w_{1}, w_{2}, \ldots, w_{t-1}\right)$$</p>
    <ul>
      <li><button class="showText" value="show" onclick="showTextPopHide(event);">Applications</button>
  <img src="/main_files/dl/nlp/rnn/9.png" alt="img" width="80%" hidden="" /></li>
    </ul>
  </li>
</ol>

<hr />

<h2 id="content2">Recurrent Neural Networks</h2>

<ol>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents2" id="bodyCoxqntents21">Recurrent Neural Networks:</strong></dt>
      <dd>An <strong>RNN</strong> is a class of artificial neural network where connections between units form a directed cycle, allowing it to exhibit dynamic temporal behavior.</dd>
      <dd>The standard RNN is a nonlinear dynamical system that maps sequences to sequences.</dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents22">The Structure of an RNN:</strong></dt>
      <dd>The RNN is parameterized with three weight matrices and three bias vectors:</dd>
      <dd>
\[\theta = [W_{hx}, W_{hh}, W_{oh}, b_h, b_o, h_0]\]
      </dd>
      <dd>These parameter completely describe the RNN.</dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents23">The Algorithm:</strong></dt>
      <dd>Given an <em>input sequence</em> \(\hat{x} = [x_1, \ldots, x_T]\), the RNN computes a sequence of hidden states \(h_1^T\) and a sequence of outputs \(y_1^T\) in the following way:<br />
<strong>for</strong> \(t\) <strong>in</strong> \([1, ..., T]\) <strong>do</strong><br />
    \(\:\:\:\:\:\:\:\)  \(u_t \leftarrow W_{hx}x_t + W_{hh}h_{t-1} + b_h\)<br />
    \(\:\:\:\:\:\:\:\)  \(h_t \leftarrow g_h(u_t)\)<br />
    \(\:\:\:\:\:\:\:\)  \(o_t \leftarrow W_{oh}h_{t} + b_o\)<br />
    \(\:\:\:\:\:\:\:\)  \(y_t \leftarrow g_y(o_t)\)</dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents24">The Loss:</strong></dt>
      <dd>The loss of an RNN is commonly a sum of per-time losses:</dd>
      <dd>
\[L(y, z) = \sum_{t=1}^TL(y_t, z_t)\]
      </dd>
      <dd>
        <ul>
          <li><strong>Language Modelling</strong>: 
  We use the <em><strong>Cross Entropy</strong></em> Loss function but predicting <em>words</em> instead of classes</li>
        </ul>
      </dd>
      <dd>
\[J^{(t)}(\theta) = - \sum_{j=1}^{\vert V \vert} y_{t, j} \log \hat{y_{t, j}}\]
      </dd>
      <dd>
\[\implies\]
      </dd>
      <dd>
\[L(y,z) = J = -\dfrac{1}{T} \sum_{t=1}^{T} \sum_{j=1}^{\vert V \vert} y_{t, j} \log \hat{y_{t, j}}\]
      </dd>
      <dd>To <strong>Evaluate</strong> the model, we use <em><strong>Preplexity</strong></em> :</dd>
      <dd>
\[2^J\]
      </dd>
      <dd>
        <blockquote>
          <p>Lower Preplexity is <em>better</em></p>
        </blockquote>
      </dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents25">Analyzing the Gradient:</strong></dt>
      <dd>Assuming the following formulation of an <strong>RNN</strong>:</dd>
      <dd>
\[h_t = Wf(h_{t-1}) + W^{(hx)}x_{[t]} \\
\hat{y_t} = W^{(S)}f(h_t)\]
      </dd>
      <dd>
        <ul>
          <li>The <strong>Total Error</strong> is the sum of each error at each time step \(t\):</li>
        </ul>
      </dd>
      <dd>
\[\dfrac{\partial E}{\partial W} = \sum_{t=1}^{T} \dfrac{\partial E_t}{\partial W}\]
      </dd>
      <dd>
        <ul>
          <li>The <strong>local Error</strong> at a time step \(t\):</li>
        </ul>
      </dd>
      <dd>
\[\dfrac{\partial E_t}{\partial W} = \sum_{k=1}^{t} \dfrac{\partial E_t}{\partial y_t} \dfrac{\partial y_t}{\partial h_t} \dfrac{\partial h_t}{\partial h_k} \dfrac{\partial h_k}{\partial W}\]
      </dd>
      <dd>
        <ul>
          <li>To compute the <em>local derivative</em> we need to compute:</li>
        </ul>
      </dd>
      <dd>
\[\dfrac{\partial h_t}{\partial h_k}\]
      </dd>
      <dd></dd>
      <dd>
\[\begin{align}
\dfrac{\partial h_t}{\partial h_k} &amp;= \prod_{j=k+1}^t \dfrac{\partial h_j}{\partial h_{j-1}} \\
&amp;= \prod_{j=k+1}^t J_{j, j-1}
\end{align}\]
      </dd>
      <dd>\(\:\:\:\:\:\:\:\:\) where each \(J_{j, j-1}\) is the <strong>jacobina matrix</strong> of the partial derivatives of each respective<br />
\(\:\:\:\:\:\:\:\:\) hidden layer.</dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents29">The Vanishing Gradient Problem:</strong></dt>
      <dd>
        <ul>
          <li><strong>Analyzing the Norms of the Jacobians</strong> of each partial:</li>
        </ul>
      </dd>
      <dd>
\[\| \dfrac{\partial h_j}{\partial h_{j-1}} \| \leq \| W^T \| \cdot \| \text{ diag}[f'(h_{j-1})] \| \leq \beta_W \beta_h\]
      </dd>
      <dd>\(\:\:\:\:\:\:\:\) where we defined the \(\beta\)s as <em>upper bounds</em> of the <em>norms</em>.</dd>
      <dd>
        <ul>
          <li><strong>The Gradient is the product of these Jacobian Matrices</strong> (each associated with a step in the forward computation):</li>
        </ul>
      </dd>
      <dd>
\[\| \dfrac{\partial h_t}{\partial h_k} \| = \| \prod_{j=k+1}^t \dfrac{\partial h_j}{\partial h_{j-1}} \| \leq (\beta_W \beta_h)^{t-k}\]
      </dd>
      <dd>
        <ul>
          <li><em><strong>Conclusion</strong></em>:<br />
  Now, as the exponential \((t-k) \rightarrow \infty\):
            <ul>
              <li><strong>If \((\beta_W \beta_h) &lt; 1\)</strong>: <br />
  \((\beta_W \beta_h)^{t-k} \rightarrow 0\).<br />
  known as <strong>Vanishing Gradient</strong>.</li>
              <li><strong>If \((\beta_W \beta_h) &gt; 1\)</strong>:<br />
  \((\beta_W \beta_h)^{t-k} \rightarrow \infty\).<br />
  known as <strong>Exploding Gradient</strong>.</li>
            </ul>
          </li>
        </ul>
      </dd>
      <dd>As the bound can become <strong>very small</strong> or <strong>very large</strong> quickly, the <em>locality assumption of gradient descent</em> breaks down.</dd>
    </dl>

    <p><br /></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents26">BPTT:</strong><br />
 <strong>for</strong> \(t\) <strong>from</strong> \(T\) <strong>to</strong> \(1\) <strong>do</strong><br />
     <!-- $$\ \ \ \ \ \ \ \ \ \ $$ $$dy_t \leftarrow g_y'(o_t) · dy_t$$   -->
 \(\begin{align}
 \ \ \ \ \ \ \ \ \ \ do_t &amp;\leftarrow dy_t · g_y'(o_t) \\
 \ \ \ \ \ \ \ \ \ \ db_o &amp;\leftarrow db_o + do_t \\
 \ \ \ \ \ \ \ \ \ \ dW_{oh} &amp;\leftarrow dW_{oh} + do_th_t^T \\
 \ \ \ \ \ \ \ \ \ \ dh_t &amp;\leftarrow dh_t + W_{oh}^T do_t \\
 \ \ \ \ \ \ \ \ \ \ du_t &amp;\leftarrow dh_t · g_h'(u_t) \\
 \ \ \ \ \ \ \ \ \ \ dW_{hx} &amp;\leftarrow dW_{hx} + du_tx_t^T \\
 \ \ \ \ \ \ \ \ \ \ db_h &amp;\leftarrow db_h + du_t \\
 \ \ \ \ \ \ \ \ \ \ dW_{hh} &amp;\leftarrow dW_{hh} + du_th_{t-1}^T \\
 \ \ \ \ \ \ \ \ \ \ dh_{t-1} &amp;\leftarrow W_{hh}^T du_t 
 \end{align}\)<br />
 <strong>Return</strong> \(\:\:\:\: d\theta = [dW_{hx}, dW_{hh}, dW_{oh}, db_h, db_o, dh_0]\)</p>

    <p><br />
 <strong>Expanded:</strong> <br />
 \(\begin{align}
     L(y, \hat{y}) &amp;= \sum_{t} L^{(t)} = -\sum_{t} \log \hat{y}_ t = -\sum_{t} \log p_{\text {model }}\left(y^{(t)} |\left\{\boldsymbol{x}^{(1)}, \ldots, \boldsymbol{x}^{(t)}\right\}\right) \\
     \hat{y}_ t &amp;= \text{softmax}(o_t) \\
     dy_{\tau} &amp;= \frac{\partial L}{\partial L^{(t)}}=1 \\
     do_t &amp;= g_y'(o_t) = \text{softmax}'(o_t) = \hat{y}_{i}^{(t)}-\mathbf{1}_{i=y^{(t)}} \\
     dh_{\tau} &amp;= W_{oh}^T do_t \\
     dh_t &amp;= \left(\frac{\partial \boldsymbol{h}^{(t+1)}}{\partial \boldsymbol{h}^{(t)}}\right)^{\top}\left(\nabla_{\boldsymbol{h}^{(t+1)}} L\right)+\left(\frac{\partial \boldsymbol{o}^{(t)}}{\partial \boldsymbol{h}^{(t)}}\right)^{\top}\left(\nabla_{\boldsymbol{o}^{(t)}} L\right) = \boldsymbol{W_{hh}}^{\top} \operatorname{diag}\left(1-\left(\boldsymbol{h}^{(t+1)}\right)^{2}\right)\left(\nabla_{\boldsymbol{h}^{(t+1)}} L\right)+\boldsymbol{W_{oh}}^{\top}\left(\nabla_{\boldsymbol{o}^{(t)}} L\right) \\
     du_t &amp;= dh_t · g_h'(u_t) = dh_t · \operatorname{tanh}'(u_t) = dh_t · \sum_{t} \operatorname{diag}\left(1-\left(h^{(t)}\right)^{2}\right) \\
     db_o &amp;= do_t \\
     dW_{oh} &amp;= do_th_t^T \\
     dW_{hx} &amp;= du_tx_t^T \\
     db_h &amp;= du_t \\
     dW_{hh} &amp;= du_th_{t-1}^T \\
     dh_{t-1} &amp;= W_{hh}^T du_t
 \end{align}\)</p>

    <p id="lst-p"><strong style="color: red">Notes:</strong></p>
    <ul>
      <li>\(dh_{\tau}\): We need to get the gradient of \(h\) at the last node/time-step \(\tau\) i.e. \(h_{\tau}\)</li>
      <li>\(dh_t\): We can then iterate backward in time to back-propagate gradients through time, from \(t=\tau-1\) down to \(t=1\), noting that \(h^{(t)}(\text { for } t&lt;\tau)\) has as <span style="color: goldenrod"><strong>descendants</strong></span> both \(\boldsymbol{o}^{(t)}\) and \(\boldsymbol{h}^{(t+1)}\).<br />
  <button class="showText" value="show" onclick="showTextPopHide(event);">From Graph</button>
    <img src="/main_files/dl/archits/rnns/9.png" alt="img" width="30%" hidden="" /></li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents27">Backpropagation Through Time:</strong>
    <ul>
      <li>We can think of the recurrent net as a layered, feed-forward net with shared weights and then train the feed-forward net with (linear) weight constraints.</li>
      <li>We can also think of this training algorithm in the time domain:
        <ul>
          <li>The forward pass builds up a stack of the activities of all the units at each time step</li>
          <li>The backward pass peels activities off the stack to compute the error derivatives at each time step</li>
          <li>After the backward pass we add together the derivatives at all the different times for each weight.</li>
        </ul>
      </li>
    </ul>

    <p><strong>Complexity:</strong><br />
 <em><strong>Linear</strong></em> in the length of the longest sequence.<br />
 <em>Minibatching</em> can be inefficient as the sequences in a batch may have different lengths.</p>
    <blockquote>
      <p>Can be alleviated w/ <strong>padding</strong>.</p>
    </blockquote>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents28">Truncated BPTT:</strong><br />
 Same as BPTT, but tries to avoid the problem of long sequences as inputs. It does that by <em>“breaking”</em> the gradient flow every nth time-step (if input is article, then n could be average length of a sentence), thus, avoiding problems of <strong>(1) Memory</strong> <strong>(2) Exploding Gradient</strong>.</p>

    <p><strong>Downsides:</strong><br />
 If there are <em>dependencies</em> between the segments where BPTT was truncated they will <strong>not be learned</strong> because the <em>gradient doesn’t flow back to teach the hidden representation about what information was useful</em>.</p>

    <p><strong>Complexity:</strong><br />
 <em><strong>Constant</strong></em> in the truncation length \(T\).<br />
 <em>Minibatching</em> is efficient as all sequences have length \(T\).</p>

    <p><strong>Notes:</strong></p>
    <ul>
      <li>In TBPTT, we <strong>Forward Propagate</strong> through-the-break/between-segments normally (through the entire comp-graph). Only the back-propagation is truncated.</li>
      <li><strong>Mini-batching</strong> on a GPU is an effective way of speeding up big matrix vector products <sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>. RNNLMs have two such products that dominate their computation: the <em>recurrent matrix</em> \(V\) and the <em>softmax matrix</em> \(W\).</li>
    </ul>
  </li>
</ol>

<p><strong>LSTMS:</strong></p>
<ul>
  <li>The core of the history/memory is captured in the <em>cell-state \(c_{n}\)</em> instead of the hidden state \(h_{n}\).</li>
  <li>(&amp;) <strong>Key Idea:</strong> The update to the cell-state \(c_{n}=c_{n-1}+\operatorname{tanh}\left(V\left[w_{n-1} ; h_{n-1}\right]+b_{c}\right)\)  here are <strong>additive</strong>. (differentiating a sum gives the identity) Making the gradient flow nicely through the sum. As opposed to the multiplicative updates to \(h_n\) in vanilla RNNs.
    <blockquote>
      <p>There is non-linear funcs applied to the history/context cell-state. It is composed of linear functions. Thus, avoids gradient shrinking.</p>
    </blockquote>
  </li>
  <li>In the recurrency of the LSTM the activation function is the identity function with a derivative of 1.0. So, the backpropagated gradient neither vanishes or explodes when passing through, but remains constant.</li>
  <li>By the selective read, write and forget mechanism (using the gating architecture) of LSTM, there exist at least one path, through which gradient can flow effectively from \(L\)  to \(\theta\). Hence no vanishing gradient.</li>
  <li>However, one must remember that, this is not the case for exploding gradient. It can be proved that, there <strong>can exist</strong> at-least one path, thorough which gradient can explode.</li>
  <li>LSTM decouples cell state (typically denoted by c) and hidden layer/output (typically denoted by h), and only do additive updates to c, which makes memories in c more stable. Thus the gradient flows through c is kept and hard to vanish (therefore the overall gradient is hard to vanish). However, other paths may cause gradient explosion.</li>
  <li>The Vanishing gradient solution for LSTM is known as <em>Constant Error Carousel</em>.</li>
  <li><a href="https://stats.stackexchange.com/questions/320919/why-can-rnns-with-lstm-units-also-suffer-from-exploding-gradients/339129#339129" value="show" onclick="iframePopA(event)"><strong>Why can RNNs with LSTM units also suffer from “exploding gradients”?</strong></a>
<a href="https://stats.stackexchange.com/questions/320919/why-can-rnns-with-lstm-units-also-suffer-from-exploding-gradients/339129#339129"></a>
    <div></div>
  </li>
  <li>
    <p><a href="https://www.cse.iitm.ac.in/~miteshk/CS7015/Slides/Teaching/pdf/Lecture15.pdf">Lecture on gradient flow paths through gates</a></p>
  </li>
  <li><a href="https://www.youtube.com/embed/eDUaRvMDs-s?start=775" value="show" onclick="iframePopA(event)"><strong>LSTMs (Lec Oxford)</strong></a>
<a href="https://www.youtube.com/embed/eDUaRvMDs-s?start=776"></a>
    <div></div>
  </li>
</ul>

<p><strong>Important Links:</strong><br />
<a href="https://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139">The unreasonable effectiveness of Character-level Language Models</a><br />
<a href="https://gist.github.com/karpathy/d4dee566867f8291f086">character-level language model with a Vanilla Recurrent Neural Network, in Python/numpy</a><br />
<a href="https://skillsmatter.com/skillscasts/6611-visualizing-and-understanding-recurrent-networks">Visualizing and Understanding Recurrent Networks - Karpathy Lec</a><br />
<a href="https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714">Cool LSTM Diagrams - blog</a><br />
<a href="https://www.youtube.com/watch?v=LHXXI4-IEns">Illustrated Guide to Recurrent Neural Networks: Understanding the Intuition</a><br />
<a href="https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/">Code LSTM in Python</a><br />
<a href="http://www.fit.vutbr.cz/~imikolov/rnnlm/thesis.pdf">Mikolov Thesis: STATISTICAL LANGUAGE MODELS BASED ON NEURAL NETWORKS</a></p>

<hr />

<h2 id="content3">RNNs Extra!</h2>

<ol>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents35">Vanishing/Exploding Gradients:</strong></dt>
      <dd>
        <ul>
          <li><strong>Exploding Gradients</strong>:
            <ul>
              <li>Truncated BPTT</li>
              <li>Clip gradients at threshold</li>
              <li>RMSprop to adjust learning rate</li>
            </ul>
          </li>
          <li><strong>Vanishing Gradient</strong>:
            <ul>
              <li>Harder to detect</li>
              <li>Weight initialization</li>
              <li>ReLu activation functions</li>
              <li>RMSprop</li>
              <li>LSTM, GRUs</li>
            </ul>
          </li>
        </ul>
      </dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents31">Applications:</strong></dt>
      <dd>
        <ul>
          <li><strong>NER</strong></li>
          <li><strong>Entity Level Sentiment in context</strong></li>
          <li><strong>Opinionated Expressions</strong></li>
        </ul>
      </dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents32">Bidirectional RNNs:</strong></dt>
      <dd>
        <ul>
          <li><strong>Motivation</strong>:<br />
  For <em>classification</em> we need to incorporate information from words both preceding and following the word being processed</li>
        </ul>
      </dd>
      <dd><img src="/main_files/dl/nlp/rnn/1.png" alt="img" width="100%" /></dd>
      <dd>\(\:\:\:\:\) Here \(h = [\overrightarrow{h};\overleftarrow{h}]\) represents (summarizes) the <em>past</em> and the <em>future</em> around a single token.</dd>
      <dd>
        <ul>
          <li><strong>Deep Bidirectional RNNs</strong>:</li>
        </ul>
      </dd>
      <dd><img src="/main_files/dl/nlp/rnn/2.png" alt="img" width="100%" /></dd>
      <dd>\(\:\:\:\:\:\) Each memory layer passes an <em>intermediate sequential representation</em> to the next.</dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents33">Math to Code:</strong></dt>
      <dd>The Parameters: \(\{W_{hx}, W_{hh}, W_{oh} ; b_h, b_o, h_o\}\)</dd>
      <dd>
\[\begin{align}
h_t &amp;= \phi(W_{hx}x_t + W_{hh}h_{t-1} + b_h) \\
h_t &amp;= \phi(\begin{bmatrix}
 W_{hx} &amp; ; &amp; W_{hh}
\end{bmatrix}   
\begin{bmatrix} x_t  \\ ;   \\ h_{t-1} \end{bmatrix} + b_h)
\end{align}\]
      </dd>
      <dd>
\[y_t = \phi'(W_{oh}h_t + b_o)\]
      </dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents34">Initial States:</strong></dt>
      <dd><img src="/main_files/dl/nlp/rnn/3.png" alt="img" width="76%" /></dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents36">Specifying the Initial States:</strong></dt>
      <dd><img src="/main_files/dl/nlp/rnn/4.png" alt="img" width="76%" /></dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents37">Teaching Signals:</strong></dt>
      <dd><img src="/main_files/dl/nlp/rnn/5.png" alt="img" width="76%" /></dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents35">Vanishing/Exploding Gradients:</strong></dt>
      <dd>
        <ul>
          <li><strong>Exploding Gradients</strong>:
            <ul>
              <li>Truncated BPTT</li>
              <li>Clip gradients at threshold</li>
              <li>RMSprop to adjust learning rate</li>
            </ul>
          </li>
          <li><strong>Vanishing Gradient</strong>:
            <ul>
              <li>Harder to detect</li>
              <li>Weight initialization</li>
              <li>ReLu activation functions</li>
              <li>RMSprop</li>
              <li>LSTM, GRUs</li>
            </ul>
          </li>
        </ul>
      </dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents39">Rectifying the Vanishing/Exploding Gradient Problem:</strong></dt>
      <dd><img src="/main_files/dl/nlp/rnn/7.png" alt="img" width="76%" /></dd>
    </dl>
  </li>
  <li>
    <dl>
      <dt><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents38">Linearity of BackProp:</strong></dt>
      <dd><img src="/main_files/dl/nlp/rnn/6.png" alt="img" width="76%" /></dd>
      <dd>The derivative update are also <strong>Correlated</strong> which is bad for SGD.</dd>
    </dl>
  </li>
</ol>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>the problem of assigning a probability to a string and text compression is exactly the same problem so if you have a good language model you also have a good text compression algorithm and both we think of it in terms of the number of bits we can compress our sequence into. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>By making them Matrix-Matrix products instead. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>


      <footer class="site-footer">
    <!--   <span class="site-footer-owner"><a href="http://localhost:8889">Ahmad Badary</a> is maintained by <a href="http://ahmedbadary.ml">Ahmad Badary</a>.</span> -->
    
<!--  -->
    <div class="wrapper">
        <div class="footer-col-wrapper">
            <div class="footer-col footer-col-1">
            <span class="site-footer-owner"><a href="http://localhost:8889">Site</a> maintained by <a href="http://ahmedbadary.ml">Ahmad Badary</a>.</span>
    <span class="site-footer-credits">
        <p>
            &copy; 2017. All rights reserved.
        </p> 
    </span>
            </div>
            <div class="footer-col footer-col-2">
            <div><p>         </p></div>
            </div>
            <div class="footer-col footer-col-3">
                <ul class="social-media-list">
                    
                      <li>
                        <a href="https://github.com/AhmedBadary">
                          <i class="fa fa-github"></i> GitHub
                        </a>
                      </li>
                    
                    
                      <li>
                        <a href="https://linkedin.com/in/ahmad-badary-656098121/">
                          <i class="fa fa-linkedin"></i> LinkedIn
                        </a>
                      </li>
                    
                    
                      <li>
                        <a href="https://www.facebook.com/ahmed.thabet.94">
                          <i class="fa fa-facebook"></i> Facebook
                        </a>
                      </li>
                    
                </ul>
            </div>
        </div>
    </div>
<!--  -->
</footer>


    </section>

  </body>

<!-- Table of Content Script -->
<script type="text/javascript">
var bodyContents = $(".bodyContents1");
$("<ol>").addClass("TOC1ul").appendTo(".TOC1");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC1ul");
     });
// 
var bodyContents = $(".bodyContents2");
$("<ol>").addClass("TOC2ul").appendTo(".TOC2");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC2ul");
     });
// 
var bodyContents = $(".bodyContents3");
$("<ol>").addClass("TOC3ul").appendTo(".TOC3");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC3ul");
     });
//
var bodyContents = $(".bodyContents4");
$("<ol>").addClass("TOC4ul").appendTo(".TOC4");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC4ul");
     });
//
var bodyContents = $(".bodyContents5");
$("<ol>").addClass("TOC5ul").appendTo(".TOC5");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC5ul");
     });
//
var bodyContents = $(".bodyContents6");
$("<ol>").addClass("TOC6ul").appendTo(".TOC6");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC6ul");
     });
//
var bodyContents = $(".bodyContents7");
$("<ol>").addClass("TOC7ul").appendTo(".TOC7");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC7ul");
     });
//
var bodyContents = $(".bodyContents8");
$("<ol>").addClass("TOC8ul").appendTo(".TOC8");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC8ul");
     });
//
var bodyContents = $(".bodyContents9");
$("<ol>").addClass("TOC9ul").appendTo(".TOC9");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC9ul");
     });

</script>

<!-- VIDEO BUTTONS SCRIPT -->
<script type="text/javascript">
  function iframePopInject(event) {
    var $button = $(event.target);
    // console.log($button.parent().next());
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $figure = $("<div>").addClass("video_container");
        $iframe = $("<iframe>").appendTo($figure);
        $iframe.attr("src", $button.attr("src"));
        // $iframe.attr("frameborder", "0");
        $iframe.attr("allowfullscreen", "true");
        $iframe.css("padding", "4px 6px");
        $button.next().css("display", "block");
        $figure.appendTo($button.next());
        $button.text("Hide Video")
    } else {
        $button.attr('value', 'show');
        $button.next().html("");
        $button.text("Show Video")
    }
}
</script>

<!-- BUTTON TRY -->
<script type="text/javascript">
  function iframePopA(event) {
    event.preventDefault();
    var $a = $(event.target).parent();
    console.log($a);
    if ($a.attr('value') == 'show') {
        $a.attr('value', 'hide');
        $figure = $("<div>");
        $iframe = $("<iframe>").addClass("popup_website_container").appendTo($figure);
        $iframe.attr("src", $a.attr("href"));
        $iframe.attr("frameborder", "1");
        $iframe.attr("allowfullscreen", "true");
        $iframe.css("padding", "4px 6px");
        $a.next().css("display", "block");
        $figure.appendTo($a.next().next());
        // $a.text("Hide Content")
        $('html, body').animate({
            scrollTop: $a.offset().top
        }, 1000);
    } else {
        $a.attr('value', 'show');
        $a.next().next().html("");
        // $a.text("Show Content")
    }

    $a.next().css("display", "inline");
}
</script>


<!-- TEXT BUTTON SCRIPT - INJECT -->
<script type="text/javascript">
  function showTextPopInject(event) {
    var $button = $(event.target);
    var txt = $button.attr("input");
    console.log(txt);
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $p = $("<p>");
        $p.html(txt);
        $button.next().css("display", "block");
        $p.appendTo($button.next());
        $button.text("Hide Content")
    } else {
        $button.attr('value', 'show');
        $button.next().html("");
        $button.text("Show Content")
    }

}
</script>

<!-- TEXT BUTTON SCRIPT - HIDDEN / HIDE / SHOW / HIDE/SHOW -->
<script type="text/javascript">
  function showTextPopHide(event) {
    var $button = $(event.target);
    // var txt = $button.attr("input");
    var txt = $button.text();
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $button.next().removeAttr("hidden");
        $button.text(txt + " - Hide Content");
    } else {
        $button.attr('value', 'show');
        $button.next().attr("hidden", "");
        $button.text(txt.replace(" - Hide Content",""));
    }
}
</script>

<!-- TEXT BUTTON SCRIPT - HIDDEN / HIDE / SHOW / HIDE/SHOW -->
<script type="text/javascript">
  function showText_withParent_PopHide(event) {
    var $button = $(event.target);
    var $parent = $button.parent();
    var txt = $button.text();
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $parent.next().removeAttr("hidden");
        $button.text(txt + " - Hide Content");
    } else {
        $button.attr('value', 'show');
        $parent.next().attr("hidden", "");
        $button.text(txt.replace(" - Hide Content",""));
    }
}
</script>

<!-- Print / Printing / printme -->
<!-- <script type="text/javascript">
i = 0

for (var i = 1; i < 6; i++) {
    var bodyContents = $(".bodyContents" + i);
    $("<p>").addClass("TOC1ul")  .appendTo(".TOC1");
    bodyContents.each(function(index, element) {
        var paragraph = $(element);
        $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC1ul");
         });
} 
</script>
 -->
 
</html>

