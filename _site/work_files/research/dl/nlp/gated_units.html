<!DOCTYPE html>
<html lang="en-us">
  <head>
  <meta charset="UTF-8">
  <title>Ahmad Badary</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <link rel="stylesheet" href="/css/normalize.css">
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="/css/cayman.css">
  <link rel="stylesheet" href="/css/style.css">
  <link rel="shortcut icon" href="/main_files/favicon.ico" />
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="/css/customStyle.css">
  <title> » Ahmad Badary</title>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
</head>

  <body>
    <nav class="main-nav">
    <a href="https://ahmedbadary.github.io/" class="main-nav-logo">
        <img src="/main_files/logo.png">
    </a>
    <ul id="menu-main" class="main-nav-items">
        <li id="menu-item-1859" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-home menu-item-1859">
            <a href="/">Home</a>
        </li>
        <li id="menu-item-2869" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-2869">
            <a href="/work">Work</a>
        </li>
        <li id="menu-item-1892" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1892">
            <a href="/projects">Projects</a>
        </li>
        <li id="menu-item-1858" class="menu-item menu-item-type-post_type menu-item-object-page current_page_parent menu-item-1858">
            <a href="/blog">Blog</a>
        </li>
        <li id="menu-item-1862" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1862">
            <a href="/about">About</a>
        </li>
    </ul>
</nav>


<section class="page-header">
  <h1 class="project-name">Gated Units <br /> RNN Architectures</h1>
  <h2 class="project-tagline"></h2>
  <a href="/#" class="btn">Home</a>
  <a href="/work" class="btn">Work-Space</a>
  <a href= /work_files/research/dl/nlp.html class="btn">Previous</a>
</section>

<!-- <div>
  <ul class="posts">
    
      <li><span>02 Jan 2014</span> &raquo; <a href="/2014/01/02/introducing-Ahmad/">Introducing Ahmad</a></li>
    
  </ul>
</div> -->


    <section class="main-content">
      
      <div class="TOC">
  <h1 id="table-of-contents">Table of Contents</h1>

  <ul class="TOC2">
    <li><a href="#content2">GRUs</a></li>
  </ul>
  <ul class="TOC3">
    <li><a href="#content3">LSTMs</a></li>
  </ul>
</div>

<hr />
<hr />

<ul>
  <li><a href="http://mlexplained.com/category/fromscratch/">Building an LSTM from Scratch in PyTorch</a></li>
  <li><a href="https://blog.echen.me/2017/05/30/exploring-lstms/">Exploring LSTMs, their Internals and How they Work (Blog!)</a></li>
</ul>

<h2 id="content2">GRUs</h2>

<ol>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents21">Gated Recurrent Units:</strong><br />
 <strong>Gated Recurrent Units (GRUs)</strong> are a class of modified (<em><strong>Gated</strong></em>) RNNs that allow them to combat the <em>vanishing gradient problem</em> by allowing them to capture more information/long range connections about the past (<em>memory</em>) and decide how strong each signal is.<br />
 <br /></p>
  </li>
  <li>
    <p id="lst-p"><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents22">Main Idea:</strong><br />
 Unlike <em>standard RNNs</em> which compute the hidden layer at the next time step directly first, <strong>GRUs</strong> computes two additional layers (<strong>gates</strong>) (Each with different weights):</p>
    <ul>
      <li><em><strong>Update Gate</strong></em>:
        <p>$$z_t = \sigma(W^{(z)}x_t + U^{(z)}h_{t-1})$$</p>
      </li>
      <li><em><strong>Reset Gate</strong></em>:
        <p>$$r_t = \sigma(W^{(r)}x_t + U^{(r)}h_{t-1})$$</p>
      </li>
    </ul>

    <p>The <strong>Update Gate</strong> and <strong>Reset Gate</strong> computed, allow us to more directly influence/manipulate what information do we care about (and want to store/keep) and what content we can ignore.</p>

    <p id="lst-p">We can view the actions of these gates from their respecting equations as:</p>
    <ul>
      <li><em><strong>New Memory Content</strong></em>:<br />
  at each hidden layer at a given time step, we compute some new memory content,<br />
  if the reset gate \(= \approx 0\), then this ignores previous memory, and only stores the new word information.
        <p>$$ \tilde{h}_ t = \tanh(Wx_t + r_t \odot Uh_{t-1})$$</p>
      </li>
      <li><em><strong>Final Memory</strong></em>:<br />
  the actual memory at a time step \(t\), combines the <em>Current</em> and <em>Previous time steps</em>,<br />
  if the <em>update gate</em> \(= \approx 0\), then this, again, ignores the <em>newly computed memory content</em>, and keeps the old memory it possessed.
        <p>$$h_ t = z_ t \odot h_ {t-1} + (1-z_t) \odot \tilde{h}_ t$$</p>
      </li>
    </ul>
  </li>
</ol>

<hr />

<h2 id="content3">Long Short-Term Memory</h2>

<ol>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents31">LSTM:</strong><br />
 The <strong>Long Short-Term Memory</strong> (LSTM) Network is a special case of the Recurrent Neural Network (RNN) that uses special gated units (a.k.a LSTM units) as building blocks for the layers of the RNN.</p>

    <p><strong>LSTM Equations:</strong></p>
    <p>
 $$\begin{align}
     f_{t}&amp;=\sigma_{g}\left(W_{f} x_{t}+U_{f} h_{t-1}+b_{f}\right) \\
     i_{t}&amp;=\sigma_{g}\left(W_{i} x_{t}+U_{i} h_{t-1}+b_{i}\right) \\
     o_{t}&amp;=\sigma_{g}\left(W_{o} x_{t}+U_{o} h_{t-1}+b_{o}\right) \\
     c_{t}&amp;=f_{t} \circ c_{t-1}+i_{t} \circ \sigma_{c}\left(W_{c} x_{t}+U_{c} h_{t-1}+b_{c}\right) \\
     h_{t}&amp;=o_{t} \circ \sigma_{h}\left(c_{t}\right)
 \end{align}$$  
 </p>
    <p>where:<br />
 \(\sigma_{g}\): sigmoid function.<br />
 \({\displaystyle \sigma_{c}}\): hyperbolic tangent function.<br />
 \({\displaystyle \sigma_{h}}\): hyperbolic tangent function or, as the peephole LSTM paper suggests, \({\displaystyle \sigma_{h}(x)=x}\).<br />
 <br /></p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents32">Architecture:</strong><br />
 The LSTM, usually, has four gates:
    <ul>
      <li><strong>Input Gate</strong>:<br />
  The input gate determines how much does the <em>current input vector (current cell)</em> matters    <br />
  It controls the extent to which a new value flows into the cell
        <p>$$i_t = \sigma(W^{(i)}x_t + U^{(i)}h_{t-1})$$</p>
      </li>
      <li><strong>Forget Gate</strong>:<br />
  Determines how much of the <em>past memory</em>, that we have kept, is still needed <br />
  It controls the extent to which a value remains in the cell
        <p>$$f_t = \sigma(W^{(f)}x_t + U^{(f)}h_{t-1})$$</p>
      </li>
      <li><strong>Output Gate</strong>: 
  Determines how much of the <em>current cell</em> matters for our <em>current prediction (i.e. passed to the sigmoid)</em><br />
  It controls the extent to which the value in the cell is used to compute the output activation of the LSTM unit
        <p>$$o_t = \sigma(W^{(o)}x_t + U^{(o)}h_{t-1})$$</p>
      </li>
      <li><strong>Memory Cell</strong>: 
  The memory cell is the cell that contains the <em>short-term memory</em> collected from each input
        <p>$$\begin{align}
  \tilde{c}_t &amp; = \tanh(W^{(c)}x_t + U^{(c)}h_{t-1}) &amp; \text{New Memory} \\
  c_t &amp; = f_t \odot c_{t-1} + i_t \odot \tilde{c}_ t &amp; \text{Final Memory}
  \end{align}$$</p>
        <p>The <strong>Final Hidden State</strong> is calculated as follows:</p>
        <p>$$h_t = o_t \odot \sigma(c_t)$$</p>
      </li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents33">Properties:</strong>
    <ul>
      <li><strong>Syntactic Invariance</strong>:<br />
  When one projects down the vectors from the <em>last time step hidden layer</em> (with PCA), one can observe the spatial localization of <em>syntacticly-similar sentences</em><br />
  <img src="/main_files/dl/nlp/9/5.png" alt="img" width="100%" /></li>
    </ul>
  </li>
</ol>

<p><strong>LSTMS:</strong></p>
<ul>
  <li>The core of the history/memory is captured in the <em>cell-state \(c_{n}\)</em> instead of the hidden state \(h_{n}\).</li>
  <li>(&amp;) <strong>Key Idea:</strong> The update to the cell-state \(c_{n}=c_{n-1}+\operatorname{tanh}\left(V\left[w_{n-1} ; h_{n-1}\right]+b_{c}\right)\)  here are <strong>additive</strong>. (differentiating a sum gives the identity) Making the gradient flow nicely through the sum. As opposed to the multiplicative updates to \(h_n\) in vanilla RNNs.
    <blockquote>
      <p>There is non-linear funcs applied to the history/context cell-state. It is composed of linear functions. Thus, avoids gradient shrinking.</p>
    </blockquote>
  </li>
  <li>In the recurrency of the LSTM the activation function is the identity function with a derivative of 1.0. So, the backpropagated gradient neither vanishes or explodes when passing through, but remains constant.</li>
  <li>By the selective read, write and forget mechanism (using the gating architecture) of LSTM, there exist at least one path, through which gradient can flow effectively from \(L\)  to \(\theta\). Hence no vanishing gradient.</li>
  <li>However, one must remember that, this is not the case for exploding gradient. It can be proved that, there <strong>can exist</strong> at-least one path, thorough which gradient can explode.</li>
  <li>LSTM decouples cell state (typically denoted by c) and hidden layer/output (typically denoted by h), and only do additive updates to c, which makes memories in c more stable. Thus the gradient flows through c is kept and hard to vanish (therefore the overall gradient is hard to vanish). However, other paths may cause gradient explosion.</li>
  <li>The Vanishing gradient solution for LSTM is known as <em>Constant Error Carousel</em>.</li>
  <li><a href="https://stats.stackexchange.com/questions/320919/why-can-rnns-with-lstm-units-also-suffer-from-exploding-gradients/339129#339129" value="show" onclick="iframePopA(event)"><strong>Why can RNNs with LSTM units also suffer from “exploding gradients”?</strong></a>
<a href="https://stats.stackexchange.com/questions/320919/why-can-rnns-with-lstm-units-also-suffer-from-exploding-gradients/339129#339129"></a>
    <div></div>
  </li>
  <li>
    <p><a href="https://www.cse.iitm.ac.in/~miteshk/CS7015/Slides/Teaching/pdf/Lecture15.pdf">Lecture on gradient flow paths through gates</a></p>
  </li>
  <li><a href="https://www.youtube.com/embed/eDUaRvMDs-s?start=775" value="show" onclick="iframePopA(event)"><strong>LSTMs (Lec Oxford)</strong></a>
<a href="https://www.youtube.com/embed/eDUaRvMDs-s?start=776"></a>
    <div></div>
  </li>
</ul>

<p><strong>Important Links:</strong><br />
<a href="https://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139">The unreasonable effectiveness of Character-level Language Models</a><br />
<a href="https://gist.github.com/karpathy/d4dee566867f8291f086">character-level language model with a Vanilla Recurrent Neural Network, in Python/numpy</a><br />
<a href="https://skillsmatter.com/skillscasts/6611-visualizing-and-understanding-recurrent-networks">Visualizing and Understanding Recurrent Networks - Karpathy Lec</a><br />
<a href="https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714">Cool LSTM Diagrams - blog</a><br />
<a href="https://www.youtube.com/watch?v=LHXXI4-IEns">Illustrated Guide to Recurrent Neural Networks: Understanding the Intuition</a><br />
<a href="https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/">Code LSTM in Python</a><br />
<a href="http://www.fit.vutbr.cz/~imikolov/rnnlm/thesis.pdf">Mikolov Thesis: STATISTICAL LANGUAGE MODELS BASED ON NEURAL NETWORKS</a></p>


      <footer class="site-footer">
    <!--   <span class="site-footer-owner"><a href="http://localhost:8889">Ahmad Badary</a> is maintained by <a href="https://ahmedbadary.github.io/">Ahmad Badary</a>.</span> -->
    
<!--  -->
    <div class="wrapper">
        <div class="footer-col-wrapper">
            <div class="footer-col footer-col-1">
            <span class="site-footer-owner"><a href="http://localhost:8889">Site</a> maintained by <a href="https://ahmedbadary.github.io/">Ahmad Badary</a>.</span>
    <span class="site-footer-credits">
        <p>
            &copy; 2017. All rights reserved.
        </p> 
    </span>
            </div>
            <div class="footer-col footer-col-2">
            <div><p>         </p></div>
            </div>
            <div class="footer-col footer-col-3">
                <ul class="social-media-list">
                    
                      <li>
                        <a href="https://github.com/AhmedBadary">
                          <i class="fa fa-github"></i> GitHub
                        </a>
                      </li>
                    
                    
                      <li>
                        <a href="https://linkedin.com/in/ahmad-badary-656098121/">
                          <i class="fa fa-linkedin"></i> LinkedIn
                        </a>
                      </li>
                    
                    
                      <li>
                        <a href="https://www.facebook.com/ahmed.thabet.94">
                          <i class="fa fa-facebook"></i> Facebook
                        </a>
                      </li>
                    
                </ul>
            </div>
        </div>
    </div>
<!--  -->
</footer>


    </section>

  </body>

<!-- Table of Content Script -->
<script type="text/javascript">
var bodyContents = $(".bodyContents1");
$("<ol>").addClass("TOC1ul").appendTo(".TOC1");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC1ul");
     });
// 
var bodyContents = $(".bodyContents2");
$("<ol>").addClass("TOC2ul").appendTo(".TOC2");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC2ul");
     });
// 
var bodyContents = $(".bodyContents3");
$("<ol>").addClass("TOC3ul").appendTo(".TOC3");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC3ul");
     });
//
var bodyContents = $(".bodyContents4");
$("<ol>").addClass("TOC4ul").appendTo(".TOC4");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC4ul");
     });
//
var bodyContents = $(".bodyContents5");
$("<ol>").addClass("TOC5ul").appendTo(".TOC5");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC5ul");
     });
//
var bodyContents = $(".bodyContents6");
$("<ol>").addClass("TOC6ul").appendTo(".TOC6");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC6ul");
     });
//
var bodyContents = $(".bodyContents7");
$("<ol>").addClass("TOC7ul").appendTo(".TOC7");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC7ul");
     });
//
var bodyContents = $(".bodyContents8");
$("<ol>").addClass("TOC8ul").appendTo(".TOC8");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC8ul");
     });
//
var bodyContents = $(".bodyContents9");
$("<ol>").addClass("TOC9ul").appendTo(".TOC9");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC9ul");
     });

</script>

<!-- VIDEO BUTTONS SCRIPT -->
<script type="text/javascript">
  function iframePopInject(event) {
    var $button = $(event.target);
    // console.log($button.parent().next());
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $figure = $("<div>").addClass("video_container");
        $iframe = $("<iframe>").appendTo($figure);
        $iframe.attr("src", $button.attr("src"));
        // $iframe.attr("frameborder", "0");
        $iframe.attr("allowfullscreen", "true");
        $iframe.css("padding", "4px 6px");
        $button.next().css("display", "block");
        $figure.appendTo($button.next());
        $button.text("Hide Video")
    } else {
        $button.attr('value', 'show');
        $button.next().html("");
        $button.text("Show Video")
    }
}
</script>

<!-- BUTTON TRY -->
<script type="text/javascript">
  function iframePopA(event) {
    event.preventDefault();
    var $a = $(event.target).parent();
    console.log($a);
    if ($a.attr('value') == 'show') {
        $a.attr('value', 'hide');
        $figure = $("<div>");
        $iframe = $("<iframe>").addClass("popup_website_container").appendTo($figure);
        $iframe.attr("src", $a.attr("href"));
        $iframe.attr("frameborder", "1");
        $iframe.attr("allowfullscreen", "true");
        $iframe.css("padding", "4px 6px");
        $a.next().css("display", "block");
        $figure.appendTo($a.next().next());
        // $a.text("Hide Content")
        $('html, body').animate({
            scrollTop: $a.offset().top
        }, 1000);
    } else {
        $a.attr('value', 'show');
        $a.next().next().html("");
        // $a.text("Show Content")
    }

    $a.next().css("display", "inline");
}
</script>


<!-- TEXT BUTTON SCRIPT - INJECT -->
<script type="text/javascript">
  function showTextPopInject(event) {
    var $button = $(event.target);
    var txt = $button.attr("input");
    console.log(txt);
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $p = $("<p>");
        $p.html(txt);
        $button.next().css("display", "block");
        $p.appendTo($button.next());
        $button.text("Hide Content")
    } else {
        $button.attr('value', 'show');
        $button.next().html("");
        $button.text("Show Content")
    }

}
</script>

<!-- TEXT BUTTON SCRIPT - HIDDEN / HIDE / SHOW / HIDE/SHOW -->
<script type="text/javascript">
  function showTextPopHide(event) {
    var $button = $(event.target);
    // var txt = $button.attr("input");
    var txt = $button.text();
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $button.next().removeAttr("hidden");
        $button.text(txt + " - Hide Content");
    } else {
        $button.attr('value', 'show');
        $button.next().attr("hidden", "");
        $button.text(txt.replace(" - Hide Content",""));
    }
}
</script>

<!-- TEXT BUTTON SCRIPT - HIDDEN / HIDE / SHOW / HIDE/SHOW -->
<script type="text/javascript">
  function showText_withParent_PopHide(event) {
    var $button = $(event.target);
    var $parent = $button.parent();
    var txt = $button.text();
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $parent.next().removeAttr("hidden");
        $button.text(txt + " - Hide Content");
    } else {
        $button.attr('value', 'show');
        $parent.next().attr("hidden", "");
        $button.text(txt.replace(" - Hide Content",""));
    }
}
</script>

<!-- Print / Printing / printme -->
<!-- <script type="text/javascript">
i = 0

for (var i = 1; i < 6; i++) {
    var bodyContents = $(".bodyContents" + i);
    $("<p>").addClass("TOC1ul")  .appendTo(".TOC1");
    bodyContents.each(function(index, element) {
        var paragraph = $(element);
        $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC1ul");
         });
} 
</script>
 -->
 
</html>

