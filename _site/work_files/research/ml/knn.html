<!DOCTYPE html>
<html lang="en-us">
  <head>
  <meta charset="UTF-8">
  <title>Ahmad Badary</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <link rel="stylesheet" href="/css/normalize.css">
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="/css/cayman.css">
  <link rel="stylesheet" href="/css/style.css">
  <link rel="shortcut icon" href="/main_files/favicon.ico" />
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="/css/customStyle.css">
  <title> » Ahmad Badary</title>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
</head>

  <body>
    <nav class="main-nav">
    <a href="https://ahmedbadary.github.io/" class="main-nav-logo">
        <img src="/main_files/logo.png">
    </a>
    <ul id="menu-main" class="main-nav-items">
        <li id="menu-item-1859" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-home menu-item-1859">
            <a href="/">Home</a>
        </li>
        <li id="menu-item-2869" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-2869">
            <a href="/work">Work</a>
        </li>
        <li id="menu-item-1892" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1892">
            <a href="/projects">Projects</a>
        </li>
        <li id="menu-item-1858" class="menu-item menu-item-type-post_type menu-item-object-page current_page_parent menu-item-1858">
            <a href="/blog">Blog</a>
        </li>
        <li id="menu-item-1862" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1862">
            <a href="/about">About</a>
        </li>
    </ul>
</nav>


<section class="page-header">
  <h1 class="project-name">KNN <br> K-Nearest Neighbor</h1>
  <h2 class="project-tagline"></h2>
  <a href="/#" class="btn">Home</a>
  <a href="/work" class="btn">Work-Space</a>
  <a href= /work_files/research/ml.html class="btn">Previous</a>
</section>

<!-- <div>
  <ul class="posts">
    
      <li><span>02 Jan 2014</span> &raquo; <a href="/2014/01/02/introducing-Ahmad/">Introducing Ahmad</a></li>
    
  </ul>
</div> -->


    <section class="main-content">
      
      <div class="TOC">
  <h1 id="table-of-contents">Table of Contents</h1>

  <ul class="TOC1">
    <li><a href="#content1">K-Nearest Neighbors (k-NN)</a></li>
  </ul>
</div>

<hr />
<hr />

<h2 id="content1">K-Nearest Neighbors (k-NN)</h2>

<ol>
  <li><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents11">KNN:</strong>  <br />
 <strong>KNN</strong> is a <em>non-parametric</em> method used for classification and regression.<br />
 It is based on the <a href="/work_files/research/dl/theory/dl_book_pt1#bodyContents32"><strong>Local Constancy (Smoothness) Prior</strong></a>, which states that “the function we learn should not change very much within a small region.”, for generalization.
    <ul>
      <li><button class="showText" value="show" onclick="showTextPopHide(event);">K-Means &amp; Local Constancy</button>
  <img src="/main_files/dl_book/7.png" alt="img" hidden="" /></li>
    </ul>

    <p><br /></p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents12">Structure:</strong><br />
 In both classification and regression, the input consists of the \(k\) closest training examples in the feature space. The output depends on whether k-NN is used for classification or regression:
    <ul>
      <li>In <strong>k-NN classification</strong>, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its \(k\) nearest neighbors (\(k\) is a positive integer, typically small). If \(k = 1\), then the object is simply assigned to the class of that single nearest neighbor.</li>
      <li>In <strong>k-NN regression</strong>, the output is the property value for the object. This value is the average of the values of \(k\) nearest neighbors.<br />
 <br /></li>
    </ul>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents13">Formal Description - Statistical Setting:</strong><br />
 Suppose we have pairs \({\displaystyle (X_{1},Y_{1}),(X_{2},Y_{2}),\dots ,(X_{n},Y_{n})}\) taking values in \({\displaystyle \mathbb {R} ^{d}\times \{1,2\}}\), where \(Y\) is the class label of \(X\), so that \({\displaystyle X|Y=r\sim P_{r}}\) for \({\displaystyle r=1,2}\) (and probability distributions \({\displaystyle P_{r}}\). Given some norm \({\displaystyle \|\cdot \|}\) on \({\displaystyle \mathbb {R} ^{d}}\) and a point \({\displaystyle x\in \mathbb {R} ^{d}}\), let \({\displaystyle (X_{(1)},Y_{(1)}),\dots ,(X_{(n)},Y_{(n)})}\) be a reordering of the training data such that \({\displaystyle \|X_{(1)}-x\|\leq \dots \leq \|X_{(n)}-x\|}\).<br />
 <br /></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents133">Choosing \(k\):</strong><br />
Nearest neighbors can produce very complex decision functions, and its behavior is highly dependent on the choice of \(k\):<br />
<img src="https://cdn.mathpix.com/snip/images/vPHUZyVTNj4bOgDpPeCA-_KUR6QvlArX_yCy0FVeeNY.original.fullsize.png" alt="img" width="50%" class="center-image" /></p>

    <p>Choosing \(k = 1\), we achieve an <em>optimal training error</em> of \(0\) because each training point will classify as itself, thus achieving \(100\%\) accuracy on itself.<br />
However, \(k = 1\) <strong>overfits</strong> to the training data, and is a terrible choice in the context of the bias-variance tradeoff.</p>

    <p>Increasing \(k\) leads to <em>increase in training error</em>, but a <em>decrease in testing error</em> and achieves <strong>better generalization</strong>.</p>

    <p>At one point, if \(k\) becomes <em>too large</em>, the algorithm will <strong>underfit</strong> the training data, and suffer from <strong>huge bias</strong>.</p>

    <p>In general, <span style="color: goldenrod">we select \(k\) using <strong>cross-validation</strong></span>.</p>

    <dl style="margin-top: 0; font-size: 72%">
      <dt><img src="https://cdn.mathpix.com/snip/images/XaIAJQLphKue6B56LLdXoXM1UMsgVyKlXKkVeW1kjB0.original.fullsize.png" alt="img" width="50%" class="center-image" /></dt>
      <dd>
\[\text{Training and Test Errors as a function of } k \:\:\:\:\:\:\:\:\:\:\:\:\]
      </dd>
    </dl>

    <p><br /></p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents144">Bias-Variance Decomposition of k-NN:</strong><br />
<button class="showText" value="show" onclick="showTextPopHide(event);">PDF (189)</button>
    <iframe hidden="" src="/main_files/ml/knn/knn_bias_var.pdf" frameborder="0" height="840" width="646" title="Layer Normalization" scrolling="auto"></iframe>

    <p><br /></p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents14">Properties:</strong>
    <ul>
      <li><strong>Computational Complexity</strong>:
        <ul>
          <li>We require \(\mathcal{O}(n)\) space to store a training set of size \(n\). There is no runtime cost during training if we do not use specialized data structures to store the data.<br />
  However, predictions take \(\mathcal{O}(n)\) time, which is costly.</li>
          <li>There has been research into <strong>Approximate Nearest Neighbors (ANN)</strong> procedures that quickly find an approximation for the nearest neighbor - some common ANN methods are <em><strong>Locality-Sensitive Hashing</strong></em> and algorithms that perform dimensionality reduction via <em><strong>randomized (Johnson-Lindenstrauss) distance-preserving projections</strong></em>.</li>
          <li>k-NN is a type of <em>instance-based learning</em>, or <em>“lazy learning”</em>, where the function is only approximated locally and all computation is deferred until classification.</li>
        </ul>
      </li>
      <li><strong>Flexibility</strong>:
        <ul>
          <li>When \(k&gt;1,\) k-NN can be modified to output predicted probabilities \(P(Y \vert X)\) by defining \(P(Y \vert X)\) as the proportion of nearest neighbors to \(X\) in the training set that have class \(Y\).</li>
          <li>k-NN can also be adapted for regression — instead of taking the majority vote, take the average of the \(y\) values for the nearest neighbors.</li>
          <li>k-NN can learn very complicated, <strong>non-linear</strong> decision boundaries (highly influenced by choice of \(k\)).</li>
        </ul>
      </li>
      <li><strong>Non-Parametric(ity)</strong>:<br />
  k-NN is a <strong>non-parametric method</strong>, which means that the number of parameters in the model grows with \(n\), the number of training points. This is as opposed to parametric methods, for which the number of parameters is independent of \(n\).</li>
      <li><strong>High-dimensional Behavior</strong>:
        <ul>
          <li>k-NN does NOT behave well in high dimensions.<br />
  As the <em>dimension increases</em>, <em>data points drift farther apart</em>, so even the nearest neighbor to a point will tend to be very far away.</li>
          <li>It is sensitive to the local structure of the data (in any/all dimension/s).</li>
        </ul>
      </li>
      <li><strong>Theoretical Guarantees/Properties</strong>:<br />
  <strong>\(1\)-NN</strong> has impressive theoretical guarantees for such a simple method:
        <ul>
          <li><em>Cover and Hart, 1967</em> prove that <span style="color: goldenrod">as the number of training samples \(n\) approaches infinity, the expected prediction error for \(1-\mathrm{NN}\) is upper bounded by \(2 \epsilon^{*}\), where \(\epsilon^{*}\) is the <strong>Bayes (optimal) error</strong></span>.</li>
          <li><em>Fix and Hodges, 1951</em> prove that <span style="color: goldenrod">as \(n\) and \(k\) approach infinity and if \(\frac{k}{n} \rightarrow 0\), then the \(k\) nearest neighbor error approaches the <em><strong>Bayes error</strong></em></span>.<br />
 <br /></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents15">Algorithm and Computational Complexity:</strong><br />
 <strong style="color: red">Training:</strong>
    <ul>
      <li><strong>Algorithm:</strong> To train this classifier, we simply store our training data for future reference.<br />
  Sometimes we store the data in a specialized structure called a <em><strong>k-d tree</strong></em>. This data structure usually allows for faster (average-case \(\mathcal{O}(\log n)\)) nearest neighbors queries.
        <blockquote>
          <p>For this reason, k-NN is sometimes referred to as <em><strong>“lazy learning”</strong></em>.</p>
        </blockquote>
      </li>
      <li><strong>Complexity</strong>: \(\:\:\:\:\mathcal{O}(1)\)</li>
    </ul>

    <p id="lst-p"><strong style="color: red">Prediction:</strong></p>
    <ul>
      <li><strong>Algorithm</strong>:
        <ol>
          <li>Compute the \(k\) closest training data points (<em>“nearest neighbors”</em>) to input point \(\boldsymbol{z}\).<br />
 “Closeness” is quantified using some metric; e.g. <strong>Euclidean distance</strong>.</li>
          <li><strong>Assignment Stage:</strong>
            <ul>
              <li><strong>Classification</strong>: Find the most common class \(y\) among these \(k\) neighbors and classify \(\boldsymbol{z}\) as \(y\) (<strong>majority vote</strong>)</li>
              <li><strong>Regression</strong>: Take the <strong>average</strong> label of the \(k\) nearest points.</li>
            </ul>
          </li>
        </ol>
      </li>
      <li><strong>Complexity</strong>: \(\:\:\:\:\mathcal{O}(N)\) <br />
 <br /></li>
    </ul>

    <p id="lst-p"><strong style="color: red">Notes:</strong></p>
    <ul>
      <li>We choose odd \(k\) for <em>binary classification</em> to break symmetry of majority vote<br />
 <br /></li>
    </ul>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents16">Behavior in High-Dimensional Space - Curse of Dimensionality:</strong><br />
 As mentioned, k-NN does NOT perform well in high-dimensional space. This is due to the “Curse of Dimensionality”.</p>

    <p><strong style="color: red" id="bodyContents16_cod">Curse of Dimensionality (CoD):</strong><br />
 To understand CoD, we first need to understand the properties of metric spaces. In high-dimensional spaces, much of our low-dimensional intuition breaks down:</p>

    <p><strong>Geometry of High-Dimensional Space:</strong><br />
 Consider a ball in \(\mathbb{R}^d\) centered at the origin with radius \(r\), and suppose we have another ball of radius \(r - \epsilon\) centered at the origin. In low dimensions, we can visually see that <em>much of the volume of the outer ball is also in the inner ball</em>.<br />
 In general, <span style="color: goldenrod">the volume of the outer ball is proportional to \(r^{d}\)</span>, while <span style="color: goldenrod">the volume of the inner ball is proportional to \((r-\epsilon)^{d}\)</span>.<br />
 Thus the <strong>ratio of the volume of the inner ball to that of the outer ball</strong> is:</p>
    <p>$$\frac{(r-\epsilon)^{d}}{r^{d}}=\left(1-\frac{\epsilon}{r}\right)^{d} \approx e^{-\epsilon d / r} \underset{d \rightarrow \infty}{\longrightarrow} 0$$</p>
    <p>Hence as \(d\) gets large, most of the volume of the outer ball is concentrated in the annular region \(\{x : r-\epsilon &lt; x &lt; r\}\) instead of the inner ball.<br />
 <img src="https://cdn.mathpix.com/snip/images/7N0Mv-hf0RhjKeHDEzLJuQuEtI156Jq8JjqeaD96PB0.original.fullsize.png" alt="img" width="70%" class="center-image" /></p>

    <p><strong>Concentration of Measure:</strong><br />
 High dimensions also make Gaussian distributions behave counter-intuitively. Suppose \(X \sim\) \(\mathcal{N}\left(0, \sigma^{2} I\right)\). If \(X_{i}\) are the components of \(X\) and \(R\) is the distance from \(X\) to the origin, then \(R^{2}=\sum_{i=1}^{d} X_{i}^{2}\). We have \(\mathbb{E}\left[R^{2}\right]=d \sigma^{2},\) so in expectation a random Gaussian will actually be reasonably far from the origin. If \(\sigma=1,\) then \(R^{2}\) is distributed <em><strong>chi-squared</strong></em> with <em>\(d\) degrees of freedom</em>.<br />
 One can show that in high dimensions, with high probability \(1-\mathcal{O}\left(e^{-d^{\epsilon}}\right)\), this multivariate Gaussian will lie within the annular region \(\left\{X :\left|R^{2}-\mathbb{E}\left[R^{2}\right]\right| \leq d^{1 / 2+\epsilon}\right\}\) where \(\mathbb{E}\left[R^{2}\right]=d \sigma^{2}\) (one possible approach is to note that as \(d \rightarrow \infty,\) the chi-squared approaches a Gaussian by the <strong>CLT</strong>, and use a <strong>Chernoff bound</strong> to show exponential decay). This phenomenon is known as <strong>Concentration of Measure</strong>.</p>

    <p>Without resorting to more complicated inequalities, we can show a simple, weaker result:<br />
 \(\bf{\text{Theorem:}}\) \(\text{If } X_{i} \sim \mathcal{N}\left(0, \sigma^{2}\right), i=1, \ldots, d \text{  are independent and } R^{2}=\sum_{i=1}^{d} X_{i}^{2}, \text{ then for every } \epsilon&gt;0, \\ 
  \text{the following holds: }\)</p>
    <p>$$\lim_{d \rightarrow \infty} P\left(\left|R^{2}-\mathbb{E}\left[R^{2}\right]\right| \geq d^{\frac{1}{2}+\epsilon}\right)=0$$</p>
    <p>Thus in the limit, the <strong style="color: goldenrod">squared radius is concentrated about its mean</strong>.</p>

    <p><button class="showText" value="show" onclick="showTextPopHide(event);">Proof.</button>
 <img src="https://cdn.mathpix.com/snip/images/2sljiV63a9o4RE6sUbYN4yAl8yV0rgz0DCx15z1yTBM.original.fullsize.png" alt="img" width="100%" hidden="" /></p>

    <p>Thus a <span style="color: goldenrod">random Gaussian will lie within a thin annular region away from the origin in high dimensions with high probability</span>, even though <em>the mode of the Gaussian bell curve is at the origin</em>. This illustrates the phenomenon in <em>high dimensions</em> where <em><strong>random data is spread very far apart</strong></em>.</p>

    <p>The k-NN classifier was conceived on the principle that <em>nearby points should be of the same class</em> - however, in high dimensions, <em>even the nearest neighbors</em> that we have to a random test point <em>will</em> tend to <em>be <strong>far</strong> away</em>, so this principle is <em>no longer useful</em>.<br />
 <br /></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents17">Improving k-NN:</strong><br />
 <strong style="color: red">(1) Obtain More Training Data:</strong><br />
 More training data allows us to counter-act the sparsity in high-dimensional space.</p>

    <p><strong style="color: red">(2) Dimensionality Reduction - Feature Selection and Feature Projection:</strong><br />
 Reduce the dimensionality of the features and/or pick better features. The best way to counteract the curse of dimensionality.</p>

    <p id="lst-p"><strong style="color: red">(3) Different Choices of Metrics/Distance Functions:</strong><br />
 We can modify the distance function. E.g.</p>
    <ul>
      <li>The family of <strong>Minkowski Distances</strong> that are induced by the \(L^p\) norms:
        <p>$$D_{p}(\mathbf{x}, \mathbf{z})=\left(\sum_{i=1}^{d}\left|x_{i}-z_{i}\right|^{p}\right)^{\frac{1}{p}}$$</p>
        <blockquote>
          <p>Without preprocessing the data, \(1-\mathrm{NN}\) with the \(L^{3}\) distance outperforms \(1-\mathrm{NN}\) with \(L^{2}\) on MNIST.</p>
        </blockquote>
      </li>
      <li>We can, also, use <strong>kernels</strong> to compute distances in a <span style="color: goldenrod"><em>different</em> feature space</span>.<br />
  For example, if \(k\) is a kernel with associated feature map \(\Phi\) and we want to compute the Euclidean distance from \(\Phi(x)\) to \(\Phi(z)\), then we have:
        <p>$$\begin{aligned}\|\Phi(\mathbf{x})-\Phi(\mathbf{z})\|_ {2}^{2} &amp;=\Phi(\mathbf{x})^{\top} \Phi(\mathbf{x})-2 \Phi(\mathbf{x})^{\top} \Phi(\mathbf{z})+\Phi(\mathbf{z})^{\top} \Phi(\mathbf{z}) \\ &amp;=k(\mathbf{x}, \mathbf{x})-2 k(\mathbf{x}, \mathbf{z})+k(\mathbf{z}, \mathbf{z}) \end{aligned}$$</p>
        <p>Thus if we define \(D(\mathrm{x}, \mathrm{z})=\sqrt{k(\mathrm{x}, \mathrm{x})-2 k(\mathrm{x}, \mathrm{z})+k(\mathrm{z}, \mathrm{z})}\) , then we can perform Euclidean nearest neighbors in \(\mathrm{\Phi}\)-space without explicitly representing \(\Phi\) by using the kernelized distance function \(D\).</p>
      </li>
    </ul>

    <p><br /></p>
  </li>
</ol>


      <footer class="site-footer">
    <!--   <span class="site-footer-owner"><a href="http://localhost:8889">Ahmad Badary</a> is maintained by <a href="http://ahmedbadary.ml">Ahmad Badary</a>.</span> -->
    
<!--  -->
    <div class="wrapper">
        <div class="footer-col-wrapper">
            <div class="footer-col footer-col-1">
            <span class="site-footer-owner"><a href="http://localhost:8889">Site</a> maintained by <a href="http://ahmedbadary.ml">Ahmad Badary</a>.</span>
    <span class="site-footer-credits">
        <p>
            &copy; 2017. All rights reserved.
        </p> 
    </span>
            </div>
            <div class="footer-col footer-col-2">
            <div><p>         </p></div>
            </div>
            <div class="footer-col footer-col-3">
                <ul class="social-media-list">
                    
                      <li>
                        <a href="https://github.com/AhmedBadary">
                          <i class="fa fa-github"></i> GitHub
                        </a>
                      </li>
                    
                    
                      <li>
                        <a href="https://linkedin.com/in/ahmad-badary-656098121/">
                          <i class="fa fa-linkedin"></i> LinkedIn
                        </a>
                      </li>
                    
                    
                      <li>
                        <a href="https://www.facebook.com/ahmed.thabet.94">
                          <i class="fa fa-facebook"></i> Facebook
                        </a>
                      </li>
                    
                </ul>
            </div>
        </div>
    </div>
<!--  -->
</footer>


    </section>

  </body>

<!-- Table of Content Script -->
<script type="text/javascript">
var bodyContents = $(".bodyContents1");
$("<ol>").addClass("TOC1ul").appendTo(".TOC1");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC1ul");
     });
// 
var bodyContents = $(".bodyContents2");
$("<ol>").addClass("TOC2ul").appendTo(".TOC2");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC2ul");
     });
// 
var bodyContents = $(".bodyContents3");
$("<ol>").addClass("TOC3ul").appendTo(".TOC3");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC3ul");
     });
//
var bodyContents = $(".bodyContents4");
$("<ol>").addClass("TOC4ul").appendTo(".TOC4");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC4ul");
     });
//
var bodyContents = $(".bodyContents5");
$("<ol>").addClass("TOC5ul").appendTo(".TOC5");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC5ul");
     });
//
var bodyContents = $(".bodyContents6");
$("<ol>").addClass("TOC6ul").appendTo(".TOC6");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC6ul");
     });
//
var bodyContents = $(".bodyContents7");
$("<ol>").addClass("TOC7ul").appendTo(".TOC7");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC7ul");
     });
//
var bodyContents = $(".bodyContents8");
$("<ol>").addClass("TOC8ul").appendTo(".TOC8");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC8ul");
     });
//
var bodyContents = $(".bodyContents9");
$("<ol>").addClass("TOC9ul").appendTo(".TOC9");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC9ul");
     });

</script>

<!-- VIDEO BUTTONS SCRIPT -->
<script type="text/javascript">
  function iframePopInject(event) {
    var $button = $(event.target);
    // console.log($button.parent().next());
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $figure = $("<div>").addClass("video_container");
        $iframe = $("<iframe>").appendTo($figure);
        $iframe.attr("src", $button.attr("src"));
        // $iframe.attr("frameborder", "0");
        $iframe.attr("allowfullscreen", "true");
        $iframe.css("padding", "4px 6px");
        $button.next().css("display", "block");
        $figure.appendTo($button.next());
        $button.text("Hide Video")
    } else {
        $button.attr('value', 'show');
        $button.next().html("");
        $button.text("Show Video")
    }
}
</script>

<!-- BUTTON TRY -->
<script type="text/javascript">
  function iframePopA(event) {
    event.preventDefault();
    var $a = $(event.target).parent();
    console.log($a);
    if ($a.attr('value') == 'show') {
        $a.attr('value', 'hide');
        $figure = $("<div>");
        $iframe = $("<iframe>").addClass("popup_website_container").appendTo($figure);
        $iframe.attr("src", $a.attr("href"));
        $iframe.attr("frameborder", "1");
        $iframe.attr("allowfullscreen", "true");
        $iframe.css("padding", "4px 6px");
        $a.next().css("display", "block");
        $figure.appendTo($a.next().next());
        // $a.text("Hide Content")
        $('html, body').animate({
            scrollTop: $a.offset().top
        }, 1000);
    } else {
        $a.attr('value', 'show');
        $a.next().next().html("");
        // $a.text("Show Content")
    }

    $a.next().css("display", "inline");
}
</script>


<!-- TEXT BUTTON SCRIPT - INJECT -->
<script type="text/javascript">
  function showTextPopInject(event) {
    var $button = $(event.target);
    var txt = $button.attr("input");
    console.log(txt);
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $p = $("<p>");
        $p.html(txt);
        $button.next().css("display", "block");
        $p.appendTo($button.next());
        $button.text("Hide Content")
    } else {
        $button.attr('value', 'show');
        $button.next().html("");
        $button.text("Show Content")
    }

}
</script>

<!-- TEXT BUTTON SCRIPT - HIDDEN / HIDE / SHOW / HIDE/SHOW -->
<script type="text/javascript">
  function showTextPopHide(event) {
    var $button = $(event.target);
    // var txt = $button.attr("input");
    var txt = $button.text();
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $button.next().removeAttr("hidden");
        $button.text(txt + " - Hide Content");
    } else {
        $button.attr('value', 'show');
        $button.next().attr("hidden", "");
        $button.text(txt.replace(" - Hide Content",""));
    }
}
</script>

<!-- TEXT BUTTON SCRIPT - HIDDEN / HIDE / SHOW / HIDE/SHOW -->
<script type="text/javascript">
  function showText_withParent_PopHide(event) {
    var $button = $(event.target);
    var $parent = $button.parent();
    var txt = $button.text();
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $parent.next().removeAttr("hidden");
        $button.text(txt + " - Hide Content");
    } else {
        $button.attr('value', 'show');
        $parent.next().attr("hidden", "");
        $button.text(txt.replace(" - Hide Content",""));
    }
}
</script>

<!-- Print / Printing / printme -->
<!-- <script type="text/javascript">
i = 0

for (var i = 1; i < 6; i++) {
    var bodyContents = $(".bodyContents" + i);
    $("<p>").addClass("TOC1ul")  .appendTo(".TOC1");
    bodyContents.each(function(index, element) {
        var paragraph = $(element);
        $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC1ul");
         });
} 
</script>
 -->
 
</html>

