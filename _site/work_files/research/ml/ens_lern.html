<!DOCTYPE html>
<html lang="en-us">
  <head>
  <meta charset="UTF-8">
  <title>Ahmad Badary</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <link rel="stylesheet" href="/css/normalize.css">
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="/css/cayman.css">
  <link rel="stylesheet" href="/css/style.css">
  <link rel="shortcut icon" href="/main_files/favicon.ico" />
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="/css/customStyle.css">
  <title> » Ahmad Badary</title>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
</head>

  <body>
    <nav class="main-nav">
    <a href="https://ahmedbadary.github.io/" class="main-nav-logo">
        <img src="/main_files/logo.png">
    </a>
    <ul id="menu-main" class="main-nav-items">
        <li id="menu-item-1859" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-home menu-item-1859">
            <a href="/">Home</a>
        </li>
        <li id="menu-item-2869" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-2869">
            <a href="/work">Work</a>
        </li>
        <li id="menu-item-1892" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1892">
            <a href="/projects">Projects</a>
        </li>
        <li id="menu-item-1858" class="menu-item menu-item-type-post_type menu-item-object-page current_page_parent menu-item-1858">
            <a href="/blog">Blog</a>
        </li>
        <li id="menu-item-1862" class="menu-item menu-item-type-post_type menu-item-object-page menu-item-1862">
            <a href="/about">About</a>
        </li>
    </ul>
</nav>


<section class="page-header">
  <h1 class="project-name">Ensemble Learning - Aggregating</h1>
  <h2 class="project-tagline"></h2>
  <a href="/#" class="btn">Home</a>
  <a href="/work" class="btn">Work-Space</a>
  <a href= /work_files/research/dl/ml.html class="btn">Previous</a>
</section>

<!-- <div>
  <ul class="posts">
    
      <li><span>02 Jan 2014</span> &raquo; <a href="/2014/01/02/introducing-Ahmad/">Introducing Ahmad</a></li>
    
  </ul>
</div> -->


    <section class="main-content">
      
      <div class="TOC">
  <h1 id="table-of-contents">Table of Contents</h1>

  <ul class="TOC1">
    <li><a href="#content1">Ensemble Learning</a></li>
  </ul>
  <ul class="TOC2">
    <li><a href="#content2">Bayes optimal classifier (Theoretical)</a></li>
  </ul>
  <ul class="TOC3">
    <li><a href="#content3">Bootstrap Aggregating (Bagging)</a></li>
  </ul>
  <ul class="TOC4">
    <li><a href="#content4">Boosting</a></li>
  </ul>
  <ul class="TOC5">
    <li><a href="#content5">Stacking</a></li>
  </ul>
  <p><!-- * [SIXTH](#content6)
  {: .TOC6} --></p>
</div>

<hr />
<hr />

<ul>
  <li><a href="https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205">Ensemble methods: bagging, boosting and stacking (in detail!)</a></li>
  <li><a href="http://www.cs.cornell.edu/courses/cs578/2005fa/CS578.bagging.boosting.lecture.pdf">Bagging Boosting (Lec Slides)</a></li>
  <li><a href="https://www.youtube.com/watch?v=ihLwJPHkMRY&amp;t=2692">Aggregation Methods (Lec! - Caltech)</a></li>
  <li><a href="https://www.youtube.com/watch?v=7kAlBa7yhDM">Aggregation in the context of Deep Learning and Representation Learning (Lec - Hinton)</a></li>
  <li><a href="https://www.youtube.com/watch?v=JacgCGtxoj0&amp;list=PLiPvV5TNogxKKwvKb1RKwkq2hm7ZvpHz0&amp;index=61">Why it helps to combine models (hinton Lec!)</a></li>
</ul>

<h2 id="content1">Ensemble Learning</h2>

<ol>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents11">Ensemble Learning:</strong><br />
 In machine learning, <strong>Ensemble Learning</strong> is a set of <strong>ensemble methods</strong> that use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.<br />
 <br /></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents13">Ensemble Theory:</strong><br />
 An <em>ensemble</em> is itself a <strong>supervised learning algorithm</strong>, because it can be trained and then used to make predictions. The trained ensemble, therefore, represents a single hypothesis. This hypothesis, however, is not necessarily contained within the hypothesis space of the models from which it is built. Thus, ensembles can be shown to have more flexibility in the functions they can represent. This flexibility can, in theory, enable them to over-fit the training data more than a single model would, but in practice, some ensemble techniques (especially <strong>bagging</strong>) tend to reduce problems related to over-fitting of the training data.</p>

    <p>Empirically, ensembles tend to yield better results when there is a significant diversity among the models. Many ensemble methods, therefore, seek to promote diversity among the models they combine.<br />
 Although perhaps non-intuitive, more random algorithms (like random decision trees) can be used to produce a stronger ensemble than very deliberate algorithms (like entropy-reducing decision trees).<br />
 Using a variety of strong learning algorithms, however, has been shown to be more effective than using techniques that attempt to dumb-down the models in order to promote diversity.<br />
 <br /></p>

    <ul>
      <li>In any network, the bias can be reduced at the cost of increased variance</li>
      <li>In a group of networks, the variance can be reduced at no cost to bias</li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents14">Types of Ensembles:</strong>
    <ul>
      <li>Bayes optimal classifier (Theoretical)</li>
      <li>Bootstrap aggregating (bagging)</li>
      <li>Boosting</li>
      <li>Bayesian parameter averaging</li>
      <li>Bayesian model combination</li>
      <li>Bucket of models</li>
      <li>Stacking
 <br /></li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents15">Applications:</strong>
    <ul>
      <li>Remote sensing
        <ul>
          <li>Land cover mapping</li>
          <li>Change detection</li>
        </ul>
      </li>
      <li>Computer security
        <ul>
          <li>Distributed denial of service</li>
          <li>Malware Detection</li>
          <li>Intrusion detection</li>
        </ul>
      </li>
      <li>Face recognition</li>
      <li>Emotion recognition</li>
      <li>Fraud detection</li>
      <li>Financial decision-making</li>
      <li>Medicine
 <br /></li>
    </ul>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents17">Ensemble Size:</strong><br />
 It is an important problem that hasn’t been well studied/addressed.</p>

    <p>More recently, a <a href="https://static.aminer.org/pdf/fa/cikm2016/shp1026-r.-bonabA.pdf">theoretical framework</a> suggested that there is an ideal number of component classifiers for an ensemble such that having more or less than this number of classifiers would deteriorate the accuracy. It is called <strong>“the law of diminishing returns in ensemble construction”</strong>. Their theoretical framework shows that <a href="https://arxiv.org/pdf/1709.02925.pdf">using the same number of independent component classifiers as class labels gives the highest accuracy</a>.<br />
 <br /></p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents1" id="bodyContents18">Notes:</strong>
    <ul>
      <li>Averaging models <strong>increases capacity</strong>.</li>
    </ul>

    <p><strong style="color: red">Ensemble Averaging:</strong><br />
 Relies</p>
  </li>
</ol>

<hr />

<h2 id="content2">Bayes optimal classifier (Theoretical)</h2>

<ol>
  <li><strong style="color: SteelBlue" class="bodyContents2" id="bodyContents21">Bayes Optimal Classifier:</strong><br />
 <a href="https://en.wikipedia.org/wiki/Ensemble_learning#Bayes_optimal_classifier">Bayes Optimal Classifier</a><br />
 <br /></li>
</ol>

<hr />

<h2 id="content3">Bootstrap Aggregating (Bagging)</h2>

<ol>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents31">Bootstrap Aggregating (Bagging):</strong><br />
 <strong>Bootstrap Aggregating (Bagging)</strong> is an ensemble meta-algorithm designed to improve the stability and accuracy of ml algorithms. It is designed to <strong>reduce variance</strong> and help to <strong>avoid overfitting</strong>.</p>

    <p>It is applicable to both <strong>classification</strong> and <strong>regression</strong> problems.</p>

    <p>Although it is usually applied to <strong>decision tree methods</strong>, it can be used with any type of method. Bagging is a special case of the model averaging approach.<br />
 <br /></p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents32">Bootstrapping:</strong><br />
 <strong>Bootstrapping</strong> is a sampling technique. From a set \(D\) of \(n\) sample points, it constructs \(m\) subsets \(D_i\), each of size \(n'\), by sampling from \(D\) <strong style="color: goldenrod">uniformly</strong> and <strong style="color: goldenrod">with replacement</strong>.
    <ul>
      <li>By sampling with replacement, <strong>some observations may be repeated</strong> in each \({\displaystyle D_{i}}\).</li>
      <li>If \(n'=n\), then for large \(n\) the set \(D_{i}\) is expected to have the fraction (\(1 - 1/e\)) (\(\approx 63.2\%\)) of the unique examples of \(D\), the rest being duplicates.</li>
    </ul>

    <p>The point of sampling with replacement is to make the re-sampling truly random. If done without replacement, the samples drawn will be dependent on the previous ones and thus not be random.<br />
 <br /></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents33">Aggregating:</strong><br />
 The predictions from the above models are aggregated to make a final combined prediction. This aggregation can be done on the basis of predictions made or the probability of the predictions made by the bootstrapped individual models.<br />
 <br /></p>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents34">The Algorithm:</strong><br />
 Bagging uses multiple weak models and aggregates the predictions from each of them to get the final prediction. The weak models should be such that each specialize in a particular part of the feature space thus enabling us to leverage predictions from each model to maximum use. As suggested by the name, it consists of two parts, bootstrapping and aggregation.</p>

    <ul>
      <li>Given a set \(D\) of \(n\) sample points,</li>
      <li><strong>Bootstrapping:</strong> Construct \(m\) <strong>bootstap samples</strong> (subsets) \(D_i\).</li>
      <li>Fit \(m\) models using the \(m\) bootstrap samples</li>
      <li><strong>Aggregating:</strong> Combine the models by:
        <ul>
          <li><strong>Regression</strong>: <span style="color: goldenrod">Averaging</span></li>
          <li><strong>Classification</strong>: <span style="color: goldenrod">Voting</span><br />
 <br /></li>
        </ul>
      </li>
    </ul>

    <p><a href="https://people.eecs.berkeley.edu/~jrs/189/lec/16.pdf">Bagging and Random Forests (Shewchuk)</a></p>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents35">Advantages:</strong>
    <ul>
      <li>Improves <strong>“unstable” procedures</strong></li>
      <li>Reduces variance \(\rightarrow\) helps avoid overfitting</li>
      <li>Ensemble models can be used to capture the linear as well as the non-linear relationships in the data.This can be accomplished by using 2 different models and forming an ensemble of the two.<br />
 <br /></li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents36">Disadvantages:</strong>
    <ul>
      <li>On the other hand, it can mildly degrade the performance of “stable” methods such as K-NN</li>
      <li>It causes a Reduction in the interpretability of the model</li>
      <li>Prone to high bias if not modeled properly</li>
      <li>Though improves accuracy, it is computationally expensive and hard to design:<br />
  It is not good for real time applications.  <br />
 <br /></li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents3" id="bodyContents37">Examples (bagging algorithms):</strong>
    <ul>
      <li><strong>Random Forests:</strong> is a bagging algorithm that further reduces variance by selecting a <strong>subset of features</strong>
        <ol>
          <li>Suppose there are N observations and M features. A sample from observation is selected randomly with replacement(Bootstrapping).</li>
          <li>A subset of features are selected to create a model with sample of observations and subset of features.</li>
          <li>Feature from the subset is selected which gives the best split on the training data.(Visit my blog on Decision Tree to know more of best split)</li>
          <li>This is repeated to create many models and every model is trained in parallel
  Prediction is given based on the aggregation of predictions from all the models.
 <br /></li>
        </ol>
      </li>
    </ul>
  </li>
</ol>

<hr />

<h2 id="content4">Boosting</h2>

<ol>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents4" id="bodyContents41">Boosting:</strong><br />
 <strong>Boosting</strong> is an ensemble meta-algorithm for primarily <strong>reducing bias</strong>, but <em>also variance</em> in supervised learning. It belongs to a family of machine learning algorithms that <strong>convert weak learners to strong ones</strong>.</p>

    <p>It is an <strong>iterative</strong> technique which adjusts the weight of an observation based on the last classification. If an observation was classified incorrectly, it tries to increase the weight of this observation and vice versa. Thus, <span style="color: goldenrod">future weak learners focus more on the examples that previous weak learners misclassified</span>.</p>

    <p>Boosting in general decreases the bias error and builds strong predictive models. However, they may sometimes over fit on the training data.</p>

    <p>Boosting <em><strong>increases the capacity</strong></em>.</p>

    <p><strong style="color: red">Summary</strong><br />
 <strong>Boosting</strong>: create different hypothesis \(h_i\)s sequentially + make each new hypothesis <strong>decorrelated</strong> with previous hypothesis.</p>
    <ul>
      <li>Assumes that this will be combined/ensembled</li>
      <li>Ensures that each new model/hypothesis will give a different/independent output<br />
 <br /></li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents4" id="bodyContents42">Motivation - “The Hypothesis Boosting Problem”:</strong><br />
 Boosting is based on a question posed by <em>Kearns</em> and <em>Valiant</em> (1988):
    <blockquote>
      <p><span style="color: blue">“Can a set of <strong>weak learners</strong> create a <em>single</em> <strong>strong learner</strong>?”:</span></p>
    </blockquote>

    <p>This question was formalized as a hypothesis called “The Hypothesis Boosting Problem”.</p>

    <p><strong style="color: red">The Hypothesis Boosting Problem:</strong><br />
 Informally, [the hypothesis boosting] problem asks whether an efficient learning algorithm […] that outputs a hypothesis whose performance is only slightly better than random guessing [i.e. a weak learner] implies the existence of an efficient algorithm that outputs a hypothesis of arbitrary accuracy [i.e. a strong learner].</p>

    <ul>
      <li>A <strong>weak learner</strong> is defined to be a classifier that is only slightly correlated with the true classification (it can label examples better than random guessing).</li>
      <li>A <strong>strong learner</strong> is a classifier that is arbitrarily well-correlated with the true classification.<br />
 <br /></li>
    </ul>

    <p><strong style="color: red">Countering BAgging Limitations:</strong><br />
 Bagging suffered from some limitations; namely, that the models can be dependent/correlated which cause the voting to be trapped in the wrong hypothesis of the weak learners. This  motivated the intuition behind Boosting:</p>
    <ul>
      <li>Instead of training parallel models, one needs to train models sequentially &amp;</li>
      <li>Each model should focus on where the previous classifier performed poorly<br />
 <br /></li>
    </ul>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents4" id="bodyContents43">Boosting Theory and Convexity:</strong><br />
 Only algorithms that are provable boosting algorithms in the <strong>probably approximately correct (PAC) learning</strong> formulation can accurately be called boosting algorithms. Other algorithms that are similar in spirit to boosting algorithms are sometimes called <strong>“leveraging algorithms”</strong>, although they are also sometimes incorrectly called boosting algorithms.<br />
 <br /></p>

    <p><strong style="color: red">Convexity:</strong><br />
 Boosting algorithms can be based on convex or non-convex optimization algorithms:</p>
    <ul>
      <li><strong>Convex Algorithms</strong>:<br />
  such as <strong>AdaBoost</strong> and <strong>LogitBoost</strong>, can be <span style="color: goldenrod">“defeated” by random noise</span> such that they can’t learn basic and learnable combinations of weak hypotheses.<br />
  This limitation was pointed out by <em>Long &amp; Servedio</em> in <em>2008</em>.</li>
      <li><strong>Non-Convex Algorithms</strong>:<br />
  such as <strong>BrownBoost</strong>, was shown to be able to learn from noisy datasets and can specifically learn the underlying classifier of the <em>“Long–Servedio dataset”</em>.<br />
 <br /></li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents4" id="bodyContents433">The Boosting MetaAlgorithm:</strong>
    <ul>
      <li><strong>Finding (defining) Weak Learners</strong>:<br />
  The algorithm defines weak learners as those that have <strong>weak rules</strong> (rules that are not powerful enough for accurate classification)</li>
      <li><strong>Identifying Weak Rules:</strong>
        <ul>
          <li>To find weak rule, we apply base learning (ML) algorithms with a different distribution. Each time base learning algorithm is applied, it generates a new weak prediction rule. This is an iterative process. After many iterations, the boosting algorithm combines these weak rules into a single strong prediction rule.</li>
        </ul>
      </li>
      <li><strong>Choosing different distribution for each round:</strong>
        <ol>
          <li>The base learner takes all the distributions and assign equal weight or attention to each observation.</li>
          <li>If there is any prediction error caused by first base learning algorithm, then we pay higher attention to observations having prediction error. Then, we apply the next base learning algorithm.</li>
          <li>Iterate Step 2 till the limit of base learning algorithm is reached or higher accuracy is achieved.</li>
        </ol>
      </li>
      <li><strong>Aggregating Outputs:</strong><br />
  Finally, it combines the outputs from weak learner and creates a strong learner which eventually improves the prediction power of the model. Boosting pays higher focus on examples which are mis-classiﬁed or have higher errors by preceding weak rules.<br />
<br /></li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents4" id="bodyContents444">Boosting Algorithms:</strong>
    <ul>
      <li>AdaBoost (Adaptive Boosting)</li>
      <li>Gradient Tree Boosting</li>
      <li>XGBoost
<br /></li>
    </ul>
  </li>
  <li>
    <p><strong style="color: SteelBlue" class="bodyContents4" id="bodyContents44">The AdaBoost Algorithm - Adaptive Boosting:</strong><br />
 <strong>AdaBoost:</strong> It works on similar method as discussed above. It fits a sequence of weak learners on different weighted training data. It starts by predicting original data set and gives equal weight to each observation. If prediction is incorrect using the first learner, then it gives higher weight to observation which have been predicted incorrectly. Being an iterative process, it continues to add learner(s) until a limit is reached in the number of models or accuracy.</p>

    <p>Mostly, we use decision stamps with AdaBoost. But, we can use any machine learning algorithms as base learner if it accepts weight on training data set. We can use AdaBoost algorithms for both classification and regression problem.</p>

    <ul>
      <li><a href="https://maelfabien.github.io/machinelearning/adaboost/#the-limits-of-bagging">The AdaBoost Boosting Algorithm in detail</a></li>
      <li><a href="https://people.eecs.berkeley.edu/~jrs/189/lec/24.pdf">AdaBoost (Shewchuk)</a></li>
      <li><a href="https://www.youtube.com/watch?v=UHBmv7qCey4">Boosting (MIT Lecture)</a><br />
 <br /></li>
    </ul>

    <p><strong style="color: red">Notes:</strong></p>
    <ul>
      <li>order of trees matter in AdaBoost<br />
 <br /></li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents4" id="bodyContents45">Advantages:</strong>
    <ul>
      <li>Decreases the Bias</li>
      <li>Better accuracy over Bagging (e.g. Random Forest)</li>
      <li>Boosting can lead to learning complex non-linear decision boundaries</li>
      <li><a href="https://www.quora.com/Why-does-Gradient-boosting-work-so-well-for-so-many-Kaggle-problems">Why does Gradient boosting work so well for so many Kaggle problems? (Quora!)</a><br />
 <br /></li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents4" id="bodyContents46">Disadvantages:</strong>
    <ul>
      <li>Reduced interpretability</li>
      <li>Harder to tune than other models, because you have so many hyperparameters and you can <strong>easily overfit</strong></li>
      <li>Computationally expensive for training (sequential) and inference <br />
 <br /></li>
    </ul>
  </li>
  <li><strong style="color: SteelBlue" class="bodyContents4" id="bodyContents48">Bagging VS Boosting:</strong> <br />
 <img src="https://cdn.mathpix.com/snip/images/fBJbH_Ej-9puFnO9piKuoN5ULBmPkMIbnhA6Qo64CU8.original.fullsize.png" alt="Bagging VS Boosting" width="80%" /></li>
</ol>

<hr />

<h2 id="content5">Stacking</h2>

<ol>
  <li><strong style="color: SteelBlue" class="bodyContents5" id="bodyContents51">Asynchronous:</strong></li>
</ol>

<!-- 2. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents52}  

3. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents53}  

4. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents54}  

5. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents55}  

6. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents56}  

7. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents57}  

8. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents5 #bodyContents58}   -->

<hr />

<!-- ## Sixth
{: #content6}

1. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents6 #bodyContents61}   -->

<!-- 2. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents6 #bodyContents62}  

3. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents6 #bodyContents63}  

4. **Asynchronous:**{: style="color: SteelBlue"}{: .bodyContents6 #bodyContents64}   -->


      <footer class="site-footer">
    <!--   <span class="site-footer-owner"><a href="http://localhost:8889">Ahmad Badary</a> is maintained by <a href="http://ahmedbadary.ml">Ahmad Badary</a>.</span> -->
    
<!--  -->
    <div class="wrapper">
        <div class="footer-col-wrapper">
            <div class="footer-col footer-col-1">
            <span class="site-footer-owner"><a href="http://localhost:8889">Site</a> maintained by <a href="http://ahmedbadary.ml">Ahmad Badary</a>.</span>
    <span class="site-footer-credits">
        <p>
            &copy; 2017. All rights reserved.
        </p> 
    </span>
            </div>
            <div class="footer-col footer-col-2">
            <div><p>         </p></div>
            </div>
            <div class="footer-col footer-col-3">
                <ul class="social-media-list">
                    
                      <li>
                        <a href="https://github.com/AhmedBadary">
                          <i class="fa fa-github"></i> GitHub
                        </a>
                      </li>
                    
                    
                      <li>
                        <a href="https://linkedin.com/in/ahmad-badary-656098121/">
                          <i class="fa fa-linkedin"></i> LinkedIn
                        </a>
                      </li>
                    
                    
                      <li>
                        <a href="https://www.facebook.com/ahmed.thabet.94">
                          <i class="fa fa-facebook"></i> Facebook
                        </a>
                      </li>
                    
                </ul>
            </div>
        </div>
    </div>
<!--  -->
</footer>


    </section>

  </body>

<!-- Table of Content Script -->
<script type="text/javascript">
var bodyContents = $(".bodyContents1");
$("<ol>").addClass("TOC1ul").appendTo(".TOC1");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC1ul");
     });
// 
var bodyContents = $(".bodyContents2");
$("<ol>").addClass("TOC2ul").appendTo(".TOC2");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC2ul");
     });
// 
var bodyContents = $(".bodyContents3");
$("<ol>").addClass("TOC3ul").appendTo(".TOC3");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC3ul");
     });
//
var bodyContents = $(".bodyContents4");
$("<ol>").addClass("TOC4ul").appendTo(".TOC4");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC4ul");
     });
//
var bodyContents = $(".bodyContents5");
$("<ol>").addClass("TOC5ul").appendTo(".TOC5");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC5ul");
     });
//
var bodyContents = $(".bodyContents6");
$("<ol>").addClass("TOC6ul").appendTo(".TOC6");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC6ul");
     });
//
var bodyContents = $(".bodyContents7");
$("<ol>").addClass("TOC7ul").appendTo(".TOC7");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC7ul");
     });
//
var bodyContents = $(".bodyContents8");
$("<ol>").addClass("TOC8ul").appendTo(".TOC8");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC8ul");
     });
//
var bodyContents = $(".bodyContents9");
$("<ol>").addClass("TOC9ul").appendTo(".TOC9");
bodyContents.each(function(index, element) {
    var paragraph = $(element);
    $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC9ul");
     });

</script>

<!-- VIDEO BUTTONS SCRIPT -->
<script type="text/javascript">
  function iframePopInject(event) {
    var $button = $(event.target);
    // console.log($button.parent().next());
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $figure = $("<div>").addClass("video_container");
        $iframe = $("<iframe>").appendTo($figure);
        $iframe.attr("src", $button.attr("src"));
        // $iframe.attr("frameborder", "0");
        $iframe.attr("allowfullscreen", "true");
        $iframe.css("padding", "4px 6px");
        $button.next().css("display", "block");
        $figure.appendTo($button.next());
        $button.text("Hide Video")
    } else {
        $button.attr('value', 'show');
        $button.next().html("");
        $button.text("Show Video")
    }
}
</script>

<!-- BUTTON TRY -->
<script type="text/javascript">
  function iframePopA(event) {
    event.preventDefault();
    var $a = $(event.target).parent();
    console.log($a);
    if ($a.attr('value') == 'show') {
        $a.attr('value', 'hide');
        $figure = $("<div>");
        $iframe = $("<iframe>").addClass("popup_website_container").appendTo($figure);
        $iframe.attr("src", $a.attr("href"));
        $iframe.attr("frameborder", "1");
        $iframe.attr("allowfullscreen", "true");
        $iframe.css("padding", "4px 6px");
        $a.next().css("display", "block");
        $figure.appendTo($a.next().next());
        // $a.text("Hide Content")
        $('html, body').animate({
            scrollTop: $a.offset().top
        }, 1000);
    } else {
        $a.attr('value', 'show');
        $a.next().next().html("");
        // $a.text("Show Content")
    }

    $a.next().css("display", "inline");
}
</script>


<!-- TEXT BUTTON SCRIPT - INJECT -->
<script type="text/javascript">
  function showTextPopInject(event) {
    var $button = $(event.target);
    var txt = $button.attr("input");
    console.log(txt);
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $p = $("<p>");
        $p.html(txt);
        $button.next().css("display", "block");
        $p.appendTo($button.next());
        $button.text("Hide Content")
    } else {
        $button.attr('value', 'show');
        $button.next().html("");
        $button.text("Show Content")
    }

}
</script>

<!-- TEXT BUTTON SCRIPT - HIDDEN / HIDE / SHOW / HIDE/SHOW -->
<script type="text/javascript">
  function showTextPopHide(event) {
    var $button = $(event.target);
    // var txt = $button.attr("input");
    var txt = $button.text();
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $button.next().removeAttr("hidden");
        $button.text(txt + " - Hide Content");
    } else {
        $button.attr('value', 'show');
        $button.next().attr("hidden", "");
        $button.text(txt.replace(" - Hide Content",""));
    }
}
</script>

<!-- TEXT BUTTON SCRIPT - HIDDEN / HIDE / SHOW / HIDE/SHOW -->
<script type="text/javascript">
  function showText_withParent_PopHide(event) {
    var $button = $(event.target);
    var $parent = $button.parent();
    var txt = $button.text();
    if ($button.attr('value') == 'show') {
        $button.attr('value', 'hide');
        $parent.next().removeAttr("hidden");
        $button.text(txt + " - Hide Content");
    } else {
        $button.attr('value', 'show');
        $parent.next().attr("hidden", "");
        $button.text(txt.replace(" - Hide Content",""));
    }
}
</script>

<!-- Print / Printing / printme -->
<!-- <script type="text/javascript">
i = 0

for (var i = 1; i < 6; i++) {
    var bodyContents = $(".bodyContents" + i);
    $("<p>").addClass("TOC1ul")  .appendTo(".TOC1");
    bodyContents.each(function(index, element) {
        var paragraph = $(element);
        $("<li>").html("<a href=#"+paragraph.attr('id')+">"+ paragraph.html().replace(':','')+" </a> ").appendTo(".TOC1ul");
         });
} 
</script>
 -->
 
</html>

